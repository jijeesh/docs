[{"anchor":"#","title":"1. Enable Istio in the Cluster","content":"This cluster uses the default Nginx controller to allow traffic into the cluster.\n\nA Rancher administrator or cluster owner can configure Rancher to deploy Istio in a Kubernetes cluster.\n\n\nIf the cluster has a Pod Security Policy enabled there are prerequisites steps\n\n\n\nFrom the Global view, navigate to the cluster where you want to enable Istio.\nClick Tools > Istio.\nOptional: Configure member access and resource limits for the Istio components. Ensure you have enough resources on your worker nodes to enable Istio.\nClick Enable.\nClick Save.\n\n\nResult: Istio is enabled at the cluster level.\n\nThe Istio application, cluster-istio, is added as an application to the cluster’s system project.\n\nWhen Istio is enabled in the cluster, the label for Istio sidecar auto injection,istio-injection=enabled, will be automatically added to each new namespace in this cluster. This automatically enables Istio sidecar injection in all new workloads that are deployed in those namespaces. You will need to manually enable Istio in preexisting namespaces and workloads.\n\nNext: Enable Istio in a Namespace\n","postref":"b74fb438ced642236354564596d8abb8","objectID":"f790e24fe0dc7a26c3e18dc83763ea10","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/tools/istio/setup/enable-istio-in-cluster/"},{"anchor":"#","title":"API Tokens","content":"By default, some cluster-level API tokens are generated with infinite time-to-live (ttl=0). In other words, API tokens with ttl=0 never expire unless you invalidate them. Tokens are not invalidated by changing a password.\n\nYou can deactivate API tokens by deleting them or by deactivating the user account.\n\nTo delete a token,\n\n\nGo to the list of all tokens in the Rancher API view at https://<Rancher-Server-IP>/v3/tokens.\n\nAccess the token you want to delete by its ID. For example, https://<Rancher-Server-IP>/v3/tokens/kubectl-shell-user-vqkqt\n\nClick Delete.\n\n\nHere is the complete list of tokens that are generated with ttl=0:\n\n\n\n\nToken\nDescription\n\n\n\n\n\nkubeconfig-*\nKubeconfig token\n\n\n\nkubectl-shell-*\nAccess to kubectl shell in the browser\n\n\n\nagent-*\nToken for agent deployment\n\n\n\ncompose-token-*\nToken for compose\n\n\n\nhelm-token-*\nToken for Helm chart deployment\n\n\n\n*-pipeline*\nPipeline token for project\n\n\n\ntelemetry-*\nTelemetry token\n\n\n\ndrain-node-*\nToken for drain (we use kubectl for drain because there is no native Kubernetes API)\n\n\n\n","postref":"fb204bf6f5a3d3134b5786322632de9b","objectID":"274d9b4aca96d5dcfeec47e63c6de8b2","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/api/api-tokens/"},{"anchor":"#","title":"Allow Unsupported Storage Drivers","content":"Available as of v2.3.0\n\nThis feature allows you to use types for storage providers and provisioners that are not enabled by default.\n\nTo enable or disable this feature, refer to the instructions on the main page about enabling experimental features.\n\n\n\n\nEnvironment Variable Key\nDefault Value\nDescription\n\n\n\n\n\nunsupported-storage-drivers\nfalse\nThis feature enables types for storage providers and provisioners that are not enabled by default.\n\n\n\n\nTypes for Persistent Volume Plugins that are Enabled by Default\n\nBelow is a list of storage types for persistent volume plugins that are enabled by default. When enabling this feature flag, any persistent volume plugins that are not on this list are considered experimental and unsupported:\n\n\n\n\nName\nPlugin\n\n\n\n\n\nAmazon EBS Disk\naws-ebs\n\n\n\nAzureFile\nazure-file\n\n\n\nAzureDisk\nazure-disk\n\n\n\nGoogle Persistent Disk\ngce-pd\n\n\n\nLonghorn\nflex-volume-longhorn\n\n\n\nVMware vSphere Volume\nvsphere-volume\n\n\n\nLocal\nlocal\n\n\n\nNetwork File System\nnfs\n\n\n\nhostPath\nhost-path\n\n\n\n\nTypes for StorageClass that are Enabled by Default\n\nBelow is a list of storage types for a StorageClass that are enabled by default. When enabling this feature flag, any persistent volume plugins that are not on this list are considered experimental and unsupported:\n\n\n\n\nName\nPlugin\n\n\n\n\n\nAmazon EBS Disk\naws-ebs\n\n\n\nAzureFile\nazure-file\n\n\n\nAzureDisk\nazure-disk\n\n\n\nGoogle Persistent Disk\ngce-pd\n\n\n\nLonghorn\nflex-volume-longhorn\n\n\n\nVMware vSphere Volume\nvsphere-volume\n\n\n\nLocal\nlocal\n\n\n\n","postref":"c05963a78c0960be947e88fa3e0420d5","objectID":"e6ca5b32c7d45648970a851bee5dc6a0","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/feature-flags/enable-not-default-storage-drivers/"},{"anchor":"#","title":"Architecture","content":"This page describes the architecture of a high-availability K3s server cluster and how it differs from a single-node server cluster.\n\nIt also describes how agent nodes are registered with K3s servers.\n\nA server node is defined as a machine (bare-metal or virtual) running the k3s server command. A worker node is defined as a machine running the k3s agent command.\n\nThis page covers the following topics:\n\n\nSingle-server setup with an embedded database\nHigh-availability K3s server with an external database\n\n\nFixed registration address for agent nodes\n\nHow agent node registration works\n\n\nSingle-server Setup with an Embedded DB\n\nThe following diagram shows an example of a cluster that has a single-node K3s server with an embedded SQLite database.\n\nIn this configuration, each agent node is registered to the same server node. A K3s user can manipulate Kubernetes resources by calling the K3s API on the server node.\n\nK3s Architecture with a Single Server\n\n\nHigh-Availability K3s Server with an External DB\n\nSingle server clusters can meet a variety of use cases, but for environments where uptime of the Kubernetes control plane is critical, you can run K3s in an HA configuration. An HA K3s cluster is comprised of:\n\n\nTwo or more server nodes that will serve the Kubernetes API and run other control plane services\nAn external datastore (as opposed to the embedded SQLite datastore used in single-server setups)\n\n\nK3s Architecture with a High-availability Server\n\n\nFixed Registration Address for Agent Nodes\n\nIn the high-availability server configuration, each node must also register with the Kubernetes API by using a fixed registration address, as shown in the diagram below.\n\nAfter registration, the agent nodes establish a connection directly to one of the server nodes.\n\n\n\nHow Agent Node Registration Works\n\nAgent nodes are registered with a websocket connection initiated by the k3s agent process, and the connection is maintained by a client-side load balancer running as part of the agent process.\n\nAgents will register with the server using the node cluster secret along with a randomly generated password for the node, stored at /etc/rancher/node/password. The server will store the passwords for individual nodes at /var/lib/rancher/k3s/server/cred/node-passwd, and any subsequent attempts must use the same password.\n\nIf the /etc/rancher/node directory of an agent is removed, the password file should be recreated for the agent, or the entry removed from the server.\n\nA unique node ID can be appended to the hostname by launching K3s servers or agents using the --with-node-id flag.\n","postref":"2e772cba3eb53ed72b5cc22935090564","objectID":"647b0f5a1fcbc39cb77c412bdc9dea44","permalink":"http://jijeesh.github.io/docs/k3s/latest/en/architecture/"},{"anchor":"#","title":"Architecture","content":"This section focuses on the Rancher server, its components, and how Rancher communicates with downstream Kubernetes clusters.\n\nFor information on the different ways that Rancher can be installed, refer to the overview of installation options.\n\nFor a list of main features of the Rancher API server, refer to the overview section.\n\nFor guidance about setting up the underlying infrastructure for the Rancher server, refer to the architecture recommendations.\n\n\nThis section assumes a basic familiarity with Docker and Kubernetes. For a brief explanation of how Kubernetes components work together, refer to the concepts page.\n\n\nThis section covers the following topics:\n\n\nRancher server architecture\nCommunicating with downstream user clusters\n\n\nThe authentication proxy\nCluster controllers and cluster agents\nNode agents\nAuthorized cluster endpoint\n\nImportant files\nTools for provisioning Kubernetes clusters\nRancher server components and source code\n\n\nRancher Server Architecture\n\nThe majority of Rancher 2.x software runs on the Rancher Server. Rancher Server includes all the software components used to manage the entire Rancher deployment.\n\nThe figure below illustrates the high-level architecture of Rancher 2.x. The figure depicts a Rancher Server installation that manages two downstream Kubernetes clusters: one created by RKE and another created by Amazon EKS (Elastic Kubernetes Service).\n\nFor the best performance and security, we recommend a dedicated Kubernetes cluster for the Rancher management server. Running user workloads on this cluster is not advised. After deploying Rancher, you can create or import clusters for running your workloads.\n\nThe diagram below shows how users can manipulate both Rancher-launched Kubernetes clusters and hosted Kubernetes clusters through Rancher’s authentication proxy:\n\nManaging Kubernetes Clusters through Rancher's Authentication Proxy\n\n\n\nYou can install Rancher on a single node, or on a high-availability Kubernetes cluster.\n\nA high-availability Kubernetes installation is recommended for production. A Docker installation may be used for development and testing purposes, but there is no migration path from a single-node to a high-availability installation. Therefore, you may want to use a Kubernetes installation from the start.\n\nThe Rancher server, regardless of the installation method, should always run on nodes that are separate from the downstream user clusters that it manages. If Rancher is installed on a high-availability Kubernetes cluster, it should run on a separate cluster from the cluster(s) it manages.\n\nCommunicating with Downstream User Clusters\n\nThis section describes how Rancher provisions and manages the downstream user clusters that run your apps and services.\n\nThe below diagram shows how the cluster controllers, cluster agents, and node agents allow Rancher to control downstream clusters.\n\nCommunicating with Downstream Clusters\n\n\n\nThe following descriptions correspond to the numbers in the diagram above:\n\n\nThe Authentication Proxy\nCluster Controllers and Cluster Agents\nNode Agents\nAuthorized Cluster Endpoint\n\n\n1. The Authentication Proxy\n\nIn this diagram, a user named Bob wants to see all pods running on a downstream user cluster called User Cluster 1. From within Rancher, he can run a kubectl command to see\nthe pods. Bob is authenticated through Rancher’s authentication proxy.\n\nThe authentication proxy forwards all Kubernetes API calls to downstream clusters. It integrates with authentication services like local authentication, Active Directory, and GitHub. On every Kubernetes API call, the authentication proxy authenticates the caller and sets the proper Kubernetes impersonation headers before forwarding the call to Kubernetes masters.\n\nRancher communicates with Kubernetes clusters using a service account, which provides an identity for processes that run in a pod.\n\nBy default, Rancher generates a kubeconfig file that contains credentials for proxying through the Rancher server to connect to the Kubernetes API server on a downstream user cluster. The kubeconfig file (kube_config_rancher-cluster.yml) contains full access to the cluster.\n\n2. Cluster Controllers and Cluster Agents\n\nEach downstream user cluster has a cluster agent, which opens a tunnel to the corresponding cluster controller within the Rancher server.\n\nThere is one cluster controller and one cluster agent for each downstream cluster. Each cluster controller:\n\n\nWatches for resource changes in the downstream cluster\nBrings the current state of the downstream cluster to the desired state\nConfigures access control policies to clusters and projects\nProvisions clusters by calling the required Docker machine drivers and Kubernetes engines, such as RKE and GKE\n\n\nBy default, to enable Rancher to communicate with a downstream cluster, the cluster controller connects to the cluster agent. If the cluster agent is not available, the cluster controller can connect to a node agent instead.\n\nThe cluster agent, also called cattle-cluster-agent, is a component that runs in a downstream user cluster. It performs the following tasks:\n\n\nConnects to the Kubernetes API of Rancher-launched Kubernetes clusters\nManages workloads, pod creation and deployment within each cluster\nApplies the roles and bindings defined in each cluster’s global policies\nCommunicates between the cluster and Rancher server (through a tunnel to the cluster controller) about events, stats, node info, and health\n\n\n3. Node Agents\n\nIf the cluster agent (also called cattle-cluster-agent) is not available, one of the node agents creates a tunnel to the cluster controller to communicate with Rancher.\n\nThe cattle-node-agent is deployed using a DaemonSet resource to make sure it runs on every node in a Rancher-launched Kubernetes cluster. It is used to interact with the nodes when performing cluster operations. Examples of cluster operations include upgrading the Kubernetes version and creating or restoring etcd snapshots.\n\n4. Authorized Cluster Endpoint\n\nAn authorized cluster endpoint allows users to connect to the Kubernetes API server of a downstream cluster without having to route their requests through the Rancher authentication proxy.\n\n\nThe authorized cluster endpoint only works on Rancher-launched Kubernetes clusters. In other words, it only works in clusters where Rancher used RKE to provision the cluster. It is not available for imported clusters, or for clusters in a hosted Kubernetes provider, such as Amazon’s EKS.\n\n\nThere are two main reasons why a user might need the authorized cluster endpoint:\n\n\nTo access a downstream user cluster while Rancher is down\nTo reduce latency in situations where the Rancher server and downstream cluster are separated by a long distance\n\n\nThe kube-api-auth microservice is deployed to provide the user authentication functionality for the authorized cluster endpoint. When you access the user cluster using kubectl, the cluster’s Kubernetes API server authenticates you by using the kube-api-auth service as a webhook.\n\nLike the authorized cluster endpoint, the kube-api-auth authentication service is also only available for Rancher-launched Kubernetes clusters.\n\n\nExample scenario: Let’s say that the Rancher server is located in the United States, and User Cluster 1 is located in Australia. A user, Alice, also lives in Australia. Alice can manipulate resources in User Cluster 1 by using the Rancher UI, but her requests will have to be sent from Australia to the Rancher server in the United States, then be proxied back to Australia, where the downstream user cluster is. The geographical distance may cause significant latency, which Alice can reduce by using the authorized cluster endpoint.\n\n\nWith this endpoint enabled for the downstream cluster, Rancher generates an extra Kubernetes context in the kubeconfig file in order to connect directly to the cluster. This file has the credentials for kubectl and helm.\n\nYou will need to use a context defined in this kubeconfig file to access the cluster if Rancher goes down. Therefore, we recommend exporting the kubeconfig file so that if Rancher goes down, you can still use the credentials in the file to access your cluster. For more information, refer to the section on accessing your cluster with kubectl and the kubeconfig file.\n\nImportant Files\n\nThe files mentioned below are needed to maintain, troubleshoot and upgrade your cluster:\n\n\nrancher-cluster.yml: The RKE cluster configuration file.\nkube_config_rancher-cluster.yml: The Kubeconfig file for the cluster, this file contains credentials for full access to the cluster. You can use this file to authenticate with a Rancher-launched Kubernetes cluster if Rancher goes down.\nrancher-cluster.rkestate: The Kubernetes cluster state file. This file contains credentials for full access to the cluster. Note: This state file is only created when using RKE v0.2.0 or higher.\n\n\n\nNote: The “rancher-cluster” parts of the two latter file names are dependent on how you name","postref":"65c227de4504b149f50d28d89a234079","objectID":"549176baddb16745c93fa03680cd63ac","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/overview/architecture/"},{"anchor":"#pilot","title":"Pilot","content":"You can visualize metrics with Grafana. Grafana lets you visualize Istio traffic data scraped by Prometheus.\n\n\nOption\nDescription\nRequired\nDefault\n\n\n\n\n\nEnable Grafana\nWhether or not to deploy the Grafana.\nYes\nTrue\n\n\n\nGrafana CPU Limit\nCPU resource limit for the Grafana pod.\nYes, when Grafana enabled\n200\n\n\n\nGrafana CPU Reservation\nCPU reservation for the Grafana pod.\nYes, when Grafana enabled\n100\n\n\n\nGrafana Memory Limit\nMemory resource limit for the Grafana pod.\nYes, when Grafana enabled\n512\n\n\n\nGrafana Memory Reservation\nMemory resource requests for the Grafana pod.\nYes, when Grafana enabled\n100\n\n\n\nGrafana Selector\nAbility to select the nodes in which Grafana pod is deployed to. To use this option, the nodes must have labels.\nNo\nn/a\n\n\n\nEnable Persistent Storage for Grafana\nEnable Persistent Storage for Grafana\nYes, when Grafana enabled\nFalse\n\n\n\nSource\nUse a Storage Class to provision a new persistent volume or Use an existing persistent volume claim\nYes, when Grafana enabled and enabled PV\nUse SC\n\n\n\nStorage Class\nStorage Class for provisioning PV for Grafana\nYes, when Grafana enabled, enabled PV and use storage class\nUse the default class\n\n\n\nPersistent Volume Size\nThe size for the PV you would like to provision for Grafana\nYes, when Grafana enabled, enabled PV and use storage class\n5Gi\n\n\n\nExisting Claim\nUse existing PVC for Grafana\nYes, when Grafana enabled, enabled PV and use existing PVC\nn/a\n\n\nYou can query for Istio metrics using Prometheus. Prometheus is an open-source systems monitoring and alerting toolkit.\n\n\nOption\nDescription\nRequired\nDefault\n\n\n\n\n\nPrometheus CPU Limit\nCPU resource limit for the Prometheus pod.\nYes\n1000\n\n\n\nPrometheus CPU Reservation\nCPU reservation for the Prometheus pod.\nYes\n750\n\n\n\nPrometheus Memory Limit\nMemory resource limit for the Prometheus pod.\nYes\n1024\n\n\n\nPrometheus Memory Reservation\nMemory resource requests for the Prometheus pod.\nYes\n750\n\n\n\nRetention for Prometheus\nHow long your Prometheus instance retains data\nYes\n6\n\n\n\nPrometheus Selector\nAbility to select the nodes in which Prometheus pod is deployed to. To use this option, the nodes must have labels.\nNo\nn/a\n\n\nThe Istio gateway allows Istio features such as monitoring and route rules to be applied to traffic entering the cluster. This gateway is a prerequisite for outside traffic to make requests to Istio.For more information, refer to the documentation.\n\n\nOption\nDescription\nRequired\nDefault\n\n\n\n\n\nEnable Ingress Gateway\nWhether or not to deploy the istio-ingressgateway.\nYes\nFalse\n\n\n\nService Type of Istio Ingress Gateway\nHow to expose the gateway. You can choose NodePort or Loadbalancer\nYes\nNodePort\n\n\n\nHttp2 Port\nThe NodePort for http2 requests\nYes\n31380\n\n\n\nHttps Port\nThe NodePort for https requests\nYes\n31390\n\n\n\nLoad Balancer IP\nIngress Gateway Load Balancer IP\nNo\nn/a\n\n\n\nLoad Balancer Source Ranges\nIngress Gateway Load Balancer Source Ranges\nNo\nn/a\n\n\n\nIngress Gateway CPU Limit\nCPU resource limit for the istio-ingressgateway pod.\nYes\n2000\n\n\n\nIngress Gateway CPU Reservation\nCPU reservation for the istio-ingressgateway pod.\nYes\n100\n\n\n\nIngress Gateway Memory Limit\nMemory resource limit for the istio-ingressgateway pod.\nYes\n1024\n\n\n\nIngress Gateway Memory Reservation\nMemory resource requests for the istio-ingressgateway pod.\nYes\n128\n\n\n\nIngress Gateway Selector\nAbility to select the nodes in which istio-ingressgateway pod is deployed to. To use this option, the nodes must have labels.\nNo\nn/a\n\n\nDistributed tracing enables users to track a request through a service mesh. This makes it easier to troubleshoot problems with latency, parallelism and serialization.\n\n\nOption\nDescription\nRequired\nDefault\n\n\n\n\n\nEnable Tracing\nWhether or not to deploy the istio-tracing.\nYes\nTrue\n\n\n\nTracing CPU Limit\nCPU resource limit for the istio-tracing pod.\nYes\n500\n\n\n\nTracing CPU Reservation\nCPU reservation for the istio-tracing pod.\nYes\n100\n\n\n\nTracing Memory Limit\nMemory resource limit for the istio-tracing pod.\nYes\n1024\n\n\n\nTracing Memory Reservation\nMemory resource requests for the istio-tracing pod.\nYes\n100\n\n\n\nTracing Selector\nAbility to select the nodes in which tracing pod is deployed to. To use this option, the nodes must have labels.\nNo\nn/a\n\n\nMixer  enforces access control and usage policies across the service mesh. It also integrates with plugins for monitoring tools such as Prometheus. The Envoy sidecar proxy passes telemetry data and monitoring data to Mixer, and Mixer passes the monitoring data to Prometheus.For more information on Mixer, policies and telemetry, refer to the documentation.\n\n\nOption\nDescription\nRequired\nDefault\n\n\n\n\n\nMixer Telemetry CPU Limit\nCPU resource limit for the istio-telemetry pod.\nYes\n4800\n\n\n\nMixer Telemetry CPU Reservation\nCPU reservation for the istio-telemetry pod.\nYes\n1000\n\n\n\nMixer Telemetry Memory Limit\nMemory resource limit for the istio-telemetry pod.\nYes\n4096\n\n\n\nMixer Telemetry Memory Reservation\nMemory resource requests for the istio-telemetry pod.\nYes\n1024\n\n\n\nEnable Mixer Policy\nWhether or not to deploy the istio-policy.\nYes\nFalse\n\n\n\nMixer Policy CPU Limit\nCPU resource limit for the istio-policy pod.\nYes, when policy enabled\n4800\n\n\n\nMixer Policy CPU Reservation\nCPU reservation for the istio-policy pod.\nYes, when policy enabled\n1000\n\n\n\nMixer Policy Memory Limit\nMemory resource limit for the istio-policy pod.\nYes, when policy enabled\n4096\n\n\n\nMixer Policy Memory Reservation\nMemory resource requests for the istio-policy pod.\nYes, when policy enabled\n1024\n\n\n\nMixer Selector\nAbility to select the nodes in which istio-policy and istio-telemetry pods are deployed to. To use this option, the nodes must have labels.\nNo\nn/a\n\n\nPilot  provides the following:\nAuthentication configuration\nService discovery for the Envoy sidecars\nTraffic management capabilities for intelligent routing (A/B tests and canary rollouts)\nConfiguration for resiliency (timeouts, retries, circuit breakers, etc)\nFor more information on Pilot, refer to the documentation.\n\n\nOption\nDescription\nRequired\nDefault\n\n\n\n\n\nPilot CPU Limit\nCPU resource limit for the istio-pilot pod.\nYes\n1000\n\n\n\nPilot CPU Reservation\nCPU reservation for the istio-pilot pod.\nYes\n500\n\n\n\nPilot Memory Limit\nMemory resource limit for the istio-pilot pod.\nYes\n4096\n\n\n\nPilot Memory Reservation\nMemory resource requests for the istio-pilot pod.\nYes\n2048\n\n\n\nTrace sampling Percentage\nTrace sampling percentage\nYes\n1\n\n\n\nPilot Selector\nAbility to select the nodes in which istio-pilot pod is deployed to. To use this option, the nodes must have labels.\nNo\nn/a\n\n\n","postref":"e945314c6f33683a878544516a911488","objectID":"ed08745b601570055f5b9e242d0f2d5c","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/tools/istio/resources/"},{"anchor":"#","title":"Cluster Access","content":"There are many ways you can interact with Kubernetes clusters that are managed by Rancher:\n\n\nRancher UI\n\nRancher provides an intuitive user interface for interacting with your clusters. All options available in the UI use the Rancher API. Therefore any action possible in the UI is also possible in the Rancher CLI or Rancher API.\n\nkubectl\n\nYou can use the Kubernetes command-line tool, kubectl, to manage   your clusters. You have two options for using kubectl:\n\n\nRancher kubectl shell\n\nInteract with your clusters by launching a kubectl shell available in the Rancher UI. This option requires no configuration actions on your part.\n\nFor more information, see Accessing Clusters with kubectl Shell.\n\nTerminal remote connection\n\nYou can also interact with your clusters by installing kubectl on your local desktop and then copying the cluster’s kubeconfig file to your local ~/.kube/config directory.\n\nFor more information, see Accessing Clusters with kubectl and a kubeconfig File.\n\n\nRancher CLI\n\nYou can control your clusters by downloading Rancher’s own command-line interface, Rancher CLI. This CLI tool can interact directly with different clusters and projects or pass them kubectl commands.\n\nRancher API\n\nFinally, you can interact with your clusters over the Rancher API. Before you use the API, you must obtain an API key. To view the different resource fields and actions for an API object, open the API UI, which can be accessed by clicking on View in API for any Rancher UI object.\n\n","postref":"ce26920a09ab4990cb2e79acce57e9b0","objectID":"0248922b10d7234e76d60e66c4dd2f6f","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/cluster-access/"},{"anchor":"#activating-deactivating-cluster-drivers","title":"Activating/Deactivating Cluster Drivers","content":"If you want to use a cluster driver that Rancher doesn’t support out-of-the-box, you can add the provider’s driver in order to start using them to create hosted kubernetes clusters.\nFrom the Global view, choose Tools > Drivers in the navigation bar.\n\nFrom the Drivers page select the Cluster Drivers tab.\n\nClick Add Cluster Driver.\n\nComplete the Add Cluster Driver form. Then click Create.\nDeveloping your own Cluster DriverIn order to develop cluster driver to add to Rancher, please refer to our example.By default, Rancher only activates drivers for the most popular cloud providers, Google GKE, Amazon EKS and Azure AKS. If you want to show or hide any node driver, you can change its status.\nFrom the Global view, choose Tools > Drivers in the navigation bar.\n\nFrom the Drivers page, select the Cluster Drivers tab.\n\nSelect the driver that you wish to Activate or Deactivate and select the appropriate icon.\n","postref":"be2403f60f2c9111216f4d36f9e46c3d","objectID":"9df8bb4813817ec441e96222dcacf1f9","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/drivers/cluster-drivers/"},{"anchor":"#","title":"Creating Credentials in the vSphere Console","content":"This section describes how to create a vSphere username and password. You will need to provide these vSphere credentials to Rancher, which allows Rancher to provision resources in vSphere.\n\nThe following table lists the permissions required for the vSphere user account:\n\n\n\n\nPrivilege Group\nOperations\n\n\n\n\n\nDatastore\nAllocateSpace  Browse  FileManagement (Low level file operations)  UpdateVirtualMachineFiles  UpdateVirtualMachineMetadata\n\n\n\nNetwork\nAssign\n\n\n\nResource\nAssignVMToPool\n\n\n\nVirtual Machine\nConfig (All)  GuestOperations (All)  Interact (All)  Inventory (All)  Provisioning (All)\n\n\n\n\nThe following steps create a role with the required privileges and then assign it to a new user in the vSphere console:\n\n\nFrom the vSphere console, go to the Administration page.\n\nGo to the Roles tab.\n\nCreate a new role.  Give it a name and select the privileges listed in the permissions table above.\n\n\n\n\n    \n    \n    \n    \n    \n    \n\n\n\nGo to the Users and Groups tab.\n\nCreate a new user. Fill out the form and then click OK. Make sure to note the username and password, because you will need it when configuring node templates in Rancher.\n\n\n\n\n    \n    \n    \n    \n    \n    \n\n\n\nGo to the Global Permissions tab.\n\nCreate a new Global Permission. Add the user you created earlier and assign it the role you created earlier. Click OK.\n\n\n\n\n    \n    \n    \n    \n    \n    \n\n\n\n\n\n\n    \n    \n    \n    \n    \n    \n\n\n\n\nResult: You now have credentials that Rancher can use to manipulate vSphere resources.\n","postref":"c6512a71b5ce93989e51c9ba5f0a2b48","objectID":"3570232c0e831f14bbf4263f189b2649","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/node-pools/vsphere/provisioning-vsphere-clusters/creating-credentials/"},{"anchor":"#","title":"Custom Network Plug-in Example","content":"The below example shows how to configure a custom network plug-in with an in-line add-on to the cluster.yml.\n\nFirst, to edit the network plug-ins, change the network section of the YAML from:\n\nnetwork: \n  options: \n    flannel_backend_type: \"vxlan\"\n  plugin: \"canal\"\n\n\nto:\n\nnetwork:\n    plugin: none\n\n\nThen, in the addons section of the cluster.yml, you can add the add-on manifest of a cluster that has the network plugin-that you want. In the below example, we are replacing the Canal plugin with a Flannel plugin by adding the add-on manifest for the cluster through the addons field:\n\naddons: |-\n    ---\n    kind: ClusterRoleBinding\n    apiVersion: rbac.authorization.k8s.io/v1\n    metadata:\n      name: flannel\n    roleRef:\n      apiGroup: rbac.authorization.k8s.io\n      kind: ClusterRole\n      name: flannel\n    subjects:\n    - kind: ServiceAccount\n      name: flannel\n      namespace: kube-system\n    ---\n    kind: ClusterRole\n    apiVersion: rbac.authorization.k8s.io/v1\n    metadata:\n      name: flannel\n    rules:\n      - apiGroups:\n          - \"\"\n        resources:\n          - pods\n        verbs:\n          - get\n      - apiGroups:\n          - \"\"\n        resources:\n          - nodes\n        verbs:\n          - list\n          - watch\n      - apiGroups:\n          - \"\"\n        resources:\n          - nodes/status\n        verbs:\n          - patch\n    ---\n    kind: ConfigMap\n    apiVersion: v1\n    metadata:\n      name: kube-flannel-cfg\n      namespace: \"kube-system\"\n      labels:\n        tier: node\n        app: flannel\n    data:\n      cni-conf.json: |\n        {\n          \"name\":\"cbr0\",\n          \"cniVersion\":\"0.3.1\",\n          \"plugins\":[\n            {\n              \"type\":\"flannel\",\n              \"delegate\":{\n                \"forceAddress\":true,\n                \"isDefaultGateway\":true\n              }\n            },\n            {\n              \"type\":\"portmap\",\n              \"capabilities\":{\n                \"portMappings\":true\n              }\n            }\n          ]\n        }\n      net-conf.json: |\n        {\n          \"Network\": \"10.42.0.0/16\",\n          \"Backend\": {\n            \"Type\": \"vxlan\"\n          }\n        }\n    ---\n    apiVersion: extensions/v1beta1\n    kind: DaemonSet\n    metadata:\n      name: kube-flannel\n      namespace: \"kube-system\"\n      labels:\n        tier: node\n        k8s-app: flannel\n    spec:\n      template:\n        metadata:\n          labels:\n            tier: node\n            k8s-app: flannel\n        spec:\n          affinity:\n            nodeAffinity:\n              requiredDuringSchedulingIgnoredDuringExecution:\n                nodeSelectorTerms:\n                  - matchExpressions:\n                    - key: beta.kubernetes.io/os\n                      operator: NotIn\n                      values:\n                        - windows\n          serviceAccountName: flannel\n          containers:\n          - name: kube-flannel\n            image: rancher/coreos-flannel:v0.10.0-rancher1\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: 300m\n                memory: 500M\n              requests:\n                cpu: 150m\n                memory: 64M\n            command: [\"/opt/bin/flanneld\",\"--ip-masq\",\"--kube-subnet-mgr\"]\n            securityContext:\n              privileged: true\n            env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            volumeMounts:\n            - name: run\n              mountPath: /run\n            - name: cni\n              mountPath: /etc/cni/net.d\n            - name: flannel-cfg\n              mountPath: /etc/kube-flannel/\n          - name: install-cni\n            image: rancher/flannel-cni:v0.3.0-rancher1\n            command: [\"/install-cni.sh\"]\n            env:\n            # The CNI network config to install on each node.\n            - name: CNI_NETWORK_CONFIG\n              valueFrom:\n                configMapKeyRef:\n                  name: kube-flannel-cfg\n                  key: cni-conf.json\n            - name: CNI_CONF_NAME\n              value: \"10-flannel.conflist\"\n            volumeMounts:\n            - name: cni\n              mountPath: /host/etc/cni/net.d\n            - name: host-cni-bin\n              mountPath: /host/opt/cni/bin/\n          hostNetwork: true\n          tolerations:\n          - operator: Exists\n            effect: NoSchedule\n          - operator: Exists\n            effect: NoExecute\n          - key: node.kubernetes.io/not-ready\n            effect: NoSchedule\n            operator: Exists\n          volumes:\n            - name: run\n              hostPath:\n                path: /run\n            - name: cni\n              hostPath:\n                path: /etc/cni/net.d\n            - name: flannel-cfg\n              configMap:\n                name: kube-flannel-cfg\n            - name: host-cni-bin\n              hostPath:\n                path: /opt/cni/bin\n      updateStrategy:\n        rollingUpdate:\n          maxUnavailable: 20%\n        type: RollingUpdate\n    ---\n    apiVersion: v1\n    kind: ServiceAccount\n    metadata:\n      name: flannel\n      namespace: kube-system\n\n\nResult: The cluster is up with the custom network plug-in.\n","postref":"c05ee7b8207b2581dff07b1374776a8a","objectID":"9ea0e0a313c60a3f73f0968999783deb","permalink":"http://jijeesh.github.io/docs/rke/latest/en/config-options/add-ons/network-plugins/custom-network-plugin-example/"},{"anchor":"#","title":"Default Alerts for Cluster Monitoring","content":"When you create a cluster, some alert rules are predefined. These alerts notify you about signs that the cluster could be unhealthy. You can receive these alerts if you configure a notifier for them.\n\nSeveral of the alerts use Prometheus expressions as the metric that triggers the alert. For more information on how expressions work, you can refer to the Rancher documentation about Prometheus expressions or the Prometheus documentation about querying metrics.\n\nAlerts for etcd\n\nEtcd is the key-value store that contains the state of the Kubernetes cluster. Rancher provides default alerts if the built-in monitoring detects a potential problem with etcd. You don’t have to enable monitoring to receive these alerts.\n\nA leader is the node that handles all client requests that need cluster consensus. For more information, you can refer to this explanation of how etcd works.\n\nThe leader of the cluster can change in response to certain events. It is normal for the leader to change, but too many changes can indicate a problem with the network or a high CPU load. With longer latencies, the default etcd configuration may cause frequent heartbeat timeouts, which trigger a new leader election.\n\n\n\n\nAlert\nExplanation\n\n\n\n\n\nA high number of leader changes within the etcd cluster are happening\nA warning alert is triggered when the leader changes more than three times in one hour.\n\n\n\nDatabase usage close to the quota 500M\nA warning alert is triggered when the size of etcd exceeds 500M.\n\n\n\nEtcd is unavailable\nA critical alert is triggered when etcd becomes unavailable.\n\n\n\nEtcd member has no leader\nA critical alert is triggered when the etcd cluster does not have a leader for at least three minutes.\n\n\n\n\nAlerts for Kubernetes Components\n\nRancher provides alerts when core Kubernetes system components become unhealthy.\n\nControllers update Kubernetes resources based on changes in etcd. The controller manager monitors the cluster desired state through the Kubernetes API server and makes the necessary changes to the current state to reach the desired state.\n\nThe scheduler service is a core component of Kubernetes. It is responsible for scheduling cluster workloads to nodes, based on various configurations, metrics, resource requirements and workload-specific requirements.\n\n\n\n\nAlert\nExplanation\n\n\n\n\n\nController Manager is unavailable\nA critical warning is triggered when the cluster’s controller-manager becomes unavailable.\n\n\n\nScheduler is unavailable\nA critical warning is triggered when the cluster’s scheduler becomes unavailable.\n\n\n\n\nAlerts for Events\n\nKubernetes events are objects that provide insight into what is happening inside a cluster, such as what decisions were made by the scheduler or why some pods were evicted from the node. In the Rancher UI, from the project view, you can see events for each workload.\n\n\n\n\nAlert\nExplanation\n\n\n\n\n\nGet warning deployment event\nA warning alert is triggered when a warning event happens on a deployment.\n\n\n\n\nAlerts for Nodes\n\nAlerts can be triggered based on node metrics. Each computing resource in a Kubernetes cluster is called a node. Nodes can be either bare-metal servers or virtual machines.\n\n\n\n\nAlert\nExplanation\n\n\n\n\n\nHigh CPU load\nA warning alert is triggered if the node uses more than 100 percent of the node’s available CPU seconds for at least three minutes.\n\n\n\nHigh node memory utilization\nA warning alert is triggered if the node uses more than 80 percent of its available memory for at least three minutes.\n\n\n\nNode disk is running full within 24 hours\nA critical alert is triggered if the disk space on the node is expected to run out in the next 24 hours based on the disk growth over the last 6 hours.\n\n\n\n\nProject-level Alerts\n\nWhen you enable monitoring for the project, some project-level alerts are provided. For details, refer to the section on project-level alerts.\n","postref":"0c41ba82279f7519244715a543405050","objectID":"a81db1b992bd3aff123c1c43d9d01311","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/tools/alerts/default-alerts/"},{"anchor":"#","title":"How Persistent Storage Works","content":"A persistent volume (PV) is a piece of storage in the Kubernetes cluster, while a persistent volume claim (PVC) is a request for storage.\n\nThere are two ways to use persistent storage in Kubernetes:\n\n\nUse an existing persistent volume\nDynamically provision new persistent volumes\n\n\nTo use an existing PV, your application will need to use a PVC that is bound to a PV, and the PV should include the minimum resources that the PVC requires.\n\nFor dynamic storage provisioning, your application will need to use a PVC that is bound to a storage class. The storage class contains the authorization to provision new persistent volumes.\n\n\n\nFor more information, refer to the official Kubernetes documentation on storage\n\nThis section covers the following topics:\n\n\nAbout persistent volume claims\n\n\nPVCs are required for both new and existing persistent storage\n\nSetting up existing storage with a PVC and PV\n\n\nBinding PVs to PVCs\n\nProvisioning new storage with a PVC and storage class\n\n\nAbout Persistent Volume Claims\n\nPersistent volume claims (PVCs) are objects that request storage resources from your cluster. They’re similar to a voucher that your deployment can redeem for storage access. A PVC is mounted into a workloads as a volume so that the workload can claim its specified share of the persistent storage.\n\nTo access persistent storage, a pod must have a PVC mounted as a volume. This PVC lets your deployment application store its data in an external location, so that if a pod fails, it can be replaced with a new pod and continue accessing its data stored externally, as though an outage never occurred.\n\nEach Rancher project contains a list of PVCs that you’ve created, available from Resources > Workloads > Volumes. (In versions prior to v2.3.0, the PVCs are in the Volumes tab.) You can reuse these PVCs when creating deployments in the future.\n\nPVCs are Required for Both New and Existing Persistent Storage\n\nA PVC is required for pods to use any persistent storage, regardless of whether the workload is intended to use storage that already exists, or the workload will need to dynamically provision new storage on demand.\n\nIf you are setting up existing storage for a workload, the workload mounts a PVC, which refers to a PV, which corresponds to existing storage infrastructure.\n\nIf a workload should request new storage, the workload mounts PVC, which refers to a storage class, which has the capability to create a new PV along with its underlying storage infrastructure.\n\nRancher lets you create as many PVCs within a project as you’d like.\n\nYou can mount PVCs to a deployment as you create it, or later, after the deployment is running.\n\nSetting up Existing Storage with a PVC and PV\n\nYour pods can store data in volumes, but if the pod fails, that data is lost. To solve this issue, Kubernetes offers persistent volumes (PVs), which are Kubernetes resources that correspond to external storage disks or file systems that your pods can access. If a pod crashes, its replacement pod can access the data in persistent storage without any data loss.\n\nPVs can represent a physical disk or file system that you host on premise, or a vendor-hosted storage resource, such as Amazon EBS or Azure Disk.\n\nCreating a persistent volume in Rancher will not create a storage volume. It only creates a Kubernetes resource that maps to an existing volume. Therefore, before you can create a persistent volume as a Kubernetes resource, you must have storage provisioned.\n\n\nImportant: PVs are created at the cluster level, which means that in a multi-tenant cluster, teams with access to separate namespaces could have access to the same PV.\n\n\nBinding PVs to PVCs\n\nWhen pods are set up to use persistent storage, they mount a persistent volume claim (PVC) that is mounted the same way as any other Kubernetes volume. When each PVC is created, the Kubernetes master considers it to be a request for storage and binds it to a PV that matches the minimum resource requirements of the PVC. Not every PVC is guaranteed to be bound to a PV. According to the Kubernetes documentation,\n\n\nClaims will remain unbound indefinitely if a matching volume does not exist. Claims will be bound as matching volumes become available. For example, a cluster provisioned with many 50Gi PVs would not match a PVC requesting 100Gi. The PVC can be bound when a 100Gi PV is added to the cluster.\n\n\nIn other words, you can create unlimited PVCs, but they will only be bound to PVs if the Kubernetes master can find a sufficient PVs that has at least the amount of disk space required by the PVC.\n\nTo dynamically provision new storage, the PVC mounted in the pod would have to correspond to a storage class instead of a persistent volume.\n\nProvisioning New Storage with a PVC and Storage Class\n\nStorage Classes allow you to create PVs dynamically without having to create persistent storage in an infrastructure provider first.\n\nFor example, if a workload is bound to a PVC and the PVC refers to an Amazon EBS Storage Class, the storage class can dynamically create an EBS volume and a corresponding PV.\n\nThe Kubernetes master will then bind the newly created PV to your workload’s PVC, allowing your workload to use the persistent storage.\n","postref":"c7452c90c9fbd77fa6d802d40ce61bb4","objectID":"b2081c5ca3d2c9fa14611f11a7e82119","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/volumes-and-storage/how-storage-works/"},{"anchor":"#","title":"How Resource Quotas Work in Rancher Projects","content":"Resource quotas in Rancher include the same functionality as the native version of Kubernetes. However, in Rancher, resource quotas have been extended so that you can apply them to projects.\n\nIn a standard Kubernetes deployment, resource quotas are applied to individual namespaces. However, you cannot apply the quota to your namespaces simultaneously with a single action. Instead, the resource quota must be applied multiple times.\n\nIn the following diagram, a Kubernetes administrator is trying to enforce a resource quota without Rancher. The administrator wants to apply a resource quota that sets the same CPU and memory limit to every namespace in his cluster (Namespace 1-4) . However, in the base version of Kubernetes, each namespace requires a unique resource quota. The administrator has to create four different resource quotas that have the same specs configured (Resource Quota 1-4) and apply them individually.\n\nBase Kubernetes: Unique Resource Quotas Being Applied to Each Namespace\n\n\nResource quotas are a little different in Rancher. In Rancher, you apply a resource quota to the project, and then the quota propagates to each namespace, whereafter Kubernetes enforces your limits using the native version of resource quotas. If you want to change the quota for a specific namespace, you can override it.\n\nThe resource quota includes two limits, which you set while creating or editing a project:\n\n\n\nProject Limits:\n\nThis set of values configures an overall resource limit for the project. If you try to add a new namespace to the project, Rancher uses the limits you’ve set to validate that the project has enough resources to accommodate the namespace.  In other words, if you try to move a namespace into a project near its resource quota, Rancher blocks you from moving the namespace.\n\nNamespace Default Limits:\n\nThis value is the default resource limit available for each namespace. When the resource quota is set on the project level, this limit is automatically propagated to each namespace in the project. Each namespace is bound to this default limit unless you override it.\n\n\nIn the following diagram, a Rancher administrator wants to apply a resource quota that sets the same CPU and memory limit for every namespace in their project (Namespace 1-4). However, in Rancher, the administrator can set a resource quota for the project (Project Resource Quota) rather than individual namespaces. This quota includes resource limits for both the entire project (Project Limit) and individual namespaces (Namespace Default Limit). Rancher then propagates the Namespace Default Limit quotas to each namespace (Namespace Resource Quota).\n\nRancher: Resource Quotas Propagating to Each Namespace\n\n\nThe following table explains the key differences between the two quota types.\n\n\n\n\nRancher Resource Quotas\nKubernetes Resource Quotas\n\n\n\n\n\nApplies to projects and namespace.\nApplies to namespaces only.\n\n\n\nCreates resource pool for all namespaces in project.\nApplies static resource limits to individual namespaces.\n\n\n\nApplies resource quotas to namespaces through propagation.\nApplies only to the assigned namespace.\n\n\n\n","postref":"c1e00a1adfe4c20644eeaf293654f16f","objectID":"3370b9d16010c0636e765d6649e3cb9d","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/project-admin/resource-quotas/quotas-for-projects/"},{"anchor":"#prerequisites","title":"Prerequisites","content":"Hardware requirements are based on the size of your K3s cluster. For production and large clusters, we recommend using a high-availability setup with an external database. The following options are recommended for the external database in production:\nMySQL\nPostgreSQL\netcd\nCPU and MemoryThe following are the minimum CPU and memory requirements for nodes in a high-availability K3s server:\n\n\nDeployment Size\nNodes\nVCPUS\nRAM\n\n\n\n\n\nSmall\nUp to 10\n2\n4 GB\n\n\n\nMedium\nUp to 100\n4\n8 GB\n\n\n\nLarge\nUp to 250\n8\n16 GB\n\n\n\nX-Large\nUp to 500\n16\n32 GB\n\n\n\nXX-Large\n500+\n32\n64 GB\n\n\nDisksThe cluster performance depends on database performance. To ensure optimal speed, we recommend always using SSD disks to back your K3s cluster. On cloud providers, you will also want to use the minimum size that allows the maximum IOPS.NetworkYou should consider increasing the subnet size for the cluster CIDR so that you don’t run out of IPs for the pods. You can do that by passing the --cluster-cidr option to K3s server upon starting.DatabaseK3s supports different databases including MySQL, PostgreSQL, MariaDB, and etcd, the following is a sizing guide for the database resources you need to run large clusters:\n\n\nDeployment Size\nNodes\nVCPUS\nRAM\n\n\n\n\n\nSmall\nUp to 10\n1\n2 GB\n\n\n\nMedium\nUp to 100\n2\n8 GB\n\n\n\nLarge\nUp to 250\n4\n16 GB\n\n\n\nX-Large\nUp to 500\n8\n32 GB\n\n\n\nXX-Large\n500+\n16\n64 GB\n\n\nThe K3s server needs port 6443 to be accessible by the nodes. The nodes need to be able to reach other nodes over UDP port 8472 (Flannel VXLAN). If you do not use Flannel and provide your own custom CNI, then port 8472 is not needed by K3s. The node should not listen on any other port. K3s uses reverse tunneling such that the nodes make outbound connections to the server and all kubelet traffic runs through that tunnel.IMPORTANT: The VXLAN port on nodes should not be exposed to the world as it opens up your cluster network to be accessed by anyone. Run your nodes behind a firewall/security group that disabled access to port 8472.If you wish to utilize the metrics server, you will need to open port 10250 on each node.Hardware requirements scale based on the size of your deployments. Minimum recommendations are outlined here.\nRAM: 512MB Minimum\nCPU: 1 Minimum\nDisksK3s performance depends on the performance of the database. To ensure optimal speed, we recommend using an SSD when possible. Disk performance will vary on ARM devices utilizing an SD card or eMMC.K3s should run on just about any flavor of Linux. However, K3s is tested on the following operating systems and their subsequent non-major releases.\nUbuntu 16.04 (amd64)\nUbuntu 18.04 (amd64)\n\n\nIf you are using Raspbian Buster, follow these steps to switch to legacy iptables.\nIf you are using Alpine Linux, follow these steps for additional setup.\n\n\nTwo nodes cannot have the same hostname. If all your nodes have the same hostname, use the --with-node-id option to append a random suffix for each node, or otherwise devise a unique name to pass with --node-name or $K3S_NODE_NAME for each node you add to the cluster.\n","postref":"641ec44ff0c14e181efaed0ddd57a8f9","objectID":"7e5357f295dcc26724692d98a0a452d4","permalink":"http://jijeesh.github.io/docs/k3s/latest/en/installation/installation-requirements/"},{"anchor":"#","title":"Installation Requirements","content":"This page describes the software, hardware, and networking requirements for the nodes where the Rancher server will be installed. The Rancher server can be installed on a single node or a high-availability Kubernetes cluster.\n\n\nIt is important to note that if you install Rancher on a Kubernetes cluster, requirements are different from the node requirements for downstream user clusters, which will run your apps and services.\n\n\nMake sure the node(s) for the Rancher server fulfill the following requirements:\n\n\nOperating Systems and Docker Requirements\nHardware Requirements\n\n\nCPU and Memory\nDisks\n\nNetworking Requirements\n\n\nNode IP Addresses\nPort Requirements\n\n\n\nFor a list of best practices that we recommend for running the Rancher server in production, refer to the best practices section.\n\nThe Rancher UI works best in Firefox or Chrome.\n\nOperating Systems and Docker Requirements\n\nRancher should work with any modern Linux distribution and any modern Docker version.\n\nRancher has been tested and is supported with Ubuntu, CentOS, Oracle Linux, RancherOS, and RedHat Enterprise Linux.\n\nFor details on which OS and Docker versions were tested with each Rancher version, refer to the support maintenance terms.\n\nAll supported operating systems are 64-bit x86.\n\nThe ntp (Network Time Protocol) package should be installed. This prevents errors with certificate validation that can occur when the time is not synchronized between the client and server.\n\nSome distributions of Linux derived from RHEL, including Oracle Linux, may have default firewall rules that block communication with Helm. This how-to guide shows how to check the default firewall rules and how to open the ports with firewalld if necessary.\n\nIf you plan to run Rancher on ARM64, see Running on ARM64 (Experimental).\n\nInstalling Docker\n\nDocker can be installed by following the steps in the official Docker documentation. Rancher also provides scripts to install Docker with one command.\n\nHardware Requirements\n\nThis section describes the CPU, memory, and disk requirements for the nodes where the Rancher server is installed.\n\nCPU and Memory\n\nHardware requirements scale based on the size of your Rancher deployment. Provision each individual node according to the requirements. The requirements are different depending on if you are installing Rancher with Docker or on a Kubernetes cluster.\n\n\n  \n  \n  These requirements apply to installing Rancher on a Kubernetes cluster.\n\n\n\n\nDeployment Size\nClusters\nNodes\nvCPUs\nRAM\n\n\n\n\n\nSmall\nUp to 5\nUp to 50\n2\n8 GB\n\n\n\nMedium\nUp to 15\nUp to 200\n4\n16 GB\n\n\n\nLarge\nUp to 50\nUp to 500\n8\n32 GB\n\n\n\nX-Large\nUp to 100\nUp to 1000\n32\n128 GB\n\n\n\nXX-Large\n100+\n1000+\nContact Rancher\nContact Rancher\n\n\n\n\n\n\n\n  These requirements apply to single node installations of Rancher.\n\n\n\n\nDeployment Size\nClusters\nNodes\nvCPUs\nRAM\n\n\n\n\n\nSmall\nUp to 5\nUp to 50\n1\n4 GB\n\n\n\nMedium\nUp to 15\nUp to 200\n2\n8 GB\n\n\n\n\n\n\n\n\n\nDisks\n\nRancher performance depends on etcd in the cluster performance. To ensure optimal speed, we recommend always using SSD disks to back your Rancher management Kubernetes cluster. On cloud providers, you will also want to use the minimum size that allows the maximum IOPS. In larger clusters, consider using dedicated storage devices for etcd data and wal directories.\n\nNetworking Requirements\n\nThis section describes the networking requirements for the node(s) where the Rancher server is installed.\n\nNode IP Addresses\n\nEach node used should have a static IP configured, regardless of whether you are installing Rancher on a single node or on an HA cluster. In case of DHCP, each node should have a DHCP reservation to make sure the node gets the same IP allocated.\n\nPort Requirements\n\nThis section describes the port requirements for nodes running the rancher/rancher container.\n\nThe port requirements are different depending on whether you are installing Rancher on a single node or on a high-availability Kubernetes cluster.\n\n\nFor a Docker installation, you only need to open the ports required to enable Rancher to communicate with downstream user clusters.\nFor a high-availability installation, the same ports need to be opened, as well as additional ports required to set up the Kubernetes cluster that Rancher is installed on.\n\n\n\n  \n  \n  Ports for Communication with Downstream Clusters\n\nTo communicate with downstream clusters, Rancher requires different ports to be open depending on the infrastructure you are using.\n\nFor example, if you are deploying Rancher on nodes hosted by an infrastructure provider, port 22 must be open for SSH.\n\nThe following diagram depicts the ports that are opened for each cluster type.\n\nPort Requirements for the Rancher Management Plane\n\n\n\nThe following tables break down the port requirements for inbound and outbound traffic:\n\nInbound Rules for Rancher Nodes\n\n\n\n\nProtocol\nPort\nSource\nDescription\n\n\n\n\n\nTCP\n80\nLoad balancer/proxy that does external SSL termination\nRancher UI/API when external SSL termination is used\n\n\n\nTCP\n443\netcd nodescontrolplane nodesworker nodeshosted/imported Kubernetesany source that needs to be able to use the Rancher UI or API\nRancher agent, Rancher UI/API, kubectl\n\n\n\n\nOutbound Rules for Rancher Nodes\n\n\n\n\nProtocol\nPort\nDestination\nDescription\n\n\n\n\n\nTCP\n22\nAny node IP from a node created using Node Driver\nSSH provisioning of nodes using Node Driver\n\n\n\nTCP\n443\n35.160.43.145/32, 35.167.242.46/32, 52.33.59.17/32\ngit.rancher.io (catalogs)\n\n\n\nTCP\n2376\nAny node IP from a node created using Node driver\nDocker daemon TLS port used by Docker Machine\n\n\n\nTCP\n6443\nHosted/Imported Kubernetes API\nKubernetes API server\n\n\n\n\nNote Rancher nodes may also require additional outbound access for any external authentication provider which is configured (LDAP for example).\n\nAdditional Port Requirements for Nodes in an HA/Kubernetes Cluster\n\nYou will need to open additional ports to launch the Kubernetes cluster that are required for a high-availability installation of Rancher.\n\nIf you follow the Rancher installation documentation for setting up a Kubernetes cluster using RKE, you will set up a cluster in which all three nodes have all three roles: etcd, controlplane, and worker. In that case, you can refer to this list of requirements for each node with all three roles:\n\nInbound Rules for Nodes with All Three Roles: etcd, Controlplane, and Worker\n\n\n\n\nProtocol\nPort\nSource\nDescription\n\n\n\n\n\nTCP\n22\nLinux worker nodes only, and any network that you want to be able to remotely access this node from.\nRemote access over SSH\n\n\n\nTCP\n80\nAny source that consumes Ingress services\nIngress controller (HTTP)\n\n\n\nTCP\n443\nAny source that consumes Ingress services\nIngress controller (HTTPS)\n\n\n\nTCP\n2376\nRancher nodes\nDocker daemon TLS port used by Docker Machine (only needed when using Node Driver/Templates)\n\n\n\nTCP\n2379\netcd nodes and controlplane nodes\netcd client requests\n\n\n\nTCP\n2380\netcd nodes and controlplane nodes\netcd peer communication\n\n\n\nTCP\n3389\nWindows worker nodes only, and any network that you want to be able to remotely access this node from.\nRemote access over RDP\n\n\n\nTCP\n6443\netcd nodes, controlplane nodes, and worker nodes\nKubernetes apiserver\n\n\n\nUDP\n8472\netcd nodes, controlplane nodes, and worker nodes\nCanal/Flannel VXLAN overlay networking\n\n\n\nTCP\n9099\nthe node itself (local traffic, not across nodes)\nCanal/Flannel livenessProbe/readinessProbe\n\n\n\nTCP\n10250\ncontrolplane nodes\nkubelet\n\n\n\nTCP\n10254\nthe node itself (local traffic, not across nodes)\nIngress controller livenessProbe/readinessProbe\n\n\n\nTCP/UDP\n30000-32767\nAny source that consumes NodePort services\nNodePort port range\n\n\n\n\nOutbound Rules for Nodes with All Three Roles: etcd, Controlplane, and Worker\n\n\n\n\nProtocol\nPort\nSource\nDestination\nDescription\n\n\n\n\n\nTCP\n22\nRKE node\nAny node configured in Cluster Configuration File\nSSH provisioning of node by RKE\n\n\n\nTCP\n443\nRancher nodes\nRancher agent\n\n\n\n\nTCP\n2379\netcd nodes\netcd client requests\n\n\n\n\nTCP\n2380\netcd nodes\netcd peer communication\n\n\n\n\nTCP\n6443\nRKE node\ncontrolplane nodes\nKubernetes API server\n\n\n\nTCP\n6443\ncontrolplane nodes\nKubernetes API server\n\n\n\n\nUDP\n8472\netcd nodes, controlplane nodes, and worker nodes\nCanal/Flannel VXLAN overlay networking\n\n\n\n\nTCP\n9099\nthe node itself (local traffic, not across nodes)\nCanal/Flannel livenessProbe/readinessProbe\n\n\n\n\nTCP\n10250\netcd nodes, controlplane nodes, and worker nodes\nkubelet\n\n\n\n\nTCP\n10254\nthe node itself (local traffic, not across nodes)\nIngress controller livenessProbe/readinessProbe\n\n\n\n\n\nThe ports that need to be opened for each node depend on the node’s Kubernetes role: etcd, controlplane, or worker. If you installed Rancher on a Kubernetes cluster that doesn’t have all three roles on each node, refer to the port requirements for the Rancher Kubernetes Engine (RKE). The RKE docs show a breakdown of the port requirements for each role.\n\n\n\n\n  Ports for Communication with Downstream Clusters\n\nTo communicate with downstream clusters, Rancher requires different ports to be open depending on the infrastructure you are using.\n\nFor examp","postref":"ea2897f43e7926e447f12959e3e727de","objectID":"1704ace7cf4c7ce12890077043ccd376","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/requirements/"},{"anchor":"#","title":"Installing Docker","content":"Docker is required to be installed on any node that runs the Rancher server.\n\nThere are a couple of options for installing Docker. One option is to refer to the official Docker documentation about how to install Docker on Linux. The steps will vary based on the Linux distribution.\n\nAnother option is to use one of Rancher’s Docker installation scripts, which are available for most recent versions of Docker.\n\nFor example, this command could be used to install Docker 18.09 on Ubuntu:\n\ncurl https://releases.rancher.com/install-docker/18.09.sh | sh\n\n\nTo find out whether a script is available for installing a certain Docker version, refer to this GitHub repository, which contains all of Rancher’s Docker installation scripts.\n","postref":"d0afd0796d449b327a6c90cde36d3569","objectID":"bdf496d9d6acb8a17b09e91d2ffc3722","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/requirements/installing-docker/"},{"anchor":"#advanced-options","title":"Advanced Options","content":"\nRecommended: Review Single Node Backup and Restoration. Although you don’t have any data you need to back up right now, we recommend creating backups after regular Rancher use.\nCreate a Kubernetes cluster: Provisioning Kubernetes Clusters.\nRefer to this page for frequently asked questions and troubleshooting tips.When installing Rancher on a single node with Docker, there are several advanced options that can be enabled:\nCustom CA Certificate\nAPI Audit Log\nTLS Settings\nAir Gap\nPersistent Data\nRunning rancher/rancher and rancher/rancher-agent on the Same Node\nRefer to this page for details.","postref":"23f1e2450e3bbf1d6210e308edd9e1b0","objectID":"6b1f663460810deb9ee3284cc8f6fe0f","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/other-installation-methods/single-node-docker/"},{"anchor":"#recommended-architecture","title":"Recommended Architecture","content":"RKE add-on install\nImportant: RKE add-on install is only supported up to Rancher v2.0.8\n\nPlease use the Rancher helm chart to install Rancher on a Kubernetes cluster. For details, see the Kubernetes Install - Installation Outline.\n\nIf you are currently using the RKE add-on install method, see Migrating from a Kubernetes Install with an RKE Add-on for details on how to move to using the Helm chart.\n\nMigrating from a Kubernetes Install with an RKE Add-on\n\nCreate Nodes and Load Balancer\nInstall Kubernetes with RKE\nInitialize Helm (tiller)\nInstall Rancher\nThe following CLI tools are required for this install. Please make sure these tools are installed and available in your $PATH\nkubectl - Kubernetes command-line tool.\nrke - Rancher Kubernetes Engine, cli for building Kubernetes clusters.\nhelm - Package management for Kubernetes. Refer to the Helm version requirements to choose a version of Helm to install Rancher.\n\nDNS for Rancher should resolve to a Layer 4 load balancer (TCP)\nThe Load Balancer should forward port TCP/80 and TCP/443 to all 3 nodes in the Kubernetes cluster.\nThe Ingress controller will redirect HTTP to HTTPS and terminate SSL/TLS on port TCP/443.\nThe Ingress controller will forward traffic to port TCP/80 on the pod in the Rancher deployment.\nKubernetes Rancher install with layer 4 load balancer, depicting SSL termination at ingress controllersKubernetes Rancher install with Layer 4 load balancer (TCP), depicting SSL termination at ingress controllers","postref":"dcb033c399d213b004beaf89b43ecb48","objectID":"00bd9ccea6dda681233db99ca6aae7df","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/helm2/"},{"anchor":"#migration-cheatsheet","title":"Migration Cheatsheet","content":"Because Rancher v1.6 defaulted to our Cattle container orchestrator, it primarily used terminology related to Cattle. However, because Rancher v2.x uses Kubernetes, it aligns with the Kubernetes naming standard. This shift could be confusing for people unfamiliar with Kubernetes, so we’ve created a table that maps terms commonly used in Rancher v1.6 to their equivalents in Rancher v2.x.\n\n\nRancher v1.6\nRancher v2.x\n\n\n\n\n\nContainer\nPod\n\n\n\nServices\nWorkload\n\n\n\nLoad Balancer\nIngress\n\n\n\nStack\nNamespace\n\n\n\nEnvironment\nProject (Administration)/Cluster (Compute)\n\n\n\nHost\nNode\n\n\n\nCatalog\nHelm\n\n\n\nPort Mapping\nHostPort (Single Node)/NodePort (All Nodes)\n\n\n\nMore detailed information on Kubernetes concepts can be found in the\nKubernetes Concepts Documentation.Next: Get Started","postref":"fff0b1b7867e1b41ec2dbbcecc4c7305","objectID":"129686ef4713b31ddc2a504b1b6d460b","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/v1.6-migration/kub-intro/"},{"anchor":"#","title":"Node Requirements for User Clusters","content":"This page describes the requirements for the nodes where your apps and services will be installed.\n\nIn this section, “user cluster” refers to a cluster running your apps, which should be separate from the cluster (or single node) running Rancher.\n\n\nIf Rancher is installed on a high-availability Kubernetes cluster, the Rancher server cluster and user clusters have different requirements. For Rancher installation requirements, refer to the node requirements in the installation section.\n\n\nMake sure the nodes for the Rancher server fulfill the following requirements:\n\n\nOperating systems and Docker requirements\nHardware Requirements\nNetworking Requirements\nOptional: Security Considerations\n\n\nOperating Systems and Docker Requirements\n\nRancher should work with any modern Linux distribution and any modern Docker version. Linux is required for the etcd and controlplane nodes of all downstream clusters. Worker nodes may run Linux or Windows Server. The capability to use Windows worker nodes in downstream clusters was added in Rancher v2.3.0.\n\nRancher works has been tested and is supported with downstream clusters running Ubuntu, CentOS, Oracle Linux, RancherOS, and RedHat Enterprise Linux. For details on which OS and Docker versions were tested with each Rancher version, refer to the support maintenance terms.\n\nAll supported operating systems are 64-bit x86.\n\nIf you plan to use ARM64, see Running on ARM64 (Experimental).\n\nFor information on how to install Docker, refer to the official Docker documentation.\n\nSome distributions of Linux derived from RHEL, including Oracle Linux, may have default firewall rules that block communication with Helm. This how-to guide shows how to check the default firewall rules and how to open the ports with firewalld if necessary.\n\nSUSE Linux may have a firewall that blocks all ports by default. In that situation, follow these steps to open the ports needed for adding a host to a custom cluster.\n\nRequirements for Windows Nodes\n\nWindows worker nodes can be used as of Rancher v2.3.0\n\nNodes with Windows Server must run Docker Enterprise Edition.\n\nWindows nodes can be used for worker nodes only. See Configuring Custom Clusters for Windows\n\nHardware Requirements\n\nThe hardware requirements for nodes with the worker role mostly depend on your workloads. The minimum to run the Kubernetes node components is 1 CPU (core) and 1GB of memory.\n\nRegarding CPU and memory, it is recommended that the different planes of Kubernetes clusters (etcd, controlplane, and workers) should be hosted on different nodes so that they can scale separately from each other.\n\nFor hardware recommendations for large Kubernetes clusters, refer to the official Kubernetes documentation on building large clusters.\n\nFor hardware recommendations for etcd clusters in production, refer to the official etcd documentation.\n\nNetworking Requirements\n\nFor a production cluster, we recommend that you restrict traffic by opening only the ports defined in the port requirements below.\n\nThe ports required to be open are different depending on how the user cluster is launched. Each of the sections below list the ports that need to be opened for different cluster creation options.\n\nFor a breakdown of the port requirements for etcd nodes, controlplane nodes, and worker nodes in a Kubernetes cluster, refer to the port requirements for the Rancher Kubernetes Engine.\n\nDetails on which ports are used in each situation are found in the following sections:\n\n\nCommonly used ports\nPort requirements for custom clusters\nPort requirements for clusters hosted by an infrastructure provider\n\n\nSecurity group for nodes on AWS EC2\n\nPort requirements for clusters hosted by a Kubernetes provider\nPort requirements for imported clusters\nPort requirements for local traffic\n\n\nCommonly Used Ports\n\nIf security isn’t a large concern and you’re okay with opening a few additional ports, you can use this table as your port reference instead of the comprehensive tables in the following sections.\n\nThese ports are typically opened on your Kubernetes nodes, regardless of what type of cluster it is.\n\n\n  \n  Click to Expand\n  \n    Commonly Used Ports Reference\n\n\n\n\nProtocol\nPort\nDescription\n\n\n\n\n\nTCP\n22\nNode driver SSH provisioning\n\n\n\nTCP\n2376\nNode driver Docker daemon TLS port\n\n\n\nTCP\n2379\netcd client requests\n\n\n\nTCP\n2380\netcd peer communication\n\n\n\nUDP\n8472\nCanal/Flannel VXLAN overlay networking\n\n\n\nUDP\n4789\nFlannel VXLAN overlay networking on Windows cluster\n\n\n\nTCP\n9099\nCanal/Flannel livenessProbe/readinessProbe\n\n\n\nTCP\n6783\nWeave Port\n\n\n\nUDP\n6783-6784\nWeave UDP Ports\n\n\n\nTCP\n10250\nkubelet API\n\n\n\nTCP\n10254\nIngress controller livenessProbe/readinessProbe\n\n\n\nTCP/UDP\n30000-32767\nNodePort port range\n\n\n\n\n  \n\n\n\nPort Requirements for Custom Clusters\n\nIf you are launching a Kubernetes cluster on your existing infrastructure, refer to these port requirements.\n\n\n  \n  Click to Expand\n  \n    The following table depicts the port requirements for Rancher Launched Kubernetes with custom nodes.\n\n\n    \n        \n          From / To\n          Rancher Nodes\n          etcd Plane Nodes\n          Control Plane Nodes\n          Worker Plane Nodes\n          External Load Balancer\n          Internet\n        \n    \n    \n        \n          Rancher Nodes (1)\n          \n          \n          \n          \n          \n          git.rancher.io (2):35.160.43.145:3235.167.242.46:3252.33.59.17:32\n        \n        \n          etcd Plane Nodes\n          443 TCP (3)\n          2379 TCP\n          \n          \n          443 TCP\n          \n        \n        \n          2380 TCP\n          \n          \n          \n        \n        \n          \n          6443 TCP\n          \n          \n        \n        \n          8472 UDP\n          \n        \n        \n          4789 UDP (7)\n          \n        \n        \n          9099 TCP (4)\n          \n          \n          \n        \n        \n          Control Plane Nodes\n          443 TCP (3)\n          2379 TCP\n          \n          \n          443 TCP\n          \n        \n        \n          2380 TCP\n          \n          \n          \n        \n        \n          \n          6443 TCP\n          \n          \n        \n        \n          8472 UDP\n          \n        \n        \n          4789 UDP (7)\n          \n        \n        \n          10250 TCP\n          \n        \n        \n          \n          9099 TCP (4)\n          \n          \n        \n        \n          \n          10254 TCP (4)\n          \n          \n        \n        \n          Worker Plane Nodes\n          443 TCP (3)\n          \n          6443 TCP\n          \n          443 TCP\n          \n        \n        \n          8472 UDP\n          \n        \n        \n          4789 UDP (7)\n          \n        \n        \n          \n          \n          9099 TCP (4)\n          \n        \n        \n          \n          \n          10254 TCP (4)\n          \n        \n        \n          External Load Balancer (5)\n          80 TCP\n          \n          \n          \n          \n          \n        \n        \n          443 TCP (6)\n          \n          \n          \n          \n          \n        \n        \n          API / UI Clients\n          80 TCP (3)\n          \n          \n          \n          80 TCP\n          \n        \n        \n          443 TCP (3)\n          \n          \n          \n          443 TCP\n          \n        \n        \n          Workload Clients\n          \n          \n          \n          30000-32767 TCP / UDP(nodeport)\n          \n          \n        \n        \n          \n          \n          80 TCP (Ingress)\n          \n          \n        \n        \n          \n          \n          443 TCP (Ingress)\n          \n          \n        \n        \n          Notes:1. Nodes running standalone server or Rancher HA deployment.2. Required to fetch Rancher chart library.3. Only without external load balancer.4. Local traffic to the node itself (not across nodes).5. Load balancer / proxy that handles tragging to the Rancher UI / API.6. Only if SSL is not terminated at external load balancer.7. Only if using Overlay mode on Windows cluster.\n        \n    \n\n\n  \n\n\n\nPort Requirements for Clusters Hosted by an Infrastructure Provider\n\nIf you are launching a Kubernetes cluster on nodes that are in an infrastructure provider such as Amazon EC2, Google Container Engine, DigitalOcean, Azure, or vSphere, these port requirements apply.\n\nThese required ports are automatically opened by Rancher during creation of clusters using cloud providers.\n\n\n  \n  Click to Expand\n  \n    The following table depicts the port requirements for Rancher Launched Kubernetes with nodes created in an Infrastructure Provider.\n\n\nNote:\nThe required ports are automatically opened by Rancher during creation of clusters in cloud providers like Amazon EC2 or DigitalOcean.\n\n\n\n    \n        \n          From / To\n          Rancher Nodes\n          etcd Plane Nodes\n          Control Plane Nodes\n          Worker Plane Nodes\n          External Load Balancer\n          Internet\n        \n    \n    \n        \n          Rancher Nodes (1)\n          \n          22 TCP\n          \n          git.rancher.i","postref":"bf33d48813329eb0c11cd81b52a4dff5","objectID":"3f07f21d9a6b2c408d4d958ed278714c","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/node-requirements/"},{"anchor":"#adding-notifiers","title":"Adding Notifiers","content":"After you set up notifiers, you can manage them. From the Global view, open the cluster that you want to manage your notifiers. Select Tools > Notifiers. You can:\nEdit their settings that you configured during their initial setup.\nClone them, to quickly setup slightly different notifiers.\nDelete them when they’re no longer necessary.\nAfter creating a notifier, set up alerts to receive notifications of Rancher system events.\nCluster owners can set up alerts at the cluster level.\nProject owners can set up alerts at the project level.\nSet up a notifier so that you can begin configuring and sending alerts.\nFrom the Global View, open the cluster that you want to add a notifier.\n\nFrom the main menu, select Tools > Notifiers. Then click Add Notifier.\n\nSelect the service you want to use as your notifier, and then fill out the form.\n\n  \n  Slack\n  \n    \nEnter a Name for the notifier.\nFrom Slack, create a webhook. For instructions, see the Slack Documentation.\nFrom Rancher, enter your Slack webhook URL.\n\nEnter the name of the channel that you want to send alert notifications in the following format: #<channelname>.\n\nBoth public and private channels are supported.\n\nClick Test. If the test is successful, the Slack channel you’re configuring for the notifier outputs Slack setting validated.\n\n\n  \n\n\n\n  \n  Email\n  \n    \nEnter a Name for the notifier.\nIn the Sender field, enter an email address available on your mail server that you want to send the notification.\nIn the Host field, enter the IP address or hostname for your SMTP server. Example: smtp.email.com\nIn the Port field, enter the port used for email. Typically, TLS uses 587 and SSL uses 465. If you’re using TLS, make sure Use TLS is selected.\nEnter a Username and Password that authenticate with the SMTP server.\nIn the Default Recipient field, enter the email address that you want to receive the notification.\nClick Test. If the test is successful, Rancher prints settings validated and you receive a test notification email.\n\n\n  \n\n\n\n  \n  PagerDuty\n  \n    \nEnter a Name for the notifier.\nFrom PagerDuty, create a Prometheus integration. For instructions, see the PagerDuty Documentation.\nFrom PagerDuty, copy the integration’s Integration Key.\nFrom Rancher, enter the key in the Service Key field.\nClick Test. If the test is successful, your PagerDuty endpoint outputs PagerDuty setting validated.\n\n\n  \n\n\n\n  \n  WebHook\n  \n    \nEnter a Name for the notifier.\nUsing the app of your choice, create a webhook URL.\nEnter your webhook URL.\nClick Test. If the test is successful, the URL you’re configuring as a notifier outputs Webhook setting validated.\n\n\n  \n\n\n\n  \n  WeChat\n  \n    Available as of v2.2.0\n\n\nEnter a Name for the notifier.\nIn the Corporation ID field, enter the “EnterpriseID” of your corporation, you could get it from Profile page.\nFrom Enterprise WeChat, create an application in the Application page, and then enter the “AgentId” and “Secret” of this application to the Application Agent ID and Application Secret fields.\nSelect the Recipient Type and then enter a corresponding id to Default Recipient field, for example, the party id, tag id or user account that you want to receive the notification. You could get contact information from Contacts page.\n\n\n  \n\n\n\nAvailable as of v2.3.0 - Select Enable for Send Resolved Alerts if you wish to notify about resolved alerts.\n\nClick Add to complete adding the notifier.\nResult: Your notifier is added to Rancher.","postref":"ceab60cc1b1470ad203e5fc5d425f448","objectID":"0eb09f92ba4853a7fe4055f112f249d5","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/tools/notifiers/"},{"anchor":"#","title":"One-time Snapshots","content":"One-time snapshots are handled differently depending on your version of RKE.\n\n\n  \n  \n  To save a snapshot of etcd from each etcd node in the cluster config file, run the rke etcd snapshot-save command.\n\nThe snapshot is saved in /opt/rke/etcd-snapshots.\n\nWhen running the command, an additional container is created to take the snapshot. When the snapshot is completed, the container is automatically removed.\n\nThe one-time snapshot can be uploaded to a S3 compatible backend by using the additional options to specify the S3 backend.\n\nTo create a local one-time snapshot, run:\n\n$ rke etcd snapshot-save --config cluster.yml --name snapshot-name   \n\n\nResult: The snapshot is saved in /opt/rke/etcd-snapshots.\n\nTo save a one-time snapshot to S3, run:\n\n$ rke etcd snapshot-save \\\n--config cluster.yml \\\n--name snapshot-name \\\n--s3 \\\n--access-key S3_ACCESS_KEY \\\n--secret-key S3_SECRET_KEY \\\n--bucket-name s3-bucket-name \\\n--folder s3-folder-name \\ # Optional - Available as of v0.3.0\n--s3-endpoint s3.amazonaws.com\n\n\nResult: The snapshot is saved in /opt/rke/etcd-snapshots as well as uploaded to the S3 backend.\n\nOptions for rke etcd snapshot-save\n\n\n\n\nOption\nDescription\nS3 Specific\n\n\n\n\n\n--name value\nSpecify snapshot name\n\n\n\n\n--config value\nSpecify an alternate cluster YAML file (default: cluster.yml) [$RKE_CONFIG]\n\n\n\n\n--s3\nEnabled backup to s3\n*\n\n\n\n--s3-endpoint value\nSpecify s3 endpoint url (default: “s3.amazonaws.com”)\n*\n\n\n\n--s3-endpoint-ca value\nSpecify a path to a CA cert file to connect to a custom s3 endpoint (optional) Available as of v0.2.5\n*\n\n\n\n--access-key value\nSpecify s3 accessKey\n*\n\n\n\n--secret-key value\nSpecify s3 secretKey\n*\n\n\n\n--bucket-name value\nSpecify s3 bucket name\n*\n\n\n\n--folder value\nSpecify folder inside  bucket where backup will be stored. This is optional. Available as of v0.3.0\n*\n\n\n\n--region value\nSpecify the s3 bucket location (optional)\n*\n\n\n\n--ssh-agent-auth\nUse SSH Agent Auth defined by SSH_AUTH_SOCK\n\n\n\n\n--ignore-docker-version\nDisable Docker version check\n\n\n\n\n\nThe --access-key and --secret-key options are not required if the etcd nodes are AWS EC2 instances that have been configured with a suitable IAM instance profile.\n\nUsing a custom CA certificate for S3\n\nAvailable as of v2.2.5\n\nThe backup snapshot can be stored on a custom S3 backup like minio. If the S3 backend uses a self-signed or custom certificate, provide a custom certificate using the --s3-endpoint-ca to connect to the S3 back end.\n\nIAM Support for Storing Snapshots in S3\n\nIn addition to API access keys, RKE supports using IAM roles for S3 authentication. The cluster etcd nodes must be assigned an IAM role that has read/write access to the designated backup bucket on S3. Also, the nodes must have network access to the S3 endpoint specified.\n\nBelow is an example IAM policy that would allow nodes to store and retrieve backups from S3:\n\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"ListObjectsInBucket\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\"s3:ListBucket\"],\n            \"Resource\": [\"arn:aws:s3:::bucket-name\"]\n        },\n        {\n            \"Sid\": \"AllObjectActions\",\n            \"Effect\": \"Allow\",\n            \"Action\": \"s3:*Object\",\n            \"Resource\": [\"arn:aws:s3:::bucket-name/*\"]\n        }\n    ]\n}\n\n\nFor details on giving an application access to S3, refer to the AWS documentation on Using an IAM Role to Grant Permissions to Applications Running on Amazon EC2 Instances.\n\n\n\n\n  To save a snapshot of etcd from each etcd node in the cluster config file, run the rke etcd snapshot-save command.\n\nWhen running the command, an additional container is created to take the snapshot. When the snapshot is completed, the container is automatically removed.\n\nRKE saves a backup of the certificates, i.e. a file named pki.bundle.tar.gz, in the same location. The snapshot and pki bundle file are required for the restore process.\n\nTo create a local one-time snapshot, run:\n\n$ rke etcd snapshot-save --config cluster.yml --name snapshot-name   \n\n\nResult: The snapshot is saved in /opt/rke/etcd-snapshots.\n\nOptions for rke etcd snapshot-save\n\n\n\n\nOption\nDescription\n\n\n\n\n\n--name value\nSpecify snapshot name\n\n\n\n--config value\nSpecify an alternate cluster YAML file (default: cluster.yml) [$RKE_CONFIG]\n\n\n\n--ssh-agent-auth\nUse SSH Agent Auth defined by SSH_AUTH_SOCK\n\n\n\n--ignore-docker-version\nDisable Docker version check\n\n\n\n\n\n\n\n\n","postref":"43ff4b9070b408e388d620b1e917d460","objectID":"4f97e070e080ae0493a849b7ffb5024f","permalink":"http://jijeesh.github.io/docs/rke/latest/en/etcd-snapshots/one-time-snapshots/"},{"anchor":"#","title":"Overview","content":"Rancher is a container management platform built for organizations that deploy containers in production. Rancher makes it easy to run Kubernetes everywhere, meet IT requirements, and empower DevOps teams.\n\nRun Kubernetes Everywhere\n\nKubernetes has become the container orchestration standard. Most cloud and virtualization vendors now offer it as standard infrastructure. Rancher users have the choice of creating Kubernetes clusters with Rancher Kubernetes Engine (RKE) or cloud Kubernetes services, such as GKE, AKS, and EKS. Rancher users can also import and manage their existing Kubernetes clusters created using any Kubernetes distribution or installer.\n\nMeet IT requirements\n\nRancher supports centralized authentication, access control, and monitoring for all Kubernetes clusters under its control. For example, you can:\n\n\nUse your Active Directory credentials to access Kubernetes clusters hosted by cloud vendors, such as GKE.\nSetup and enforce access control and security policies across all users, groups, projects, clusters, and clouds.\nView the health and capacity of your Kubernetes clusters from a single-pane-of-glass.\n\n\nEmpower DevOps Teams\n\nRancher provides an intuitive user interface for DevOps engineers to manage their application workload. The user does not need to have in-depth knowledge of Kubernetes concepts to start using Rancher. Rancher catalog contains a set of useful DevOps tools. Rancher is certified with a wide selection of cloud native ecosystem products, including, for example, security tools, monitoring systems, container registries, and storage and networking drivers.\n\nThe following figure illustrates the role Rancher plays in IT and DevOps organizations. Each team deploys their applications on the public or private clouds they choose. IT administrators gain visibility and enforce policies across all users, clusters, and clouds.\n\n\n\nFeatures of the Rancher API Server\n\nThe Rancher API server is built on top of an embedded Kubernetes API server and an etcd database. It implements the following functionalities:\n\nAuthorization and Role-Based Access Control\n\n\nUser management: The Rancher API server manages user identities that correspond to external authentication providers like Active Directory or GitHub, in addition to local users.\nAuthorization: The Rancher API server manages access control and security policies.\n\n\nWorking with Kubernetes\n\n\nProvisioning Kubernetes clusters: The Rancher API server can provision Kubernetes on existing nodes, or perform Kubernetes upgrades.\nCatalog management: Rancher provides the ability to use a catalog of Helm charts that make it easy to repeatedly deploy applications.\nManaging projects: A project is a group of multiple namespaces and access control policies within a cluster. A project is a Rancher concept, not a Kubernetes concept, which allows you manage multiple namespaces as a group and perform Kubernetes operations in them. The Rancher UI provides features for project administration and for managing applications within projects.\nPipelines: Setting up a pipeline can help developers deliver new software as quickly and efficiently as possible. Within Rancher, you can configure pipelines for each of your Rancher projects.\nIstio: Our integration with Istio is designed so that a Rancher operator, such as an administrator or cluster owner, can deliver Istio to developers. Then developers can use Istio to enforce security policies, troubleshoot problems, or manage traffic for green/blue deployments, canary deployments, or A/B testing.\n\n\nWorking with Cloud Infrastructure\n\n\nTracking nodes: The Rancher API server tracks identities of all the nodes in all clusters.\nSetting up infrastructure:  When configured to use a cloud provider, Rancher can dynamically provision new nodes and persistent storage in the cloud.\n\n\nCluster Visibility\n\n\nLogging: Rancher can integrate with a variety of popular logging services and tools that exist outside of your Kubernetes clusters. Logging can be set up at the cluster level or at the project level.\nMonitoring: Using Rancher, you can monitor the state and processes of your cluster nodes, Kubernetes components, and software deployments through integration with Prometheus, a leading open-source monitoring solution. Monitoring can be configured at the cluster level or at the project level.\nAlerting: To keep your clusters and applications healthy and driving your organizational productivity forward, you need to stay informed of events occurring in your clusters and projects, both planned and unplanned. To help you stay informed of these events, you can configure alerts at the cluster level or at the project level.\n\n\nEditing Downstream Clusters with Rancher\n\nThe options and settings available for an existing cluster change based on the method that you used to provision it. For example, only clusters provisioned by RKE have Cluster Options available for editing.\n\nAfter a cluster is created with Rancher, a cluster administrator can manage cluster membership, enable pod security policies, and manage node pools, among other options.\n\nThe following table summarizes the options and settings available for each cluster type:\n\n\n\n\nRancher Capability\nRKE Launched\nHosted Kubernetes Cluster\nImported Cluster\n\n\n\n\n\nManage member roles\n✓\n✓\n✓\n\n\n\nEdit cluster options\n✓\n\n\n\n\n\nManage node pools\n✓\n\n\n\n\n\n","postref":"26d1e79e0ef0cef0242616a485f385d0","objectID":"2859e5d83eb6f2fa4ebe182428fc5d93","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/overview/"},{"anchor":"#","title":"Overview of RKE","content":"Rancher Kubernetes Engine (RKE) is a CNCF-certified Kubernetes distribution that runs entirely within Docker containers. It works on bare-metal and virtualized servers. RKE solves the problem of installation complexity, a common issue in the Kubernetes community. With RKE, the installation and operation of Kubernetes is both simplified and easily automated, and it’s entirely independent of the operating system and platform you’re running. As long as you can run a supported version of Docker, you can deploy and run Kubernetes with RKE.\n","postref":"876c0cc22dc880fa29e9cf07474b5257","objectID":"4be0f7e6faea8d1c4799abad6a7c4d0a","permalink":"http://jijeesh.github.io/docs/rke/latest/en/"},{"anchor":"#","title":"Overview of RancherOS","content":"RancherOS is the smallest, easiest way to run Docker in production.  Every process in RancherOS is a container managed by Docker. This includes system services such as udev and syslog.  Because it only includes the services necessary to run Docker, RancherOS is significantly smaller than most traditional operating systems. By removing unnecessary libraries and services, requirements for security patches and other maintenance are also reduced. This is possible because, with Docker, users typically package all necessary libraries into their containers.\n\nAnother way in which RancherOS is designed specifically for running Docker is that it always runs the latest version of Docker. This allows users to take advantage of the latest Docker capabilities and bug fixes.\n\nLike other minimalist Linux distributions, RancherOS boots incredibly quickly. Starting Docker containers is nearly instant, similar to starting any other process. This speed is ideal for organizations adopting microservices and autoscaling.\n\nDocker is an open-source platform designed for developers, system admins, and DevOps. It is used to build, ship, and run containers, using a simple and powerful command line interface (CLI). To get started with Docker, please visit the Docker user guide.\n\nHardware Requirements\n\n\nMemory Requirements\n\n\n\n\n\nPlatform\nRAM requirement(>=v1.5.x)\nRAM requirement(v1.4.x)\n\n\n\n\n\nBaremetal\n1GB\n1280MB\n\n\n\nVirtualBox\n1GB\n1280MB\n\n\n\nVMWare\n1GB\n1280MB (rancheros.iso)  2048MB (rancheros-vmware.iso)\n\n\n\nGCE\n1GB\n1280MB\n\n\n\nAWS\n1GB\n1.7GB\n\n\n\n\nYou can adjust memory requirements by custom building RancherOS, please refer to reduce-memory-requirements\n\nHow RancherOS Works\n\nEverything in RancherOS is a Docker container. We accomplish this by launching two instances of Docker. One is what we call System Docker and is the first process on the system. All other system services, like ntpd, syslog, and console, are running in Docker containers. System Docker replaces traditional init systems like systemd and is used to launch additional system services.\n\nSystem Docker runs a special container called Docker, which is another Docker daemon responsible for managing all of the user’s containers. Any containers that you launch as a user from the console will run inside this Docker. This creates isolation from the System Docker containers and ensures that normal user commands don’t impact system services.\n\nWe created this separation not only for the security benefits, but also to make sure that commands like docker rm -f $(docker ps -qa) don’t delete the entire OS.\n\n\n\n\n    \n    \n    \n    \n    \n    \n\n\n\nRunning RancherOS\n\nTo get started with RancherOS, head over to our Quick Start Guide.\n\nLatest Release\n\nPlease check our repository for the latest release in our README.\n\n\n\n","postref":"9e337e126a809e439ec5a9ee6dfe0f21","objectID":"8495b865fc284fbc09abddfb57ef94ea","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/"},{"anchor":"#","title":"Overview of RancherOS","content":"RancherOS is the smallest, easiest way to run Docker in production.  Every process in RancherOS is a container managed by Docker. This includes system services such as udev and syslog.  Because it only includes the services necessary to run Docker, RancherOS is significantly smaller than most traditional operating systems. By removing unnecessary libraries and services, requirements for security patches and other maintenance are also reduced. This is possible because, with Docker, users typically package all necessary libraries into their containers.\n\nAnother way in which RancherOS is designed specifically for running Docker is that it always runs the latest version of Docker. This allows users to take advantage of the latest Docker capabilities and bug fixes.\n\nLike other minimalist Linux distributions, RancherOS boots incredibly quickly. Starting Docker containers is nearly instant, similar to starting any other process. This speed is ideal for organizations adopting microservices and autoscaling.\n\nDocker is an open-source platform designed for developers, system admins, and DevOps. It is used to build, ship, and run containers, using a simple and powerful command line interface (CLI). To get started with Docker, please visit the Docker user guide.\n\nHardware Requirements\n\n\nMemory Requirements\n\n\n\n\n\nPlatform\nRAM requirement(>=v1.5.x)\nRAM requirement(v1.4.x)\n\n\n\n\n\nBaremetal\n1GB\n1280MB\n\n\n\nVirtualBox\n1GB\n1280MB\n\n\n\nVMWare\n1GB\n1280MB (rancheros.iso)  2048MB (rancheros-vmware.iso)\n\n\n\nGCE\n1GB\n1280MB\n\n\n\nAWS\n1GB\n1.7GB\n\n\n\n\nYou can adjust memory requirements by custom building RancherOS, please refer to reduce-memory-requirements\n\nHow RancherOS Works\n\nEverything in RancherOS is a Docker container. We accomplish this by launching two instances of Docker. One is what we call System Docker and is the first process on the system. All other system services, like ntpd, syslog, and console, are running in Docker containers. System Docker replaces traditional init systems like systemd and is used to launch additional system services.\n\nSystem Docker runs a special container called Docker, which is another Docker daemon responsible for managing all of the user’s containers. Any containers that you launch as a user from the console will run inside this Docker. This creates isolation from the System Docker containers and ensures that normal user commands don’t impact system services.\n\nWe created this separation not only for the security benefits, but also to make sure that commands like docker rm -f $(docker ps -qa) don’t delete the entire OS.\n\n\n\n\n    \n    \n    \n    \n    \n    \n\n\n\nRunning RancherOS\n\nTo get started with RancherOS, head over to our Quick Start Guide.\n\nLatest Release\n\nPlease check our repository for the latest release in our README.\n\n\n\n","postref":"9976686b53e6abe1bea61218b104bfb9","objectID":"2363078d30773dec706718f55fb96d31","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/overview/"},{"anchor":"#node-exporter","title":"Node Exporter","content":"\nPrerequisite: Configure one or more storage class to use as persistent storage for your Prometheus or Grafana pod.\nBy default, when you enable Prometheus for either a cluster or project, all monitoring data that Prometheus collects is stored on its own pod. With local storage, if the Prometheus or Grafana pods fail, all the data is lost. Rancher recommends configuring an external persistent storage to the cluster. With the external persistent storage, if the Prometheus or Grafana pods fail, the new pods can recover using data from the persistent storage.When enabling persistent storage for Prometheus or Grafana, specify the size of the persistent volume and select the storage class.The node exporter is a popular open source exporter, which exposes the metrics for hardware and *NIX kernels OS. It is designed to monitor the host system. However, there are still issues with namespaces when running it in a container, mostly around filesystem mount spaces. In order to monitor actual network metrics for the container network, the node exporter must be deployed with the hostNetwork mode.When configuring Prometheus and enabling the node exporter, enter a host port in the Node Exporter Host Port that will not produce port conflicts with existing applications. The host port chosen must be open to allow internal traffic between Prometheus and the Node Exporter.\nWarning: In order for Prometheus to collect the metrics of the node exporter, after enabling cluster monitoring, you must open the Node Exporter Host Port in the host firewall rules to allow intranet access. By default, 9796 is used as that host port.\n","postref":"1fa1af288ee6983ee98258f7afb83ccb","objectID":"112aa1a5bf3bd9fcac3a702969811454","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/tools/monitoring/prometheus/"},{"anchor":"#","title":"Provisioning Kubernetes Clusters in vSphere","content":"This section explains how to configure Rancher with vSphere credentials, provision nodes in vSphere, and set up Kubernetes clusters on those nodes.\n\nPrerequisites\n\nThis section describes the requirements for setting up vSphere so that Rancher can provision VMs and clusters.\n\nThe node templates are documented and tested with the vSphere Web Services API version 6.5.\n\n\nCreate credentials in vSphere\nNetwork permissions\nValid ESXi License for vSphere API Access\n\n\nCreate Credentials in vSphere\n\nBefore proceeding to create a cluster, you must ensure that you have a vSphere user with sufficient permissions. When you set up a node template, the template will need to use these vSphere credentials.\n\nRefer to this how-to guide for instructions on how to create a user in vSphere with the required permissions. These steps result in a username and password that you will need to provide to Rancher, which allows Rancher to provision resources in vSphere.\n\nNetwork Permissions\n\nIt must be ensured that the hosts running the Rancher server are able to establish the following network connections:\n\n\nTo the vSphere API on the vCenter server (usually port 443/TCP).\nTo the Host API (port 443/TCP) on all ESXi hosts used to instantiate virtual machines for the clusters (only required with Rancher prior to v2.3.3 or when using the ISO creation method in later versions).\nTo port 22/TCP and 2376/TCP on the created VMs\n\n\nSee Node Networking Requirements for a detailed list of port requirements applicable for creating nodes on an infrastructure provider.\n\nValid ESXi License for vSphere API Access\n\nThe free ESXi license does not support API access. The vSphere servers must have a valid or evaluation ESXi license.\n\nCreating Clusters in vSphere with Rancher\n\nThis section describes how to set up vSphere credentials, node templates, and vSphere clusters using the Rancher UI.\n\nYou will need to do the following:\n\n\nCreate a node template using vSphere credentials\nCreate a Kubernetes cluster using the node template\nOptional: Provision storage\n\n\nEnable the vSphere cloud provider for the cluster\n\n\n\nConfiguration References\n\nFor details on configuring the node template, refer to the node template configuration reference.\n\nRancher uses the RKE library to provision Kubernetes clusters. For details on configuring clusters in vSphere, refer to the cluster configuration reference in the RKE documentation.\n\nNote that the vSphere cloud provider must be enabled to allow dynamic provisioning of volumes.\n\n1. Create a Node Template Using vSphere Credentials\n\nTo create a cluster, you need to create at least one vSphere node template that specifies how VMs are created in vSphere.\n\nAfter you create a node template, it is saved, and you can re-use it whenever you create additional vSphere clusters.\n\nTo create a node template,\n\n\nLog in with an administrator account to the Rancher UI.\n\nFrom the user settings menu, select Node Templates.\n\nClick Add Template and then click on the vSphere icon.\n\n\nThen, configure your template:\n\n\nA. Configure the vSphere credential\nB. Configure node scheduling\nC. Configure instances and operating systems\nD. Add networks\nE. If not already enabled, enable disk UUIDs\nF. Optional: Configure node tags and custom attributes\nG. Optional: Configure cloud-init\nH. Saving the node template\n\n\nA. Configure the vSphere Credential\n\nThe steps for configuring your vSphere credentials for the cluster are different depending on your version of Rancher.\n\n\n  \n  \n  Your account access information is in a cloud credential. Cloud credentials are stored as Kubernetes secrets.\n\nYou can use an existing cloud credential or create a new one. To create a new cloud credential,\n\n\nClick Add New.\nIn the Name field, enter a name for your vSphere credentials.\nIn the vCenter or ESXi Server field, enter the vCenter or ESXi hostname/IP. ESXi is the virtualization platform where you create and run virtual machines and virtual appliances. vCenter Server is the service through which you manage multiple hosts connected in a network and pool host resources.\nOptional: In the Port field, configure the port of the vCenter or ESXi server.\nIn the Username and Password fields, enter your vSphere login username and password.\nClick Create.\n\n\nResult: The node template has the credentials required to provision nodes in vSphere.\n\n\n\n\n  In the Account Access section, enter the vCenter FQDN or IP address and the credentials for the vSphere user account.\n\n\n\n\n\n\nB. Configure Node Scheduling\n\nChoose what hypervisor the virtual machine will be scheduled to. The configuration options depend on your version of Rancher.\n\n\n  \n  \n  The fields in the Scheduling section should auto-populate with the data center and other scheduling options that are available to you in vSphere.\n\n\nIn the Data Center field, choose the data center where the VM will be scheduled.\nOptional: Select a Resource Pool. Resource pools can be used to partition available CPU and memory resources of a standalone host or cluster, and they can also be nested.\nIf you have a data store cluster, you can toggle the Data Store field. This lets you select a data store cluster where your VM will be scheduled to. If the field is not toggled, you can select an individual disk.\nOptional: Select a folder where the VM will be placed. The VM folders in this dropdown menu directly correspond to your VM folders in vSphere. Note: The folder name should be prefaced with vm/ in your vSphere config file.\nOptional: Choose a specific host to create the VM on. Leave this field blank for a standalone ESXi or for a cluster with DRS (Distributed Resource Scheduler). If specified, the host system’s pool will be used and the Resource Pool parameter will be ignored.\n\n\n\n\n\n  In the Scheduling section, enter:\n\n\nThe name/path of the Data Center to create the VMs in\nThe name of the VM Network to attach to\n\nThe name/path of the Datastore to store the disks in\n\n\n\n\n\n\n\n\n\nC. Configure Instances and Operating Systems\n\nDepending on the Rancher version there are different options available to configure instances.\n\n\n  \n  \n  In the Instance Options section, configure the number of vCPUs, memory, and disk size for the VMs created by this template.\n\nIn the Creation method field, configure the method used to provision VMs in vSphere. Available options include creating VMs that boot from a RancherOS ISO or creating VMs by cloning from an existing virtual machine or VM template.\n\nThe existing VM or template may use any modern Linux operating system that is configured with support for cloud-init using the NoCloud datasource.\n\nChoose the way that the VM will be created:\n\n\nDeploy from template: Data Center: Choose a VM template that exists in the data center that you selected.\nDeploy from template: Content Library: First, select the Content Library that contains your template, then select the template from the populated list Library templates.\nClone an existing virtual machine: In the Virtual machine field, choose an existing VM that the new VM will be cloned from.\nInstall from boot2docker ISO: Ensure that the OS ISO URL field contains the URL of a VMware ISO release for RancherOS (rancheros-vmware.iso). Note that this URL must be accessible from the nodes running your Rancher server installation.\n\n\n\n\n\n  In the Instance Options section, configure the number of vCPUs, memory, and disk size for the VMs created by this template.\n\nOnly VMs booting from RancherOS ISO are supported.\n\nEnsure that the OS ISO URL contains the URL of the VMware ISO release for RancherOS: rancheros-vmware.iso.\n\n![image](http://jijeesh.github.io/docs/img/rancher/vsphere-node-template-1.png)\n\n\n\n\n\n\n\nD. Add Networks\n\nAvailable as of v2.3.3\n\nThe node template now allows a VM to be provisioned with multiple networks. In the Networks field, you can now click Add Network to add any networks available to you in vSphere.\n\nE. If Not Already Enabled, Enable Disk UUIDs\n\nIn order to provision nodes with RKE, all nodes must be configured with disk UUIDs.\n\nAs of Rancher v2.0.4, disk UUIDs are enabled in vSphere node templates by default.\n\nIf you are using Rancher prior to v2.0.4, refer to these instructions for details on how to enable a UUID with a Rancher node template.\n\nF. Optional: Configure Node Tags and Custom Attributes\n\nThe way to attach metadata to the VM is different depending on your Rancher version.\n\n\n  \n  \n  Optional: Add vSphere tags and custom attributes. Tags allow you to attach metadata to objects in the vSphere inventory to make it easier to sort and search for these objects.\n\nFor tags, all your vSphere tags will show up as options to select from in your node template.\n\nIn the custom attributes, Rancher will let you select all the custom attributes you have already set up in vSphere. The custom attributes are keys and you can enter values for each one.\n\n\nNote: Custom attributes are a legacy feature that will eventually be removed from vSphere. These attributes allow you to attach metadata to objects in the vSphere inventory to make it e","postref":"1c6dea262f361f5402bb7250d6fd3b92","objectID":"f50fcd3dbfba3bf5548d092d9d6c56cf","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/node-pools/vsphere/provisioning-vsphere-clusters/"},{"anchor":"#","title":"Questions about Upgrading to Rancher v2.x","content":"This page contains frequently asked questions about the changes between Rancher v1.x and v2.x, and how to upgrade from Rancher v1.x to v2.x.\n\nKubernetes\n\nWhat does it mean when you say Rancher v2.x is built on Kubernetes?\n\nRancher v2.x is a complete container management platform built 100% on Kubernetes leveraging its Custom Resource and Controller framework.  All features are written as a CustomResourceDefinition (CRD) which extends the existing Kubernetes API and can leverage native features such as RBAC.\n\n\n\nDo you plan to implement upstream Kubernetes, or continue to work on your own fork?\n\nWe’re still going to provide our distribution when you select the default option of having us create your Kubernetes cluster, but it will be very close to upstream.\n\n\n\nDoes this release mean that we need to re-train our support staff in Kubernetes?\n\nYes.  Rancher will offer the native Kubernetes functionality via kubectl but will also offer our own UI dashboard to allow you to deploy Kubernetes workload without having to understand the full complexity of Kubernetes.  However, to fully leverage Kubernetes, we do recommend understanding Kubernetes.  We do plan on improving our UX with subsequent releases to make Kubernetes easier to use.\n\n\n\nIs a Rancher compose going to make a Kubernetes pod? Do we have to learn both now? We usually use the filesystem layer of files, not the UI.\n\nNo.  Unfortunately, the differences were enough such that we cannot support Rancher compose anymore in 2.x.  We will be providing both a tool and guides to help with this migration.\n\n\n\nIf we use Kubernetes native YAML files for creating resources, should we expect that to work as expected, or do we need to use Rancher/Docker compose files to deploy infrastructure?\n\nAbsolutely.\n\nCattle\n\nHow does Rancher v2.x affect Cattle?\n\nCattle will not supported in v2.x as Rancher has been re-architected to be based on Kubernetes. You can, however, expect majority of Cattle features you use will exist and function similarly on Kubernetes. We will develop migration tools in Rancher v2.1 to help you transform your existing Rancher Compose files into Kubernetes YAML files.\n\n\n\nCan I migrate existing Cattle workloads into Kubernetes?\n\nYes. In the upcoming Rancher v2.1 release we will provide a tool to help translate existing Cattle workloads in Compose format to Kubernetes YAML format.  You will then be able to deploy those workloads on the v2.x platform.\n\nFeature Changes\n\nCan we still add our own infrastructure services, which had a separate view/filter in 1.6.x?\n\nYes. You can manage Kubernetes storage, networking, and its vast ecosystem of add-ons.\n\n\n\nAre there changes to default roles available now or going forward? Will the Kubernetes alignment impact plans for roles/RBAC?\n\nThe default roles will be expanded to accommodate the new Rancher 2.x features, and will also take advantage of the Kubernetes RBAC (Role-Based Access Control) capabilities to give you more flexibility.\n\n\n\nWill there be any functions like network policies to separate a front-end container from a back-end container through some kind of firewall in v2.x?\n\nYes. You can do so by leveraging Kubernetes’ network policies.\n\n\n\nWhat about the CLI? Will that work the same way with the same features?\n\nYes. Definitely.\n\nEnvironments & Clusters\n\nCan I still create templates for environments and clusters?\n\nStarting with 2.0, the concept of an environment has now been changed to a Kubernetes cluster as going forward, only the Kubernetes orchestration engine is supported.\n\nKubernetes RKE Templates is on our roadmap for 2.x. Please refer to our Release Notes and documentation for all the features that we currently support.\n\n\n\nCan you still add an existing host to an environment? (i.e. not provisioned directly from Rancher)\n\nYes. We still provide you with the same way of executing our Rancher agents directly on hosts.\n\nUpgrading/Migrating\n\nHow would the migration from v1.x to v2.x work?\n\nDue to the technical difficulty in transforming a Docker container into a pod running Kubernetes, upgrading will require users to “replay” those workloads from v1.x into new v2.x environments. We plan to ship with a tool in v2.1 to translate existing Rancher Compose files into Kubernetes YAML files.  You will then be able to deploy those workloads on the v2.x platform.\n\n\n\nIs it possible to upgrade from Rancher v1.x to v2.x without any disruption to Cattle and Kubernetes clusters?\n\nAt this time, we are still exploring this scenario and taking feedback. We anticipate that you will need to launch a new Rancher instance and then relaunch on v2.x. Once you’ve moved to v2.x, upgrades will be in place, as they are in v1.6.\n\nSupport\n\nAre you planning some long-term support releases for Rancher v1.6?\n\nThat is definitely the focus of the v1.6 stream. We’re continuing to improve that release, fix bugs, and maintain it. New releases of the v1.6 stream are announced in the Rancher forums. The Rancher wiki contains the v1.6 release notes.\n","postref":"80c80205d9c05f8709e2d9b0a55cd66b","objectID":"a344b0195944100a4f2b3cff0384f28a","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/faq/upgrades-to-2x/"},{"anchor":"#using-rancheros","title":"Using RancherOS","content":"Deploying a Docker ContainerLet’s try to deploy a normal Docker container on the Docker daemon.  The RancherOS Docker daemon is identical to any other Docker environment, so all normal Docker commands work.$ docker run -d nginx\nYou can see that the nginx container is up and running:$ docker ps\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS               NAMES\ne99c2c4b8b30        nginx               \"nginx -g 'daemon off\"   12 seconds ago      Up 11 seconds       80/tcp, 443/tcp     drunk_ptolemy\nDeploying A System Service ContainerThe following is a simple Docker container to set up Linux-dash, which is a minimal low-overhead web dashboard for monitoring Linux servers. The Dockerfile will be like this:FROM hwestphal/nodebox\nMAINTAINER hussein.galal.ahmed.11@gmail.com\n\nRUN opkg-install unzip\nRUN curl -k -L -o master.zip https://github.com/afaqurk/linux-dash/archive/master.zip\nRUN unzip master.zip\nWORKDIR linux-dash-master\nRUN npm install\n\nENTRYPOINT [\"node\",\"server\"]\nUsing the hwestphal/nodebox image, which uses a Busybox image and installs node.js and npm. We downloaded the source code of Linux-dash, and then ran the server. Linux-dash will run on port 80 by default.To run this container in System Docker use the following command:$ sudo system-docker run -d --net=host --name busydash husseingalal/busydash\nIn the command, we used --net=host to tell System Docker not to containerize the container’s networking, and use the host’s networking instead. After running the container, you can see the monitoring server by accessing http://<IP_OF_MACHINE>.To make the container survive during the reboots, you can create the /opt/rancher/bin/start.sh script, and add the Docker start line to launch the Docker at each startup.$ sudo mkdir -p /opt/rancher/bin\n$ echo \"sudo system-docker start busydash\" | sudo tee -a /opt/rancher/bin/start.sh\n$ sudo chmod 755 /opt/rancher/bin/start.sh\nUsing ROSAnother useful command that can be used with RancherOS is ros which can be used to control and configure the system.$ sudo ros -v\nros version 0.0.1\nRancherOS state is controlled by a cloud config file. ros is used to edit the configuration of the system, to see for example the dns configuration of the system:$ sudo ros config get rancher.network.dns.nameservers\n- 8.8.8.8\n- 8.8.4.4\nWhen using the native Busybox console, any changes to the console will be lost after reboots, only changes to /home or /opt will be persistent. You can use the ros console switch command to switch to a persistent console and replace the native Busybox console. For example, to switch to the Ubuntu console:$ sudo ros console switch ubuntu\nConclusionRancherOS is a simple Linux distribution ideal for running Docker.  By embracing containerization of system services and leveraging Docker for management, RancherOS hopes to provide a very reliable, and easy to manage OS for running containers.","postref":"402203ac56263b8fcfe8c86012864335","objectID":"5824c764fc0ce86a25cc3a4f5ff87e11","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/quick-start-guide/"},{"anchor":"#","title":"Rancher 2.x","content":"What’s New?\n\nRancher was originally built to work with multiple orchestrators, and it included its own orchestrator called Cattle. With the rise of Kubernetes in the marketplace, Rancher now exclusively deploys and manages multiple Kubernetes clusters running anywhere, on any provider. It can provision Kubernetes from a hosted provider, provision compute nodes and then install Kubernetes onto them, or inherit existing Kubernetes clusters running anywhere.\n\nOne Rancher server installation can manage hundreds of Kubernetes clusters from the same interface.\n\nRancher adds significant value on top of Kubernetes, first by centralizing role-based access control (RBAC) for all of the clusters and giving global admins the ability to control cluster access from one location. It then enables detailed monitoring and alerting for clusters and their resources, ships logs to external providers, and integrates directly with Helm via the Application Catalog. If you have an external CI/CD system, you can plug it into Rancher, but if you don’t, Rancher even includes a pipeline engine to help you automatically deploy and upgrade workloads.\n\nRancher is a complete container management platform for Kubernetes, giving you the tools to successfully run Kubernetes anywhere.\n","postref":"f83a79e40f336c447241dd6aeac2d8cb","objectID":"98a814c86295a23d9a3cccb0e62dfa3e","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/"},{"anchor":"#","title":"Recommended Cluster Architecture","content":"There are three roles that can be assigned to nodes: etcd, controlplane and worker.\n\nSeparating Worker Nodes from Nodes with Other Roles\n\nWhen designing your cluster(s), you have two options:\n\n\nUse dedicated nodes for each role. This ensures resource availability for the components needed for the specified role. It also strictly isolates network traffic between each of the roles according to the port requirements.\nAssign the etcd and controlplane roles to the same nodes. These nodes must meet the hardware requirements for both roles.\n\n\nIn either case, the worker role should not be used or added to nodes with the etcd or controlplane role.\n\nTherefore, each node should have one of the following role configurations:\n\n\netcd\ncontrolplane\nBoth etcd and controlplane\nworker\n\n\nRecommended Number of Nodes with Each Role\n\nThe cluster should have:\n\n\nAt least three nodes with the role etcd to survive losing one node. Increase this count for higher node fault toleration, and spread them across (availability) zones to provide even better fault tolerance.\nAt least two nodes with the role controlplane for master component high availability.\nAt least two nodes with the role worker for workload rescheduling upon node failure.\n\n\nFor more information on what each role is used for, refer to the section on roles for nodes in Kubernetes.\n\nNumber of Controlplane Nodes\n\nAdding more than one node with the controlplane role makes every master component highly available.\n\nNumber of etcd Nodes\n\nThe number of nodes that you can lose at once while maintaining cluster availability is determined by the number of nodes assigned the etcd role. For a cluster with n members, the minimum is (n/2)+1. Therefore, we recommend creating an  etcd node in 3 different availability zones within a region to survive the loss of one availability zone. If you use only two zones, you can only survive the loss of the zone where you don’t lose the majority of nodes.\n\n\n\n\nNodes with etcd role\nMajority\nFailure Tolerance\n\n\n\n\n\n1\n1\n0\n\n\n\n2\n2\n0\n\n\n\n3\n2\n1\n\n\n\n4\n3\n1\n\n\n\n5\n3\n2\n\n\n\n6\n4\n2\n\n\n\n7\n4\n3\n\n\n\n8\n5\n3\n\n\n\n9\n5\n4\n\n\n\n\nReferences:\n\n\nOfficial etcd documentation on optimal etcd cluster size\nOfficial Kubernetes documentation on operating etcd clusters for Kubernetes\n\n\nNumber of Worker Nodes\n\nAdding more than one node with the worker role will make sure your workloads can be rescheduled if a node fails.\n\nWhy Production Requirements are Different for the Rancher Cluster and the Clusters Running Your Applications\n\nYou may have noticed that our Kubernetes Install instructions do not meet our definition of a production-ready cluster, as there are no dedicated nodes for the worker role. However, for your Rancher installation, this three node cluster is valid, because:\n\n\nIt allows one etcd node failure.\nIt maintains multiple instances of the master components by having multiple controlplane nodes.\nNo other workloads than Rancher itself should be created on this cluster.\n\n\nReferences\n\n\nKubernetes: Master Components\n\n","postref":"af153e1fd033be8a590359ccf8283a01","objectID":"123a3ebcfe2ee0bb55dde38e755f692f","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/production/recommended-architecture/"},{"anchor":"#","title":"Roles for Nodes in Kubernetes","content":"This section describes the roles for etcd nodes, controlplane nodes, and worker nodes in Kubernetes, and how the roles work together in a cluster.\n\nThis diagram is applicable to Kubernetes clusters launched with Rancher using RKE..\n\n\nLines show the traffic flow between components. Colors are used purely for visual aid\n\netcd\n\nNodes with the etcd role run etcd, which is a consistent and highly available key value store used as Kubernetes’ backing store for all cluster data. etcd replicates the data to each node.\n\n\nNote: Nodes with the etcd role are shown as Unschedulable in the UI, meaning no pods will be scheduled to these nodes by default.\n\n\ncontrolplane\n\nNodes with the controlplane role run the Kubernetes master components (excluding etcd, as it’s a separate role). See Kubernetes: Master Components for a detailed list of components.\n\n\nNote: Nodes with the controlplane role are shown as Unschedulable in the UI, meaning no pods will be scheduled to these nodes by default.\n\n\nkube-apiserver\n\nThe Kubernetes API server (kube-apiserver) scales horizontally. Each node with the role controlplane will be added to the NGINX proxy on the nodes with components that need to access the Kubernetes API server. This means that if a node becomes unreachable, the local NGINX proxy on the node will forward the request to another Kubernetes API server in the list.\n\nkube-controller-manager\n\nThe Kubernetes controller manager uses leader election using an endpoint in Kubernetes. One instance of the kube-controller-manager will create an entry in the Kubernetes endpoints and updates that entry in a configured interval. Other instances will see an active leader and wait for that entry to expire (for example, when a node is unresponsive).\n\nkube-scheduler\n\nThe Kubernetes scheduler uses leader election using an endpoint in Kubernetes. One instance of the kube-scheduler will create an entry in the Kubernetes endpoints and updates that entry in a configured interval. Other instances will see an active leader and wait for that entry to expire (for example, when a node is unresponsive).\n\nworker\n\nNodes with the worker role run the Kubernetes node components. See Kubernetes: Node Components for a detailed list of components.\n\nReferences\n\n\nKubernetes: Node Components\n\n","postref":"3ac825645dc3c34b5674605f89fbdfee","objectID":"71fa259d309fc66ae0a64aecfe8bc4b9","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/production/nodes-and-roles/"},{"anchor":"#","title":"Security Scans","content":"Available as of v2.4.0-alpha1\n\nRancher can run a security scan to check whether Kubernetes is deployed according to security best practices as defined in the CIS Kubernetes Benchmark.\n\nThe Center for Internet Security (CIS) is a 501©(3) nonprofit organization, formed in October 2000, with a mission is to “identify, develop, validate, promote, and sustain best practice solutions for cyber defense and build and lead communities to enable an environment of trust in cyberspace”. The organization is headquartered in East Greenbush, New York, with members including large corporations, government agencies, and academic institutions.\n\nCIS Benchmarks are best practices for the secure configuration of a target system. CIS Benchmarks are developed through the generous volunteer efforts of subject matter experts, technology vendors, public and private community members, and the CIS Benchmark Development team.\n\nThe Benchmark provides recommendations of two types: Scored and Not Scored. We run tests related to only Scored recommendations.\n\nWhen Rancher runs a CIS Security Scan on a cluster, it generates a report showing the results of each test, including a summary with the number of passed, skipped and failed tests. The report also includes remediation steps for any failed tests.\n\nTo check clusters for CIS Kubernetes Benchmark compliance, the security scan leverages kube-bench, an open-source tool from Aqua Security.\n\nAbout the Generated Report\n\nEach scan generates a report can be viewed in the Rancher UI and can be downloaded in CSV format.\n\nTo determine which version of the Benchmark to use in the scan, Rancher chooses a version that is appropriate for the cluster’s Kubernetes version. The Benchmark version is included in the generated report.\n\nEach test in the report is identified by its corresponding Scored test in the Benchmark. For example, if a cluster fails test 1.3.6, you can look up the description and rationale for the section 1.3.6 in the Benchmark itself, or in Rancher’s hardening guide for the Kubernetes version that the cluster is using. Recommendations marked as Not Scored in the Benchmark are not included in the report.\n\nSimilarly, for information on how to manually audit the test result, you could look up section 1.3.6 in Rancher’s self-assessment guide for the corresponding Kubernetes version.\n\nPrerequisites\n\nTo run security scans on a cluster and access the generated reports, you must be an Administrator or Cluster Owner.\n\nRancher can only run security scans on clusters that were created with RKE, which includes custom clusters and clusters that Rancher created in an infrastructure provider such as Amazon EC2 or GCE. Imported clusters and clusters in hosted Kubernetes providers can’t be scanned by Rancher.\n\nThe security scan cannot run in a cluster that has Windows nodes.\n\nRunning a Scan\n\n\nFrom the cluster view in Rancher, click Tools > CIS Scans.\nClick Run Scan.\n\n\nResult: A report is generated and displayed in the CIS Scans page. To see details of the report, click the report’s name.\n\nSkipping a Test\n\n\nFrom the cluster view in Rancher, click Tools > CIS Scans.\nClick the name of the report that has tests you want to skip.\nA Skip button is displayed next to each failed test. Click Skip for each test that should be skipped.\n\n\nResult: The tests will be skipped on the next scan.\n\nTo re-run the security scan, go to the top of the page and click Run Scan.\n\nUn-skipping a Test\n\n\nFrom the cluster view in Rancher, click Tools > CIS Scans.\nClick the name of the report that has tests you want to un-skip.\nAn Unskip button is displayed next to each skipped test. Click Unskip for each test that should not be skipped.\n\n\nResult: The tests will not be skipped on the next scan.\n\nTo re-run the security scan, go to the top of the page and click Run Scan.\n\nDeleting a Report\n\n\nFrom the cluster view in Rancher, click Tools > CIS Scans.\nGo to the report that should be deleted.\nClick the Ellipsis (…) > Delete.\nClick Delete.\n\n","postref":"b382875af487cd260d5a8c7a9e3ded7c","objectID":"81f38fe678bda917fc636593f7577865","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/security/security-scan/"},{"anchor":"#","title":"Setting up Existing Storage","content":"This section describes how to set up existing persistent storage for workloads in Rancher.\n\n\nThis section assumes that you understand the Kubernetes concepts of persistent volumes and persistent volume claims. For more information, refer to the section on how storage works.\n\n\nTo set up storage, follow these steps:\n\n\nSet up persistent storage in an infrastructure provider.\nAdd a persistent volume that refers to the persistent storage.\nAdd a persistent volume claim that refers to the persistent volume.\nMount the persistent volume claim as a volume in your workload.\n\n\nPrerequisites\n\n\nTo create a persistent volume as a Kubernetes resource, you must have the Manage Volumes role.\nIf you are provisioning storage for a cluster hosted in the cloud, the storage and cluster hosts must have the same cloud provider.\n\n\n1. Set up persistent storage in an infrastructure provider\n\nCreating a persistent volume in Rancher will not create a storage volume. It only creates a Kubernetes resource that maps to an existing volume. Therefore, before you can create a persistent volume as a Kubernetes resource, you must have storage provisioned.\n\nThe steps to set up a persistent storage device will differ based on your infrastructure. We provide examples of how to set up storage using vSphere, NFS, or Amazon’s EBS.\n\n2. Add a persistent volume that refers to the persistent storage\n\nThese steps describe how to set up a persistent volume at the cluster level in Kubernetes.\n\n\nFrom the cluster view, select Storage > Persistent Volumes.\n\nClick Add Volume.\n\nEnter a Name for the persistent volume.\n\nSelect the Volume Plugin for the disk type or service that you’re using. When adding storage to a cluster that’s hosted by a cloud provider, use the cloud provider’s plug-in for cloud storage. For example, if you have a Amazon EC2 cluster and you want to use cloud storage for it, you must use the Amazon EBS Disk volume plugin.\n\nEnter the Capacity of your volume in gigabytes.\n\nComplete the Plugin Configuration form. Each plugin type requires information specific to the vendor of disk type. For help regarding each plugin’s form and the information that’s required, refer to the plug-in’s vendor documentation.\n\nOptional: In the Customize form, configure the access modes. This options sets how many nodes can access the volume, along with the node read/write permissions. The Kubernetes Documentation includes a table that lists which access modes are supported by the plugins available.\n\nOptional: In the Customize form, configure the mount options. Each volume plugin allows you to specify additional command line options during the mounting process. Consult each plugin’s vendor documentation for the mount options available.\n\nClick Save.\n\n\nResult: Your new persistent volume is created.\n\n3. Add a persistent volume claim that refers to the persistent volume\n\nThese steps describe how to set up a PVC in the namespace where your stateful workload will be deployed.\n\n\nGo to the project containing a workload that you want to add a persistent volume claim to.\n\nThen click the Volumes tab and click Add Volume. (In versions prior to v2.3.0, click Workloads on the main navigation bar, then Volumes.)\n\nEnter a Name for the volume claim.\n\nSelect the Namespace of the workload that you want to add the persistent storage to.\n\nIn the section called Use an existing persistent volume, go to the Persistent Volume drop-down and choose the persistent volume that you created.\n\nOptional: From Customize, select the Access Modes that you want to use.\n\nClick Create.\n\n\nResult: Your PVC is created. You can now attach it to any workload in the project.\n\n4. Mount the persistent volume claim as a volume in your workload\n\nMount PVCs to stateful workloads so that your applications can store their data.\n\nYou can mount PVCs during the deployment of a workload, or following workload creation.\n\nThe following steps describe how to assign existing storage to a new workload that is a stateful set:\n\n\nFrom the Project view, go to the Workloads tab.\nClick Deploy.\nEnter a name for the workload.\nNext to the Workload Type field, click More Options.\nClick Stateful set of 1 pod. Optionally, configure the number of pods.\nChoose the namespace where the workload will be deployed.\nExpand the Volumes section and click Add Volume > Use an existing persistent volume (claim)..\nIn the Persistent Volume Claim field, select the PVC that you created.\nIn the Mount Point field, enter the path that the workload will use to access the volume.\nClick Launch.\n\n\nResult: When the workload is deployed, it will make a request for the specified amount of disk space to the Kubernetes master. If a PV with the specified resources is available when the workload is deployed, the Kubernetes master will bind the PV to the PVC.\n\nThe following steps describe how to assign persistent storage to an existing workload:\n\n\nFrom the Project view, go to the Workloads tab.\nGo to the workload that you want to add the persistent storage to. The workload type should be a stateful set. Click Ellipsis (…) > Edit.\nExpand the Volumes section and click Add Volume > Use an existing persistent volume (claim)..\nIn the Persistent Volume Claim field, select the PVC that you created.\nIn the Mount Point field, enter the path that the workload will use to access the volume.\nClick Save.\n\n\nResult: The workload will make a request for the specified amount of disk space to the Kubernetes master. If a PV with the specified resources is available when the workload is deployed, the Kubernetes master will bind the PV to the PVC.\n","postref":"20df283310871c0733d273f582b6c43c","objectID":"1962c8786659d76cf3246d93db7252f0","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/volumes-and-storage/attaching-existing-storage/"},{"anchor":"#","title":"Troubleshooting etcd Nodes","content":"This section contains commands and tips for troubleshooting nodes with the etcd role.\n\nThis page covers the following topics:\n\n\nChecking if the etcd Container is Running\netcd Container Logging\netcd Cluster and Connectivity Checks\n\n\nCheck etcd Members on all Nodes\nCheck Endpoint Status\nCheck Endpoint Health\nCheck Connectivity on Port TCP/2379\nCheck Connectivity on Port TCP/2380\n\netcd Alarms\netcd Space Errors\nLog Level\netcd Content\n\n\nWatch Streaming Events\nQuery etcd Directly\n\nReplacing Unhealthy etcd Nodes\n\n\nChecking if the etcd Container is Running\n\nThe container for etcd should have status Up. The duration shown after Up is the time the container has been running.\n\ndocker ps -a -f=name=etcd$\n\n\nExample output:\n\nCONTAINER ID        IMAGE                         COMMAND                  CREATED             STATUS              PORTS               NAMES\n605a124503b9        rancher/coreos-etcd:v3.2.18   \"/usr/local/bin/et...\"   2 hours ago         Up 2 hours                              etcd\n\n\netcd Container Logging\n\nThe logging of the container can contain information on what the problem could be.\n\ndocker logs etcd\n\n\n\n\n\nLog\nExplanation\n\n\n\n\n\nhealth check for peer xxx could not connect: dial tcp IP:2380: getsockopt: connection refused\nA connection to the address shown on port 2380 cannot be established. Check if the etcd container is running on the host with the address shown.\n\n\n\nxxx is starting a new election at term x\nThe etcd cluster has lost its quorum and is trying to establish a new leader. This can happen when the majority of the nodes running etcd go down/unreachable.\n\n\n\nconnection error: desc = \"transport: Error while dialing dial tcp 0.0.0.0:2379: i/o timeout\"; Reconnecting to {0.0.0.0:2379 0  <nil>}\nThe host firewall is preventing network communication.\n\n\n\nrafthttp: request cluster ID mismatch\nThe node with the etcd instance logging rafthttp: request cluster ID mismatch is trying to join a cluster that has already been formed with another peer. The node should be removed from the cluster, and re-added.\n\n\n\nrafthttp: failed to find member\nThe cluster state (/var/lib/etcd) contains wrong information to join the cluster. The node should be removed from the cluster, the state directory should be cleaned and the node should be re-added.\n\n\n\n\netcd Cluster and Connectivity Checks\n\nThe address where etcd is listening depends on the address configuration of the host etcd is running on. If an internal address is configured for the host etcd is running on, the endpoint for etcdctl needs to be specified explicitly. If any of the commands respond with Error:  context deadline exceeded, the etcd instance is unhealthy (either quorum is lost or the instance is not correctly joined in the cluster)\n\nCheck etcd Members on all Nodes\n\nOutput should contain all the nodes with the etcd role and the output should be identical on all nodes.\n\nCommand:\n\ndocker exec etcd etcdctl member list\n\n\nCommand when using etcd version lower than 3.3.x (Kubernetes 1.13.x and lower) and --internal-address was specified when adding the node:\n\ndocker exec etcd sh -c \"etcdctl --endpoints=\\$ETCDCTL_ENDPOINT member list\"\n\n\nExample output:\n\nxxx, started, etcd-xxx, https://IP:2380, https://IP:2379,https://IP:4001\nxxx, started, etcd-xxx, https://IP:2380, https://IP:2379,https://IP:4001\nxxx, started, etcd-xxx, https://IP:2380, https://IP:2379,https://IP:4001\n\n\nCheck Endpoint Status\n\nThe values for RAFT TERM should be equal and RAFT INDEX should be not be too far apart from each other.\n\nCommand:\n\ndocker exec etcd etcdctl endpoint status --endpoints=$(docker exec etcd /bin/sh -c \"etcdctl member list | cut -d, -f5 | sed -e 's/ //g' | paste -sd ','\") --write-out table\n\n\nCommand when using etcd version lower than 3.3.x (Kubernetes 1.13.x and lower) and --internal-address was specified when adding the node:\n\ndocker exec etcd etcdctl endpoint status --endpoints=$(docker exec etcd /bin/sh -c \"etcdctl --endpoints=\\$ETCDCTL_ENDPOINT member list | cut -d, -f5 | sed -e 's/ //g' | paste -sd ','\") --write-out table\n\n\nExample output:\n\n+-----------------+------------------+---------+---------+-----------+-----------+------------+\n| ENDPOINT        |        ID        | VERSION | DB SIZE | IS LEADER | RAFT TERM | RAFT INDEX |\n+-----------------+------------------+---------+---------+-----------+-----------+------------+\n| https://IP:2379 | 333ef673fc4add56 |  3.2.18 |   24 MB |     false |        72 |      66887 |\n| https://IP:2379 | 5feed52d940ce4cf |  3.2.18 |   24 MB |      true |        72 |      66887 |\n| https://IP:2379 | db6b3bdb559a848d |  3.2.18 |   25 MB |     false |        72 |      66887 |\n+-----------------+------------------+---------+---------+-----------+-----------+------------+\n\n\nCheck Endpoint Health\n\nCommand:\n\ndocker exec etcd etcdctl endpoint health --endpoints=$(docker exec etcd /bin/sh -c \"etcdctl member list | cut -d, -f5 | sed -e 's/ //g' | paste -sd ','\")\n\n\nCommand when using etcd version lower than 3.3.x (Kubernetes 1.13.x and lower) and --internal-address was specified when adding the node:\n\ndocker exec etcd etcdctl endpoint health --endpoints=$(docker exec etcd /bin/sh -c \"etcdctl --endpoints=\\$ETCDCTL_ENDPOINT member list | cut -d, -f5 | sed -e 's/ //g' | paste -sd ','\")\n\n\nExample output:\n\nhttps://IP:2379 is healthy: successfully committed proposal: took = 2.113189ms\nhttps://IP:2379 is healthy: successfully committed proposal: took = 2.649963ms\nhttps://IP:2379 is healthy: successfully committed proposal: took = 2.451201ms\n\n\nCheck Connectivity on Port TCP/2379\n\nCommand:\n\nfor endpoint in $(docker exec etcd /bin/sh -c \"etcdctl member list | cut -d, -f5\"); do\n   echo \"Validating connection to ${endpoint}/health\"\n   docker run --net=host -v $(docker inspect kubelet --format '{{ range .Mounts }}{{ if eq .Destination \"/etc/kubernetes\" }}{{ .Source }}{{ end }}{{ end }}')/ssl:/etc/kubernetes/ssl:ro appropriate/curl -s -w \"\\n\" --cacert $(docker exec etcd printenv ETCDCTL_CACERT) --cert $(docker exec etcd printenv ETCDCTL_CERT) --key $(docker exec etcd printenv ETCDCTL_KEY) \"${endpoint}/health\"\ndone\n\n\nCommand when using etcd version lower than 3.3.x (Kubernetes 1.13.x and lower) and --internal-address was specified when adding the node:\n\nfor endpoint in $(docker exec etcd /bin/sh -c \"etcdctl --endpoints=\\$ETCDCTL_ENDPOINT member list | cut -d, -f5\"); do\n  echo \"Validating connection to ${endpoint}/health\";\n  docker run --net=host -v $(docker inspect kubelet --format '{{ range .Mounts }}{{ if eq .Destination \"/etc/kubernetes\" }}{{ .Source }}{{ end }}{{ end }}')/ssl:/etc/kubernetes/ssl:ro appropriate/curl -s -w \"\\n\" --cacert $(docker exec etcd printenv ETCDCTL_CACERT) --cert $(docker exec etcd printenv ETCDCTL_CERT) --key $(docker exec etcd printenv ETCDCTL_KEY) \"${endpoint}/health\"\ndone\n\n\nExample output:\n\nValidating connection to https://IP:2379/health\n{\"health\": \"true\"}\nValidating connection to https://IP:2379/health\n{\"health\": \"true\"}\nValidating connection to https://IP:2379/health\n{\"health\": \"true\"}\n\n\nCheck Connectivity on Port TCP/2380\n\nCommand:\n\nfor endpoint in $(docker exec etcd /bin/sh -c \"etcdctl member list | cut -d, -f4\"); do\n  echo \"Validating connection to ${endpoint}/version\";\n  docker run --net=host -v $(docker inspect kubelet --format '{{ range .Mounts }}{{ if eq .Destination \"/etc/kubernetes\" }}{{ .Source }}{{ end }}{{ end }}')/ssl:/etc/kubernetes/ssl:ro appropriate/curl --http1.1 -s -w \"\\n\" --cacert $(docker exec etcd printenv ETCDCTL_CACERT) --cert $(docker exec etcd printenv ETCDCTL_CERT) --key $(docker exec etcd printenv ETCDCTL_KEY) \"${endpoint}/version\"\ndone\n\n\nCommand when using etcd version lower than 3.3.x (Kubernetes 1.13.x and lower) and --internal-address was specified when adding the node:\n\nfor endpoint in $(docker exec etcd /bin/sh -c \"etcdctl --endpoints=\\$ETCDCTL_ENDPOINT member list | cut -d, -f4\"); do\n  echo \"Validating connection to ${endpoint}/version\";\n  docker run --net=host -v $(docker inspect kubelet --format '{{ range .Mounts }}{{ if eq .Destination \"/etc/kubernetes\" }}{{ .Source }}{{ end }}{{ end }}')/ssl:/etc/kubernetes/ssl:ro appropriate/curl --http1.1 -s -w \"\\n\" --cacert $(docker exec etcd printenv ETCDCTL_CACERT) --cert $(docker exec etcd printenv ETCDCTL_CERT) --key $(docker exec etcd printenv ETCDCTL_KEY) \"${endpoint}/version\"\ndone\n\n\nExample output:\n\nValidating connection to https://IP:2380/version\n{\"etcdserver\":\"3.2.18\",\"etcdcluster\":\"3.2.0\"}\nValidating connection to https://IP:2380/version\n{\"etcdserver\":\"3.2.18\",\"etcdcluster\":\"3.2.0\"}\nValidating connection to https://IP:2380/version\n{\"etcdserver\":\"3.2.18\",\"etcdcluster\":\"3.2.0\"}\n\n\netcd Alarms\n\netcd will trigger alarms, for instance when it runs out of space.\n\nCommand:\n\ndocker exec etcd etcdctl alarm list\n\n\nCommand when using etcd version lower than 3.3.x (Kubernetes 1.13.x and lower) and --internal-address was specified when adding the node:\n\ndocker exec etcd sh -c \"etcdctl --endpoints=\\$ETCDCTL_ENDPOINT alarm list\"\n\n\nExample output when NOSPACE alarm is triggered:\n\nmemberID:x","postref":"285e8feea78090bb5d15de4adc16101b","objectID":"5838f5baaab26aee8741f036ce749241","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/troubleshooting/kubernetes-components/etcd/"},{"anchor":"#managing-members","title":"Managing Members","content":"Available as of v2.3.0The default length (TTL) of each user session is adjustable. The default session length is 16 hours.\nFrom the Global view, click on Settings.\nIn the Settings page, find auth-user-session-ttl-minutes and click Edit.\nEnter the amount of time in minutes a session length should last and click Save.\nResult: Users are automatically logged out of Rancher after the set number of minutes.Rancher maintains information about each user that logs in through an authentication provider. This information includes whether the user is allowed to access your Rancher server and the list of groups that the user belongs to. Rancher keeps this user information so that the CLI, API, and kubectl can accurately reflect the access that the user has based on their group membership in the authentication provider.Whenever a user logs in to the UI using an authentication provider, Rancher automatically updates this user information.Automatically Refreshing User InformationAvailable as of v2.2.0Rancher will periodically refresh the user information even before a user logs in through the UI. You can control how often Rancher performs this refresh.  From the Global view, click on Settings. Two settings control this behavior:\nauth-user-info-max-age-seconds\n\nThis setting controls how old a user’s information can be before Rancher refreshes it. If a user makes an API call (either directly or by using the Rancher CLI or kubectl) and the time since the user’s last refresh is greater than this setting, then Rancher will trigger a refresh. This setting defaults to 3600 seconds, i.e. 1 hour.\n\nauth-user-info-resync-cron\n\nThis setting controls a recurring schedule for resyncing authentication provider information for all users. Regardless of whether a user has logged in or used the API recently, this will cause the user to be refreshed at the specified interval. This setting defaults to 0 0 * * *, i.e. once a day at midnight. See the Cron documentation for more information on valid values for this setting.\n\nNote: Since SAML does not support user lookup, SAML-based authentication providers do not support periodically refreshing user information. User information will only be refreshed when the user logs into the Rancher UI.\nManually Refreshing User InformationIf you are not sure the last time Rancher performed an automatic refresh of user information, you can perform a manual refresh of all users.\nFrom the Global view, click on Users in the navigation bar.\n\nClick on Refresh Group Memberships.\nResults: Rancher refreshes the user information for all users. Requesting this refresh will update which users can access Rancher as well as all the groups that each user belongs to.\nNote: Since SAML does not support user lookup, SAML-based authentication providers do not support the ability to manually refresh user information. User information will only be refreshed when the user logs into the Rancher UI.\nWhen adding a user or group to a resource, you can search for users or groups by beginning to type their name. The Rancher server will query the authentication provider to find users and groups that match what you’ve entered. Searching is limited to the authentication provider that you are currently logged in with. For example, if you’ve enabled GitHub authentication but are logged in using a local user account, you will not be able to search for GitHub users or groups.All users, whether they are local users or from an authentication provider, can be viewed and managed. From the Global view, click on Users.\nSAML Provider Caveats:\n\n\nSAML Protocol does not support search or lookup for users or groups. Therefore, there is no validation on users or groups when adding them to Rancher.\nWhen adding users, the exact user IDs (i.e. UID Field) must be entered correctly. As you type the user ID, there will be no search for other  user IDs that may match.\n\nWhen adding groups, you must select the group from the drop-down that is next to the text box. Rancher assumes that any input from the text box is a user.\n\n\nThe group drop-down shows only the groups that you are a member of. You will not be able to add groups that you are not a member of.\n\n\n","postref":"f6dab07788cac69662fb3c55879adff8","objectID":"7eba39e095c53f7a21a315a47408cb3f","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/authentication/user-groups/"},{"anchor":"#","title":"2. Enable Istio in a Namespace","content":"You will need to manually enable Istio in each namespace that you want to be tracked or controlled by Istio. When Istio is enabled in a namespace, the Envoy sidecar proxy will be automatically injected into all new workloads that are deployed in the namespace.\n\nThis namespace setting will only affect new workloads in the namespace. Any preexisting workloads will need to be re-deployed to leverage the sidecar auto injection.\n\n\nPrerequisite: To enable Istio in a namespace, the cluster must have Istio enabled.\n\n\n\nIn the Rancher UI, go to the cluster view. Click the Projects/Namespaces tab.\nGo to the namespace where you want to enable the Istio sidecar auto injection and click the Ellipsis (…).\nClick Edit.\nIn the Istio sidecar auto injection section, click Enable.\nClick Save.\n\n\nResult: The namespace now has the label istio-injection=enabled. All new workloads deployed in this namespace will have the Istio sidecar injected by default.\n\nVerifying that Automatic Istio Sidecar Injection is Enabled\n\nTo verify that Istio is enabled, deploy a hello-world workload in the namespace. Go to the workload and click the pod name. In the Containers section, you should see the istio-proxy container.\n\nExcluding Workloads from Being Injected with the Istio Sidecar\n\nIf you need to exclude a workload from getting injected with the Istio sidecar, use the following annotation on the workload:\n\nsidecar.istio.io/inject: “false”\n\n\nTo add the annotation to a workload,\n\n\nFrom the Global view, open the project that has the workload that should not have the sidecar.\nClick Resources > Workloads.\nGo to the workload that should not have the sidecar and click Ellipsis (…) > Edit.\nClick Show Advanced Options. Then expand the Labels & Annotations section.\nClick Add Annotation.\nIn the Key field, enter sidecar.istio.io/inject.\nIn the Value field, enter false.\nClick Save.\n\n\nResult: The Istio sidecar will not be injected into the workload.\n\nNext: Set up Taints and Tolerations\n","postref":"d0be517253f2007697b4b4196028b297","objectID":"29ddf97d31b20958b959de4cffe1d69a","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/tools/istio/setup/enable-istio-in-namespace/"},{"anchor":"#","title":"About High-availability Installations","content":"We recommend using Helm, a Kubernetes package manager, to install Rancher on a dedicated Kubernetes cluster. This is called a high-availability Kubernetes installation because increased availability is achieved by running Rancher on multiple nodes.\n\nIn a standard installation, Kubernetes is first installed on three nodes that are hosted in an infrastructure provider such as Amazon’s EC2 or Google Compute Engine.\n\nThen Helm is used to install Rancher on top of the Kubernetes cluster. Helm uses Rancher’s Helm chart to install a replica of Rancher on each of the three nodes in the Kubernetes cluster. We recommend using a load balancer to direct traffic to each replica of Rancher in the cluster, in order to increase Rancher’s availability.\n\nThe Rancher server data is stored on etcd. This etcd database also runs on all three nodes, and requires an odd number of nodes so that it can always elect a leader with a majority of the etcd cluster. If the etcd database cannot elect a leader, etcd can fail, requiring the cluster to be restored from backup.\n\nFor information on how Rancher works, regardless of the installation method, refer to the architecture section.\n\nRecommended Architecture\n\n\nDNS for Rancher should resolve to a layer 4 load balancer\nThe Load Balancer should forward port TCP/80 and TCP/443 to all 3 nodes in the Kubernetes cluster.\nThe Ingress controller will redirect HTTP to HTTPS and terminate SSL/TLS on port TCP/443.\nThe Ingress controller will forward traffic to port TCP/80 on the pod in the Rancher deployment.\n\n\nKubernetes Rancher install with layer 4 load balancer, depicting SSL termination at ingress controllers\n\nKubernetes Rancher install with Layer 4 load balancer (TCP), depicting SSL termination at ingress controllers\n","postref":"3acab8f3d91da335f22a462ec1b79d87","objectID":"ebf4ac1c738bcc100e3e867612e8dac2","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/how-ha-works/"},{"anchor":"#","title":"Alerts","content":"To keep your clusters and applications healthy and driving your organizational productivity forward, you need to stay informed of events occurring in your clusters and projects, both planned and unplanned. When an event occurs, your alert is triggered, and you are sent a notification. You can then, if necessary, follow up with corrective actions.\n\nNotifiers and alerts are built on top of the Prometheus Alertmanager. Leveraging these tools, Rancher can notify cluster owners and project owners of events they need to address.\n\nBefore you can receive alerts, you must configure one or more notifier in Rancher.\n\nWhen you create a cluster, some alert rules are predefined. You can receive these alerts if you configure a notifier for them.\n\nFor details about what triggers the predefined alerts, refer to the documentation on default alerts.\n\nThis section covers the following topics:\n\n\nAlert event examples\nUrgency levels\nScope of alerts\nAdding cluster alerts\nManaging cluster alerts\n\n\nAlert Event Examples\n\nSome examples of alert events are:\n\n\nA Kubernetes master component entering an unhealthy state.\nA node or workload error occurring.\nA scheduled deployment taking place as planned.\nA node’s hardware resources becoming overstressed.\n\n\nUrgency Levels\n\nYou can set an urgency level for each alert. This urgency appears in the notification you receive, helping you to prioritize your response actions. For example, if you have an alert configured to inform you of a routine deployment, no action is required. These alerts can be assigned a low priority level. However, if a deployment fails, it can critically impact your organization, and you need to react quickly. Assign these alerts a high priority level.\n\nScope of Alerts\n\nThe scope for alerts can be set at either the cluster level or project level.\n\nAt the cluster level, Rancher monitors components in your Kubernetes cluster, and sends you alerts related to:\n\n\nThe state of your nodes.\nThe system services that manage your Kubernetes cluster.\nThe resource events from specific system services.\nThe Prometheus expression cross the thresholds\n\n\nAdding Cluster Alerts\n\nAs a cluster owner, you can configure Rancher to send you alerts for cluster events.\n\n\nPrerequisite: Before you can receive cluster alerts, you must add a notifier.\n\n\n\nFrom the Global view, navigate to the cluster that you want to configure cluster alerts for. Select Tools > Alerts. Then click Add Alert Group.\n\nEnter a Name for the alert that describes its purpose, you could group alert rules for the different purpose.\n\nBased on the type of alert you want to create, complete one of the instruction subsets below.\n\n  \n  System Service Alerts\n  \n    This alert type monitor for events that affect one of the Kubernetes master components, regardless of the node it occurs on.\n\n\nSelect the System Services option, and then select an option from the drop-down.\n\n\ncontroller-manager\netcd\nscheduler\n\n\nSelect the urgency level of the alert. The options are:\n\n\nCritical: Most urgent\nWarning: Normal urgency\nInfo: Least urgent\n\n\nSelect the urgency level based on the importance of the service and how many nodes fill the role within your cluster. For example, if you’re making an alert for the etcd service, select Critical. If you’re making an alert for redundant schedulers, Warning is more appropriate.\n\n\nConfigure advanced options. By default, the below options will apply to all alert rules within the group. You can disable these advanced options when configuring a specific rule.\n\n\nGroup Wait Time: How long to wait to buffer alerts of the same group before sending initially, default to 30 seconds.\nGroup Interval Time: How long to wait before sending an alert that has been added to a group which contains already fired alerts, default to 30 seconds.\nRepeat Wait Time: How long to wait before sending an alert that has been added to a group which contains already fired alerts, default to 1 hour.\n\n\n\n  \n\n\n\n  \n  Resource Event Alerts\n  \n    This alert type monitors for specific events that are thrown from a resource type.\n\n\nChoose the type of resource event that triggers an alert. The options are:\n\n\nNormal: triggers an alert when any standard resource event occurs.\nWarning: triggers an alert when unexpected resource events occur.\n\n\nSelect a resource type from the Choose a Resource drop-down that you want to trigger an alert.\n\n\nDaemonSet\nDeployment\nNode\nPod\nStatefulSet\n\n\nSelect the urgency level of the alert.\n\n\nCritical: Most urgent\nWarning: Normal urgency\n\nInfo: Least urgent\n\n\nSelect the urgency level of the alert by considering factors such as how often the event occurs or its importance. For example:\n\nIf you set a normal alert for pods, you’re likely to receive alerts often, and individual pods usually self-heal, so select an urgency of Info.\n\nIf you set a warning alert for StatefulSets, it’s very likely to impact operations, so select an urgency of Critical.\n\n\nConfigure advanced options. By default, the below options will apply to all alert rules within the group. You can disable these advanced options when configuring a specific rule.\n\n\nGroup Wait Time: How long to wait to buffer alerts of the same group before sending initially, default to 30 seconds.\nGroup Interval Time: How long to wait before sending an alert that has been added to a group which contains already fired alerts, default to 30 seconds.\nRepeat Wait Time: How long to wait before sending an alert that has been added to a group which contains already fired alerts, default to 1 hour.\n\n\n\n  \n\n\n\n  \n  Node Alerts\n  \n    This alert type monitors for events that occur on a specific node.\n\n\nSelect the Node option, and then make a selection from the Choose a Node drop-down.\n\nChoose an event to trigger the alert.\n\n\nNot Ready: Sends you an alert when the node is unresponsive.\nCPU usage over: Sends you an alert when the node raises above an entered percentage of its processing allocation.\nMem usage over: Sends you an alert when the node raises above an entered percentage of its memory allocation.\n\n\nSelect the urgency level of the alert.\n\n\nCritical: Most urgent\nWarning: Normal urgency\nInfo: Least urgent\n\n\nSelect the urgency level of the alert based on its impact on operations. For example, an alert triggered when a node’s CPU raises above 60% deems an urgency of Info, but a node that is Not Ready deems an urgency of Critical.\n\n\nConfigure advanced options. By default, the below options will apply to all alert rules within the group. You can disable these advanced options when configuring a specific rule.\n\n\nGroup Wait Time: How long to wait to buffer alerts of the same group before sending initially, default to 30 seconds.\nGroup Interval Time: How long to wait before sending an alert that has been added to a group which contains already fired alerts, default to 30 seconds.\nRepeat Wait Time: How long to wait before sending an alert that has been added to a group which contains already fired alerts, default to 1 hour.\n\n\n\n  \n\n\n\n  \n  Node Selector Alerts\n  \n    This alert type monitors for events that occur on any node on marked with a label. For more information, see the Kubernetes documentation for Labels.\n\n\nSelect the Node Selector option, and then click Add Selector to enter a key value pair for a label. This label should be applied to one or more of your nodes. Add as many selectors as you’d like.\n\nChoose an event to trigger the alert.\n\n\nNot Ready: Sends you an alert when selected nodes are unresponsive.\nCPU usage over: Sends you an alert when selected nodes raise above an entered percentage of processing allocation.\nMem usage over: Sends you an alert when selected nodes raise above an entered percentage of memory allocation.\n\n\nSelect the urgency level of the alert.\n\n\nCritical: Most urgent\nWarning: Normal urgency\nInfo: Least urgent\n\n\nSelect the urgency level of the alert based on its impact on operations. For example, an alert triggered when a node’s CPU raises above 60% deems an urgency of Info, but a node that is Not Ready deems an urgency of Critical.\n\n\nConfigure advanced options. By default, the below options will apply to all alert rules within the group. You can disable these advanced options when configuring a specific rule.\n\n\nGroup Wait Time: How long to wait to buffer alerts of the same group before sending initially, default to 30 seconds.\nGroup Interval Time: How long to wait before sending an alert that has been added to a group which contains already fired alerts, default to 30 seconds.\nRepeat Wait Time: How long to wait before sending an alert that has been added to a group which contains already fired alerts, default to 1 hour.\n\n\n\n  \n\n\n\n  \n  Metric Expression Alerts\n  \n    This alert type monitors for the overload from Prometheus expression querying, it would be available after you enable monitoring.\n\n\nInput or select an Expression, the drop down shows the original metrics from Prometheus, including:\n\n\nNode\nContainer\nETCD\nKubernetes Components\nKubernetes Resources\nFluentd (supported by Logging)\nCluste","postref":"7b071e5d9f7d7ae80b162cc0c16aadb4","objectID":"4aa5f2f647c393ec4c4a4cab2b594036","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/tools/alerts/"},{"anchor":"#","title":"Dynamically Provisioning New Storage in Rancher","content":"This section describes how to provision new persistent storage for workloads in Rancher.\n\n\nThis section assumes that you understand the Kubernetes concepts of storage classes and persistent volume claims. For more information, refer to the section on how storage works.\n\n\nTo provision new storage for your workloads, follow these steps:\n\n\nAdd a storage class and configure it to use your storage provider.\nAdd a persistent volume claim that refers to the storage class.\nMount the persistent volume claim as a volume for your workload.\n\n\nPrerequisites\n\n\nTo set up persistent storage, the Manage Volumes role is required.\nIf you are provisioning storage for a cluster hosted in the cloud, the storage and cluster hosts must have the same cloud provider.\nThe cloud provider must be enabled. For details on enabling cloud providers, refer to this page.\nMake sure your storage provisioner is available to be enabled.\n\n\nThe following storage provisioners are enabled by default:\n\n\n\n\nName\nPlugin\n\n\n\n\n\nAmazon EBS Disk\naws-ebs\n\n\n\nAzureFile\nazure-file\n\n\n\nAzureDisk\nazure-disk\n\n\n\nGoogle Persistent Disk\ngce-pd\n\n\n\nLonghorn\nflex-volume-longhorn\n\n\n\nVMware vSphere Volume\nvsphere-volume\n\n\n\nLocal\nlocal\n\n\n\nNetwork File System\nnfs\n\n\n\nhostPath\nhost-path\n\n\n\n\nTo use a storage provisioner that is not on the above list, you will need to use a feature flag to enable unsupported storage drivers.\n\n1. Add a storage class and configure it to use your storage provider\n\nThese steps describe how to set up a storage class at the cluster level.\n\n\nGo to the cluster for which you want to dynamically provision persistent storage volumes.\n\nFrom the cluster view, select Storage > Storage Classes. Click Add Class.\n\nEnter a Name for your storage class.\n\nFrom the Provisioner drop-down, select the service that you want to use to dynamically provision storage volumes. For example, if you have a Amazon EC2 cluster and you want to use cloud storage for it, use the Amazon EBS Disk provisioner.\n\nFrom the Parameters section, fill out the information required for the service to dynamically provision storage volumes. Each provisioner requires different information to dynamically provision storage volumes. Consult the service’s documentation for help on how to obtain this information.\n\nClick Save.\n\n\nResult: The storage class is available to be consumed by a PVC.\n\nFor full information about the storage class parameters, refer to the official Kubernetes documentation..\n\n2. Add a persistent volume claim that refers to the storage class\n\nThese steps describe how to set up a PVC in the namespace where your stateful workload will be deployed.\n\n\nGo to the project containing a workload that you want to add a PVC to.\n\nFrom the main navigation bar, choose Resources > Workloads. (In versions prior to v2.3.0, choose Workloads on the main navigation bar.) Then select the Volumes tab. Click Add Volume.\n\nEnter a Name for the volume claim.\n\nSelect the Namespace of the volume claim.\n\nIn the Source field, click Use a Storage Class to provision a new persistent volume.\n\nGo to the Storage Class drop-down and select the storage class that you created.\n\nEnter a volume Capacity.\n\nOptional: Expand the Customize section and select the Access Modes that you want to use.\n\nClick Create.\n\n\nResult: Your PVC is created. You can now attach it to any workload in the project.\n\n3. Mount the persistent volume claim as a volume for your workload\n\nMount PVCs to workloads so that your applications can store their data.\n\nYou can mount PVCs during the deployment of a workload, or following workload creation.\n\nTo attach the PVC to a new workload,\n\n\nCreate a workload as you would in Deploying Workloads.\nFor Workload Type, select Stateful set of 1 pod.\nExpand the Volumes section and click Add Volume > Add a New Persistent Volume (Claim).\nIn the Persistent Volume Claim section, select the newly created persistent volume claim that is attached to the storage class.\nIn the Mount Point field, enter the path that the workload will use to access the volume.\nClick Launch.\n\n\nResult: When the workload is deployed, it will make a request for the specified amount of disk space to the Kubernetes master. If a PV with the specified resources is available when the workload is deployed, the Kubernetes master will bind the PV to the PVC.\n\nTo attach the PVC to an existing workload,\n\n\nGo to the project that has the workload that will have the PVC attached.\nGo to the workload that will have persistent storage and click Ellipsis (…) > Edit.\nExpand the Volumes section and click Add Volume > Add a New Persistent Volume (Claim).\nIn the Persistent Volume Claim section, select the newly created persistent volume claim that is attached to the storage class.\nIn the Mount Point field, enter the path that the workload will use to access the volume.\nClick Save.\n\n\nResult: The workload will make a request for the specified amount of disk space to the Kubernetes master. If a PV with the specified resources is available when the workload is deployed, the Kubernetes master will bind the PV to the PVC. If not, Rancher will provision new persistent storage.\n","postref":"ee6530b0b5723aa8d0fe54be7becda44","objectID":"ccd81af6f0c283341fdebd841aec9591","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/volumes-and-storage/provisioning-new-storage/"},{"anchor":"#","title":"Enabling Disk UUIDs for vSphere VMs","content":"In order to provision nodes with RKE, all nodes must be configured with disk UUIDs. This is required so that attached VMDKs present a consistent UUID to the VM, allowing the disk to be mounted properly.\n\nDepending on whether you are provisioning the VMs using the vSphere node driver in Rancher or using your own scripts or third-party tools, there are different methods available to enable disk UUIDs for VMs:\n\n\nUsing the vSphere console\nUsing the GOVC CLI tool\nUsing a Rancher node template\n\n\nUsing the vSphere Console\n\nThe required property can be set while creating or modifying VMs in the vSphere Console:\n\n\nFor each VM navigate to the tab VM Options and click on Edit Configuration.\n\nAdd the parameter disk.EnableUUID with a value of TRUE.\n\n\n\n\n    \n    \n    \n    \n    \n    \n\n\n\n\nUsing the GOVC CLI tool\n\nYou can also modify properties of VMs with the govc command-line tool to enable disk UUIDs:\n$ govc vm.change -vm <vm-path> -e disk.enableUUID=TRUE\nUsing a Rancher Node Template\n\nIn Rancher v2.0.4+, disk UUIDs are enabled in vSphere node templates by default.\n\nIf you are using Rancher prior to v2.0.4, refer to the Rancher documentation. for details on how to enable a UUID with a Rancher node template.\n","postref":"218374635c19b05dbcc252d3dcea0c6a","objectID":"8f0801f19b75a7fb99392282eced18fd","permalink":"http://jijeesh.github.io/docs/rke/latest/en/config-options/cloud-providers/vsphere/enabling-uuid/"},{"anchor":"#","title":"Installing Rancher in an Air Gapped Environment with Helm 2","content":"\nAfter Helm 3 was released, the Rancher installation instructions were updated to use Helm 3.\n\nIf you are using Helm 2, we recommend migrating to Helm 3 because it is simpler to use and more secure than Helm 2.\n\nThis section provides a copy of the older instructions for installing Rancher on a Kubernetes cluster using Helm 2 in an air air gap environment, and it is intended to be used if upgrading to Helm 3 is not feasible.\n\n\nThis section is about installations of Rancher server in an air gapped environment. An air gapped environment could be where Rancher server will be installed offline, behind a firewall, or behind a proxy.\n\nThroughout the installations instructions, there will be tabs for either a high availability Kubernetes installation or a single-node Docker installation.\n\nAir Gapped Kubernetes Installations\n\nThis section covers how to install Rancher on a Kubernetes cluster in an air gapped environment.\n\nA Kubernetes installation is comprised of three nodes running the Rancher server components on a Kubernetes cluster. The persistence layer (etcd) is also replicated on these three nodes, providing redundancy and data duplication in case one of the nodes fails.\n\nAir Gapped Docker Installations\n\nThese instructions also cover how to install Rancher on a single node in an air gapped environment.\n\nThe Docker installation is for Rancher users that are wanting to test out Rancher. Instead of running on a Kubernetes cluster, you install the Rancher server component on a single node using a docker run command. Since there is only one node and a single Docker container, if the node goes down, there is no copy of the etcd data available on other nodes and you will lose all the data of your Rancher server.\n\n\nImportant: If you install Rancher following the Docker installation guide, there is no upgrade path to transition your Docker Installation to a Kubernetes Installation.\n\n\nInstead of running the Docker installation, you have the option to follow the Kubernetes Install guide, but only use one node to install Rancher. Afterwards, you can scale up the etcd nodes in your Kubernetes cluster to make it a Kubernetes Installation.\n\nInstallation Outline\n\n\n1. Prepare your Node(s)\n2. Collect and Publish Images to your Private Registry\n3. Launch a Kubernetes Cluster with RKE\n4. Install Rancher\n\n\nNext: Prepare your Node(s)\n","postref":"f975ad02b2c674268e82845b604a2c86","objectID":"1bfb864cf40de7d3db94bc61c733b2a6","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/air-gap-helm2/"},{"anchor":"#activating-deactivating-node-drivers","title":"Activating/Deactivating Node Drivers","content":"If you want to use a node driver that Rancher doesn’t support out-of-the-box, you can add that provider’s driver in order to start using them to create node templates and eventually node pools for your Kubernetes cluster.\nFrom the Global view, choose Tools > Drivers in the navigation bar. From the Drivers page, select the Node Drivers tab. In version prior to v2.2.0, you can select Node Drivers directly in the navigation bar.\n\nClick Add Node Driver.\n\nComplete the Add Node Driver form. Then click Create.\nDeveloping your own node driverNode drivers are implemented with Docker Machine.By default, Rancher only activates drivers for the most popular cloud providers, Amazon EC2, Azure, DigitalOcean and vSphere. If you want to show or hide any node driver, you can change its status.\nFrom the Global view, choose Tools > Drivers in the navigation bar. From the Drivers page, select the Node Drivers tab. In version prior to v2.2.0, you can select Node Drivers directly in the navigation bar.\n\nSelect the driver that you wish to Activate or Deactivate and select the appropriate icon.\n","postref":"75329fd42bc8d8b8fc58c5082af3be13","objectID":"5ecfdc15a883d65a796ff938dd7b329e","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/drivers/node-drivers/"},{"anchor":"#","title":"Overriding the Default Limit for a Namespace","content":"Although the Namespace Default Limit propagates from the project to each namespace, in some cases, you may need to increase (or decrease) the performance for a specific namespace. In this situation, you can override the default limits by editing the namespace.\n\nIn the diagram below, the Rancher administrator has a resource quota in effect for their project. However, the administrator wants to override the namespace limits for Namespace 3 so that it performs better. Therefore, the administrator raises the namespace limits for Namespace 3 so that the namespace can access more resources.\n\nNamespace Default Limit Override\n\n\nHow to: Editing Namespace Resource Quotas\n\nEditing Namespace Resource Quotas\n\nIf there is a resource quota configured for a project, you can override the namespace default limit to provide a specific namespace with access to more (or less) project resources.\n\n\nFrom the Global view, open the cluster that contains the namespace for which you want to edit the resource quota.\n\nFrom the main menu, select Projects/Namespaces.\n\nFind the namespace for which you want to edit the resource quota. Select Ellipsis (…) > Edit.\n\nEdit the Resource Quota Limits.  These limits determine the resources available to the namespace. The limits must be set within the configured project limits.\n\nFor more information about each Resource Type, see Resource Quota Types.\n\n\nNote:\n\n\nIf a resource quota is not configured for the project, these options will not be available.\nIf you enter limits that exceed the configured project limits, Rancher will not let you save your edits.\n\n\n\n\nResult: The namespace’s default resource quota is overwritten with your override.\n","postref":"6856b7abc028181c31c7697c173cc2ba","objectID":"b457af88f5145d9ebfeea0f1497aa5c7","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/project-admin/resource-quotas/override-namespace-default/"},{"anchor":"#","title":"Recurring Snapshots","content":"Recurring snapshots are handled differently based on your version of RKE.\n\n\n  \n  \n  To schedule automatic recurring etcd snapshots, you can enable the etcd-snapshot service with extra configuration options. etcd-snapshot runs in a service container alongside the etcd container. By default, the etcd-snapshot service takes a snapshot for every node that has the etcd role and stores them to local disk in /opt/rke/etcd-snapshots.\n\nIf you set up the options for S3, the snapshot will also be uploaded to the S3 backend.\n\nSnapshot Service Logging\n\nWhen a cluster is launched with the etcd-snapshot service enabled, you can view the etcd-rolling-snapshots logs to confirm backups are being created automatically.\n\n$ docker logs etcd-rolling-snapshots\n\ntime=\"2018-05-04T18:39:16Z\" level=info msg=\"Initializing Rolling Backups\" creation=1m0s retention=24h0m0s\ntime=\"2018-05-04T18:40:16Z\" level=info msg=\"Created backup\" name=\"2018-05-04T18:40:16Z_etcd\" runtime=108.332814ms\ntime=\"2018-05-04T18:41:16Z\" level=info msg=\"Created backup\" name=\"2018-05-04T18:41:16Z_etcd\" runtime=92.880112ms\ntime=\"2018-05-04T18:42:16Z\" level=info msg=\"Created backup\" name=\"2018-05-04T18:42:16Z_etcd\" runtime=83.67642ms\ntime=\"2018-05-04T18:43:16Z\" level=info msg=\"Created backup\" name=\"2018-05-04T18:43:16Z_etcd\" runtime=86.298499ms\n\n\nOptions for the Etcd-Snapshot Service\n\n\n\n\nOption\nDescription\nS3 Specific\n\n\n\n\n\ninterval_hours\nThe duration in hours between recurring backups.  This supercedes the creation option (which was used in RKE prior to v0.2.0) and will override it if both are specified.\n\n\n\n\nretention\nThe number of snapshots to retain before rotation. This supercedes the retention option and will override it if both are specified.\n\n\n\n\nbucket_name\nS3 bucket name where backups will be stored\n*\n\n\n\nfolder\nFolder inside S3 bucket where backups will be stored. This is optional. Available as of v0.3.0\n*\n\n\n\naccess_key\nS3 access key with permission to access the backup bucket.\n*\n\n\n\nsecret_key\nS3 secret key with permission to access the backup bucket.\n*\n\n\n\nregion\nS3 region for the backup bucket. This is optional.\n*\n\n\n\nendpoint\nS3 regions endpoint for the backup bucket.\n*\n\n\n\ncustom_ca\nCustom certificate authority to use when connecting to the endpoint. Only required for private S3 compatible storage solutions. Available for RKE v0.2.5+.\n*\n\n\n\n\nThe --access-key and --secret-key options are not required if the etcd nodes are AWS EC2 instances that have been configured with a suitable IAM instance profile.\n\nUsing a custom CA certificate for S3\n\nThe backup snapshot can be stored on a custom S3 backup like minio. If the S3 backend uses a self-signed or custom certificate, provide a custom certificate using the option custom_ca to connect to the S3 backend.\n\nIAM Support for Storing Snapshots in S3\n\nIn addition to API access keys, RKE supports using IAM roles for S3 authentication. The cluster etcd nodes must be assigned an IAM role that has read/write access to the designated backup bucket on S3. Also, the nodes must have network access to the S3 endpoint specified.\n\nBelow is an example IAM policy that would allow nodes to store and retrieve backups from S3:\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"ListObjectsInBucket\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\"s3:ListBucket\"],\n            \"Resource\": [\"arn:aws:s3:::bucket-name\"]\n        },\n        {\n            \"Sid\": \"AllObjectActions\",\n            \"Effect\": \"Allow\",\n            \"Action\": \"s3:*Object\",\n            \"Resource\": [\"arn:aws:s3:::bucket-name/*\"]\n        }\n    ]\n}\nFor details on giving an application access to S3, refer to the AWS documentation on Using an IAM Role to Grant Permissions to Applications Running on Amazon EC2 Instances.\n\nConfiguring the Snapshot Service in YAML\nservices:\n  etcd:\n    backup_config:\n      interval_hours: 12\n      retention: 6\n      s3backupconfig:\n        access_key: S3_ACCESS_KEY\n        secret_key: S3_SECRET_KEY\n        bucket_name: s3-bucket-name\n        region: \"\"\n        folder: \"\" # Optional - Available as of v0.3.0\n        endpoint: s3.amazonaws.com\n\n\n\n  To schedule automatic recurring etcd snapshots, you can enable the etcd-snapshot service with extra configuration options. etcd-snapshot runs in a service container alongside the etcd container. By default, the etcd-snapshot service takes a snapshot for every node that has the etcd role and stores them to local disk in /opt/rke/etcd-snapshots.\n\nRKE saves a backup of the certificates, i.e. a file named pki.bundle.tar.gz, in the same location. The snapshot and pki bundle file are required for the restore process in versions prior to v0.2.0.\n\nSnapshot Service Logging\n\nWhen a cluster is launched with the etcd-snapshot service enabled, you can view the etcd-rolling-snapshots logs to confirm backups are being created automatically.\n\n$ docker logs etcd-rolling-snapshots\n\ntime=\"2018-05-04T18:39:16Z\" level=info msg=\"Initializing Rolling Backups\" creation=1m0s retention=24h0m0s\ntime=\"2018-05-04T18:40:16Z\" level=info msg=\"Created backup\" name=\"2018-05-04T18:40:16Z_etcd\" runtime=108.332814ms\ntime=\"2018-05-04T18:41:16Z\" level=info msg=\"Created backup\" name=\"2018-05-04T18:41:16Z_etcd\" runtime=92.880112ms\ntime=\"2018-05-04T18:42:16Z\" level=info msg=\"Created backup\" name=\"2018-05-04T18:42:16Z_etcd\" runtime=83.67642ms\ntime=\"2018-05-04T18:43:16Z\" level=info msg=\"Created backup\" name=\"2018-05-04T18:43:16Z_etcd\" runtime=86.298499ms\n\n\nOptions for the Local Etcd-Snapshot Service\n\n\n\n\nOption\nDescription\n\n\n\n\n\nSnapshot\nBy default, the recurring snapshot service is disabled. To enable the service, you need to define it as part of etcd and set it to true.\n\n\n\nCreation\nBy default, the snapshot service will take snapshots every 5 minutes (5m0s). You can change the time between snapshots as part of the creation directive for the etcd service.\n\n\n\nRetention\nBy default, all snapshots are saved for 24 hours (24h) before being deleted and purged. You can change how long to store a snapshot as part of the retention directive for the etcd service.\n\n\n\n\nConfiguring the Snapshot Service in YAML\nservices:\n    etcd:\n      snapshot: true\n      creation: 5m0s\n      retention: 24h\n\n\n\n\n","postref":"f89cc6e7cfe36095a93a4380f609da6d","objectID":"01cc359636e9a566b5a91e8a25f4ad46","permalink":"http://jijeesh.github.io/docs/rke/latest/en/etcd-snapshots/recurring-snapshots/"},{"anchor":"#","title":"Setup Guide","content":"This section describes how to enable Istio and start using it in your projects.\n\nThis section assumes that you have Rancher installed, and you have a Rancher-provisioned Kubernetes cluster where you would like to set up Istio.\n\nIf you use Istio for traffic management, you will need to allow external traffic to the cluster. In that case, you will need to follow all of the steps below.\n\n\nQuick Setup If you don’t need external traffic to reach Istio, and you just want to set up Istio for monitoring and tracing traffic within the cluster, skip the steps for setting up the Istio gateway and setting up Istio’s components for traffic management.\n\n\n\nEnable Istio in the cluster.\nEnable Istio in all the namespaces where you want to use it.\nSelect the nodes where the main Istio components will be deployed.\nAdd deployments and services that have the Istio sidecar injected.\nSet up the Istio gateway. \nSet up Istio’s components for traffic management.\nGenerate traffic and see Istio in action.\n\n\nPrerequisites\n\nThis guide assumes you have already installed Rancher, and you have already provisioned a separate Kubernetes cluster on which you will install Istio.\n\nThe nodes in your cluster must meet the CPU and memory requirements.\n\nThe workloads and services that you want to be controlled by Istio must meet Istio’s requirements.\n","postref":"3bf2ce9e77f2e6ea0f8af6df1df287b1","objectID":"625b4f5014fd73e548a501c5b14b7205","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/tools/istio/setup/"},{"anchor":"#","title":"Troubleshooting Controlplane Nodes","content":"This section applies to nodes with the controlplane role.\n\nCheck if the Controlplane Containers are Running\n\nThere are three specific containers launched on nodes with the controlplane role:\n\n\nkube-apiserver\nkube-controller-manager\nkube-scheduler\n\n\nThe containers should have status Up. The duration shown after Up is the time the container has been running.\n\ndocker ps -a -f=name='kube-apiserver|kube-controller-manager|kube-scheduler'\n\n\nExample output:\n\nCONTAINER ID        IMAGE                                COMMAND                  CREATED             STATUS              PORTS               NAMES\n26c7159abbcc        rancher/hyperkube:v1.11.5-rancher1   \"/opt/rke-tools/en...\"   3 hours ago         Up 3 hours                              kube-apiserver\nf3d287ca4549        rancher/hyperkube:v1.11.5-rancher1   \"/opt/rke-tools/en...\"   3 hours ago         Up 3 hours                              kube-scheduler\nbdf3898b8063        rancher/hyperkube:v1.11.5-rancher1   \"/opt/rke-tools/en...\"   3 hours ago         Up 3 hours                              kube-controller-manager\n\n\nControlplane Container Logging\n\n\nNote: If you added multiple nodes with the controlplane role, both kube-controller-manager and kube-scheduler use a leader election process to determine the leader. Only the current leader will log the performed actions. See Kubernetes leader election how to retrieve the current leader.\n\n\nThe logging of the containers can contain information on what the problem could be.\n\ndocker logs kube-apiserver\ndocker logs kube-controller-manager\ndocker logs kube-scheduler\n\n","postref":"a2a79babdbb966b8426ea0ee552c08c8","objectID":"2b35114751a3010bb1f7d7a3896e3f36","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/troubleshooting/kubernetes-components/controlplane/"},{"anchor":"#","title":"UI for Istio Virtual Services and Destination Rules","content":"Available as of v2.3.0\n\nThis feature enables a UI that lets you create, read, update and delete virtual services and destination rules, which are traffic management features of Istio.\n\n\nPrerequisite: Turning on this feature does not enable Istio. A cluster administrator needs to enable Istio for the cluster in order to use the feature.\n\n\nTo enable or disable this feature, refer to the instructions on the main page about enabling experimental features.\n\n\n\n\nEnvironment Variable Key\nDefault Value\nStatus\nAvailable as of\n\n\n\n\n\nistio-virtual-service-ui\nfalse\nExperimental\nv2.3.0\n\n\n\nistio-virtual-service-ui\ntrue\nGA\nv2.3.2\n\n\n\n\nAbout this Feature\n\nA central advantage of Istio’s traffic management features is that they allow dynamic request routing, which is useful for canary deployments, blue/green deployments, or A/B testing.\n\nWhen enabled, this feature turns on a page that lets you configure some traffic management features of Istio using the Rancher UI. Without this feature, you need to use kubectl to manage traffic with Istio.\n\nThe feature enables two UI tabs: one tab for Virtual Services and another for Destination Rules.\n\n\nVirtual services intercept and direct traffic to your Kubernetes services, allowing you to direct percentages of traffic from a request to different services. You can use them to define a set of routing rules to apply when a host is addressed. For details, refer to the Istio documentation.\nDestination rules serve as the single source of truth about which service versions are available to receive traffic from virtual services. You can use these resources to define policies that apply to traffic that is intended for a service after routing has occurred. For details, refer to the Istio documentation.\n\n\nTo see these tabs,\n\n\nGo to the project view in Rancher and click Resources > Istio.\nYou will see tabs for Traffic Graph, which has the Kiali network visualization integrated into the UI, and Traffic Metrics, which shows metrics for the success rate and request volume of traffic to your services, among other metrics. Next to these tabs, you should see the tabs for Virtual Services and Destination Rules.\n\n","postref":"4f124fc6f75e00100aa9d321f5ea4d7e","objectID":"7bb0856b1248db71fe7bb2d0b64ca5fb","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/feature-flags/istio-virtual-service-ui/"},{"anchor":"#rancher-dashboard","title":"Rancher Dashboard","content":"If you’ve enabled monitoring at either the cluster level or project level, Rancher automatically creates a link to Grafana instance. Use this link to view monitoring data.Grafana allows you to query, visualize, alert, and ultimately, understand your cluster and workload data. For more information on Grafana and its capabilities, visit the Grafana website.AuthenticationRancher determines which users can access the new Grafana instance, as well as the objects they can view within it, by validating them against the user’s cluster or project roles. In other words, a user’s access in Grafana mirrors their access in Rancher.When you go to the Grafana instance, you will be logged in with the username admin and the password admin. If you log out and log in again, you will be prompted to change your password. You will only have access to the URL of the Grafana instance if you have access to view the corresponding metrics in Rancher. So for example, if your Rancher permissions are scoped to the project level, you won’t be able to see the Grafana instance for cluster-level metrics.Accessing the Cluster-level Grafana Instance\nFrom the Global view, navigate to a cluster that has monitoring enabled.\n\nGo to the System project view. This project is where the cluster-level Grafana instance runs.\n\nClick Apps. In versions prior to v2.2.0, choose Catalog Apps on the main navigation bar.\n\nGo to the cluster-monitoring application.\n\nIn the cluster-monitoring application, there are two /index.html links: one that leads to a Grafana instance and one that leads to a Prometheus instance. When you click the Grafana link, it will redirect you to a new webpage for Grafana, which shows metrics for the cluster.\n\nYou will be signed in to the Grafana instance automatically. The default username is admin and the default password is admin. For security, we recommend that you log out of Grafana, log back in with the admin password, and change your password.\nResults: You are logged into Grafana from the Grafana instance. After logging in, you can view the preset Grafana dashboards, which are imported via the Grafana provisioning mechanism, so you cannot modify them directly. For now, if you want to configure your own dashboards, clone the original and modify the new copy.\nNote: This is only available if you’ve enabled monitoring at the cluster level. Project specific analytics must be viewed using the project’s Grafana instance.\nRancher’s dashboards are available at multiple locations:\nCluster Dashboard: From the Global view, navigate to the cluster.\nNode Metrics: From the Global view, navigate to the cluster. Select Nodes. Find the individual node and click on its name. Click Node Metrics.\nWorkload Metrics: From the Global view, navigate to the project. From the main navigation bar, choose Resources > Workloads. (In versions prior to v2.3.0, choose Workloads on the main navigation bar.) Find the individual workload and click on its name. Click Workload Metrics.\nPod Metrics: From the Global view, navigate to the project. Select Workloads > Workloads. Find the individual workload and click on its name. Find the individual pod and click on its name. Click Pod Metrics.\nContainer Metrics: From the Global view, navigate to the project. From the main navigation bar, choose Resources > Workloads. (In versions prior to v2.3.0, choose Workloads on the main navigation bar.) Find the individual workload and click on its name. Find the individual pod and click on its name. Find the individual container and click on its name. Click Container Metrics.\nPrometheus metrics are displayed and are denoted with the Grafana icon. If you click on the icon, the metrics will open a new tab in Grafana.Within each Prometheus metrics widget, there are several ways to customize your view.\nToggle between two views:\n\n\nDetail: Displays graphs and charts that let you view each event in a Prometheus time series\nSummary Displays events in a Prometheus time series that are outside the norm.\n\nChange the range of the time series that you’re viewing to see a more refined or expansive data sample.\nCustomize the data sample to display data between specific dates and times.\nWhen analyzing these metrics, don’t be concerned about any single standalone metric in the charts and graphs. Rather, you should establish a baseline for your metrics over the course of time, e.g. the range of values that your components usually operate within and are considered normal. After you establish the baseline, be on the lookout for any large deltas in the charts and graphs, as these big changes usually indicate a problem that you need to investigate.","postref":"0a9be7308ac272209b8a93db206daaa6","objectID":"c2e8e9a7773fa87d6ba0526f7bb287fa","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/tools/monitoring/viewing-metrics/"},{"anchor":"#","title":"3. Select the Nodes Where Istio Components Will be Deployed","content":"\nPrerequisite: Your cluster needs a worker node that can designated for Istio. The worker node should meet the resource requirements.\n\n\nThis section describes how use node selectors to configure Istio components to be deployed on a designated node.\n\nIn larger deployments, it is strongly advised that Istio’s infrastructure be placed on dedicated nodes in the cluster by adding a node selector for each Istio component.\n\nAdding a Label to the Istio Node\n\nFirst, add a label to the node where Istio components should be deployed. This label can have any key-value pair. For this example, we will use the key istio and the value enabled.\n\n\nFrom the cluster view, go to the Nodes tab.\nGo to a worker node that will host the Istio components and click Ellipsis (…) > Edit.\nExpand the Labels & Annotations section.\nClick Add Label.\nIn the fields that appear, enter istio for the key and enabled for the value.\nClick Save.\n\n\nResult: A worker node has the label that will allow you to designate it for Istio components.\n\nConfiguring Istio Components to Use the Labeled Node\n\nConfigure each Istio component to be deployed to the node with the Istio label. Each Istio component can be configured individually, but in this tutorial, we will configure all of the components to be scheduled on the same node for the sake of simplicity.\n\nFor larger deployments, it is recommended to schedule each component of Istio onto separate nodes.\n\n\nFrom the cluster view, click Tools > Istio.\nExpand the Pilot section and click Add Selector in the form that appears. Enter the node selector label that you added to the Istio node. In our case, we are using the key istio and the value enabled.\nRepeat the previous step for the Mixer and Tracing sections.\nClick Save.\n\n\nResult: The Istio components will be deployed on the Istio node.\n\nNext: Add Deployments and Services\n","postref":"4655c9f7b94fb96277dcd9281c714a3d","objectID":"0888a7837a3624c6814f1040e5773202","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/tools/istio/setup/node-selectors/"},{"anchor":"#","title":"Architecture Recommendations","content":"Kubernetes cluster. If you are installing Rancher on a single node, the main architecture recommendation that applies to your installation is that the node running Rancher should be separate from downstream clusters.\n\nThis section covers the following topics:\n\n\nSeparation of Rancher and User Clusters\nWhy HA is Better for Rancher in Production\nRecommended Load Balancer Configuration for Kubernetes Installations\nEnvironment for Kubernetes Installations\nRecommended Node Roles for Kubernetes Installations\nArchitecture for an Authorized Cluster Endpoint\n\n\nSeparation of Rancher and User Clusters\n\nA user cluster is a downstream Kubernetes cluster that runs your apps and services.\n\nIf you have a Docker installation of Rancher, the node running the Rancher server should be separate from your downstream clusters.\n\nIn Kubernetes Installations of Rancher, the Rancher server cluster should also be separate from the user clusters.\n\n\n\nWhy HA is Better for Rancher in Production\n\nWe recommend installing the Rancher server on a three-node Kubernetes cluster for production, primarily because it protects the Rancher server data. The Rancher server stores its data in etcd in both single-node and Kubernetes Installations.\n\nWhen Rancher is installed on a single node, if the node goes down, there is no copy of the etcd data available on other nodes and you could lose the data on your Rancher server.\n\nBy contrast, in the high-availability installation,\n\n\nThe etcd data is replicated on three nodes in the cluster, providing redundancy and data duplication in case one of the nodes fails.\nA load balancer serves as the single point of contact for clients, distributing network traffic across multiple servers in the cluster and helping to prevent any one server from becoming a point of failure. Note: This example of how to configure an NGINX server as a basic layer 4 load balancer (TCP).\n\n\nRecommended Load Balancer Configuration for Kubernetes Installations\n\nWe recommend the following configurations for the load balancer and Ingress controllers:\n\n\nThe DNS for Rancher should resolve to a Layer 4 load balancer (TCP)\nThe Load Balancer should forward port TCP/80 and TCP/443 to all 3 nodes in the Kubernetes cluster.\nThe Ingress controller will redirect HTTP to HTTPS and terminate SSL/TLS on port TCP/443.\nThe Ingress controller will forward traffic to port TCP/80 on the pod in the Rancher deployment.\n\n\nRancher installed on a Kubernetes cluster with layer 4 load balancer, depicting SSL termination at ingress controllers\n\nRancher installed on a Kubernetes cluster with Layer 4 load balancer (TCP), depicting SSL termination at ingress controllers\n\nEnvironment for Kubernetes Installations\n\nIt is strongly recommended to install Rancher on a Kubernetes cluster on hosted infrastructure such as Amazon’s EC2 or Google Compute Engine.\n\nFor the best performance and greater security, we recommend a dedicated Kubernetes cluster for the Rancher management server. Running user workloads on this cluster is not advised. After deploying Rancher, you can create or import clusters for running your workloads.\n\nIt is not recommended to install Rancher on top of a managed Kubernetes service such as Amazon’s EKS or Google Kubernetes Engine. These hosted Kubernetes solutions do not expose etcd to a degree that is manageable for Rancher, and their customizations can interfere with Rancher operations.\n\nRecommended Node Roles for Kubernetes Installations\n\nWe recommend installing Rancher on a Kubernetes cluster in which each node has all three Kubernetes roles: etcd, controlplane, and worker.\n\nComparing Node Roles for the Rancher Server Cluster and User Clusters\n\nOur recommendation for node roles on the Rancher server cluster contrast with our recommendations for the downstream user clusters that run your apps and services. We recommend that each node in a user cluster should have a single role for stability and scalability.\n\n\n\nKubernetes only requires at least one node with each role and does not require nodes to be restricted to one role. However, for the clusters that run your apps, we recommend separate roles for each node so that workloads on worker nodes don’t interfere with the Kubernetes master or cluster data as your services scale.\n\nWe recommend that downstream user clusters should have at least:\n\n\nThree nodes with only the etcd role to maintain a quorum if one node is lost, making the state of your cluster highly available\nTwo nodes with only the controlplane role to make the master component highly available\nOne or more nodes with only the worker role to run the Kubernetes node components, as well as the workloads for your apps and services\n\n\nWith that said, it is safe to use all three roles on three nodes when setting up the Rancher server because:\n\n\nIt allows one etcd node failure.\nIt maintains multiple instances of the master components by having multiple controlplane nodes.\nNo other workloads than Rancher itself should be created on this cluster.\n\n\nBecause no additional workloads will be deployed on the Rancher server cluster, in most cases it is not necessary to use the same architecture that we recommend for the scalability and reliability of user clusters.\n\nFor more best practices for user clusters, refer to the production checklist or our best practices guide.\n\nArchitecture for an Authorized Cluster Endpoint\n\nIf you are using an authorized cluster endpoint, we recommend creating an FQDN pointing to a load balancer which balances traffic across your nodes with the controlplane role.\n\nIf you are using private CA signed certificates on the load balancer, you have to supply the CA certificate, which will be included in the generated kubeconfig file to validate the certificate chain. See the documentation on kubeconfig files and API keys for more information.\n","postref":"647f60f83d125165de02f8ace3e5f1cf","objectID":"cb1a1363ae1b2ee62817c7c707e4b82b","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/overview/architecture-recommendations/"},{"anchor":"#finding-node-metrics","title":"Finding Node Metrics","content":"Workload metrics display the hardware utilization for a Kubernetes workload. You can also view metrics for deployments, stateful sets and so on.\nFrom the Global view, navigate to the project that you want to view workload metrics.\n\nFrom the main navigation bar, choose Resources > Workloads. In versions prior to v2.3.0, choose Workloads on the main navigation bar.\n\nSelect a specific workload and click on its name.\n\nIn the Pods section, select a specific pod and click on its name.\n\n\nView the Pod Metrics: Click on Pod Metrics.\nView the Container Metrics: In the Containers section, select a specific container and click on its name. Click on Container Metrics.\n\nGet expressions for Workload MetricsAlthough the Dashboard for a cluster primarily displays data sourced from Prometheus, it also displays information for cluster logging, provided that you have configured Rancher to use a logging service.Get expressions for Rancher Logging Metrics\nFrom the Global view, navigate to the cluster that you want to view metrics.\n\nSelect Nodes in the navigation bar.\n\nSelect a specific node and click on its name.\n\nClick on Node Metrics.\nGet expressions for Cluster MetricsEtcd Metrics\nNote: Only supported for Rancher launched Kubernetes clusters.\nEtcd metrics display the operations of the etcd database on each of your cluster nodes. After establishing a baseline of normal etcd operational metrics, observe them for abnormal deltas between metric refreshes, which indicate potential issues with etcd. Always address etcd issues immediately!You should also pay attention to the text at the top of the etcd metrics, which displays leader election statistics. This text indicates if etcd currently has a leader, which is the etcd instance that coordinates the other etcd instances in your cluster. A large increase in leader changes implies etcd is unstable. If you notice a change in leader election statistics, you should investigate them for issues.Some of the biggest metrics to look out for:\nEtcd has a leader\n\netcd is usually deployed on multiple nodes and elects a leader to coordinate its operations. If etcd does not have a leader, its operations are not being coordinated.\n\nNumber of leader changes\n\nIf this statistic suddenly grows, it usually indicates network communication issues that constantly force the cluster to elect a new leader.\nGet expressions for Etcd MetricsKubernetes Components MetricsKubernetes components metrics display data about the cluster’s individual Kubernetes components. Primarily, it displays information about connections and latency for each component: the API server, controller manager, scheduler, and ingress controller.\nNote: The metrics for the controller manager, scheduler and ingress controller are only supported for Rancher launched Kubernetes clusters.\nWhen analyzing Kubernetes component metrics, don’t be concerned about any single standalone metric in the charts and graphs that display. Rather, you should establish a baseline for metrics considered normal following a period of observation, e.g. the range of values that your components usually operate within and are considered normal. After you establish this baseline, be on the lookout for large deltas in the charts and graphs, as these big changes usually indicate a problem that you need to investigate.Some of the more important component metrics to monitor are:\nAPI Server Request Latency\n\nIncreasing API response times indicate there’s a generalized problem that requires investigation.\n\nAPI Server Request Rate\n\nRising API request rates usually coincide with increased API response times. Increased request rates also indicate a generalized problem requiring investigation.\n\nScheduler Preemption Attempts\n\nIf you see a spike in scheduler preemptions, it’s an indication that you’re running out of hardware resources, as Kubernetes is recognizing it doesn’t have enough resources to run all your pods and is prioritizing the more important ones.\n\nScheduling Failed Pods\n\nFailed pods can have a variety of causes, such as unbound persistent volume claims, exhausted hardware resources, non-responsive nodes, etc.\n\nIngress Controller Request Process Time\n\nHow fast ingress is routing connections to your cluster services.\nGet expressions for Kubernetes Component Metrics","postref":"4ec4f42b91d97b6d1d92e8e8504dfb2d","objectID":"38603018698523b953e81fd0f3ae5d5e","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/tools/monitoring/cluster-metrics/"},{"anchor":"#","title":"Enabling Disk UUIDs in Node Templates","content":"As of Rancher v2.0.4, disk UUIDs are enabled in vSphere node templates by default.\n\nFor Rancher prior to v2.0.4, we recommend configuring a vSphere node template to automatically enable disk UUIDs because they are required for Rancher to manipulate vSphere resources.\n\nTo enable disk UUIDs for all VMs created for a cluster,\n\n\nNavigate to the Node Templates in the Rancher UI while logged in as an administrator.\n\nAdd or edit an existing vSphere node template.\n\nUnder Instance Options click on Add Parameter.\n\nEnter disk.enableUUID as key with a value of TRUE.\n\n\n\n\n    \n    \n    \n    \n    \n    \n\n\n\nClick Create or Save.\n\n\nResult: The disk UUID is enabled in the vSphere node template.\n","postref":"5140089502300da17ae20e142cc03984","objectID":"c0f7e8731ecfb4a9b827de6a7f68a94d","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/node-pools/vsphere/provisioning-vsphere-clusters/enabling-uuids/"},{"anchor":"#","title":"Installing Rancher in an Air Gapped Environment","content":"This section is about installations of Rancher server in an air gapped environment. An air gapped environment could be where Rancher server will be installed offline, behind a firewall, or behind a proxy.\n\nThroughout the installations instructions, there will be tabs for either a high availability Kubernetes installation or a single-node Docker installation.\n\nAir Gapped Kubernetes Installations\n\nThis section covers how to install Rancher on a Kubernetes cluster in an air gapped environment.\n\nA Kubernetes install is composed of three nodes running the Rancher server components on a Kubernetes cluster. The persistence layer (etcd) is also replicated on these three nodes, providing redundancy and data duplication in case one of the nodes fails.\n\nAir Gapped Docker Installations\n\nThese instructions also cover how to install Rancher on a single node in an air gapped environment.\n\nThe Docker installation is for Rancher users that are wanting to test out Rancher. Instead of running on a Kubernetes cluster, you install the Rancher server component on a single node using a docker run command. Since there is only one node and a single Docker container, if the node goes down, there is no copy of the etcd data available on other nodes and you will lose all the data of your Rancher server.\n\n\nImportant: If you install Rancher following the Docker installation guide, there is no upgrade path to transition your Docker Installation to a Kubernetes Installation.\n\n\nInstead of running the Docker installation, you have the option to follow the Kubernetes Install guide, but only use one node to install Rancher. Afterwards, you can scale up the etcd nodes in your Kubernetes cluster to make it a Kubernetes Installation.\n\nInstallation Outline\n\n\n1. Prepare your Node(s)\n2. Collect and Publish Images to your Private Registry\n3. Launch a Kubernetes Cluster with RKE\n4. Install Rancher\n\n\nNext: Prepare your Node(s)\n","postref":"674ce18461c766b1388d809c13aa3586","objectID":"11ce7ef62de1692ff4abe11e30eac86c","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/other-installation-methods/air-gap/"},{"anchor":"#required-cli-tools","title":"Required CLI Tools","content":"RKE add-on install\nImportant: RKE add-on install is only supported up to Rancher v2.0.8\n\nPlease use the Rancher Helm chart to install Rancher on a Kubernetes cluster. For details, see the Kubernetes Install - Installation Outline.\n\nIf you are currently using the RKE add-on install method, see Migrating from a Kubernetes Install with an RKE Add-on for details on how to move to using the helm chart.\n\nMigrating from a high-availability Kubernetes Install with an RKE Add-on\nInstalling Rancher with Helm 2: This section provides a copy of the older high-availability Rancher installation instructions that used Helm 2, and it is intended to be used if upgrading to Helm 3 is not feasible.\n\nCreate Nodes and Load Balancer\nInstall Kubernetes with RKE\nInstall Rancher\nThe following CLI tools are required for this install. Please make sure these tools are installed and available in your $PATH\nkubectl - Kubernetes command-line tool.\nrke - Rancher Kubernetes Engine, cli for building Kubernetes clusters.\nhelm - Package management for Kubernetes. Refer to the Helm version requirements to choose a version of Helm to install Rancher.\n","postref":"fe9f613e8d2bfd1caaa338b67f0959d0","objectID":"fd71137863479ee52391d595ae37cfcb","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/k8s-install/"},{"anchor":"#related-links","title":"Related Links","content":"Logging Architecture","postref":"bfe4bdcd4c97efdaa0ea5202b101c246","objectID":"52c2436ff9726152482b05540ee20b9b","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/tools/logging/"},{"anchor":"#","title":"Restoring from Backup","content":"The details of restoring your cluster from backup are different depending on your version of RKE.\n\n\n  \n  \n  If there is a disaster with your Kubernetes cluster, you can use rke etcd snapshot-restore to recover your etcd. This command reverts etcd to a specific snapshot and should be run on an etcd node of the the specific cluster that has suffered the disaster.\n\nThe following actions will be performed when you run the command:\n\n\nSyncs the snapshot or downloads the snapshot from S3, if necessary.\nChecks snapshot checksum across etcd nodes to make sure they are identical.\nDeletes your current cluster and cleans old data by running rke remove. This removes the entire Kubernetes cluster, not just the etcd cluster.\nRebuilds the etcd cluster from the chosen snapshot.\nCreates a new cluster by running rke up.\nRestarts cluster system pods.\n\n\n\nWarning: You should back up any important data in your cluster before running rke etcd snapshot-restore because the command deletes your current Kubernetes cluster and replaces it with a new one.\n\n\nThe snapshot used to restore your etcd cluster can either be stored locally in /opt/rke/etcd-snapshots or from a S3 compatible backend.\n\nExample of Restoring from a Local Snapshot\n\nTo restore etcd from a local snapshot, run:\n\n$ rke etcd snapshot-restore --config cluster.yml --name mysnapshot\n\n\nThe snapshot is assumed to be located in /opt/rke/etcd-snapshots.\n\nNote: The pki.bundle.tar.gz file is not needed because RKE v0.2.0 changed how the Kubernetes cluster state is stored.\n\nExample of Restoring from a Snapshot in S3\n\n\nPrerequisite: Ensure your cluster.rkestate is present before starting the restore, because this contains your certificate data for the cluster.\n\n\nWhen restoring etcd from a snapshot located in S3, the command needs the S3 information in order to connect to the S3 backend and retrieve the snapshot.\n$ rke etcd snapshot-restore \\\n--config cluster.yml \\\n--name snapshot-name \\\n--s3 \\\n--access-key S3_ACCESS_KEY \\\n--secret-key S3_SECRET_KEY \\\n--bucket-name s3-bucket-name \\\n--folder s3-folder-name \\ # Optional - Available as of v0.3.0\n--s3-endpoint s3.amazonaws.com\nNote: if you were restoring a cluster that had Rancher installed, the Rancher UI should start up after a few minutes; you don’t need to re-run Helm.\n\nOptions for rke etcd snapshot-restore\n\n\n\n\nOption\nDescription\nS3 Specific\n\n\n\n\n\n--name value\nSpecify snapshot name\n\n\n\n\n--config value\nSpecify an alternate cluster YAML file (default: cluster.yml) [$RKE_CONFIG]\n\n\n\n\n--s3\nEnabled backup to s3\n*\n\n\n\n--s3-endpoint value\nSpecify s3 endpoint url (default: “s3.amazonaws.com”)\n*\n\n\n\n--access-key value\nSpecify s3 accessKey\n*\n\n\n\n--secret-key value\nSpecify s3 secretKey\n*\n\n\n\n--bucket-name value\nSpecify s3 bucket name\n*\n\n\n\n--folder value\nSpecify folder inside  bucket where backup will be stored. This is optional.  This is optional. Available as of v0.3.0\n*\n\n\n\n--region value\nSpecify the s3 bucket location (optional)\n*\n\n\n\n--ssh-agent-auth\nUse SSH Agent Auth defined by SSH_AUTH_SOCK\n\n\n\n\n--ignore-docker-version\nDisable Docker version check\n\n\n\n\n\n\n\n\n  If there is a disaster with your Kubernetes cluster, you can use rke etcd snapshot-restore to recover your etcd. This command reverts etcd to a specific snapshot and should be run on an etcd node of the the specific cluster that has suffered the disaster.\n\nThe following actions will be performed when you run the command:\n\n\nRemoves the old etcd cluster\nRebuilds the etcd cluster using the local snapshot\n\n\nBefore you run this command, you must:\n\n\nRun rke remove to remove your Kubernetes cluster and clean the nodes\nDownload your etcd snapshot from S3, if applicable. Place the etcd snapshot and the pki.bundle.tar.gz file in /opt/rke/etcd-snapshots. Manually sync the snapshot across all etcd nodes.\n\n\nAfter the restore, you must rebuild your Kubernetes cluster with rke up.\n\n\nWarning: You should back up any important data in your cluster before running rke etcd snapshot-restore because the command deletes your current etcd cluster and replaces it with a new one.\n\n\nExample of Restoring from a Local Snapshot\n\nTo restore etcd from a local snapshot, run:\n\n$ rke etcd snapshot-restore --config cluster.yml --name mysnapshot\n\n\nThe snapshot is assumed to be located in /opt/rke/etcd-snapshots.\n\nThe snapshot must be manually synched across all etcd nodes.\n\nThe pki.bundle.tar.gz file is also expected to be in the same location.\n\nOptions for rke etcd snapshot-restore\n\n\n\n\nOption\nDescription\n\n\n\n\n\n--name value\nSpecify snapshot name\n\n\n\n--config value\nSpecify an alternate cluster YAML file (default: cluster.yml) [$RKE_CONFIG]\n\n\n\n--ssh-agent-auth\nUse SSH Agent Auth defined by SSH_AUTH_SOCK\n\n\n\n--ignore-docker-version\nDisable Docker version check\n\n\n\n\n\n\n\n\n","postref":"9cc89efa79354256ea80d0d065155dff","objectID":"065518b3031f755b2f9ef0740ae412dd","permalink":"http://jijeesh.github.io/docs/rke/latest/en/etcd-snapshots/restoring-from-backup/"},{"anchor":"#","title":"Role-based Access Control","content":"This section describes the permissions required to access Istio features and how to configure access to the Kiali and Jaeger visualizations.\n\nCluster-level Access\n\nBy default, only cluster administrators can:\n\n\nEnable Istio for the cluster\nConfigure resource allocations for Istio\nView each UI for Prometheus, Grafana, Kiali, and Jaeger\n\n\nProject-level Access\n\nAfter Istio is enabled in a cluster, project owners and members have permission to:\n\n\nEnable and disable Istio sidecar auto-injection for namespaces\nAdd the Istio sidecar to workloads\nView the traffic metrics and traffic graph for the cluster\nView the Kiali and Jaeger visualizations if cluster administrators give access to project members\nConfigure Istio’s resources (such as the gateway, destination rules, or virtual services) with kubectl (This does not apply to read-only project members)\n\n\nAccess to Visualizations\n\nBy default,  the Kiali and Jaeger visualizations are restricted to the cluster owner because the information in them could be sensitive.\n\nJaeger provides a UI for a distributed tracing system, which is useful for root cause analysis and for determining what causes poor performance.\n\nKiali provides a diagram that shows the services within a service mesh and how they are connected.\n\nRancher supports giving groups permission to access Kiali and Jaeger, but not individuals.\n\nTo configure who has permission to access the Kiali and Jaeger UI,\n\n\nGo to the cluster view and click Tools > Istio.\nThen go to the Member Access section. If you want to restrict access to certain groups, choose Allow cluster owner and specified members to access Kiali and Jaeger UI. Search for the groups that you want to have access to Kiali and Jaeger. If you want all members to have access to the tools, click Allow all members to access Kiali and Jaeger UI.\nClick Save.\n\n\nResult: The access levels for Kiali and Jaeger have been updated.\n\nSummary of Default Permissions for Istio Users\n\n\n\n\nPermission\nCluster Administrators\nProject Owners\nProject Members\nRead-only Project Members\n\n\n\n\n\nEnable and disable Istio for the cluster\n✓\n\n\n\n\n\n\nConfigure Istio resource limits\n✓\n\n\n\n\n\n\nControl who has access to Kiali and the Jaeger UI\n✓\n\n\n\n\n\n\nEnable and disable Istio for a namespace\n✓\n✓\n✓\n\n\n\n\nEnable and disable Istio on workloads\n✓\n✓\n✓\n\n\n\n\nConfigure Istio with kubectl\n✓\n✓\n✓\n\n\n\n\nView Prometheus UI and Grafana UI\n✓\n\n\n\n\n\n\nView Kiali UI and Jaeger UI (Configurable)\n✓\n\n\n\n\n\n\nView Istio project dashboard, including traffic metrics*\n✓\n✓\n✓\n✓\n\n\n\n\n\nBy default, only the cluster owner will see the traffic graph. Project members will see only a subset of traffic metrics. Project members cannot see the traffic graph because it comes from Kiali, and access to Kiali is restricted to cluster owners by default.\n\n","postref":"00cafb755794ec85ad988980b40a35f0","objectID":"b1115f72ecb3217e3aa8db7b54be95eb","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/tools/istio/rbac/"},{"anchor":"#","title":"Setting Container Default Resource Limits","content":"Available as of v2.2.0\n\nWhen setting resource quotas, if you set anything related to CPU or Memory (i.e. limits or reservations) on a project / namespace, all containers will require a respective CPU or Memory field set during creation. See the Kubernetes documentation for more details on why this is required.\n\nTo avoid setting these limits on each and every container during workload creation, a default container resource limit can be specified on the namespace.\n\nEditing the Container Default Resource Limit\n\nAvailable as of v2.2.0\n\nEdit container default resource limit when:\n\n\nYou have a CPU or Memory resource quota set on a project, and want to supply the corresponding default values for a container.\nYou want to edit the default container resource limit.\n\n\n\nFrom the Global view, open the cluster containing the project to which you want to edit the container default resource limit.\nFrom the main menu, select Projects/Namespaces.\nFind the project that you want to edit the container default resource limit. From that project, select Ellipsis (…) > Edit.\nExpand Container Default Resource Limit and edit the values.\n\n\nResource Limit Propagation\n\nWhen the default container resource limit is set at a project level, the parameter will be propagated to any namespace created in the project after the limit has been set. For any existing namespace in a project, this limit will not be automatically propagated. You will need to manually set the default container resource limit for any existing namespaces in the project in order for it to be used when creating any containers.\n\n\nNote: Prior to v2.2.0, you could not launch catalog applications that did not have any limits set. With v2.2.0, you can set a default container resource limit on a project and launch any catalog applications.\n\n\nOnce a container default resource limit is configured on a namespace, the default will be pre-populated for any containers created in that namespace. These limits/reservations can always be overridden during workload creation.\n\nContainer Resource Quota Types\n\nThe following resource limits can be configured:\n\n\n\n\nResource Type\nDescription\n\n\n\n\n\nCPU Limit\nThe maximum amount of CPU (in millicores) allocated to the container.\n\n\n\nCPU Reservation\nThe minimum amount of CPU (in millicores) guaranteed to the container.\n\n\n\nMemory Limit\nThe maximum amount of memory (in bytes) allocated to the container.\n\n\n\nMemory Reservation\nThe minimum amount of memory (in bytes) guaranteed to the container.\n\n\n\n","postref":"d3c388dd0dc8283dcf5fa2b2cc3b946c","objectID":"fff18011608684e99338095db1d63304","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/project-admin/resource-quotas/override-container-default/"},{"anchor":"#","title":"Troubleshooting nginx-proxy","content":"The nginx-proxy container is deployed on every node that does not have the controlplane role. It provides access to all the nodes with the controlplane role by dynamically generating the NGINX configuration based on available nodes with the controlplane role.\n\nCheck if the Container is Running\n\nThe container is called nginx-proxy and should have status Up. The duration shown after Up is the time the container has been running.\n\ndocker ps -a -f=name=nginx-proxy\n\n\nExample output:\n\ndocker ps -a -f=name=nginx-proxy\nCONTAINER ID        IMAGE                       COMMAND                  CREATED             STATUS              PORTS               NAMES\nc3e933687c0e        rancher/rke-tools:v0.1.15   \"nginx-proxy CP_HO...\"   3 hours ago         Up 3 hours                              nginx-proxy\n\n\nCheck Generated NGINX Configuration\n\nThe generated configuration should include the IP addresses of the nodes with the controlplane role. The configuration can be checked using the following command:\n\ndocker exec nginx-proxy cat /etc/nginx/nginx.conf\n\n\nExample output:\n\nerror_log stderr notice;\n\nworker_processes auto;\nevents {\n  multi_accept on;\n  use epoll;\n  worker_connections 1024;\n}\n\nstream {\n        upstream kube_apiserver {\n            \n            server ip_of_controlplane_node1:6443;\n            \n            server ip_of_controlplane_node2:6443;\n            \n        }\n\n        server {\n            listen        6443;\n            proxy_pass    kube_apiserver;\n            proxy_timeout 30;\n            proxy_connect_timeout 2s;\n\n        }\n\n}\n\n\nnginx-proxy Container Logging\n\nThe logging of the containers can contain information on what the problem could be.\n\ndocker logs nginx-proxy\n\n","postref":"7ee6077d70f1996b89da79fc651981cd","objectID":"281b4a8e0c2e2f0c744d92cadb5a55ab","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/troubleshooting/kubernetes-components/nginx-proxy/"},{"anchor":"#scaling-etcd-disk-performance","title":"Scaling etcd disk performance","content":"You can follow the recommendations from the etcd docs on how to tune the disk priority on the host.Additionally, to reduce IO contention on the disks for etcd, you can use a dedicated device for the data and wal directory. Based on etcd best practices, mirroring RAID configurations are unnecessary because etcd replicates data between the nodes in the cluster. You can use stripping RAID configurations to increase available IOPS.To implement this solution in an RKE cluster, the /var/lib/etcd/data and /var/lib/etc/wal directories will need to have disks mounted and formatted on the underlying host. In the extra_args directive of the etcd service, you must include the wal_dir directory. Without specifying the wal_dir, etcd process will try to manipulate the underlying wal mount with insufficient permissions.# RKE cluster.yml\n---\nservices:\n  etcd:\n    extra_args:\n      data-dir: '/var/lib/rancher/etcd/data/'\n      wal-dir: '/var/lib/rancher/etcd/wal/wal_dir'\n    extra_binds:\n      - '/var/lib/etcd/data:/var/lib/rancher/etcd/data'\n      - '/var/lib/etcd/wal:/var/lib/rancher/etcd/wal'","postref":"cd12c77835d18f80838e94ab8e6b4198","objectID":"4f2ae193af9c7d7b3c7c6087c6a99bc3","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/etcd/"},{"anchor":"#","title":"Upgrading","content":"If RancherOS has released a new version and you want to learn how to upgrade your OS, we make it easy using the ros os command.\n\nSince RancherOS is a kernel and initrd, the upgrade process is downloading a new kernel and initrd, and updating the boot loader to point to it. The old kernel and initrd are not removed. If there is a problem with your upgrade, you can select the old kernel from the Syslinux bootloader.\n\nBefore upgrading to any version, please review the release notes on our releases page in GitHub to review any updates in the release.\n\n\nNote: If you are using docker-machine then you will not be able to upgrade your RancherOS version. You need to delete and re-create the machine.\n\n\nVersion Control\n\nFirst, let’s check what version you have running on your system.\n\n$ sudo ros os version\nv0.4.5\n\n\nIf you just want to find out the available releases from the command line, it’s a simple command.\n\n# List all available releases\n$ sudo ros os list\nrancher/os:v0.4.0 remote\nrancher/os:v0.4.1 remote\nrancher/os:v0.4.2 remote\nrancher/os:v0.4.3 remote\nrancher/os:v0.4.4 remote\nrancher/os:v0.4.5 remote\nrancher/os:v0.5.0 local\n\n\nThe local/remote label shows which images are available to System Docker locally versus which need to be pulled from Docker Hub. If you choose to upgrade to a version that is remote, we will automatically pull that image during the upgrade.\n\nUpgrading\n\nLet’s walk through upgrading! The ros os upgrade command will automatically upgrade to the current release of RancherOS. The current release is designated as the most recent release of RancherOS.\n\n$ sudo ros os upgrade\nUpgrading to rancher/os:v0.5.0\n\n\nConfirm that you want to continue and the final step will be to confirm that you want to reboot.\n\nContinue [y/N]: y\n...\n...\n...\nContinue with reboot [y/N]: y\nINFO[0037] Rebooting\n\n\nAfter rebooting, you can check that your version has been updated.\n\n$ sudo ros -v\nros version v0.5.0\n\n\n\nNote: If you are booting from ISO and have not installed to disk, your upgrade will not be saved. You can view our guide to installing to disk.\n\n\nUpgrading to a Specific Version\n\nIf you are a couple of versions behind the current version, use the -i option to pick the version that you want to upgrade to.\n\n$ sudo ros os upgrade -i rancher/os:v0.5.0\nUpgrading to rancher/os:v0.5.0\nContinue [y/N]: y\n...\n...\n...\nContinue with reboot [y/N]: y\nINFO[0082] Rebooting\n\n\nBypassing The Prompts\n\nWe have added the ability to bypass the prompts. Use the -f or --force option when upgrading. Your machine will automatically be rebooted and you’ll just need to log back in when it’s done.\n\nIf you want to bypass the prompts, but you don’t want to immediately reboot, you can add --no-reboot to avoid rebooting immediately.\n\nRolling back an Upgrade\n\nIf you’ve upgraded your RancherOS and something’s not working anymore, you can easily rollback your upgrade.\n\nThe ros os upgrade command works for rolling back. We’ll use the -i option to “upgrade” to a specific version. All you need to do is pick the previous version! Same as before, you will be prompted to confirm your upgrade version as well as confirm your reboot.\n\n$ sudo ros -v\nros version v0.4.5\n$ sudo ros os upgrade -i rancher/os:v0.4.4\nUpgrading to rancher/os:v0.4.4\nContinue [y/N]: y\n...\n...\n...\nContinue with reboot [y/N]: y\nINFO[0082] Rebooting\n\n\nAfter rebooting, the rollback will be complete.\n\n$ sudo ros -v\nros version 0.4.4\n\n\n\n\n\nNote: If you are using a persistent console and in the current version’s console, rolling back is not supported. For example, rolling back to v0.4.5 when using a v0.5.0 persistent console is not supported.\n\n\nStaging an Upgrade\n\nDuring an upgrade, the template of the upgrade is downloaded from the rancher/os repository. You can download this template ahead of time so that it’s saved locally. This will decrease the time it takes to upgrade. We’ll use the -s option to stage the specific template. You will need to specify the image name with the -i option, otherwise it will automatically stage the current version.\n\n$ sudo ros os upgrade -s -i rancher/os:v0.5.0\n\n\nCustom Upgrade Sources\n\nIn the upgrade key, the url is used to find the list of available and current versions of RancherOS. This can be modified to track custom builds and releases.\n#cloud-config\nrancher:\n  upgrade:\n    url: https://releases.rancher.com/os/releases.yml\n    image: rancher/os\nUpgrade Notes for v1.4.0+\n\nIf you are upgrading to v1.4.0+, please review these notes that could alter your RancherOS settings.\n\nDue to changes in the location of user-docker’s data-root, after upgrading to v1.4.0+, you must move or copy the files of user-docker’s data-root. If you do not do this, your data will NOT be available.\n\n#!/bin/bash\n\nold_docker_root=\"/proc/1/root/var/lib/docker\"\nnew_docker_root=\"/proc/1/root/var/lib/user-docker\"\n\nsystem-docker stop docker\ncp -a $old_docker_root/* $new_docker_root\nsystem-docker start docker\n\n\nIf you had another bridge IP set for system-docker, you may need to explicitly set it again depending on your upgrade path. Before re-setting it, you can confirm if it’s set.\n\n# Check to see if docker bridge IP is set\n$ sudo ros config get rancher.system_docker.bip\n\n# If it is no longer set, re-set the setting\n$ sudo ros config set rancher.system_docker.bip 10.0.0.1/16\n\n","postref":"30ffa52aa547de4502841f3a9f03da17","objectID":"2c288d6cf6ffd1595dd3b7bbd894229b","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/upgrading/"},{"anchor":"#","title":"vSphere Configuration Reference","content":"This section shows an example of how to configure the vSphere cloud provider.\n\nThe vSphere cloud provider must be enabled to allow dynamic provisioning of volumes.\n\nFor more details on deploying a Kubernetes cluster on vSphere, refer to the official cloud provider documentation.\n\n\nNote: This documentation reflects the new vSphere Cloud Provider configuration schema introduced in Kubernetes v1.9 which differs from previous versions.\n\n\nvSphere Configuration Example\n\nGiven the following:\n\n\nVMs in the cluster are running in the same datacenter eu-west-1 managed by the vCenter vc.example.com.\nThe vCenter has a user provisioner with password secret with the required roles assigned, see Prerequisites.\nThe vCenter has a datastore named ds-1 which should be used to store the VMDKs for volumes.\nA vm/kubernetes folder exists in vCenter.\n\n\nThe corresponding configuration for the provider would then be as follows:\n(...)\ncloud_provider:\n  name: vsphere\n  vsphereCloudProvider:\n    virtual_center:\n      vc.example.com:\n        user: provisioner\n        password: secret\n        port: 443\n        datacenters: /us-west-1\n    workspace:\n      server: vc.example.com\n      folder: /us-west-1/folder/myvmfolder\n      default-datastore: /us-west-1/datastore/ds-1\n      datacenter: /us-west-1\n      resourcepool-path: /us-west-1/host/hn1/resources/myresourcepool\nConfiguration Options\n\nThe vSphere configuration options are divided into 5 groups:\n\n\nglobal\nvirtual_center\nworkspace\ndisk\nnetwork\n\n\nglobal\n\nThe main purpose of global options is to be able to define a common set of configuration parameters that will be inherited by all vCenters defined under the virtual_center directive unless explicitly defined there.\n\nAccordingly, the global directive accepts the same configuration options that are available under the virtual_center directive. Additionally it accepts a single parameter that can only be specified here:\n\n\n\n\nglobal Options\nType\nRequired\nDescription\n\n\n\n\n\ninsecure-flag\nboolean\n\nSet to true if the vCenter/ESXi uses a self-signed certificate.\n\n\n\n\nExample:\n(...)\n    global:\n      insecure-flag: true\nvirtual_center\n\nThis configuration directive specifies the vCenters that are managing the nodes in the cluster. You must define at least one vCenter/ESXi server. If the nodes span multiple vCenters then all must be defined.\n\nEach vCenter is defined by adding a new entry under the virtual_center directive with the vCenter IP or FQDN as the name. All required parameters must be provided for each vCenter unless they are already defined under the global directive.\n\n\n\n\nvirtual_center Options\nType\nRequired\nDescription\n\n\n\n\n\nuser\nstring\n*\nvCenter/ESXi user used to authenticate with this server.\n\n\n\npassword\nstring\n*\nUser’s password.\n\n\n\nport\nstring\n\nPort to use to connect to this server. Defaults to 443.\n\n\n\ndatacenters\nstring\n*\nComma-separated list of all datacenters in which cluster nodes are running in.\n\n\n\nsoap-roundtrip-count\nuint\n\nRound tripper count for API requests to the vCenter (num retries = value - 1).\n\n\n\n\n\nThe following additional options (introduced in Kubernetes v1.11) are not yet supported in RKE.\n\n\n\n\n\nvirtual_center Options\nType\nRequired\nDescription\n\n\n\n\n\nsecret-name\nstring\n\nName of secret resource containing credential key/value pairs. Can be specified in lieu of user/password parameters.\n\n\n\nsecret-namespace\nstring\n\nNamespace in which the secret resource was created in.\n\n\n\nca-file\nstring\n\nPath to CA cert file used to verify the vCenter certificate.\n\n\n\n\nExample:\n(...)\n    virtual_center:\n      172.158.111.1: {}  # This vCenter inherits all it's properties from global options\n      172.158.110.2:     # All required options are set explicitly\n        user: vc-user\n        password: othersecret\n        datacenters: eu-west-2\nworkspace\n\nThis configuration group specifies how storage for volumes is created in vSphere.\nThe following configuration options are available:\n\n\n\n\nworkspace Options\nType\nRequired\nDescription\n\n\n\n\n\nserver\nstring\n*\nIP or FQDN of the vCenter/ESXi that should be used for creating the volumes. Must match one of the vCenters defined under the virtual_center directive.\n\n\n\ndatacenter\nstring\n*\nName of the datacenter that should be used for creating volumes. For ESXi enter ha-datacenter.\n\n\n\nfolder\nstring\n*\nPath of folder in which to create dummy VMs used for volume provisioning (relative from the root folder in vCenter), e.g. “vm/kubernetes”.\n\n\n\ndefault-datastore\nstring\n\nName of default datastore to place VMDKs if neither datastore or storage policy are specified in the volume options of a PVC. If datastore is located in a storage folder or is a member of a datastore cluster, specify the full path.\n\n\n\nresourcepool-path\nstring\n\nAbsolute or relative path to the resource pool where the dummy VMs for Storage policy based provisioning should be created. If a relative path is specified, it is resolved with respect to the datacenter’s host folder. Examples: /<dataCenter>/host/<hostOrClusterName>/Resources/<poolName>, Resources/<poolName>. For standalone ESXi specify Resources.\n\n\n\n\nExample:\n(...)\n    workspace:\n      server: 172.158.111.1 # matches IP of vCenter defined in the virtual_center block\n      datacenter: eu-west-1\n      folder: vm/kubernetes\n      default-datastore: ds-1\ndisk\n\nThe following configuration options are available under the disk directive:\n\n\n\n\ndisk Options\nType\nRequired\nDescription\n\n\n\n\n\nscsicontrollertype\nstring\n\nSCSI controller type to use when attaching block storage to VMs. Must be one of: lsilogic-sas or pvscsi. Default: pvscsi.\n\n\n\n\nnetwork\n\nThe following configuration options are available under the network directive:\n\n\n\n\nnetwork Options\nType\nRequired\nDescription\n\n\n\n\n\npublic-network\nstring\n\nName of public VM Network to which the VMs in the cluster are connected. Used to determine public IP addresses of VMs.\n\n\n\n","postref":"aaa8d114716e8719f58709e031a10bf0","objectID":"9529bcd084d3af5d416c30a8c1a8bdfa","permalink":"http://jijeesh.github.io/docs/rke/latest/en/config-options/cloud-providers/vsphere/config-reference/"},{"anchor":"#","title":"4. Add Deployments and Services with the Istio Sidecar","content":"\nPrerequisite: To enable Istio for a workload, the cluster and namespace must have Istio enabled.\n\n\nEnabling Istio in a namespace only enables automatic sidecar injection for new workloads. To enable the Envoy sidecar for existing workloads, you need to enable it manually for each workload.\n\nTo inject the Istio sidecar on an existing workload in the namespace, go to the workload, click the Ellipsis (…), and click Redeploy. When the workload is redeployed, it will have the Envoy sidecar automatically injected.\n\nWait a few minutes for the workload to upgrade to have the istio sidecar. Click it and go to the Containers section. You should be able to see istio-init and istio-proxy alongside your original workload. This means the Istio sidecar is enabled for the workload. Istio is doing all the wiring for the sidecar envoy. Now Istio can do all the features automatically if you enable them in the yaml.\n\n3. Add Deployments and Services\n\nNext we add the Kubernetes resources for the sample deployments and services for the BookInfo app in Istio’s documentation.\n\n\nGo to the project inside the cluster you want to deploy the workload on.\nIn Workloads, click Import YAML.\nCopy the below resources into the form.\nClick Import.\n\n\nThis will set up the following sample resources from Istio’s example BookInfo app:\n\nDetails service and deployment:\n\n\nA details Service\nA ServiceAccount for bookinfo-details\nA details-v1 Deployment\n\n\nRatings service and deployment:\n\n\nA ratings Service\nA ServiceAccount for bookinfo-ratings\nA ratings-v1 Deployment\n\n\nReviews service and deployments (three versions):\n\n\nA reviews Service\nA ServiceAccount for bookinfo-reviews\nA reviews-v1 Deployment\nA reviews-v2 Deployment\nA reviews-v3 Deployment\n\n\nProductpage service and deployment:\n\nThis is the main page of the app, which will be visible from a web browser. The other services will be called from this page.\n\n\nA productpage service\nA ServiceAccount for bookinfo-productpage\nA productpage-v1 Deployment\n\n\nResource YAML\n# Copyright 2017 Istio Authors\n#\n#   Licensed under the Apache License, Version 2.0 (the \"License\");\n#   you may not use this file except in compliance with the License.\n#   You may obtain a copy of the License at\n#\n#       http://www.apache.org/licenses/LICENSE-2.0\n#\n#   Unless required by applicable law or agreed to in writing, software\n#   distributed under the License is distributed on an \"AS IS\" BASIS,\n#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#   See the License for the specific language governing permissions and\n#   limitations under the License.\n\n##################################################################################################\n# Details service\n##################################################################################################\napiVersion: v1\nkind: Service\nmetadata:\n  name: details\n  labels:\n    app: details\n    service: details\nspec:\n  ports:\n  - port: 9080\n    name: http\n  selector:\n    app: details\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: bookinfo-details\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: details-v1\n  labels:\n    app: details\n    version: v1\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: details\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: details\n        version: v1\n    spec:\n      serviceAccountName: bookinfo-details\n      containers:\n      - name: details\n        image: docker.io/istio/examples-bookinfo-details-v1:1.15.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9080\n---\n##################################################################################################\n# Ratings service\n##################################################################################################\napiVersion: v1\nkind: Service\nmetadata:\n  name: ratings\n  labels:\n    app: ratings\n    service: ratings\nspec:\n  ports:\n  - port: 9080\n    name: http\n  selector:\n    app: ratings\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: bookinfo-ratings\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ratings-v1\n  labels:\n    app: ratings\n    version: v1\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ratings\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: ratings\n        version: v1\n    spec:\n      serviceAccountName: bookinfo-ratings\n      containers:\n      - name: ratings\n        image: docker.io/istio/examples-bookinfo-ratings-v1:1.15.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9080\n---\n##################################################################################################\n# Reviews service\n##################################################################################################\napiVersion: v1\nkind: Service\nmetadata:\n  name: reviews\n  labels:\n    app: reviews\n    service: reviews\nspec:\n  ports:\n  - port: 9080\n    name: http\n  selector:\n    app: reviews\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: bookinfo-reviews\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: reviews-v1\n  labels:\n    app: reviews\n    version: v1\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: reviews\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: reviews\n        version: v1\n    spec:\n      serviceAccountName: bookinfo-reviews\n      containers:\n      - name: reviews\n        image: docker.io/istio/examples-bookinfo-reviews-v1:1.15.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9080\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: reviews-v2\n  labels:\n    app: reviews\n    version: v2\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: reviews\n      version: v2\n  template:\n    metadata:\n      labels:\n        app: reviews\n        version: v2\n    spec:\n      serviceAccountName: bookinfo-reviews\n      containers:\n      - name: reviews\n        image: docker.io/istio/examples-bookinfo-reviews-v2:1.15.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9080\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: reviews-v3\n  labels:\n    app: reviews\n    version: v3\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: reviews\n      version: v3\n  template:\n    metadata:\n      labels:\n        app: reviews\n        version: v3\n    spec:\n      serviceAccountName: bookinfo-reviews\n      containers:\n      - name: reviews\n        image: docker.io/istio/examples-bookinfo-reviews-v3:1.15.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9080\n---\n##################################################################################################\n# Productpage services\n##################################################################################################\napiVersion: v1\nkind: Service\nmetadata:\n  name: productpage\n  labels:\n    app: productpage\n    service: productpage\nspec:\n  ports:\n  - port: 9080\n    name: http\n  selector:\n    app: productpage\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: bookinfo-productpage\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: productpage-v1\n  labels:\n    app: productpage\n    version: v1\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: productpage\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: productpage\n        version: v1\n    spec:\n      serviceAccountName: bookinfo-productpage\n      containers:\n      - name: productpage\n        image: docker.io/istio/examples-bookinfo-productpage-v1:1.15.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9080\n---\nNext: Set up the Istio Gateway\n","postref":"0bd8e196e425c8529c53998d29092f1d","objectID":"72c41aebcf93d00f9de2795b45e2de61","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/tools/istio/setup/deploy-workloads/"},{"anchor":"#developing","title":"Developing","content":"If you find any bugs or are having any trouble, please contact us by filing an issue.If you have any updates to our documentation, please make any PRs to our docs repo.\nAll of repositories are located within our main GitHub page.RancherOS Repo: This repo contains the bulk of the RancherOS code.RancherOS Services Repo: This repo is where any system-services can be contributed.RancherOS Images Repo: This repo is for the corresponding service images.Development is easiest done with QEMU on Linux. OS X works too, although QEMU doesn’t have KVM support. If you are running Linux in a virtual machine, then we recommend you run VMWare Fusion/Workstation and enable VT-x support.  Then, QEMU will have KVM support and run sufficiently fast inside your Linux VM.BuildingRequirements:\nbash\nmake\nDocker 1.10.3+\n$ make\nThe build will run in Docker containers, and when the build is done, the vmlinuz, initrd, and ISO should be in dist/artifacts.If you’re building a version of RancherOS used for development and not for a release, you can instead run make dev. This will run faster than the standard build by avoiding building the installer.tar and rootfs.tar.gz artifacts which are not needed by QEMU.TestingRun make integration-tests to run the all integration tests in a container, or ./scripts/integration-tests to run them outside a container (they use QEMU to test the OS.)To run just one integration test, or a group of them (using regex’s like .*Console.*, you can set the RUNTEST environment variable:$ RUNTEST=TestPreload make integration-test\nRunningPrerequisites: QEMU, coreutils, cdrtools/genisoimage/mkisofs.\nOn OS X, brew is recommended to install those. On Linux, use your distro package manager.To launch RancherOS in QEMU from your dev version, you can either use make run, or customise the vm using ./scripts/run and its options. You can use --append your.kernel=params here and --cloud-config your-cloud-config.yml to configure the RancherOS instance you’re launching.You can SSH in using ./scripts/ssh.  Your SSH keys should have been populated (if you didn’t provide your own cloud-config) so you won’t need a password.  If you don’t have SSH keys, or something is wrong with your cloud-config, then the password is “rancher”.If you’re on OS X, you can run RancherOS using xhyve instead of QEMU: just pass --xhyve to ./scripts/run and ./scripts/ssh.Debugging and logging.You can enable extra log information in the console by setting them using sudo ros config set,\nor as kernel boot parameters.\nEnable all logging by setting rancher.debug true\nor you can set rancher.docker.debug, rancher.system_docker.debug, rancher.bootstrap_docker.debug, or rancher.log individually.You will also be able to view the debug logging information by running dmesg as root.","postref":"56a0b24be9898c87fdaf6e996e78f3ec","objectID":"e55e920c628582e7ed007c8fc3b303fd","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/about/"},{"anchor":"#","title":"Disabling Istio","content":"This section describes how to disable Istio in a cluster, namespace, or workload.\n\nDisable Istio in a Cluster\n\nTo disable Istio,\n\n\nFrom the Global view, navigate to the cluster that you want to disable Istio for.\nClick Tools > Istio.\nClick Disable, then click the red button again to confirm the disable action.\n\n\nResult: The cluster-istio application in the cluster’s system project gets removed. The Istio sidecar cannot be deployed on any workloads in the cluster.\n\nDisable Istio in a Namespace\n\n\nIn the Rancher UI, go to the project that has the namespace where you want to disable Istio.\nOn the Workloads tab, you will see a list of namespaces and the workloads deployed in them. Go to the namespace where you want to disable and click the Ellipsis (…) > Disable Istio Auto Injection.\n\n\nResult: When workloads are deployed in this namespace, they will not have the Istio sidecar.\n\nRemove the Istio Sidecar from a Workload\n\nDisable Istio in the namespace, then redeploy the workloads with in it. They will be deployed without the Istio sidecar.\n","postref":"9d4793f1f53aecb05fb22be4ece5609a","objectID":"fe75e9a6df28aeef0b30eb1b778db83b","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/tools/istio/disabling-istio/"},{"anchor":"#","title":"Example Scenarios","content":"These example scenarios for backup and restore are different based on your version of RKE.\n\n\n  \n  \n  This walkthrough will demonstrate how to restore an etcd cluster from a local snapshot with the following steps:\n\n\nBack up the cluster\nSimulate a node failure\nAdd a new etcd node to the cluster\nRestore etcd on the new node from the backup\nConfirm that cluster operations are restored\n\n\nIn this example, the Kubernetes cluster was deployed on two AWS nodes.\n\n\n\n\nName\nIP\nRole\n\n\n\n\n\nnode1\n10.0.0.1\n[controlplane, worker]\n\n\n\nnode2\n10.0.0.2\n[etcd]\n\n\n\n\n1. Back Up the Cluster\n\nTake a local snapshot of the Kubernetes cluster.\n\nYou can upload this snapshot directly to an S3 backend with the S3 options.\n\n$ rke etcd snapshot-save --name snapshot.db --config cluster.yml\n\n\n<img\n        srcset=\"http://jijeesh.github.io/docs/img/rke/rke-etcd-backup_hu5fdc6707027e33c146de8439bc4ea0be_196393_200x0_resize_box_2.png 200w, http://jijeesh.github.io/docs/img/rke/rke-etcd-backup_hu5fdc6707027e33c146de8439bc4ea0be_196393_400x0_resize_box_2.png 400w, http://jijeesh.github.io/docs/img/rke/rke-etcd-backup_hu5fdc6707027e33c146de8439bc4ea0be_196393_600x0_resize_box_2.png 600w, http://jijeesh.github.io/docs/img/rke/rke-etcd-backup_hu5fdc6707027e33c146de8439bc4ea0be_196393_800x0_resize_box_2.png 800w, http://jijeesh.github.io/docs/img/rke/rke-etcd-backup_hu5fdc6707027e33c146de8439bc4ea0be_196393_1000x0_resize_box_2.png 1000w\"\n        src=\"http://jijeesh.github.io/docs/img/rke/rke-etcd-backup_hu5fdc6707027e33c146de8439bc4ea0be_196393_600x0_resize_box_2.png\"\n        alt =\"etcd snapshot\"\n>\n\n\n2. Simulate a Node Failure\n\nTo simulate the failure, let’s power down node2.\n\nroot@node2:~# poweroff\n\n\n\n\n\nName\nIP\nRole\n\n\n\n\n\nnode1\n10.0.0.1\n[controlplane, worker]\n\n\n\nnode2\n10.0.0.2\n[etcd]\n\n\n\n\n3. Add a New etcd Node to the Kubernetes Cluster\n\nBefore updating and restoring etcd, you will need to add the new node into the Kubernetes cluster with the etcd role. In the cluster.yml, comment out the old node and add in the new node.\nnodes:\n    - address: 10.0.0.1\n      hostname_override: node1\n      user: ubuntu\n      role:\n        - controlplane\n        - worker\n#    - address: 10.0.0.2\n#      hostname_override: node2\n#      user: ubuntu\n#      role:\n#       - etcd\n    - address: 10.0.0.3\n      hostname_override: node3\n      user: ubuntu\n      role:\n        - etcd\n4. Restore etcd on the New Node from the Backup\n\n\nPrerequisite: Ensure your cluster.rkestate is present before starting the restore, because this contains your certificate data for the cluster.\n\n\nAfter the new node is added to the cluster.yml, run the rke etcd snapshot-restore to launch etcd from the backup:\n\n$ rke etcd snapshot-restore --name snapshot.db --config cluster.yml\n\n\nThe snapshot is expected to be saved at /opt/rke/etcd-snapshots.\n\nIf you want to directly retrieve the snapshot from S3, add in the S3 options.\n\n\nNote: As of v0.2.0, the file pki.bundle.tar.gz is no longer required for the restore process because the certificates required to restore are preserved within the cluster.rkestate.\n\n\n5. Confirm that Cluster Operations are Restored\n\nThe rke etcd snapshot-restore command triggers rke up using the new cluster.yml. Confirm that your Kubernetes cluster is functional by checking the pods on your cluster.\n\n> kubectl get pods                                                    \nNAME                     READY     STATUS    RESTARTS   AGE\nnginx-65899c769f-kcdpr   1/1       Running   0          17s\nnginx-65899c769f-pc45c   1/1       Running   0          17s\nnginx-65899c769f-qkhml   1/1       Running   0          17s\n\n\n\n\n\n  This walkthrough will demonstrate how to restore an etcd cluster from a local snapshot with the following steps:\n\n\nTake a local snapshot of the cluster\nStore the snapshot externally\nSimulate a node failure\nRemove the Kubernetes cluster and clean the nodes\nRetrieve the backup and place it on a new node\nAdd a new etcd node to the Kubernetes cluster\nRestore etcd on the new node from the backup\nRestore Operations on the Cluster\n\n\nExample Scenario of restoring from a Local Snapshot\n\nIn this example, the Kubernetes cluster was deployed on two AWS nodes.\n\n\n\n\nName\nIP\nRole\n\n\n\n\n\nnode1\n10.0.0.1\n[controlplane, worker]\n\n\n\nnode2\n10.0.0.2\n[etcd]\n\n\n\n\n\n\n1. Take a Local Snapshot of the Cluster\n\nBack up the Kubernetes cluster by taking a local snapshot:\n\n$ rke etcd snapshot-save --name snapshot.db --config cluster.yml\n\n\n<img\n        srcset=\"http://jijeesh.github.io/docs/img/rke/rke-etcd-backup_hu5fdc6707027e33c146de8439bc4ea0be_196393_200x0_resize_box_2.png 200w, http://jijeesh.github.io/docs/img/rke/rke-etcd-backup_hu5fdc6707027e33c146de8439bc4ea0be_196393_400x0_resize_box_2.png 400w, http://jijeesh.github.io/docs/img/rke/rke-etcd-backup_hu5fdc6707027e33c146de8439bc4ea0be_196393_600x0_resize_box_2.png 600w, http://jijeesh.github.io/docs/img/rke/rke-etcd-backup_hu5fdc6707027e33c146de8439bc4ea0be_196393_800x0_resize_box_2.png 800w, http://jijeesh.github.io/docs/img/rke/rke-etcd-backup_hu5fdc6707027e33c146de8439bc4ea0be_196393_1000x0_resize_box_2.png 1000w\"\n        src=\"http://jijeesh.github.io/docs/img/rke/rke-etcd-backup_hu5fdc6707027e33c146de8439bc4ea0be_196393_600x0_resize_box_2.png\"\n        alt =\"etcd snapshot\"\n>\n\n\n\n\n2. Store the Snapshot Externally\n\nAfter taking the etcd snapshot on node2, we recommend saving this backup in a persistent place. One of the options is to save the backup and pki.bundle.tar.gz file on an S3 bucket or tape backup.\n\n# If you're using an AWS host and have the ability to connect to S3\nroot@node2:~# s3cmd mb s3://rke-etcd-backup\nroot@node2:~# s3cmd \\\n  /opt/rke/etcd-snapshots/snapshot.db \\\n  /opt/rke/etcd-snapshots/pki.bundle.tar.gz \\\n  s3://rke-etcd-backup/\n\n\n\n\n3. Simulate a Node Failure\n\nTo simulate the failure, let’s power down node2.\n\nroot@node2:~# poweroff\n\n\n\n\n\nName\nIP\nRole\n\n\n\n\n\nnode1\n10.0.0.1\n[controlplane, worker]\n\n\n\nnode2\n10.0.0.2\n[etcd]\n\n\n\n\n\n\n4. Remove the Kubernetes Cluster and Clean the Nodes\n\nThe following command removes your cluster and cleans the nodes so that the cluster can be restored without any conflicts:\n\nrke remove --config rancher-cluster.yml\n\n\n\n\n5. Retrieve the Backup and Place it On a New Node\n\nBefore restoring etcd and running rke up, we need to retrieve the backup saved on S3 to a new node, e.g. node3.\n\n# Make a Directory\nroot@node3:~# mkdir -p /opt/rke/etcdbackup\n\n# Get the Backup from S3\nroot@node3:~# s3cmd get \\\n  s3://rke-etcd-backup/snapshot.db \\\n  /opt/rke/etcd-snapshots/snapshot.db\n\n# Get the pki bundle from S3\nroot@node3:~# s3cmd get \\\n  s3://rke-etcd-backup/pki.bundle.tar.gz \\\n  /opt/rke/etcd-snapshots/pki.bundle.tar.gz\n\n\n\nNote: If you had multiple etcd nodes, you would have to manually sync the snapshot and pki.bundle.tar.gz across all of the etcd nodes in the cluster.\n\n\n\n\n6. Add a New etcd Node to the Kubernetes Cluster\n\nBefore updating and restoring etcd, you will need to add the new node into the Kubernetes cluster with the etcd role. In the cluster.yml, comment out the old node and add in the new node. `\nnodes:\n    - address: 10.0.0.1\n      hostname_override: node1\n      user: ubuntu\n      role:\n        - controlplane\n        - worker\n#    - address: 10.0.0.2\n#      hostname_override: node2\n#      user: ubuntu\n#      role:\n#       - etcd\n    - address: 10.0.0.3\n      hostname_override: node3\n      user: ubuntu\n      role:\n        - etcd\n\n\n7. Restore etcd on the New Node from the Backup\n\nAfter the new node is added to the cluster.yml, run the rke etcd snapshot-restore command to launch etcd from the backup:\n\n$ rke etcd snapshot-restore --name snapshot.db --config cluster.yml\n\n\nThe snapshot and pki.bundle.tar.gz file are expected to be saved at /opt/rke/etcd-snapshots on each etcd node.\n\n\n\n8. Restore Operations on the Cluster\n\nFinally, we need to restore the operations on the cluster. We will make the Kubernetes API point to the new etcd by running rke up again using the new cluster.yml.\n\n$ rke up --config cluster.yml\n\n\nConfirm that your Kubernetes cluster is functional by checking the pods on your cluster.\n\n> kubectl get pods                                                    \nNAME                     READY     STATUS    RESTARTS   AGE\nnginx-65899c769f-kcdpr   1/1       Running   0          17s\nnginx-65899c769f-pc45c   1/1       Running   0          17s\nnginx-65899c769f-qkhml   1/1       Running   0          17s\n\n\n\n\n\n\n","postref":"d53c9b6d5a4613ebae438c2ab1090dab","objectID":"176a83c92dedc9ab051aab415954a444","permalink":"http://jijeesh.github.io/docs/rke/latest/en/etcd-snapshots/example-scenarios/"},{"anchor":"#in-this-document","title":"In This Document","content":"\nCPU Utilization\n\n\n\n\nCatalog\nExpression\n\n\n\n\n\nDetail\ncfs throttled secondssum(rate(container_cpu_cfs_throttled_seconds_total{namespace=\"$namespace\",pod_name=~\"$podName\",container_name!=\"\"}[5m])) by (pod_name)user secondssum(rate(container_cpu_user_seconds_total{namespace=\"$namespace\",pod_name=~\"$podName\",container_name!=\"\"}[5m])) by (pod_name)system secondssum(rate(container_cpu_system_seconds_total{namespace=\"$namespace\",pod_name=~\"$podName\",container_name!=\"\"}[5m])) by (pod_name)usage secondssum(rate(container_cpu_usage_seconds_total{namespace=\"$namespace\",pod_name=~\"$podName\",container_name!=\"\"}[5m])) by (pod_name)\n\n\n\nSummary\ncfs throttled secondssum(rate(container_cpu_cfs_throttled_seconds_total{namespace=\"$namespace\",pod_name=~\"$podName\",container_name!=\"\"}[5m]))user secondssum(rate(container_cpu_user_seconds_total{namespace=\"$namespace\",pod_name=~\"$podName\",container_name!=\"\"}[5m]))system secondssum(rate(container_cpu_system_seconds_total{namespace=\"$namespace\",pod_name=~\"$podName\",container_name!=\"\"}[5m]))usage secondssum(rate(container_cpu_usage_seconds_total{namespace=\"$namespace\",pod_name=~\"$podName\",container_name!=\"\"}[5m]))\n\n\n\n\nMemory Utilization\n\n\n\n\nCatalog\nExpression\n\n\n\n\n\nDetail\nsum(container_memory_working_set_bytes{namespace=\"$namespace\",pod_name=~\"$podName\", container_name!=\"\"}) by (pod_name)\n\n\n\nSummary\nsum(container_memory_working_set_bytes{namespace=\"$namespace\",pod_name=~\"$podName\", container_name!=\"\"})\n\n\n\n\nNetwork Packets\n\n\n\n\nCatalog\nExpression\n\n\n\n\n\nDetail\nreceive-packetssum(rate(container_network_receive_packets_total{namespace=\"$namespace\",pod_name=~\"$podName\",container_name!=\"\"}[5m])) by (pod_name)receive-droppedsum(rate(container_network_receive_packets_dropped_total{namespace=\"$namespace\",pod_name=~\"$podName\",container_name!=\"\"}[5m])) by (pod_name)receive-errorssum(rate(container_network_receive_errors_total{namespace=\"$namespace\",pod_name=~\"$podName\",container_name!=\"\"}[5m])) by (pod_name)transmit-packetssum(rate(container_network_transmit_packets_total{namespace=\"$namespace\",pod_name=~\"$podName\",container_name!=\"\"}[5m])) by (pod_name)transmit-droppedsum(rate(container_network_transmit_packets_dropped_total{namespace=\"$namespace\",pod_name=~\"$podName\",container_name!=\"\"}[5m])) by (pod_name)transmit-errorssum(rate(container_network_transmit_errors_total{namespace=\"$namespace\",pod_name=~\"$podName\",container_name!=\"\"}[5m])) by (pod_name)\n\n\n\nSummary\nreceive-packetssum(rate(container_network_receive_packets_total{namespace=\"$namespace\",pod_name=~\"$podName\",container_name!=\"\"}[5m]))receive-droppedsum(rate(container_network_receive_packets_dropped_total{namespace=\"$namespace\",pod_name=~\"$podName\",container_name!=\"\"}[5m]))receive-errorssum(rate(container_network_receive_errors_total{namespace=\"$namespace\",pod_name=~\"$podName\",container_name!=\"\"}[5m]))transmit-packetssum(rate(container_network_transmit_packets_total{namespace=\"$namespace\",pod_name=~\"$podName\",container_name!=\"\"}[5m]))transmit-droppedsum(rate(container_network_transmit_packets_dropped_total{namespace=\"$namespace\",pod_name=~\"$podName\",container_name!=\"\"}[5m]))transmit-errorssum(rate(container_network_transmit_errors_total{namespace=\"$namespace\",pod_name=~\"$podName\",container_name!=\"\"}[5m]))\n\n\n\n\nNetwork I/O\n\n\n\n\nCatalog\nExpression\n\n\n\n\n\nDetail\nreceivesum(rate(container_network_receive_bytes_total{namespace=\"$namespace\",pod_name=~\"$podName\",container_name!=\"\"}[5m])) by (pod_name)transmitsum(rate(container_network_transmit_bytes_total{namespace=\"$namespace\",pod_name=~\"$podName\",container_name!=\"\"}[5m])) by (pod_name)\n\n\n\nSummary\nreceivesum(rate(container_network_receive_bytes_total{namespace=\"$namespace\",pod_name=~\"$podName\",container_name!=\"\"}[5m]))transmitsum(rate(container_network_transmit_bytes_total{namespace=\"$namespace\",pod_name=~\"$podName\",container_name!=\"\"}[5m]))\n\n\n\n\nDisk I/O\n\n\n\n\nCatalog\nExpression\n\n\n\n\n\nDetail\nreadsum(rate(container_fs_reads_bytes_total{namespace=\"$namespace\",pod_name=~\"$podName\",container_name!=\"\"}[5m])) by (pod_name)writesum(rate(container_fs_writes_bytes_total{namespace=\"$namespace\",pod_name=~\"$podName\",container_name!=\"\"}[5m])) by (pod_name)\n\n\n\nSummary\nreadsum(rate(container_fs_reads_bytes_total{namespace=\"$namespace\",pod_name=~\"$podName\",container_name!=\"\"}[5m]))writesum(rate(container_fs_writes_bytes_total{namespace=\"$namespace\",pod_name=~\"$podName\",container_name!=\"\"}[5m]))\n\n\n\nPod Metrics\nCPU Utilization\n\n\n\n\nCatalog\nExpression\n\n\n\n\n\nDetail\ncfs throttled secondssum(rate(container_cpu_cfs_throttled_seconds_total{container_name!=\"POD\",namespace=\"$namespace\",pod_name=\"$podName\", container_name!=\"\"}[5m])) by (container_name)usage secondssum(rate(container_cpu_usage_seconds_total{container_name!=\"POD\",namespace=\"$namespace\",pod_name=\"$podName\", container_name!=\"\"}[5m])) by (container_name)system secondssum(rate(container_cpu_system_seconds_total{container_name!=\"POD\",namespace=\"$namespace\",pod_name=\"$podName\", container_name!=\"\"}[5m])) by (container_name)user secondssum(rate(container_cpu_user_seconds_total{container_name!=\"POD\",namespace=\"$namespace\",pod_name=\"$podName\", container_name!=\"\"}[5m])) by (container_name)\n\n\n\nSummary\ncfs throttled secondssum(rate(container_cpu_cfs_throttled_seconds_total{container_name!=\"POD\",namespace=\"$namespace\",pod_name=\"$podName\", container_name!=\"\"}[5m]))usage secondssum(rate(container_cpu_usage_seconds_total{container_name!=\"POD\",namespace=\"$namespace\",pod_name=\"$podName\", container_name!=\"\"}[5m]))system secondssum(rate(container_cpu_system_seconds_total{container_name!=\"POD\",namespace=\"$namespace\",pod_name=\"$podName\", container_name!=\"\"}[5m]))user secondssum(rate(container_cpu_user_seconds_total{container_name!=\"POD\",namespace=\"$namespace\",pod_name=\"$podName\", container_name!=\"\"}[5m]))\n\n\n\n\nMemory Utilization\n\n\n\n\nCatalog\nExpression\n\n\n\n\n\nDetail\nsum(container_memory_working_set_bytes{container_name!=\"POD\",namespace=\"$namespace\",pod_name=\"$podName\",container_name!=\"\"}) by (container_name)\n\n\n\nSummary\nsum(container_memory_working_set_bytes{container_name!=\"POD\",namespace=\"$namespace\",pod_name=\"$podName\",container_name!=\"\"})\n\n\n\n\nNetwork Packets\n\n\n\n\nCatalog\nExpression\n\n\n\n\n\nDetail\nreceive-packetssum(rate(container_network_receive_packets_total{namespace=\"$namespace\",pod_name=\"$podName\",container_name!=\"\"}[5m]))receive-droppedsum(rate(container_network_receive_packets_dropped_total{namespace=\"$namespace\",pod_name=\"$podName\",container_name!=\"\"}[5m]))receive-errorssum(rate(container_network_receive_errors_total{namespace=\"$namespace\",pod_name=\"$podName\",container_name!=\"\"}[5m]))transmit-packetssum(rate(container_network_transmit_packets_total{namespace=\"$namespace\",pod_name=\"$podName\",container_name!=\"\"}[5m]))transmit-droppedsum(rate(container_network_transmit_packets_dropped_total{namespace=\"$namespace\",pod_name=\"$podName\",container_name!=\"\"}[5m]))transmit-errorssum(rate(container_network_transmit_errors_total{namespace=\"$namespace\",pod_name=\"$podName\",container_name!=\"\"}[5m]))\n\n\n\nSummary\nreceive-packetssum(rate(container_network_receive_packets_total{namespace=\"$namespace\",pod_name=\"$podName\",container_name!=\"\"}[5m]))receive-droppedsum(rate(container_network_receive_packets_dropped_total{namespace=\"$namespace\",pod_name=\"$podName\",container_name!=\"\"}[5m]))receive-errorssum(rate(container_network_receive_errors_total{namespace=\"$namespace\",pod_name=\"$podName\",container_name!=\"\"}[5m]))transmit-packetssum(rate(container_network_transmit_packets_total{namespace=\"$namespace\",pod_name=\"$podName\",container_name!=\"\"}[5m]))transmit-droppedsum(rate(container_network_transmit_packets_dropped_total{namespace=\"$namespace\",pod_name=\"$podName\",container_name!=\"\"}[5m]))transmit-errorssum(rate(container_network_transmit_errors_total{namespace=\"$namespace\",pod_name=\"$podName\",container_name!=\"\"}[5m]))\n\n\n\n\nNetwork I/O\n\n\n\n\nCatalog\nExpression\n\n\n\n\n\nDetail\nreceivesum(rate(container_network_receive_bytes_total{namespace=\"$namespace\",pod_name=\"$podName\",container_name!=\"\"}[5m]))transmitsum(rate(container_network_transmit_bytes_total{namespace=\"$namespace\",pod_name=\"$podName\",container_name!=\"\"}[5m]))\n\n\n\nSummary\nreceivesum(rate(container_network_receive_bytes_total{namespace=\"$namespace\",pod_name=\"$podName\",container_name!=\"\"}[5m]))transmitsum(rate(container_network_transmit_bytes_total{namespace=\"$namespace\",pod_name=\"$podName\",container_name!=\"\"}[5m]))\n\n\n\n\nDisk I/O\n\n\n\n\nCatalog\nExpression\n\n\n\n\n\nDetail\nreadsum(rate(container_fs_reads_bytes_total{namespace=\"$namespace\",pod_name=\"$podName\",container_name!=\"\"}[5m])) by (container_name)writesum(rate(container_fs_writes_bytes_total{namespace=\"$namespace\",pod_name=\"$podName\",container_name!=\"\"}[5m])) by (container_name)\n\n\n\nSummary\nreadsum(rate(container_fs_reads_bytes_total{namespace=\"$namespace\",pod_name=\"$podName\",container_name!=\"\"}[5m]))writesum(rate(container_fs_writes_bytes_total{namespace=\"$namespace\",pod_name=\"$podName\",container_name!=\"\"}[5m]))\n\n\n\nContainer Metrics\nCPU Utilization\n\n\n\n\nCatalog\nExpression\n\n\n\n\n\ncfs throttled seconds\nsum(rate(container_cpu_cfs_throttled_seconds_to","postref":"cc633033db28ccda9b76c61e92097056","objectID":"8ad2aba20f9fd626e949587102b0fd75","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/tools/monitoring/expression/"},{"anchor":"#","title":"FAQ and Troubleshooting","content":"How Do I Know if My Certificates are in PEM Format?\n\nYou can recognize the PEM format by the following traits:\n\n  The file begins with the following header: -----BEGIN CERTIFICATE-----\n  The header is followed by a long string of characters. Like, really long.\n  The file ends with a footer: -----END CERTIFICATE-----\n\n\nPEM Certificate Example:\n\n----BEGIN CERTIFICATE-----\nMIIGVDCCBDygAwIBAgIJAMiIrEm29kRLMA0GCSqGSIb3DQEBCwUAMHkxCzAJBgNV\n... more lines\nVWQqljhfacYPgp8KJUJENQ9h5hZ2nSCrI+W00Jcw4QcEdCI8HL5wmg==\n-----END CERTIFICATE-----\n\n\nPEM Certificate Key Example:\n\n-----BEGIN RSA PRIVATE KEY-----\nMIIGVDCCBDygAwIBAgIJAMiIrEm29kRLMA0GCSqGSIb3DQEBCwUAMHkxCzAJBgNV\n... more lines\nVWQqljhfacYPgp8KJUJENQ9h5hZ2nSCrI+W00Jcw4QcEdCI8HL5wmg==\n-----END RSA PRIVATE KEY-----\n\n\nIf your key looks like the example below, see How Can I Convert My Certificate Key From\n    PKCS8 to PKCS1?\n  \n\n    -----BEGIN PRIVATE KEY-----\nMIIGVDCCBDygAwIBAgIJAMiIrEm29kRLMA0GCSqGSIb3DQEBCwUAMHkxCzAJBgNV\n... more lines\nVWQqljhfacYPgp8KJUJENQ9h5hZ2nSCrI+W00Jcw4QcEdCI8HL5wmg==\n-----END PRIVATE KEY-----\n\n\n    How Can I Convert My Certificate Key From PKCS8 to PKCS1?\n\n    If you are using a PKCS8 certificate key file, Rancher will log the following line:\n\n    ListenConfigController cli-config [listener] failed with : failed to read private key: asn1: structure error: tags don't match (2 vs {class:0 tag:16 length:13 isCompound:true})\n\n\n    To make this work, you will need to convert the key from PKCS8 to PKCS1 using the command below:\n\n    openssl rsa -in key.pem -out convertedkey.pem\n\n\n    You can now use convertedkey.pem as certificate key file for Rancher.\n\n    What is the Order of Certificates if I Want to Add My Intermediate(s)?\n\n    The order of adding certificates is as follows:\n\n    -----BEGIN CERTIFICATE-----\n%YOUR_CERTIFICATE%\n-----END CERTIFICATE-----\n-----BEGIN CERTIFICATE-----\n%YOUR_INTERMEDIATE_CERTIFICATE%\n-----END CERTIFICATE-----\n\n\n    How Do I Validate My Certificate Chain?\n\n    You can validate the certificate chain by using the openssl binary. If the output of the command\n      (see the command example below) ends with Verify return code: 0 (ok), your certificate chain is\n      valid. The ca.pem file must be the same as you added to the rancher/rancher container.\n      When using a certificate signed by a recognized Certificate Authority, you can omit the -CAfile\n      parameter.\n\n    Command:\n    openssl s_client -CAfile ca.pem -connect rancher.yourdomain.com:443\n...\n    Verify return code: 0 (ok)\n\n\n","postref":"c61ce70edb92367785e88656f05475d7","objectID":"cf4582e9ce346967e9d3f8567a366281","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/other-installation-methods/single-node-docker/troubleshooting/"},{"anchor":"#","title":"Integrating Rancher and Prometheus for Cluster Monitoring","content":"Available as of v2.2.0\n\nUsing Rancher, you can monitor the state and processes of your cluster nodes, Kubernetes components, and software deployments through integration with Prometheus, a leading open-source monitoring solution.\n\nThis section covers the following topics:\n\n\nAbout Prometheus\nMonitoring scope\nEnabling cluster monitoring\nResource consumption\n\n\nResource consumption of Prometheus pods\nResource consumption of other pods\n\n\n\nAbout Prometheus\n\nPrometheus provides a time series of your data, which is, according to Prometheus documentation:\n\nYou can configure these services to collect logs at either the cluster level or the project level. This page describes how to enable monitoring for a cluster. For details on enabling monitoring for a project, refer to the project administration section.\n\n\nA stream of timestamped values belonging to the same metric and the same set of labeled dimensions, along with comprehensive statistics and metrics of the monitored cluster.\n\n\nIn other words, Prometheus lets you view metrics from your different Rancher and Kubernetes objects. Using timestamps, Prometheus lets you query and view these metrics in easy-to-read graphs and visuals, either through the Rancher UI or Grafana, which is an analytics viewing platform deployed along with Prometheus.\n\nBy viewing data that Prometheus scrapes from your cluster control plane, nodes, and deployments, you can stay on top of everything happening in your cluster. You can then use these analytics to better run your organization: stop system emergencies before they start, develop maintenance strategies, restore crashed servers, etc.\n\nMulti-tenancy support in terms of cluster-only and project-only Prometheus instances are also supported.\n\nMonitoring Scope\n\nUsing Prometheus, you can monitor Rancher at both the cluster level and project level. For each cluster and project that is enabled for monitoring, Rancher deploys a Prometheus server.\n\n\nCluster monitoring allows you to view the health of your Kubernetes cluster. Prometheus collects metrics from the cluster components below, which you can view in graphs and charts.\n\n\nKubernetes control plane\netcd database\nAll nodes (including workers)\n\n\nProject monitoring allows you to view the state of pods running in a given project. Prometheus collects metrics from the project’s deployed HTTP and TCP/UDP workloads.\n\n\nEnabling Cluster Monitoring\n\nAs an administrator or cluster owner, you can configure Rancher to deploy Prometheus to monitor your Kubernetes cluster.\n\n\nFrom the Global view, navigate to the cluster that you want to configure cluster monitoring.\n\nSelect Tools > Monitoring in the navigation bar.\n\nSelect Enable to show the Prometheus configuration options. Review the resource consumption recommendations to ensure you have enough resources for Prometheus and on your worker nodes to enable monitoring. Enter in your desired configuration options.\n\nClick Save.\n\n\nResult: The Prometheus server will be deployed as well as two monitoring applications. The two monitoring applications, cluster-monitoring and monitoring-operator, are added as an application to the cluster’s system project. After the applications are active, you can start viewing cluster metrics through the Rancher dashboard or directly from Grafana.\n\nResource Consumption\n\nWhen enabling cluster monitoring, you need to ensure your worker nodes and Prometheus pod have enough resources. The tables below provides a guide of how much resource consumption will be used. In larger deployments, it is strongly advised that the monitoring infrastructure be placed on dedicated nodes in the cluster.\n\nResource Consumption of Prometheus Pods\n\nThis table is the resource consumption of the Prometheus pod, which is based on the number of all the nodes in the cluster. The count of nodes includes the worker, control plane and etcd nodes. Total disk space allocation should be approximated by the rate * retention period set at the cluster level. When enabling cluster level monitoring, you should adjust the CPU and Memory limits and reservation.\n\n\n\n\nNumber of Cluster Nodes\nCPU (milli CPU)\nMemory\nDisk\n\n\n\n\n\n5\n500\n650 MB\n~1 GB/Day\n\n\n\n50\n2000\n2 GB\n~5 GB/Day\n\n\n\n256\n4000\n6 GB\n~18 GB/Day\n\n\n\n\nAdditional pod resource requirements for cluster level monitoring.\n\n\n\n\nWorkload\nContainer\nCPU - Request\nMem - Request\nCPU - Limit\nMem - Limit\nConfigurable\n\n\n\n\n\nPrometheus\nprometheus\n750m\n750Mi\n1000m\n1000Mi\nY\n\n\n\n\nprometheus-proxy\n50m\n50Mi\n100m\n100Mi\nY\n\n\n\n\nprometheus-auth\n100m\n100Mi\n500m\n200Mi\nY\n\n\n\n\nprometheus-config-reloader\n-\n-\n50m\n50Mi\nN\n\n\n\n\nrules-configmap-reloader\n-\n-\n100m\n25Mi\nN\n\n\n\nGrafana\ngrafana-init-plugin-json-copy\n50m\n50Mi\n50m\n50Mi\nY\n\n\n\n\ngrafana-init-plugin-json-modify\n50m\n50Mi\n50m\n50Mi\nY\n\n\n\n\ngrafana\n100m\n100Mi\n200m\n200Mi\nY\n\n\n\n\ngrafana-proxy\n50m\n50Mi\n100m\n100Mi\nY\n\n\n\nKube-State Exporter\nkube-state\n100m\n130Mi\n100m\n200Mi\nY\n\n\n\nNode Exporter\nexporter-node\n200m\n200Mi\n200m\n200Mi\nY\n\n\n\nOperator\nprometheus-operator\n100m\n50Mi\n200m\n100Mi\nY\n\n\n\n\nResource Consumption of Other Pods\n\nBesides the Prometheus pod, there are components that are deployed that require additional resources on the worker nodes.\n\n\n\n\nPod\nCPU (milli CPU)\nMemory (MB)\n\n\n\n\n\nNode Exporter (Per Node)\n100\n30\n\n\n\nKube State Cluster Monitor\n100\n130\n\n\n\nGrafana\n100\n150\n\n\n\nPrometheus Cluster Monitoring Nginx\n50\n50\n\n\n\n","postref":"ff4e98989a906389f459230f4b38c4bd","objectID":"8d05e53d0bb3771b428d3c85d63df013","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/tools/monitoring/"},{"anchor":"#","title":"Kubernetes Concepts","content":"This page explains concepts related to Kubernetes that are important for understanding how Rancher works. The descriptions below provide a simplified interview of Kubernetes components. For more details, refer to the official documentation on Kubernetes components.\n\nThis section covers the following topics:\n\n\nAbout Docker\nAbout Kubernetes\nWhat is a Kubernetes Cluster?\nRoles for Nodes in Kubernetes Clusters\n\n\netcd Nodes\nControlplane Nodes\nWorker Nodes\n\nAbout Helm\n\n\nAbout Docker\n\nDocker is the container packaging and runtime standard. Developers build container images from Dockerfiles and distribute container images from Docker registries. Docker Hub is the most popular public registry. Many organizations also set up private Docker registries. Docker is primarily used to manage containers on individual nodes.\n\n\nNote: Although Rancher 1.6 supported Docker Swarm clustering technology, it is no longer supported in Rancher 2.x due to the success of Kubernetes.\n\n\nAbout Kubernetes\n\nKubernetes is the container cluster management standard. YAML files specify containers and other resources that form an application. Kubernetes performs functions such as scheduling, scaling, service discovery, health check, secret management, and configuration management.\n\nWhat is a Kubernetes Cluster?\n\nA cluster is a group of computers that work together as a single system.\n\nA Kubernetes Cluster is a cluster that uses the Kubernetes container-orchestration system to deploy, maintain, and scale Docker containers, allowing your organization to automate application operations.\n\nRoles for Nodes in Kubernetes Clusters\n\nEach computing resource in a Kubernetes cluster is called a node. Nodes can be either bare-metal servers or virtual machines. Kubernetes classifies nodes into three types: etcd nodes, control plane nodes, and worker nodes.\n\nA Kubernetes cluster consists of at least one etcd, controlplane, and worker node.\n\netcd Nodes\n\nRancher uses etcd as a data store in both single node and high-availability installations. In Kubernetes, etcd is also a role for nodes that store the cluster state.\n\nThe state of a Kubernetes cluster is maintained in etcd.  The etcd nodes run the etcd database.\n\nThe etcd database component is a distributed key-value store used as Kubernetes storage for all cluster data, such as cluster coordination and state management. It is recommended to run etcd on multiple nodes so that there’s always a backup available for failover.\n\nAlthough you can run etcd on just one node, etcd requires a majority of nodes, a quorum, to agree on updates to the cluster state. The cluster should always contain enough healthy etcd nodes to form a quorum. For a cluster with n members, a quorum is (n/2)+1. For any odd-sized cluster, adding one node will always increase the number of nodes necessary for a quorum.\n\nThree etcd nodes is generally sufficient for smaller clusters and five etcd nodes for large clusters.\n\nControlplane Nodes\n\nControlplane nodes run the Kubernetes API server, scheduler, and controller manager. These nodes take care of routine tasks to ensure that your cluster maintains your configuration. Because all cluster data is stored on your etcd nodes, control plane nodes are stateless. You can run control plane on a single node, although two or more nodes are recommended for redundancy. Additionally, a single node can share the control plane and etcd roles.\n\nWorker Nodes\n\nEach worker node runs the following:\n\n\nKubelets: An agent that monitors the state of the node, ensuring your containers are healthy.\nWorkloads: The containers and pods that hold your apps, as well as other types of deployments.\n\n\nWorker nodes also run storage and networking drivers, and ingress controllers when required. You create as many worker nodes as necessary to run your  workloads.\n\nAbout Helm\n\nFor high-availability installations of Rancher, Helm is the tool used to install Rancher on a Kubernetes cluster.\n\nHelm is the package management tool of choice for Kubernetes. Helm charts provide templating syntax for Kubernetes YAML manifest documents. With Helm we can create configurable deployments instead of just using static files. For more information about creating your own catalog of deployments, check out the docs at https://helm.sh/.\n\nFor more information on service accounts and cluster role binding, refer to the Kubernetes documentation.\n","postref":"6a2b5114d36d29dfa7e4fdf407a15fbf","objectID":"694001d802f5352cecd4a1b0ab5d6638","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/overview/concepts/"},{"anchor":"#","title":"Other Installation Methods","content":"Docker Installations\n\nThe single-node Docker installation is for Rancher users that are wanting to test out Rancher. Instead of running on a Kubernetes cluster using Helm, you install the Rancher server component on a single node using a docker run command.\n\nSince there is only one node and a single Docker container, if the node goes down, there is no copy of the etcd data available on other nodes and you will lose all the data of your Rancher server.\n\nAir Gapped Installations\n\nFollow these steps to install the Rancher server in an air gapped environment.\n\nAn air gapped environment could be where Rancher server will be installed offline, behind a firewall, or behind a proxy.\n","postref":"60b1f91813abf0faa16790774c0c807c","objectID":"e30b7289148f3fe749d81d209096b8cd","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/other-installation-methods/"},{"anchor":"#","title":"Resource Quota Type Reference","content":"When you create a resource quota, you are configuring the pool of resources available to the project. You can set the following resource limits for the following resource types.\n\n\n\n\nResource Type\nDescription\n\n\n\n\n\nCPU Limit*\nThe maximum amount of CPU (in millicores) allocated to the project/namespace.1\n\n\n\nCPU Reservation*\nThe minimum amount of CPU (in millicores) guaranteed to the project/namespace.1\n\n\n\nMemory Limit*\nThe maximum amount of memory (in bytes) allocated to the project/namespace.1\n\n\n\nMemory Reservation*\nThe minimum amount of memory (in bytes) guaranteed to the project/namespace.1\n\n\n\nStorage Reservation\nThe minimum amount of storage (in gigabytes) guaranteed to the project/namespace.\n\n\n\nServices Load Balancers\nThe maximum number of load balancers services that can exist in the project/namespace.\n\n\n\nServices Node Ports\nThe maximum number of node port services that can exist in the project/namespace.\n\n\n\nPods\nThe maximum number of pods that can exist in the project/namespace in a non-terminal state (i.e., pods with a state of .status.phase in (Failed, Succeeded) equal to true).\n\n\n\nServices\nThe maximum number of services that can exist in the project/namespace.\n\n\n\nConfigMaps\nThe maximum number of ConfigMaps that can exist in the project/namespace.\n\n\n\nPersistent Volume Claims\nThe maximum number of persistent volume claims that can exist in the project/namespace.\n\n\n\nReplications Controllers\nThe maximum number of replication controllers that can exist in the project/namespace.\n\n\n\nSecrets\nThe maximum number of secrets that can exist in the project/namespace.\n\n\n\n\n\n* When setting resource quotas, if you set anything related to CPU or Memory (i.e. limits or reservations) on a project / namespace, all containers will require a respective CPU or Memory field set during creation. As of v2.2.0, a container default resource limit can be set at the same time to avoid the need to explicitly set these limits for every workload. See the Kubernetes documentation for more details on why this is required.\n\n","postref":"b990654f4de8cbb6f00a5b04c4d3f72b","objectID":"501ebe5bac035771584b2e40013b9954","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/project-admin/resource-quotas/quota-type-reference/"},{"anchor":"#","title":"Troubleshooting Worker Nodes and Generic Components","content":"This section applies to every node as it includes components that run on nodes with any role.\n\nCheck if the Containers are Running\n\nThere are three specific containers launched on nodes with the controlplane role:\n\n\nkubelet\nkube-proxy\n\n\nThe containers should have status Up. The duration shown after Up is the time the container has been running.\n\ndocker ps -a -f=name='kubelet|kube-proxy'\n\n\nExample output:\n\nCONTAINER ID        IMAGE                                COMMAND                  CREATED             STATUS              PORTS               NAMES\n158d0dcc33a5        rancher/hyperkube:v1.11.5-rancher1   \"/opt/rke-tools/en...\"   3 hours ago         Up 3 hours                              kube-proxy\na30717ecfb55        rancher/hyperkube:v1.11.5-rancher1   \"/opt/rke-tools/en...\"   3 hours ago         Up 3 hours                              kubelet\n\n\nContainer Logging\n\nThe logging of the containers can contain information on what the problem could be.\n\ndocker logs kubelet\ndocker logs kube-proxy\n\n","postref":"fe7604cfa01bfff3ad81845cef0ed341","objectID":"266a71146f9471af5549172020c94b8a","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/troubleshooting/kubernetes-components/worker-and-generic/"},{"anchor":"#","title":"Troubleshooting vSphere Clusters","content":"If you are experiencing issues while provisioning a cluster with enabled vSphere Cloud Provider or while creating vSphere volumes for your workloads, you should inspect the logs of the following K8s services:\n\n\ncontroller-manager (Manages volumes in vCenter)\nkubelet: (Mounts vSphere volumes to pods)\n\n\nIf your cluster is not configured with external Cluster Logging, you will need to SSH into nodes to get the logs of the kube-controller-manager (running on one of the control plane nodes) and the kubelet (pertaining to the node where the stateful pod has been scheduled).\n\nThe easiest way to create a SSH session with a node is the Rancher CLI tool.\n\n\nConfigure the Rancher CLI for your cluster.\n\nRun the following command to get a shell to the corresponding nodes:\n$ rancher ssh <nodeName>\n\nInspect the logs of the controller-manager and kubelet containers looking for errors related to the vSphere cloud provider:\n$ docker logs --since 15m kube-controller-manager\n$ docker logs --since 15m kubelet\n\n","postref":"c3409d0f51886e47bc88dfd72962fb5f","objectID":"c1b5d56ebcc2b9535ffdb3b4f08032da","permalink":"http://jijeesh.github.io/docs/rke/latest/en/config-options/cloud-providers/vsphere/troubleshooting/"},{"anchor":"#","title":"vSphere Node Template Configuration Reference","content":"The tables below describe the configuration options available in the vSphere node template:\n\n\nAccount access\nInstance options\nScheduling options\n\n\nAccount Access\n\nThe account access parameters are different based on the Rancher version.\n\n\n  \n  \n  \n\n\nParameter\nRequired\nDescription\n\n\n\n\n\nCloud Credentials\n*\nYour vSphere account access information, stored in a cloud credential.\n\n\n\n\n\n\n\n  \n\n\nParameter\nRequired\nDescription\n\n\n\n\n\nvCenter or ESXi Server\n*\nIP or FQDN of the vCenter or ESXi server used for managing VMs.\n\n\n\nPort\n*\nPort to use when connecting to the server. Defaults to 443.\n\n\n\nUsername\n*\nvCenter/ESXi user to authenticate with the server.\n\n\n\nPassword\n*\nUser’s password.\n\n\n\n\n\n\n\n\n\nInstance Options\n\nThe options for creating and configuring an instance are different depending on your Rancher version.\n\n\n  \n  \n  \n\n\nParameter\nRequired\nDescription\n\n\n\n\n\nCPUs\n*\nNumber of vCPUS to assign to VMs.\n\n\n\nMemory\n*\nAmount of memory to assign to VMs.\n\n\n\nDisk\n*\nSize of the disk (in MB) to attach to the VMs.\n\n\n\nCreation method\n*\nThe method for setting up an operating system on the node. The operating system can be installed from an ISO or from a VM template. Depending on the creation method, you will also have to specify a VM template, content library, existing VM, or ISO. For more information on creation methods, refer to the section on configuring instances.\n\n\n\nCloud Init\n\nURL of a cloud-config.yml file or URL to provision VMs with. This file allows further customization of the operating system, such as network configuration, DNS servers, or system daemons. The operating system must support cloud-init.\n\n\n\nNetworks\n\nName(s) of the network to attach the VM to.\n\n\n\nConfiguration Parameters used for guestinfo\n\nAdditional configuration parameters for the VMs. These correspond to the Advanced Settings in the vSphere console. Example use cases include providing RancherOS guestinfo parameters or enabling disk UUIDs for the VMs (disk.EnableUUID=TRUE).\n\n\n\n\n\n\n\n  \n\n\nParameter\nRequired\nDescription\n\n\n\n\n\nCPUs\n*\nNumber of vCPUS to assign to VMs.\n\n\n\nMemory\n*\nAmount of memory to assign to VMs.\n\n\n\nDisk\n*\nSize of the disk (in MB) to attach to the VMs.\n\n\n\nCloud Init\n\nURL of a RancherOS cloud-config file to provision VMs with. This file allows further customization of the RancherOS operating system, such as network configuration, DNS servers, or system daemons.\n\n\n\nOS ISO URL\n*\nURL of a RancherOS vSphere ISO file to boot the VMs from. You can find URLs for specific versions in the Rancher OS GitHub Repo.\n\n\n\nConfiguration Parameters\n\nAdditional configuration parameters for the VMs. These correspond to the Advanced Settings in the vSphere console. Example use cases include providing RancherOS guestinfo parameters or enabling disk UUIDs for the VMs (disk.EnableUUID=TRUE).\n\n\n\n\n\n\n\n\n\nScheduling Options\n\nThe options for scheduling VMs to a hypervisor are different depending on your Rancher version.\n\n  \n  \n  \n\n\nParameter\nRequired\nDescription\n\n\n\n\n\nData Center\n*\nName/path of the datacenter to create VMs in.\n\n\n\nResource Pool\n\nName of the resource pool to schedule the VMs in. Leave blank for standalone ESXi. If not specified, the default resource pool is used.\n\n\n\nData Store\n*\nIf you have a data store cluster, you can toggle the Data Store field. This lets you select a data store cluster where your VM will be scheduled to. If the field is not toggled, you can select an individual disk.\n\n\n\nFolder\n\nName of a folder in the datacenter to create the VMs in. Must already exist. The folder name should be prefaced with vm/ in your vSphere config file.\n\n\n\nHost\n\nThe IP of the host system to schedule VMs in. If specified, the host system’s pool will be used and the Pool parameter will be ignored.\n\n\n\n\n\n\n\n  \n\n\nParameter\nRequired\nDescription\n\n\n\n\n\nData Center\n*\nName/path of the datacenter to create VMs in.\n\n\n\nPool\n\nName/path of the resource pool to schedule the VMs in. If not specified, the default resource pool is used.\n\n\n\nHost\n\nName/path of the host system to schedule VMs in. If specified, the host system’s pool will be used and the Pool parameter will be ignored.\n\n\n\nNetwork\n*\nName of the VM network to attach VMs to.\n\n\n\nData Store\n*\nDatastore to store the VM disks.\n\n\n\nFolder\n\nName of a folder in the datacenter to create the VMs in. Must already exist. The folder name should be prefaced with vm/ in your vSphere config file.\n\n\n\n\n\n\n\n\n","postref":"58fa36d188f83f3f2ab613cd37083f28","objectID":"88cc9e8b1dc8c4f9cad6100806c43ba9","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/node-pools/vsphere/provisioning-vsphere-clusters/node-template-reference/"},{"anchor":"#","title":"5. Set up the Istio Gateway","content":"The gateway to each cluster can have its own port or load balancer, which is unrelated to a service mesh. By default, each Rancher-provisioned cluster has one NGINX ingress controller allowing traffic into the cluster.\n\nYou can use the NGINX ingress controller with or without Istio installed. If this is the only gateway to your cluster, Istio will be able to route traffic from service to service, but Istio will not be able to receive traffic from outside the cluster.\n\nTo allow Istio to receive external traffic, you need to enable Istio’s gateway, which works as a north-south proxy for external traffic. When you enable the Istio gateway, the result is that your cluster will have two ingresses.\n\nYou will also need to set up a Kubernetes gateway for your services. This Kubernetes resource points to Istio’s implementation of the ingress gateway to the cluster.\n\nYou can route traffic into the service mesh with a load balancer or just Istio’s NodePort gateway. This section describes how to set up the NodePort gateway.\n\nFor more information on the Istio gateway, refer to the Istio documentation.\n\n\n\nEnable the Istio Gateway\n\nThe ingress gateway is a Kubernetes service that will be deployed in your cluster. There is only one Istio gateway per cluster.\n\n\nGo to the cluster where you want to allow outside traffic into Istio.\nClick Tools > Istio.\nExpand the Ingress Gateway section.\nUnder Enable Ingress Gateway, click True. The default type of service for the Istio gateway is NodePort. You can also configure it as a load balancer.\nOptionally, configure the ports, service types, node selectors and tolerations, and resource requests and limits for this service. The default resource requests for CPU and memory are the minimum recommended resources.\nClick Save.\n\n\nResult: The gateway is deployed, which allows Istio to receive traffic from outside the cluster.\n\nAdd a Kubernetes Gateway that Points to the Istio Gateway\n\nTo allow traffic to reach Ingress, you will also need to provide a Kubernetes gateway resource in your YAML that points to Istio’s implementation of the ingress gateway to the cluster.\n\n\nGo to the namespace where you want to deploy the Kubernetes gateway and click Import YAML.\nUpload the gateway YAML as a file or paste it into the form. An example gateway YAML is provided below.\nClick Import.\n\napiVersion: networking.istio.io/v1alpha3\nkind: Gateway\nmetadata:\n  name: bookinfo-gateway\nspec:\n  selector:\n    istio: ingressgateway # use istio default controller\n  servers:\n    - port:\n        number: 80\n        name: http\n        protocol: HTTP\n      hosts:\n        - \"*\"\n---\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: bookinfo\nspec:\n  hosts:\n  - \"*\"\n  gateways:\n  - bookinfo-gateway\n  http:\n  - match:\n    - uri:\n        exact: /productpage\n    - uri:\n        prefix: /static\n    - uri:\n        exact: /login\n    - uri:\n        exact: /logout\n    - uri:\n        prefix: /api/v1/products\n    route:\n    - destination:\n        host: productpage\n        port:\n          number: 9080\nResult: You have configured your gateway resource so that Istio can receive traffic from outside the cluster.\n\nConfirm that the resource exists by running:\n\nkubectl get gateway -A\n\n\nThe result should be something like this:\n\nNAME               AGE\nbookinfo-gateway   64m\n\n\nAccess the ProductPage Service from a Web Browser\n\nTo test and see if the BookInfo app deployed correctly, the app can be viewed a web browser using the Istio controller IP and port, combined with the request name specified in your Kubernetes gateway resource:\n\nhttp://<IP of Istio controller>:<Port of istio controller>/productpage\n\nTo get the ingress gateway URL and port,\n\n\nGo to the System project in your cluster.\nWithin the System project, go to Resources > Workloads then scroll down to the istio-system namespace.\nWithin istio-system, there is a workload named istio-ingressgateway. Under the name of this workload, you should see links, such as 80/tcp.\nClick one of those links. This should show you the URL of the ingress gateway in your web browser. Append /productpage to the URL.\n\n\nResult: You should see the BookInfo app in the web browser.\n\nFor help inspecting the Istio controller URL and ports, try the commands the Istio documentation.\n\nTroubleshooting\n\nThe official Istio documentation suggests kubectl commands to inspect the correct ingress host and ingress port for external requests.\n\nConfirming that the Kubernetes Gateway Matches Istio’s Ingress Controller\n\nYou can try the steps in this section to make sure the Kubernetes gateway is configured properly.\n\nIn the gateway resource, the selector refers to Istio’s default ingress controller by its label, in which the key of the label is istio and the value is ingressgateway.  To make sure the label is appropriate for the gateway, do the following:\n\n\nGo to the System project in your cluster.\nWithin the System project, go to the namespace istio-system.\nWithin istio-system, there is a workload named istio-ingressgateway.\nClick the name of this workload and go to the Labels and Annotations section. You should see that it has the key istio and the value ingressgateway. This confirms that the selector in the Gateway resource matches Istio’s default ingress controller.\n\n\nNext: Set up Istio’s Components for Traffic Management\n","postref":"32402e0a179f04c0acc001ed054685bc","objectID":"610e014fd82db46bff20a925c7afc704","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/tools/istio/setup/gateway/"},{"anchor":"#","title":"Advanced Options for Docker Installs","content":"When installing Rancher, there are several advanced options that can be enabled:\n\n\nCustom CA Certificate\nAPI Audit Log\nTLS Settings\nAir Gap\nPersistent Data\nRunning rancher/rancher and rancher/rancher-agent on the Same Node\n\n\nCustom CA Certificate\n\nIf you want to configure Rancher to use a CA root certificate to be used when validating services, you would start the Rancher container sharing the directory that contains the CA root certificate.\n\nUse the command example to start a Rancher container with your private CA certificates mounted.\n\n\nThe volume flag (-v) should specify the host directory containing the CA root certificates.\nThe environment variable flag (-e) in combination with SSL_CERT_DIR and directory declares an environment variable that specifies the mounted CA root certificates directory location inside the container.\nPassing environment variables to the Rancher container can be done using -e KEY=VALUE or --env KEY=VALUE.\nMounting a host directory inside the container can be done using -v host-source-directory:container-destination-directory or --volume host-source-directory:container-destination-directory.\n\n\nThe example below is based on having the CA root certificates in the /host/certs directory on the host and mounting this directory on /container/certs inside the Rancher container.\n\ndocker run -d --restart=unless-stopped \\\n  -p 80:80 -p 443:443 \\\n  -v /host/certs:/container/certs \\\n  -e SSL_CERT_DIR=\"/container/certs\" \\\n  rancher/rancher:latest\n\n\nAPI Audit Log\n\nThe API Audit Log records all the user and system transactions made through Rancher server.\n\nThe API Audit Log writes to /var/log/auditlog inside the rancher container by default. Share that directory as a volume and set your AUDIT_LEVEL to enable the log.\n\nSee API Audit Log for more information and options.\n\ndocker run -d --restart=unless-stopped \\\n  -p 80:80 -p 443:443 \\\n  -v /var/log/rancher/auditlog:/var/log/auditlog \\\n  -e AUDIT_LEVEL=1 \\\n  rancher/rancher:latest\n\n\nTLS settings\n\nAvailable as of v2.1.7\n\nTo set a different TLS configuration, you can use the CATTLE_TLS_MIN_VERSION and CATTLE_TLS_CIPHERS environment variables. For example, to configure TLS 1.0 as minimum accepted TLS version:\n\ndocker run -d --restart=unless-stopped \\\n  -p 80:80 -p 443:443 \\\n  -e CATTLE_TLS_MIN_VERSION=\"1.0\" \\\n  rancher/rancher:latest\n\n\nSee TLS settings for more information and options.\n\nAir Gap\n\nIf you are visiting this page to complete an air gap installation, you must prepend your private registry URL to the server tag when running the installation command in the option that you choose. Add <REGISTRY.DOMAIN.COM:PORT> with your private registry URL in front of rancher/rancher:latest.\n\nExample:\n\n <REGISTRY.DOMAIN.COM:PORT>/rancher/rancher:latest\n\n\nPersistent Data\n\n\n  Rancher uses etcd as datastore. When using the Docker Install, the embedded etcd is\n    being used. The persistent data is at the following path in the container: /var/lib/rancher. You can\n    bind mount a host volume to this location to preserve data on the host it is running on. When using RancherOS,\n    please check what persistent storage\n      directories you can use to store the data.\n\n  Command:\n\n  docker run -d --restart=unless-stopped \\\n  -p 80:80 -p 443:443 \\\n  -v /opt/rancher:/var/lib/rancher \\\n  rancher/rancher:latest\n\n\n\n\nRunning rancher/rancher and rancher/rancher-agent on the Same Node\n\nIn the situation where you want to use a single node to run Rancher and to be able to add the same node to a cluster, you have to adjust the host ports mapped for the rancher/rancher container.\n\nIf a node is added to a cluster, it deploys the nginx ingress controller which will use port 80 and 443. This will conflict with the default ports we advise to expose for the rancher/rancher container.\n\nPlease note that this setup is not recommended for production use, but can be convenient for development/demo purposes.\n\nTo change the host ports mapping, replace the following part -p 80:80 -p 443:443 with -p 8080:80 -p 8443:443:\n\ndocker run -d --restart=unless-stopped \\\n  -p 8080:80 -p 8443:443 \\\n  rancher/rancher:latest\n\n","postref":"ef24cf03073b0102d091ebecca3c29f1","objectID":"27e08a9e9beaada21464167cccd708b8","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/other-installation-methods/single-node-docker/advanced/"},{"anchor":"#","title":"Example Scenarios","content":"These example scenarios describe how an organization could use templates to standardize cluster creation.\n\n\nEnforcing templates: Administrators might want to enforce one or more template settings for everyone if they want all new Rancher-provisioned clusters to have those settings.\nSharing different templates with different users: Administrators might give different templates to basic and advanced users, so that basic users have more restricted options and advanced users have more discretion when creating clusters.\nUpdating template settings: If an organization’s security and DevOps teams decide to embed best practices into the required settings for new clusters, those best practices could change over time. If the best practices change, a template can be updated to a new revision and clusters created from the template can upgrade to the new version of the template.\nSharing ownership of a template: When a template owner no longer wants to maintain a template, or wants to delegate ownership of the template, this scenario describes how template ownership can be shared.\n\n\nEnforcing a Template Setting for Everyone\n\nLet’s say there is an organization in which the administrators decide that all new clusters should be created with Kubernetes version 1.14.\n\n\nFirst, an administrator creates a template which specifies the Kubernetes version as 1.14 and marks all other settings as Allow User Override.\nThe administrator makes the template public.\nThe administrator turns on template enforcement.\n\n\nResults:\n\n\nAll Rancher users in the organization have access to the template.\nAll new clusters created by standard users with this template will use Kubernetes 1.14 and they are unable to use a different Kubernetes version. By default, standard users don’t have permission to create templates, so this template will be the only template they can use unless more templates are shared with them.\nAll standard users must use a cluster template to create a new cluster. They cannot create a cluster without using a template.\n\n\nIn this way, the administrators enforce the Kubernetes version across the organization, while still allowing end users to configure everything else.\n\nTemplates for Basic and Advanced Users\n\nLet’s say an organization has both basic and advanced users. Administrators want the basic users to be required to use a template, while the advanced users and administrators create their clusters however they want.\n\n\nFirst, an administrator turns on RKE template enforcement. This means that every standard user in Rancher will need to use an RKE template when they create a cluster.\n\nThe administrator then creates two templates:\n\n\nOne template for basic users, with almost every option specified except for access keys\nOne template for advanced users, which has most or all options has Allow User Override turned on\n\n\nThe administrator shares the advanced template with only the advanced users.\n\nThe administrator makes the template for basic users public, so the more restrictive template is an option for everyone who creates a Rancher-provisioned cluster.\n\n\nResult: All Rancher users, except for administrators, are required to use a template when creating a cluster. Everyone has access to the restrictive template, but only advanced users have permission to use the more permissive template. The basic users are more restricted, while advanced users have more freedom when configuring their Kubernetes clusters.\n\nUpdating Templates and Clusters Created with Them\n\nLet’s say an organization has a template that requires clusters to use Kubernetes v1.14. However, as time goes on, the administrators change their minds. They decide they want users to be able to upgrade their clusters to use newer versions of Kubernetes.\n\nIn this organization, many clusters were created with a template that requires Kubernetes v1.14. Because the template does not allow that setting to be overridden, the users who created the cluster cannot directly edit that setting.\n\nThe template owner has several options for allowing the cluster creators to upgrade Kubernetes on their clusters:\n\n\nSpecify Kubernetes v1.15 on the template: The template owner can create a new template revision that specifies Kubernetes v1.15. Then the owner of each cluster that uses that template can upgrade their cluster to a new revision of the template. This template upgrade allows the cluster creator to upgrade Kubernetes to v1.15 on their cluster.\nAllow any Kubernetes version on the template: When creating a template revision, the template owner can also mark the the Kubernetes version as Allow User Override using the switch near that setting on the Rancher UI. This will allow clusters that upgrade to this template revision to use any version of Kubernetes.\nAllow the latest minor Kubernetes version on the template: The template owner can also create a template revision in which the Kubernetes version is defined as Latest v1.14 (Allows patch version upgrades). This means clusters that use that revision will be able to get patch version upgrades, but major version upgrades will not be allowed.\n\n\nAllowing Other Users to Control and Share a Template\n\nLet’s say Alice is a Rancher administrator. She owns an RKE template that reflects her organization’s agreed-upon best practices for creating a cluster.\n\nBob is an advanced user who can make informed decisions about cluster configuration. Alice trusts Bob to create new revisions of her template as the best practices get updated over time. Therefore, she decides to make Bob an owner of the template.\n\nTo share ownership of the template with Bob, Alice adds Bob as an owner of her template.\n\nThe result is that as a template owner, Bob is in charge of version control for that template. Bob can now do all of the following:\n\n\nRevise the template when the best practices change\nDisable outdated revisions of the template so that no new clusters can be created with it\nDelete the whole template if the organization wants to go in a different direction\nSet a certain revision as default when users create a cluster with it. End users of the template will still be able to choose which revision they want to create the cluster with.\nShare the template with specific users, make the template available to all Rancher users, or share ownership of the template with another user.\n\n","postref":"88e0d452d01ec6e7cb391c86ac3b808c","objectID":"769df7ba3ecc6100d286f6edc1813ad6","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/rke-templates/example-scenarios/"},{"anchor":"#","title":"Generic troubleshooting","content":"\nImportant: RKE add-on install is only supported up to Rancher v2.0.8\n\nPlease use the Rancher Helm chart to install Rancher on a Kubernetes cluster. For details, see the Kubernetes Install - Installation Outline.\n\nIf you are currently using the RKE add-on install method, see Migrating from a Kubernetes Install with an RKE Add-on for details on how to move to using the helm chart.\n\n\nBelow are steps that you can follow to determine what is wrong in your cluster.\n\nDouble check if all the required ports are opened in your (host) firewall\n\nDouble check if all the required ports are opened in your (host) firewall.\n\nAll nodes should be present and in Ready state\n\nTo check, run the command:\n\nkubectl --kubeconfig kube_config_rancher-cluster.yml get nodes\n\n\nIf a node is not shown in this output or a node is not in Ready state, you can check the logging of the kubelet container. Login to the node and run docker logs kubelet.\n\nAll pods/jobs should be in Running/Completed state\n\nTo check, run the command:\n\nkubectl --kubeconfig kube_config_rancher-cluster.yml get pods --all-namespaces\n\n\nIf a pod is not in Running state, you can dig into the root cause by running:\n\nDescribe pod\n\nkubectl --kubeconfig kube_config_rancher-cluster.yml describe pod POD_NAME -n NAMESPACE\n\n\nPod container logs\n\nkubectl --kubeconfig kube_config_rancher-cluster.yml logs POD_NAME -n NAMESPACE\n\n\nIf a job is not in Completed state, you can dig into the root cause by running:\n\nDescribe job\n\nkubectl --kubeconfig kube_config_rancher-cluster.yml describe job JOB_NAME -n NAMESPACE\n\n\nLogs from the containers of pods of the job\n\nkubectl --kubeconfig kube_config_rancher-cluster.yml logs -l job-name=JOB_NAME -n NAMESPACE\n\n\nCheck ingress\n\nIngress should have the correct HOSTS (showing the configured FQDN) and ADDRESS (address(es) it will be routed to).\n\nkubectl --kubeconfig kube_config_rancher-cluster.yml get ingress --all-namespaces\n\n\nList all Kubernetes cluster events\n\nKubernetes cluster events are stored, and can be retrieved by running:\n\nkubectl --kubeconfig kube_config_rancher-cluster.yml get events --all-namespaces\n\n\nCheck Rancher container logging\n\nkubectl --kubeconfig kube_config_rancher-cluster.yml logs -l app=cattle -n cattle-system\n\n\nCheck NGINX ingress controller logging\n\nkubectl --kubeconfig kube_config_rancher-cluster.yml logs -l app=ingress-nginx -n ingress-nginx\n\n\nCheck if overlay network is functioning correctly\n\nThe pod can be scheduled to any of the hosts you used for your cluster, but that means that the NGINX ingress controller needs to be able to route the request from NODE_1 to NODE_2. This happens over the overlay network. If the overlay network is not functioning, you will experience intermittent TCP/HTTP connection failures due to the NGINX ingress controller not being able to route to the pod.\n\nTo test the overlay network, you can launch the following DaemonSet definition. This will run an alpine container on every host, which we will use to run a ping test between containers on all hosts.\n\n\nSave the following file as ds-alpine.yml\n\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: alpine\nspec:\n  selector:\n      matchLabels:\n        name: alpine\n  template:\n    metadata:\n      labels:\n        name: alpine\n    spec:\n      tolerations:\n      - effect: NoExecute\n        key: \"node-role.kubernetes.io/etcd\"\n        value: \"true\"\n      - effect: NoSchedule\n        key: \"node-role.kubernetes.io/controlplane\"\n        value: \"true\"\n      containers:\n      - image: alpine\n        imagePullPolicy: Always\n        name: alpine\n        command: [\"sh\", \"-c\", \"tail -f /dev/null\"]\n        terminationMessagePath: /dev/termination-log\n\n\nLaunch it using kubectl --kubeconfig kube_config_rancher-cluster.yml create -f ds-alpine.yml\n\nWait until kubectl --kubeconfig kube_config_rancher-cluster.yml rollout status ds/alpine -w returns: daemon set \"alpine\" successfully rolled out.\n\nRun the following command to let each container on every host ping each other (it’s a single line command).\n\necho \"=> Start\"; kubectl --kubeconfig kube_config_rancher-cluster.yml get pods -l name=alpine -o jsonpath='{range .items[*]}{@.metadata.name}{\" \"}{@.spec.nodeName}{\"\\n\"}{end}' | while read spod shost; do kubectl --kubeconfig kube_config_rancher-cluster.yml get pods -l name=alpine -o jsonpath='{range .items[*]}{@.status.podIP}{\" \"}{@.spec.nodeName}{\"\\n\"}{end}' | while read tip thost; do kubectl --kubeconfig kube_config_rancher-cluster.yml --request-timeout='10s' exec $spod -- /bin/sh -c \"ping -c2 $tip > /dev/null 2>&1\"; RC=$?; if [ $RC -ne 0 ]; then echo $shost cannot reach $thost; fi; done; done; echo \"=> End\"\n\n\nWhen this command has finished running, the output indicating everything is correct is:\n\n=> Start\n=> End\n\n\n\nIf you see error in the output, that means that the required ports for overlay networking are not opened between the hosts indicated.\n\nExample error output of a situation where NODE1 had the UDP ports blocked.\n\n=> Start\ncommand terminated with exit code 1\nNODE2 cannot reach NODE1\ncommand terminated with exit code 1\nNODE3 cannot reach NODE1\ncommand terminated with exit code 1\nNODE1 cannot reach NODE2\ncommand terminated with exit code 1\nNODE1 cannot reach NODE3\n=> End\n\n","postref":"da6e48d9fcdac9442c7ce5297d0cbcf0","objectID":"e217895991916332780c5895a47bcf93","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/helm2/rke-add-on/troubleshooting/generic-troubleshooting/"},{"anchor":"#","title":"Generic troubleshooting","content":"\nImportant: RKE add-on install is only supported up to Rancher v2.0.8\n\nPlease use the Rancher helm chart to install Rancher on a Kubernetes cluster. For details, see the Kubernetes Install - Installation Outline.\n\nIf you are currently using the RKE add-on install method, see Migrating from a Kubernetes Install with an RKE Add-on for details on how to move to using the helm chart.\n\n\nBelow are steps that you can follow to determine what is wrong in your cluster.\n\nDouble check if all the required ports are opened in your (host) firewall\n\nDouble check if all the required ports are opened in your (host) firewall.\n\nAll nodes should be present and in Ready state\n\nTo check, run the command:\n\nkubectl --kubeconfig kube_config_rancher-cluster.yml get nodes\n\n\nIf a node is not shown in this output or a node is not in Ready state, you can check the logging of the kubelet container. Login to the node and run docker logs kubelet.\n\nAll pods/jobs should be in Running/Completed state\n\nTo check, run the command:\n\nkubectl --kubeconfig kube_config_rancher-cluster.yml get pods --all-namespaces\n\n\nIf a pod is not in Running state, you can dig into the root cause by running:\n\nDescribe pod\n\nkubectl --kubeconfig kube_config_rancher-cluster.yml describe pod POD_NAME -n NAMESPACE\n\n\nPod container logs\n\nkubectl --kubeconfig kube_config_rancher-cluster.yml logs POD_NAME -n NAMESPACE\n\n\nIf a job is not in Completed state, you can dig into the root cause by running:\n\nDescribe job\n\nkubectl --kubeconfig kube_config_rancher-cluster.yml describe job JOB_NAME -n NAMESPACE\n\n\nLogs from the containers of pods of the job\n\nkubectl --kubeconfig kube_config_rancher-cluster.yml logs -l job-name=JOB_NAME -n NAMESPACE\n\n\nCheck ingress\n\nIngress should have the correct HOSTS (showing the configured FQDN) and ADDRESS (address(es) it will be routed to).\n\nkubectl --kubeconfig kube_config_rancher-cluster.yml get ingress --all-namespaces\n\n\nList all Kubernetes cluster events\n\nKubernetes cluster events are stored, and can be retrieved by running:\n\nkubectl --kubeconfig kube_config_rancher-cluster.yml get events --all-namespaces\n\n\nCheck Rancher container logging\n\nkubectl --kubeconfig kube_config_rancher-cluster.yml logs -l app=cattle -n cattle-system\n\n\nCheck NGINX ingress controller logging\n\nkubectl --kubeconfig kube_config_rancher-cluster.yml logs -l app=ingress-nginx -n ingress-nginx\n\n\nCheck if overlay network is functioning correctly\n\nThe pod can be scheduled to any of the hosts you used for your cluster, but that means that the NGINX ingress controller needs to be able to route the request from NODE_1 to NODE_2. This happens over the overlay network. If the overlay network is not functioning, you will experience intermittent TCP/HTTP connection failures due to the NGINX ingress controller not being able to route to the pod.\n\nTo test the overlay network, you can launch the following DaemonSet definition. This will run an alpine container on every host, which we will use to run a ping test between containers on all hosts.\n\n\nSave the following file as ds-alpine.yml\n\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: alpine\nspec:\n  selector:\n      matchLabels:\n        name: alpine\n  template:\n    metadata:\n      labels:\n        name: alpine\n    spec:\n      tolerations:\n      - effect: NoExecute\n        key: \"node-role.kubernetes.io/etcd\"\n        value: \"true\"\n      - effect: NoSchedule\n        key: \"node-role.kubernetes.io/controlplane\"\n        value: \"true\"\n      containers:\n      - image: alpine\n        imagePullPolicy: Always\n        name: alpine\n        command: [\"sh\", \"-c\", \"tail -f /dev/null\"]\n        terminationMessagePath: /dev/termination-log\n\n\nLaunch it using kubectl --kubeconfig kube_config_rancher-cluster.yml create -f ds-alpine.yml\n\nWait until kubectl --kubeconfig kube_config_rancher-cluster.yml rollout status ds/alpine -w returns: daemon set \"alpine\" successfully rolled out.\n\nRun the following command to let each container on every host ping each other (it’s a single line command).\n\necho \"=> Start\"; kubectl --kubeconfig kube_config_rancher-cluster.yml get pods -l name=alpine -o jsonpath='{range .items[*]}{@.metadata.name}{\" \"}{@.spec.nodeName}{\"\\n\"}{end}' | while read spod shost; do kubectl --kubeconfig kube_config_rancher-cluster.yml get pods -l name=alpine -o jsonpath='{range .items[*]}{@.status.podIP}{\" \"}{@.spec.nodeName}{\"\\n\"}{end}' | while read tip thost; do kubectl --kubeconfig kube_config_rancher-cluster.yml --request-timeout='10s' exec $spod -- /bin/sh -c \"ping -c2 $tip > /dev/null 2>&1\"; RC=$?; if [ $RC -ne 0 ]; then echo $shost cannot reach $thost; fi; done; done; echo \"=> End\"\n\n\nWhen this command has finished running, the output indicating everything is correct is:\n\n=> Start\n=> End\n\n\n\nIf you see error in the output, that means that the required ports for overlay networking are not opened between the hosts indicated.\n\nExample error output of a situation where NODE1 had the UDP ports blocked.\n\n=> Start\ncommand terminated with exit code 1\nNODE2 cannot reach NODE1\ncommand terminated with exit code 1\nNODE3 cannot reach NODE1\ncommand terminated with exit code 1\nNODE1 cannot reach NODE2\ncommand terminated with exit code 1\nNODE1 cannot reach NODE3\n=> End\n\n","postref":"a6cbcf59cf16789f97424e0e6e9591cb","objectID":"119a7fc03c9b44ee6158bbc8142e5513","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/rke-add-on/troubleshooting/generic-troubleshooting/"},{"anchor":"#","title":"Istio","content":"Available as of v2.3.0\n\nIstio is an open-source tool that makes it easier for DevOps teams to observe, control, troubleshoot, and secure the traffic within a complex network of microservices.\n\nAs a network of microservices changes and grows, the interactions between them can become more difficult to manage and understand. In such a situation, it is useful to have a service mesh as a separate infrastructure layer. Istio’s service mesh lets you manipulate traffic between microservices without changing the microservices directly.\n\nOur integration of Istio is designed so that a Rancher operator, such as an administrator or cluster owner, can deliver Istio to developers. Then developers can use Istio to enforce security policies, troubleshoot problems, or manage traffic for green/blue deployments, canary deployments, or A/B testing.\n\nThis service mesh provides features that include but are not limited to the following:\n\n\nTraffic management features\nEnhanced monitoring and tracing\nService discovery and routing\nSecure connections and service-to-service authentication with mutual TLS\nLoad balancing\nAutomatic retries, backoff, and circuit breaking\n\n\nAfter Istio is enabled in a cluster, you can leverage Istio’s control plane functionality with kubectl.\n\nRancher’s Istio integration comes with comprehensive visualization aids:\n\n\nTrace the root cause of errors with Jaeger. Jaeger is an open-source tool that provides a UI for a distributed tracing system, which is useful for root cause analysis and for determining what causes poor performance. Distributed tracing allows you to view an entire chain of calls, which might originate with a user request and traverse dozens of microservices.\nGet the full picture of your microservice architecture with Kiali. Kiali provides a diagram that shows the services within a service mesh and how they are connected, including the traffic rates and latencies between them. You can check the health of the service mesh, or drill down to see the incoming and outgoing requests to a single component.\nGain insights from time series analytics with Grafana dashboards. Grafana is an analytics platform that allows you to query, visualize, alert on and understand the data gathered by Prometheus.\nWrite custom queries for time series data with the Prometheus UI. Prometheus is a systems monitoring and alerting toolkit. Prometheus scrapes data from your cluster, which is then used by Grafana. A Prometheus UI is also integrated into Rancher, and lets you write custom queries for time series data and see the results in the UI.\n\n\nPrerequisites\n\nBefore enabling Istio, we recommend that you confirm that your Rancher worker nodes have enough CPU and memory to run all of the components of Istio.\n\nSetup Guide\n\nRefer to the setup guide for instructions on how to set up Istio and use it in a project.\n\nDisabling Istio\n\nTo remove Istio components from a cluster, namespace, or workload, refer to the section on disabling Istio.\n\nAccessing Visualizations\n\n\nBy default, only cluster owners have access to Jaeger and Kiali. For instructions on how to allow project members to access them, refer to Access to Visualizations.\n\n\nAfter Istio is set up in a cluster, Grafana, Prometheus, Jaeger, and Kiali are available in the Rancher UI.\n\nYour access to the visualizations depend on your role. Grafana and Prometheus are only available for cluster owners. The Kiali and Jaeger UIs are available only to cluster owners by default, but cluster owners can allow project members to access them by editing the Istio settings. When you go to your project and click Resources > Istio, you can go to each UI for Kiali, Jaeger, Grafana, and Prometheus by clicking their icons in the top right corner of the page.\n\nTo see the visualizations, go to the cluster where Istio is set up and click Tools > Istio. You should see links to each UI at the top of the page.\n\nYou can also get to the visualization tools from the project view.\n\nViewing the Kiali Traffic Graph\n\n\nFrom the project view in Rancher, click Resources > Istio.\nIf you are a cluster owner, you can go to the Traffic Graph tab. This tab has the Kiali network visualization integrated into the UI.\n\n\nViewing Traffic Metrics\n\nIstio’s monitoring features provide visibility into the performance of all your services.\n\n\nFrom the project view in Rancher, click Resources > Istio.\nGo to the Traffic Metrics tab. After traffic is generated in your cluster, you should be able to see metrics for Success Rate, Request Volume, 4xx Response Count, Project 5xx Response Count and Request Duration. Cluster owners can see all of the metrics, while project members can see a subset of the metrics.\n\n\nArchitecture\n\nIstio installs a service mesh that uses Envoy sidecar proxies to intercept traffic to each workload. These sidecars intercept and manage service-to-service communication, allowing fine-grained observation and control over traffic within the cluster.\n\nOnly workloads that have the Istio sidecar injected can be tracked and controlled by Istio.\n\nEnabling Istio in Rancher enables monitoring in the cluster, and enables Istio in all new namespaces that are created in a cluster. You need to manually enable Istio in preexisting namespaces.\n\nWhen a namespace has Istio enabled, new workloads deployed in the namespace will automatically have the Istio sidecar. You need to manually enable Istio in preexisting workloads.\n\nFor more information on the Istio sidecar, refer to the Istio docs.\n\nTwo Ingresses\n\nBy default, each Rancher-provisioned cluster has one NGINX ingress controller allowing traffic into the cluster. To allow Istio to receive external traffic, you need to enable the Istio ingress gateway for the cluster. The result is that your cluster will have two ingresses.\n\n\n","postref":"795176f80950b4125b6150aa24c5507c","objectID":"e0b0d23f1cdf45aa17ca42cedf7e3461","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/tools/istio/"},{"anchor":"#operating-system","title":"Operating System","content":"Your SSH server system-wide configuration file, located at /etc/ssh/sshd_config, must include this line that allows TCP forwarding:AllowTcpForwarding yes\n\n        RKE node:Node that runs the rke commands\n        RKE node - Outbound rules\n\n    \n        \n          Protocol\n          Port\n          Source\n          Destination\n          Description\n        \n    \n    \n        \n          TCP\n          22\n          RKE node\n          Any node configured in Cluster Configuration File\n          SSH provisioning of node by RKE\n        \n        \n          TCP\n          6443\n          RKE node\n          controlplane nodes\n          Kubernetes apiserver\n        \n    \n\n\t\n\tetcd nodes:Nodes with the role etcd\n\tetcd nodes - Inbound rules\n\t\n\t    \n\t        Protocol\n\t        Port\n\t        Source\n\t        Description\n\t    \n            \n                TCP\n                2376\n                Rancher nodes\n                Docker daemon TLS port used by Docker Machine(only needed when using Node Driver/Templates)\n            \n\t    \n\t        TCP\n\t        2379\n\t        etcd nodescontrolplane nodes\n\t        etcd client requests\n\t    \n\t    \n\t        TCP\n\t        2380\n\t        etcd nodescontrolplane nodes\n\t        etcd peer communication\n\t    \n\t    \n\t\tUDP\n\t        8472\n\t        etcd nodescontrolplane nodesworker nodes\n\t        Canal/Flannel VXLAN overlay networking\n\t    \n\t    \n\t\tTCP\n\t        9099\n\t        etcd node itself (local traffic, not across nodes)See Local node traffic\n\t        Canal/Flannel livenessProbe/readinessProbe\n\t    \n\t    \n\t        TCP\n\t        10250\n\t        controlplane nodes\n\t        kubelet\n\t    \n\t\n\tetcd nodes - Outbound rules\n\t\n\t    \n\t        Protocol\n\t        Port\n\t        Destination\n\t        Description\n\t    \n\t    \n\t        TCP\n\t        443\n\t        Rancher nodes\n\t        Rancher agent\n\t    \n\t    \n\t        TCP\n\t        2379\n\t        etcd nodes\n\t        etcd client requests\n\t    \n\t    \n\t        TCP\n\t        2380\n\t        etcd nodes\n\t        etcd peer communication\n\t    \n\t    \n\t        TCP\n\t        6443\n\t        controlplane nodes\n\t        Kubernetes apiserver\n\t    \n\t    \n\t\tUDP\n\t        8472\n\t        etcd nodescontrolplane nodesworker nodes\n\t        Canal/Flannel VXLAN overlay networking\n\t    \n\t    \n\t\tTCP\n\t        9099\n\t        etcd node itself (local traffic, not across nodes)See Local node traffic\n\t        Canal/Flannel livenessProbe/readinessProbe\n\t    \n\t\n\tcontrolplane nodes:Nodes with the role controlplane\n\tcontrolplane nodes - Inbound rules\n\t\n\t    \n\t        Protocol\n\t        Port\n\t        Source\n\t        Description\n\t    \n\t    \n\t        TCP\n\t        80\n\t        Any that consumes Ingress services\n\t        Ingress controller (HTTP)\n\t    \n\t    \n\t        TCP\n\t        443\n\t        Any that consumes Ingress services\n\t        Ingress controller (HTTPS)\n\t    \n            \n                TCP\n                2376\n                Rancher nodes\n                Docker daemon TLS port used by Docker Machine(only needed when using Node Driver/Templates)\n            \n\t    \n\t        TCP\n\t        6443\n\t        etcd nodescontrolplane nodesworker nodes\n\t        Kubernetes apiserver\n\t    \n\t    \n\t\tUDP\n\t        8472\n\t        etcd nodescontrolplane nodesworker nodes\n\t        Canal/Flannel VXLAN overlay networking\n\t    \n\t    \n\t\tTCP\n\t        9099\n\t        controlplane node itself (local traffic, not across nodes)See Local node traffic\n\t        Canal/Flannel livenessProbe/readinessProbe\n\t    \n\t    \n\t        TCP\n\t        10250\n\t        controlplane nodes\n\t        kubelet\n\t    \n\t    \n\t        TCP\n\t        10254\n\t        controlplane node itself (local traffic, not across nodes)See Local node traffic\n\t        Ingress controller livenessProbe/readinessProbe\n\t    \n\t    \n\t        TCP/UDP\n\t        30000-32767\n\t        Any source that consumes NodePort services\n\t        NodePort port range\n\t    \n\t\n\tcontrolplane nodes - Outbound rules\n\t\n\t    \n\t        Protocol\n\t        Port\n\t        Destination\n\t        Description\n\t    \n\t    \n\t        TCP\n\t        443\n\t        Rancher nodes\n\t        Rancher agent\n\t    \n\t    \n\t        TCP\n\t        2379\n\t        etcd nodes\n\t        etcd client requests\n\t    \n\t    \n\t        TCP\n\t        2380\n\t        etcd nodes\n\t        etcd peer communication\n\t    \n\t    \n\t\tUDP\n\t        8472\n\t        etcd nodescontrolplane nodesworker nodes\n\t        Canal/Flannel VXLAN overlay networking\n\t    \n\t    \n\t\tTCP\n\t        9099\n\t        controlplane node itself (local traffic, not across nodes)See Local node traffic\n\t        Canal/Flannel livenessProbe/readinessProbe\n\t    \n\t    \n\t        TCP\n\t        10250\n\t        etcd nodescontrolplane nodesworker nodes\n\t        kubelet\n\t    \n\t    \n\t        TCP\n\t        10254\n\t        controlplane node itself (local traffic, not across nodes)See Local node traffic\n\t        Ingress controller livenessProbe/readinessProbe\n\t    \n\t\n\tworker nodes:Nodes with the role worker\n\tworker nodes - Inbound rules\n\t\n\t    \n\t        Protocol\n\t        Port\n\t        Source\n\t        Description\n\t\t\n\t\t\n\t\t\tTCP\n\t\t\t22\n\t\t\t\n\t\t\t\t\n\t\t\t\t\tLinux worker nodes only\n\t\t\t\t\tAny network that you want to be able to remotely access this node from.\n\t\t\t\t\n\t\t\t\n\t\t\tRemote access over SSH\n\t\t\n\t\t\n\t\t\tTCP\n\t\t\t3389\n\t\t\t\n\t\t\t\t\n\t\t\t\t\tWindows worker nodes only\n\t\t\t\t\tAny network that you want to be able to remotely access this node from.\n\t\t\t\t\n\t\t\t\n\t\t\tRemote access over RDP\n\t\t\n\t    \n\t        TCP\n\t        80\n\t        Any that consumes Ingress services\n\t        Ingress controller (HTTP)\n\t    \n\t    \n\t        TCP\n\t        443\n\t        Any that consumes Ingress services\n\t        Ingress controller (HTTPS)\n\t    \n            \n                TCP\n                2376\n                Rancher nodes\n                Docker daemon TLS port used by Docker Machine(only needed when using Node Driver/Templates)\n            \n\t    \n\t\tUDP\n\t        8472\n\t        etcd nodescontrolplane nodesworker nodes\n\t        Canal/Flannel VXLAN overlay networking\n\t    \n\t    \n\t\tTCP\n\t        9099\n\t        worker node itself (local traffic, not across nodes)See Local node traffic\n\t        Canal/Flannel livenessProbe/readinessProbe\n\t    \n\t    \n\t        TCP\n\t        10250\n\t        controlplane nodes\n\t        kubelet\n\t    \n\t    \n\t        TCP\n\t        10254\n\t        worker node itself (local traffic, not across nodes)See Local node traffic\n\t        Ingress controller livenessProbe/readinessProbe\n\t    \n\t    \n\t        TCP/UDP\n\t        30000-32767\n\t        Any source that consumes NodePort services\n\t        NodePort port range\n\t    \n\t\n\tworker nodes - Outbound rules\n\t\n\t    \n\t        Protocol\n\t        Port\n\t        Destination\n\t        Description\n\t    \n\t    \n\t        TCP\n\t        443\n\t        Rancher nodes\n\t        Rancher agent\n\t    \n\t    \n\t        TCP\n\t        6443\n\t        controlplane nodes\n\t        Kubernetes apiserver\n\t    \n\t    \n\t\tUDP\n\t        8472\n\t        etcd nodescontrolplane nodesworker nodes\n\t        Canal/Flannel VXLAN overlay networking\n\t    \n\t    \n\t\tTCP\n\t        9099\n\t        worker node itself (local traffic, not across nodes)See Local node traffic\n\t        Canal/Flannel livenessProbe/readinessProbe\n\t    \n\t    \n\t        TCP\n\t        10254\n\t        worker node itself (local traffic, not across nodes)See Local node traffic\n\t        Ingress controller livenessProbe/readinessProbe\n\t    \n\t\n    \n    Information on local node traffic\n    Kubernetes healthchecks (livenessProbe and readinessProbe) are executed on the host itself. On most nodes, this is allowed by default. When you have applied strict host firewall (i.e. iptables) policies on the node, or when you are using nodes that have multiple interfaces (multihomed), this traffic gets blocked. In this case, you have to explicitly allow this traffic in your host firewall, or in case of public/private cloud hosted machines (i.e. AWS or OpenStack), in your security group configuration. Keep in mind that when using a security group as Source or Destination in your security group, that this only applies to the private interface of the nodes/instances.\n    \t\nIf you are using an external firewall, make sure you have this port opened between the machine you are using to run rke and the nodes that you are going to use in the cluster.Opening port TCP/6443 using iptables# Open TCP/6443 for all\niptables -A INPUT -p tcp --dport 6443 -j ACCEPT\n\n# Open TCP/6443 for one specific IP\niptables -A INPUT -p tcp -s your_ip_here --dport 6443 -j ACCEPT\nOpening port TCP/6443 using firewalld# Open TCP/6443 for all\nfirewall-cmd --zone=public --add-port=6443/tcp --permanent\nfirewall-cmd --reload\n\n# Open TCP/6443 for one specific IP\nfirewall-cmd --permanent --zone=public --add-rich-rule='\n  rule family=\"ipv4\"\n  source address=\"your_ip_here/32\"\n  port protocol=\"tcp\" port=\"6443\" accept'\nfirewall-cmd --reload\nThis section describes the requirements for Docker, Kubernetes, and SSH.OpenSSHIn order to SSH into each node, OpenSSH 7.0+ must be installed on each node.KubernetesRefer to the RKE release notes for the supported versions of Kubernetes.DockerEach Kubernetes version supports different Docker ver","postref":"fb141bb2eb74f8adcc69bca54dca556c","objectID":"e32ba1b723fffc862ea1b25cbaf20948","permalink":"http://jijeesh.github.io/docs/rke/latest/en/os/"},{"anchor":"#","title":"Resources, References, and Advanced Options","content":"When installing Rancher, there are several advanced options that can be enabled during installation. Within each install guide, these options are presented. Learn more about these options:\n\n\n\n\nAdvanced Option\nAvailable as of\n\n\n\n\n\nCustom CA Certificate\nv2.0.0\n\n\n\nAPI Audit Log\nv2.0.0\n\n\n\nTLS Settings\nv2.1.7\n\n\n\netcd configuration\nv2.2.0\n\n\n\nLocal System Charts for Air Gap Installations\nv2.3.0\n\n\n\n","postref":"5bf92da126d1a47d11cadb5d855ea395","objectID":"58c22a0fb3b3e805f046f4fd8ac26bbf","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/"},{"anchor":"#","title":"Troubleshooting","content":"As of v0.1.9, the rke-bundle-cert container is removed on both success and failure of a restore. To debug any issues, you will need to look at the logs generated from rke.\n\nAs of v0.1.8 and below, the rke-bundle-cert container is left over from a failed etcd restore. If you are having an issue with restoring an etcd snapshot then you can do the following on each etcd nodes before attempting to do another restore:\n\ndocker container rm --force rke-bundle-cert\n\n\nThe rke-bundle-cert container is usually removed when a backup or restore of etcd succeeds. Whenever something goes wrong, the rke-bundle-cert container will be left over. You can look\nat the logs or inspect the container to see what the issue is.\n\ndocker container logs --follow rke-bundle-cert\ndocker container inspect rke-bundle-cert\n\n\nThe important thing to note is the mounts of the container and location of the pki.bundle.tar.gz.\n","postref":"5666a5ce666365de523c0fc6e85edf06","objectID":"bba6b793730722be9e3363c337e3a6c8","permalink":"http://jijeesh.github.io/docs/rke/latest/en/etcd-snapshots/troubleshooting/"},{"anchor":"#","title":"6. Set up Istio's Components for Traffic Management","content":"A central advantage of traffic management in Istio is that it allows dynamic request routing. Some common applications for dynamic request routing include canary deployments and blue/green deployments. The two key resources in Istio traffic management are virtual services and destination rules.\n\n\nVirtual services intercept and direct traffic to your Kubernetes services, allowing you to divide percentages of traffic from a request to different services. You can use them to define a set of routing rules to apply when a host is addressed.\nDestination rules serve as the single source of truth about which service versions are available to receive traffic from virtual services. You can use these resources to define policies that apply to traffic that is intended for a service after routing has occurred.\n\n\nThis section describes how to add an example virtual service that corresponds to the reviews microservice in the sample BookInfo app. The purpose of this service is to divide traffic between two versions of the reviews service.\n\nIn this example, we take the traffic to the reviews service and intercept it so that 50 percent of it goes to v1 of the service and 50 percent goes to v2.\n\nAfter this virtual service is deployed, we will generate traffic and see from the Kiali visualization that traffic is being routed evenly between the two versions of the service.\n\nTo deploy the virtual service and destination rules for the reviews service,\n\n\nGo to the project view and click Import YAML.\nCopy resources below into the form.\nClick Import.\n\n\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: reviews\nspec:\n  hosts:\n  - reviews\n  http:\n  - route:\n    - destination:\n        host: reviews\n        subset: v1\n      weight: 50\n    - destination:\n        host: reviews\n        subset: v3\n      weight: 50\n---\napiVersion: networking.istio.io/v1alpha3\nkind: DestinationRule\nmetadata:\n  name: reviews\nspec:\n  host: reviews\n  subsets:\n  - name: v1\n    labels:\n      version: v1\n  - name: v2\n    labels:\n      version: v2\n  - name: v3\n    labels:\n      version: v3\n\n\nResult: When you generate traffic to this service (for example, by refreshing the ingress gateway URL), the Kiali traffic graph will reflect that traffic to the reviews service is divided evenly between v1 and v3.\n\nNext: Generate and View Traffic\n","postref":"2e8a2c0c93e2c1493efe8a2ac1939167","objectID":"f044f199bce089a25ded792cf6ca7d76","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/tools/istio/setup/set-up-traffic-management/"},{"anchor":"#","title":"7. Generate and View Traffic","content":"This section describes how to view the traffic that is being managed by Istio.\n\nThe Kiali Traffic Graph\n\nRancher integrates a Kiali graph into the Rancher UI. The Kiali graph provides a powerful way to visualize the topology of your Istio service mesh. It shows you which services communicate with each other.\n\nTo see the traffic graph,\n\n\nFrom the project view in Rancher, click Resources > Istio.\nGo to the Traffic Graph tab. This tab has the Kiali network visualization integrated into the UI.\n\n\nIf you refresh the URL to the BookInfo app several times, you should be able to see green arrows on the Kiali graph showing traffic to v1 and v3 of the reviews service. The control panel on the right side of the graph lets you configure details including how many minutes of the most recent traffic should be shown on the graph.\n\nFor additional tools and visualizations, you can go to each UI for Kiali, Jaeger, Grafana, and Prometheus by clicking their icons in the top right corner of the page.\n\nViewing Traffic Metrics\n\nIstio’s monitoring features provide visibility into the performance of all your services.\n\n\nFrom the project view in Rancher, click Resources > Istio.\nGo to the Traffic Metrics tab. After traffic is generated in your cluster, you should be able to see metrics for Success Rate, Request Volume, 4xx Response Count, Project 5xx Response Count and Request Duration.\n\n","postref":"c284397506de9510db1e71da809829fa","objectID":"bff69401639324688d21bd15094bb58a","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/tools/istio/setup/view-traffic/"},{"anchor":"#install-script","title":"Install Script","content":"K3s provides an installation script that is a convenient way to install it as a service on systemd or openrc based systems. This script is available at https://get.k3s.io. To install K3s using this method, just run:curl -sfL https://get.k3s.io | sh -After running this installation:\nThe K3s service will be configured to automatically restart after node reboots or if the process crashes or is killed\nAdditional utilities will be installed, including kubectl, crictl, ctr, k3s-killall.sh, and k3s-uninstall.sh\nA kubeconfig file will be written to /etc/rancher/k3s/k3s.yaml and the kubectl installed by K3s will automatically use it\nTo install on worker nodes and add them to the cluster, run the installation script with the K3S_URL and K3S_TOKEN environment variables. Here is an example showing how to join a worker node:curl -sfL https://get.k3s.io | K3S_URL=https://myserver:6443 K3S_TOKEN=mynodetoken sh -Setting the K3S_URL parameter causes K3s to run in worker mode. The K3s agent will register with the K3s server listening at the supplied URL. The value to use for K3S_TOKEN is stored at /var/lib/rancher/k3s/server/node-token on your server node.Note: Each machine must have a unique hostname. If your machines do not have unique hostnames, pass the K3S_NODE_NAME environment variable and provide a value with a valid and unique hostname for each node.","postref":"b492db157d4564d8ce87f3dfcffb718c","objectID":"556d902f9c4d6c06056c580f678c7bf1","permalink":"http://jijeesh.github.io/docs/k3s/latest/en/quick-start/"},{"anchor":"#","title":"Template Creator Permissions","content":"Administrators have the permission to create RKE templates, and only administrators can give that permission to other users.\n\nFor more information on administrator permissions, refer to the documentation on global permissions.\n\nGiving Users Permission to Create Templates\n\nTemplates can only be created by users who have the global permission Create RKE Templates.\n\nAdministrators have the global permission to create templates, and only administrators can give that permission to other users.\n\nFor information on allowing users to modify existing templates, refer to Sharing Templates.\n\nAdministrators can give users permission to create RKE templates in two ways:\n\n\nBy editing the permissions of an individual user\nBy changing the default permissions of new users\n\n\nAllowing a User to Create Templates\n\nAn administrator can individually grant the role Create RKE Templates to any existing user by following these steps:\n\n\nFrom the global view, click the Users tab. Choose the user you want to edit and click the Vertical Ellipsis (…) > Edit.\nIn the Global Permissions section, choose Custom and select the Create RKE Templates role along with any other roles the user should have. Click Save.\n\n\nResult: The user has permission to create RKE templates.\n\nAllowing New Users to Create Templates by Default\n\nAlternatively, the administrator can give all new users the default permission to create RKE templates by following the following steps. This will not affect the permissions of existing users.\n\n\nFrom the Global view, click Security > Roles.\nUnder the Global roles tab, go to the role Create RKE Templates and click the Vertical Ellipsis (…) > Edit.\nSelect the option Yes: Default role for new users and click Save.\n\n\nResult: Any new user created in this Rancher installation will be able to create RKE templates. Existing users will not get this permission.\n\nRevoking Permission to Create Templates\n\nAdministrators can remove a user’s permission to create templates with the following steps:\n\n\nFrom the global view, click the Users tab. Choose the user you want to edit and click the Vertical Ellipsis (…) > Edit.\nIn the Global Permissions section, un-check the box for Create RKE Templates. In this section, you can change the user back to a standard user, or give the user a different set of custom permissions.\nClick Save.\n\n\nResult: The user cannot create RKE templates.\n","postref":"01f60dd5678e3d86954b636b866900d8","objectID":"6871070a690b68f40334869784841766","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/rke-templates/creator-permissions/"},{"anchor":"#","title":"Upgrade Basics","content":"You can upgrade K3s by using the installation script, or by manually installing the binary of the desired version.\n\n\nNote: When upgrading, upgrade server nodes first one at a time, then any worker nodes.\n\n\nUpgrade K3s Using the Installation Script\n\nTo upgrade K3s from an older version you can re-run the installation script using the same flags, for example:\ncurl -sfL https://get.k3s.io | sh -\nIf you want to upgrade to specific version you can run the following command:\ncurl -sfL https://get.k3s.io | INSTALL_K3S_VERSION=vX.Y.Z-rc1 sh -\nManually Upgrade K3s Using the Binary\n\nOr to manually upgrade K3s:\n\n\nDownload the desired version of the K3s binary from releases\nCopy the downloaded binary to /usr/local/bin/k3s (or your desired location)\nStop the old k3s binary\nLaunch the new k3s binary\n\n\nRestarting K3s\n\nRestarting K3s is supported by the installation script for systemd and openrc.\nTo restart manually for systemd use:\nsudo systemctl restart k3s\nTo restart manually for openrc use:\nsudo service k3s restart","postref":"404392b559b3a865117cf7f2b0e02236","objectID":"4f5dbe0bc033c0411962b17c973b0139","permalink":"http://jijeesh.github.io/docs/k3s/latest/en/upgrades/basic/"},{"anchor":"#","title":"Automated Upgrades","content":"\nNote: This feature is available as of v1.17.4+k3s1\n\n\nOverview\n\nYou can manage K3s cluster upgrades using Rancher’s system-upgrade-controller. This is a Kubernetes-native approach to cluster upgrades. It leverages a custom resource definition (CRD), the plan, and a controller that schedules upgrades based on the configured plans.\n\nA plan defines upgrade policies and requirements. This documentation will provide plans with defaults appropriate for upgrading a K3s cluster. For more advanced plan configuration options, please review the CRD.\n\nThe controller schedules upgrades by monitoring plans and selecting nodes to run upgrade jobs on. A plan defines which nodes should be upgraded through a label selector. When a job has run to completion successfully, the controller will label the node on which it ran accordingly.\n\n\nNote: The upgrade job that is launched must be highly privileged. It is configured with the following:\n\n\nHost IPC, NET, and PID namespaces\nThe CAP_SYS_BOOT capability\nHost root mounted at /host with read and write permissions\n\n\n\nFor more details on the design and architecture of the system-upgrade-controller or its integration with K3s, see the following Git repositories:\n\n\nsystem-upgrade-controller\nk3s-upgrade\n\n\nTo automate upgrades in this manner you must:\n\n\nInstall the system-upgrade-controller into your cluster\nConfigure plans\n\n\nInstall the system-upgrade-controller\n\nThe system-upgrade-controller can be installed as a deployment into your cluster. The deployment requires a service-account, clusterRoleBinding, and a configmap. To install these components, run the following command:\n\nkubectl apply -f https://github.com/rancher/system-upgrade-controller/releases/download/v0.4.0/system-upgrade-controller.yaml\n\n\nThe controller can be configured and customized via the previously mentioned configmap, but the controller must be redeployed for the changes to be applied.\n\nConfigure plans\n\nIt is recommended that you minimally create two plans: a plan for upgrading server (master) nodes and a plan for upgrading agent (worker) nodes. As needed, you can create additional plans to control the rollout of the upgrade across nodes. The following two example plans will upgrade your cluster to K3s v1.17.4+k3s1. Once the plans are created, the controller will pick them up and begin to upgrade your cluster.\n\n# Server plan\napiVersion: upgrade.cattle.io/v1\nkind: Plan\nmetadata:\n  name: server-plan\n  namespace: system-upgrade\nspec:\n  concurrency: 1\n  cordon: true\n  nodeSelector:\n    matchExpressions:\n    - key: node-role.kubernetes.io/master\n      operator: In\n      values:\n      - \"true\"\n  serviceAccountName: system-upgrade\n  upgrade:\n    image: rancher/k3s-upgrade\n  version: v1.17.4+k3s1\n---\n# Agent plan\napiVersion: upgrade.cattle.io/v1\nkind: Plan\nmetadata:\n  name: agent-plan\n  namespace: system-upgrade\nspec:\n  concurrency: 1\n  cordon: true\n  nodeSelector:\n    matchExpressions:\n    - key: node-role.kubernetes.io/master\n      operator: DoesNotExist\n  prepare:\n    args:\n    - prepare\n    - server-plan\n    image: rancher/k3s-upgrade:v1.17.4-k3s1\n  serviceAccountName: system-upgrade\n  upgrade:\n    image: rancher/k3s-upgrade\n  version: v1.17.4+k3s1\n\n\nThere are a few important things to call out regarding these plans:\n\nFirst, the plans must be created in the same namespace where the controller was deployed.\n\nSecond, the concurrency field indicates how many nodes can be upgraded at the same time.\n\nThird, the server-plan targets server nodes by specifying a label selector that selects nodes with the node-role.kubernetes.io/master label. The agent-plan targets agent nodes by specifying a label selector that select nodes without that label.\n\nFourth, the prepare step in the agent-plan will cause upgrade jobs for that plan to wait for the server-plan to complete before they execute.\n\nFifth, both plans have the version field set to v1.17.4+k3s1. Alternatively, you can omit the version field and set the channel field to a URL that resolves to a release of K3s. This will cause the controller to monitor that URL and upgrade the cluster any time it resolves to a new release. This is designed specifically to work with the latest release functionality of GitHub. Thus, you can configure your plans with the following channel to ensure your cluster is always automatically upgraded to the latest release of K3s:\n\napiVersion: upgrade.cattle.io/v1\nkind: Plan\n...\nspec:\n  ...\n  channel: https://github.com/rancher/k3s/releases/latest\n\n\n\nAs stated, the upgrade will begin as soon as the controller detects that a plan was created. Updating a plan will cause the controller to re-evaluate the plan and determine if another upgrade is needed.\n\nYou can monitor the progress of an upgrade by viewing the plan and jobs via kubectl:\n\nkubectl -n system-upgrade get plans -o yaml\nkubectl -n system-upgrade get jobs -o yaml\n\n","postref":"402252dbbed67c2351ae0ad3802bb155","objectID":"524edb204b3cb55bb49d6172c1e88ec6","permalink":"http://jijeesh.github.io/docs/k3s/latest/en/upgrades/automated/"},{"anchor":"#","title":"Failed to get job complete status","content":"\nImportant: RKE add-on install is only supported up to Rancher v2.0.8\n\nPlease use the Rancher Helm chart to install Rancher on a Kubernetes cluster. For details, see the Kubernetes Install - Installation Outline.\n\nIf you are currently using the RKE add-on install method, see Migrating from a Kubernetes Install with an RKE Add-on for details on how to move to using the helm chart.\n\n\nTo debug issues around this error, you will need to download the command-line tool kubectl. See Install and Set Up kubectl how to download kubectl for your platform.\n\nWhen you have made changes to rancher-cluster.yml, you will have to run rke remove --config rancher-cluster.yml to clean the nodes, so it cannot conflict with previous configuration errors.\n\nFailed to deploy addon execute job [rke-user-includes-addons]: Failed to get job complete status\n\nSomething is wrong in the addons definitions, you can run the following command to get the root cause in the logging of the job:\n\nkubectl --kubeconfig kube_config_rancher-cluster.yml logs -l job-name=rke-user-addon-deploy-job -n kube-system\n\n\nerror: error converting YAML to JSON: yaml: line 9:\n\nThe structure of the addons definition in rancher-cluster.yml is wrong. In the different resources specified in the addons section, there is a error in the structure of the YAML. The pointer  yaml line 9 references to the line number of the addon that is causing issues.\n\nThings to check\n\n\nIs each of the base64 encoded certificate string placed directly after the key, for example: tls.crt: LS01..., there should be no newline/space before, in between or after.\nIs the YAML properly formatted, each indentation should be 2 spaces as shown in the template files.\nVerify the integrity of your certificate by running this command cat MyCertificate | base64 -d on Linux, cat MyCertificate | base64 -D on Mac OS . If any error exists, the command output will tell you.\n\n\n\nError from server (BadRequest): error when creating “/etc/config/rke-user-addon.yaml”: Secret in version “v1” cannot be handled as a Secret\n\nThe base64 string of one of the certificate strings is wrong. The log message will try to show you what part of the string is not recognized as valid base64.\n\nThings to check\n\n\nCheck if the base64 string is valid by running one of the commands below:\n\n# MacOS\necho BASE64_CRT | base64 -D\n# Linux\necho BASE64_CRT | base64 -d\n# Windows\ncertutil -decode FILENAME.base64 FILENAME.verify\n\n\n\n\n\nThe Ingress “cattle-ingress-http” is invalid: spec.rules[0].host: Invalid value: “IP”: must be a DNS name, not an IP address\n\nThe host value can only contain a host name, as it is needed by the ingress controller to match the hostname and pass to the correct backend.\n","postref":"21b481300426eca052a0472eb953ac77","objectID":"f28fea7014658f61247cc024462c39e4","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/helm2/rke-add-on/troubleshooting/job-complete-status/"},{"anchor":"#","title":"Failed to get job complete status","content":"\nImportant: RKE add-on install is only supported up to Rancher v2.0.8\n\nPlease use the Rancher helm chart to install Rancher on a Kubernetes cluster. For details, see the Kubernetes Install - Installation Outline.\n\nIf you are currently using the RKE add-on install method, see Migrating from a Kubernetes Install with an RKE Add-on for details on how to move to using the helm chart.\n\n\nTo debug issues around this error, you will need to download the command-line tool kubectl. See Install and Set Up kubectl how to download kubectl for your platform.\n\nWhen you have made changes to rancher-cluster.yml, you will have to run rke remove --config rancher-cluster.yml to clean the nodes, so it cannot conflict with previous configuration errors.\n\nFailed to deploy addon execute job [rke-user-includes-addons]: Failed to get job complete status\n\nSomething is wrong in the addons definitions, you can run the following command to get the root cause in the logging of the job:\n\nkubectl --kubeconfig kube_config_rancher-cluster.yml logs -l job-name=rke-user-addon-deploy-job -n kube-system\n\n\nerror: error converting YAML to JSON: yaml: line 9:\n\nThe structure of the addons definition in rancher-cluster.yml is wrong. In the different resources specified in the addons section, there is a error in the structure of the YAML. The pointer  yaml line 9 references to the line number of the addon that is causing issues.\n\nThings to check\n\n\nIs each of the base64 encoded certificate string placed directly after the key, for example: tls.crt: LS01..., there should be no newline/space before, in between or after.\nIs the YAML properly formatted, each indentation should be 2 spaces as shown in the template files.\nVerify the integrity of your certificate by running this command cat MyCertificate | base64 -d on Linux, cat MyCertificate | base64 -D on Mac OS . If any error exists, the command output will tell you.\n\n\n\nError from server (BadRequest): error when creating “/etc/config/rke-user-addon.yaml”: Secret in version “v1” cannot be handled as a Secret\n\nThe base64 string of one of the certificate strings is wrong. The log message will try to show you what part of the string is not recognized as valid base64.\n\nThings to check\n\n\nCheck if the base64 string is valid by running one of the commands below:\n\n# MacOS\necho BASE64_CRT | base64 -D\n# Linux\necho BASE64_CRT | base64 -d\n# Windows\ncertutil -decode FILENAME.base64 FILENAME.verify\n\n\n\n\n\nThe Ingress “cattle-ingress-http” is invalid: spec.rules[0].host: Invalid value: “IP”: must be a DNS name, not an IP address\n\nThe host value can only contain a host name, as it is needed by the ingress controller to match the hostname and pass to the correct backend.\n","postref":"d5c84037c05ead58d8e2e78134d33a1d","objectID":"7e5f6836fbef7df12ae471f1b09d7053","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/rke-add-on/troubleshooting/job-complete-status/"},{"anchor":"#","title":"Installation","content":"This section contains instructions for installing K3s in various environments. Please ensure you have met the Installation Requirements before you begin installing K3s.\n\nInstallation and Configuration Options provides guidance on the options available to you when installing K3s.\n\nHigh Availability with an External DB details how to set up an HA K3s cluster backed by an external datastore such as MySQL, PostgreSQL, or etcd.\n\nHigh Availability with Embedded DB (Experimental) details how to set up an HA K3s cluster that leverages a built-in distributed database.\n\nAir-Gap Installation details how to set up K3s in environments that do not have direct access to the Internet.\n\nUninstalling\n\nIf you installed K3s with the help of the install.sh script, an uninstall script is generated during installation. The script is created on your node at /usr/local/bin/k3s-uninstall.sh (or as k3s-agent-uninstall.sh).\n","postref":"d59c468f13321097acde0e673611ebb8","objectID":"70f716824d2af4cbbd4c43a3f35792e5","permalink":"http://jijeesh.github.io/docs/k3s/latest/en/installation/"},{"anchor":"#","title":"Installation Options","content":"This page focuses on the options that can be used when you set up K3s for the first time:\n\n\nInstallation script options\nInstalling K3s from the binary\nRegistration options for the K3s server\nRegistration options for the K3s agent\n\n\nFor more advanced options, refer to this page.\n\nInstallation Script Options\n\nAs mentioned in the Quick-Start Guide, you can use the installation script available at https://get.k3s.io to install K3s as a service on systemd and openrc based systems.\n\nThe simplest form of this command is as follows:\ncurl -sfL https://get.k3s.io | sh -\nWhen using this method to install K3s, the following environment variables can be used to configure the installation:\n\n\nINSTALL_K3S_SKIP_DOWNLOAD\n\nIf set to true will not download K3s hash or binary.\n\nINSTALL_K3S_SYMLINK\n\nIf set to ‘skip’ will not create symlinks, ‘force’ will overwrite, default will symlink if command does not exist in path.\n\nINSTALL_K3S_SKIP_START\n\nIf set to true will not start K3s service.\n\nINSTALL_K3S_VERSION\n\nVersion of K3s to download from github. Will attempt to download the latest version if not specified.\n\nINSTALL_K3S_BIN_DIR\n\nDirectory to install K3s binary, links, and uninstall script to, or use /usr/local/bin as the default.\n\nINSTALL_K3S_BIN_DIR_READ_ONLY\n\nIf set to true will not write files to INSTALL_K3S_BIN_DIR, forces setting INSTALL_K3S_SKIP_DOWNLOAD=true.\n\nINSTALL_K3S_SYSTEMD_DIR\n\nDirectory to install systemd service and environment files to, or use /etc/systemd/system as the default.\n\nINSTALL_K3S_EXEC\n\nCommand with flags to use for launching K3s in the service. If the command is not specified, it will default to “agent” if K3S_URL is set or “server” if it is not set.\n\nThe final systemd command resolves to a combination of this environment variable and script args. To illustrate this, the following commands result in the same behavior of registering a server without flannel:\n curl ... | INSTALL_K3S_EXEC=\"--no-flannel\" sh -s -\n curl ... | INSTALL_K3S_EXEC=\"server --no-flannel\" sh -s -\n curl ... | INSTALL_K3S_EXEC=\"server\" sh -s - --no-flannel\n curl ... | sh -s - server --no-flannel\n curl ... | sh -s - --no-flannel\n\nINSTALL_K3S_NAME\n\n\nName of systemd service to create, will default from the K3s exec command if not specified. If specified the name will be prefixed with ‘k3s-’.\n\n\nINSTALL_K3S_TYPE\n\n\nType of systemd service to create, will default from the K3s exec command if not specified.\n\n\nEnvironment variables which begin with K3S_ will be preserved for the systemd and openrc services to use. Setting K3S_URL without explicitly setting an exec command will default the command to “agent”. When running the agent K3S_TOKEN must also be set.\n\nInstalling K3s from the Binary\n\nAs stated, the installation script is primarily concerned with configuring K3s to run as a service. If you choose to not use the script, you can run K3s simply by downloading the binary from our release page, placing it on your path, and executing it. The K3s binary supports the following commands:\n\n\n\n\nCommand\nDescription\n\n\n\n\n\nk3s server\nRun the K3s management server, which will also launch Kubernetes control plane components such as the API server, controller-manager, and scheduler.\n\n\n\nk3s agent\nRun the K3s node agent. This will cause K3s to run as a worker node, launching the Kubernetes node services kubelet and kube-proxy.\n\n\n\nk3s kubectl\nRun an embedded kubectl CLI. If the KUBECONFIG environment variable is not set, this will automatically attempt to use the config file that is created at /etc/rancher/k3s/k3s.yaml when launching a K3s server node.\n\n\n\nk3s crictl\nRun an embedded crictl. This is a CLI for interacting with Kubernetes’s container runtime interface (CRI). Useful for debugging.\n\n\n\nk3s ctr\nRun an embedded ctr. This is a CLI for containerd, the container daemon used by K3s. Useful for debugging.\n\n\n\nk3s help\nShows a list of commands or help for one command\n\n\n\n\nThe k3s server and k3s agent commands have additional configuration options that can be viewed with k3s server --help or k3s agent --help. For convenience, that help text is presented here:\n\nRegistration Options for the K3s Server\n\nNAME:\n   k3s server - Run management server\n\nUSAGE:\n   k3s server [OPTIONS]\n\nOPTIONS:\n   -v value                                   (logging) Number for the log level verbosity (default: 0)\n   --vmodule value                            (logging) Comma-separated list of pattern=N settings for file-filtered logging\n   --log value, -l value                      (logging) Log to file\n   --alsologtostderr                          (logging) Log to standard error as well as file (if set)\n   --bind-address value                       (listener) k3s bind address (default: 0.0.0.0)\n   --https-listen-port value                  (listener) HTTPS listen port (default: 6443)\n   --advertise-address value                  (listener) IP address that apiserver uses to advertise to members of the cluster (default: node-external-ip/node-ip)\n   --advertise-port value                     (listener) Port that apiserver uses to advertise to members of the cluster (default: listen-port) (default: 0)\n   --tls-san value                            (listener) Add additional hostname or IP as a Subject Alternative Name in the TLS cert\n   --data-dir value, -d value                 (data) Folder to hold state default /var/lib/rancher/k3s or ${HOME}/.rancher/k3s if not root\n   --cluster-cidr value                       (networking) Network CIDR to use for pod IPs (default: \"10.42.0.0/16\")\n   --service-cidr value                       (networking) Network CIDR to use for services IPs (default: \"10.43.0.0/16\")\n   --cluster-dns value                        (networking) Cluster IP for coredns service. Should be in your service-cidr range (default: 10.43.0.10)\n   --cluster-domain value                     (networking) Cluster Domain (default: \"cluster.local\")\n   --flannel-backend value                    (networking) One of 'none', 'vxlan', 'ipsec', 'host-gw', or 'wireguard' (default: \"vxlan\")\n   --token value, -t value                    (cluster) Shared secret used to join a server or agent to a cluster [$K3S_TOKEN]\n   --token-file value                         (cluster) File containing the cluster-secret/token [$K3S_TOKEN_FILE]\n   --write-kubeconfig value, -o value         (client) Write kubeconfig for admin client to this file [$K3S_KUBECONFIG_OUTPUT]\n   --write-kubeconfig-mode value              (client) Write kubeconfig with this mode [$K3S_KUBECONFIG_MODE]\n   --kube-apiserver-arg value                 (flags) Customized flag for kube-apiserver process\n   --kube-scheduler-arg value                 (flags) Customized flag for kube-scheduler process\n   --kube-controller-manager-arg value        (flags) Customized flag for kube-controller-manager process\n   --kube-cloud-controller-manager-arg value  (flags) Customized flag for kube-cloud-controller-manager process\n   --datastore-endpoint value                 (db) Specify etcd, Mysql, Postgres, or Sqlite (default) data source name [$K3S_DATASTORE_ENDPOINT]\n   --datastore-cafile value                   (db) TLS Certificate Authority file used to secure datastore backend communication [$K3S_DATASTORE_CAFILE]\n   --datastore-certfile value                 (db) TLS certification file used to secure datastore backend communication [$K3S_DATASTORE_CERTFILE]\n   --datastore-keyfile value                  (db) TLS key file used to secure datastore backend communication [$K3S_DATASTORE_KEYFILE]\n   --default-local-storage-path value         (storage) Default local storage path for local provisioner storage class\n   --disable value                            (components) Do not deploy packaged components and delete any deployed components (valid items: coredns, servicelb, traefik, local-storage, metrics-server)\n   --disable-scheduler                        (components) Disable Kubernetes default scheduler\n   --disable-cloud-controller                 (components) Disable k3s default cloud controller manager\n   --disable-network-policy                   (components) Disable k3s default network policy controller\n   --node-name value                          (agent/node) Node name [$K3S_NODE_NAME]\n   --with-node-id                             (agent/node) Append id to node name\n   --node-label value                         (agent/node) Registering and starting kubelet with set of labels\n   --node-taint value                         (agent/node) Registering kubelet with set of taints\n   --docker                                   (agent/runtime) Use docker instead of containerd\n   --container-runtime-endpoint value         (agent/runtime) Disable embedded containerd and use alternative CRI implementation\n   --pause-image value                        (agent/runtime) Customized pause image for containerd or docker sandbox (default: \"docker.io/rancher/pause:3.1\")\n   --private-registry value                   (agent/runtime) Private registry configuration file (default: \"/etc/rancher/k3s/","postref":"c8929255ef751ae1d0b48d896700aaf5","objectID":"a653777c2a6ec8b3d439da8865642744","permalink":"http://jijeesh.github.io/docs/k3s/latest/en/installation/install-options/"},{"anchor":"#","title":"Cluster Access","content":"The kubeconfig file is used to configure access to the Kubernetes cluster. It is required to be set up properly in order to access the Kubernetes API such as with kubectl or for installing applications with Helm. You may set the kubeconfig by either exporting the KUBECONFIG environment variable or by specifying a flag for kubectl and helm. Refer to the examples below for details.\n\nLeverage the KUBECONFIG environment variable:\n\nexport KUBECONFIG=/etc/rancher/k3s/k3s.yaml\nkubectl get pods --all-namespaces\nhelm ls --all-namespaces\n\n\nOr specify the location of the kubeconfig file per command:\n\nkubectl --kubeconfig /etc/rancher/k3s/k3s.yaml get pods --all-namespaces\nhelm --kubeconfig /etc/rancher/k3s/k3s.yaml ls --all-namespaces\n\n\nAccessing the Cluster from Outside with kubectl\n\nCopy /etc/rancher/k3s/k3s.yaml on your machine located outside the cluster as ~/.kube/config. Then replace “localhost” with the IP or name of your K3s server. kubectl can now manage your K3s cluster.\n","postref":"69589867793601079ff7dd4bbfb3c661","objectID":"b35336e9a4fa42886273f511d81640f4","permalink":"http://jijeesh.github.io/docs/k3s/latest/en/cluster-access/"},{"anchor":"#outline","title":"Outline","content":"In Rancher v1.6, stacks were used to group together the services that belong to your application. In v2.x, you need to create namespaces, which are the v2.x equivalent of stacks, for the same purpose.In Rancher v2.x, namespaces are child objects to projects. When you create a project, a default namespace is added to the project, but you can create your own to parallel your stacks from v1.6.During migration, if you don’t explicitly define which namespace a service should be deployed to, it’s deployed to the default namespace.Just like v1.6, Rancher v2.x supports service discovery within and across namespaces (we’ll get to service discovery soon).Next: Migrate Your ServicesBegin work in Rancher v2.x by using it to provision a new Kubernetes cluster, which is similar to an environment in v1.6. This cluster will host your application deployments.A cluster and project in combined together in Rancher v2.x is equivalent to a v1.6 environment. A cluster is the compute boundary (i.e., your hosts) and a project is an administrative boundary (i.e., a grouping of namespaces used to assign access rights to users).There’s more basic info on provisioning clusters in the headings below, but for full information, see Provisioning Kubernetes Clusters.ClustersIn Rancher v1.6, compute nodes were added to an environment. Rancher v2.x eschews the term environment for cluster, as Kubernetes uses this term for a team of computers instead of environment.Rancher v2.x lets you launch a Kubernetes cluster anywhere. Host your cluster using:\nA hosted Kubernetes provider.\nA pool of nodes from an infrastructure provider. Rancher launches Kubernetes on the nodes.\nAny custom node(s). Rancher can launch Kubernetes on the nodes, be they bare metal servers, virtual machines, or cloud hosts on a less popular infrastructure provider.\nProjectsAdditionally, Rancher v2.x introduces projects, which are objects that divide clusters into different application groups that are useful for applying user permissions. This model of clusters and projects allow for multi-tenancy because hosts are owned by the cluster, and the cluster can be further divided into multiple projects where users can manage their apps, but not those of others.When you create a cluster, two projects are automatically created:\nThe System project, which includes system namespaces where important Kubernetes resources are running (like ingress controllers and cluster dns services)\nThe Default project.\nHowever, for production environments, we recommend creating your own project and giving it a descriptive name.After provisioning a new cluster and project, you can authorize your users to access and use project resources. Similarly to Rancher v1.6 environments, Rancher v2.x allows you to assign users to projects. By assigning users to projects, you can limit what applications and resources a user can access.After your Rancher v2.x Server is installed, we recommend configuring external authentication (like Active Directory or GitHub) so that users can log into Rancher using their single sign-on. For a full list of supported authentication providers and instructions on how to configure them, see Authentication.Rancher v2.x AuthenticationLocal UsersAlthough we recommend using an external authentication provider, Rancher v1.6 and v2.x both offer support for users local to Rancher. However, these users cannot be migrated from Rancher v1.6 to v2.x. If you used local users in Rancher v1.6 and want to continue this practice in v2.x, you’ll need to manually recreate these user accounts and assign them access rights.As a best practice, you should use a hybrid of external and local authentication. This practice provides access to Rancher should your external authentication experience an interruption, as you can still log in using a local user account. Set up a few local accounts as administrative users of Rancher.SAML Authentication ProvidersIn Rancher v1.6, we encouraged our SAML users to use Shibboleth, as it was the only SAML authentication option we offered. However, to better support their minor differences, we’ve added more fully tested SAML providers for v2.x: Ping Identity, Microsoft ADFS, and FreeIPA.The first step in migrating from v1.6 to v2.x is to install the Rancher v2.x Server side-by-side with your v1.6 Server, as you’ll need your old install during the migration process. Due to the architecture changes between v1.6 and v2.x, there is no direct path for upgrade. You’ll have to install v2.x independently and then migrate your v1.6 services to v2.x.New for v2.x, all communication to Rancher Server is encrypted. The procedures below instruct you not only on installation of Rancher, but also creation and installation of these certificates.Before installing v2.x, provision one host or more to function as your Rancher Server(s). You can find the requirements for these hosts in Server Requirements.After provisioning your node(s), install Rancher:\nDocker Install\n\nFor development environments, Rancher can be installed on a single node using Docker. This installation procedure deploys a single Rancher container to your host.\n\nKubernetes Install\n\nFor production environments where your user base requires constant access to your cluster, we recommend installing Rancher in a high availability Kubernetes installation. This installation procedure provisions a three-node cluster and installs Rancher on each node using a Helm chart.\n\n\nImportant Difference: Although you could install Rancher v1.6 in a high-availability Kubernetes configuration using an external database and a Docker command on each node, Rancher v2.x in a Kubernetes install requires an existing Kubernetes cluster. Review Kubernetes Install for full requirements.\n\n\nA. Install Rancher v2.x\nB. Configure Authentication\nC. Provision a Cluster and Project\nD. Create Stacks\n","postref":"7a990fcbc331890d82699e3752e40014","objectID":"f62f78ed564378beeb2c85739f22f631","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/v1.6-migration/get-started/"},{"anchor":"#before-you-start","title":"Before You Start","content":"This procedure creates a backup that you can restore if Rancher encounters a disaster scenario.\nUsing a remote Terminal connection, log into the node running your Rancher Server.\n\nStop the container currently running Rancher Server. Replace <RANCHER_CONTAINER_NAME> with the name of your Rancher container.\n\ndocker stop <RANCHER_CONTAINER_NAME>\n\n\nUse the command below, replacing each placeholder, to create a data container from the Rancher container that you just stopped.\n\ndocker create --volumes-from <RANCHER_CONTAINER_NAME> --name rancher-data-<DATE> rancher/rancher:<RANCHER_CONTAINER_TAG>\n\n\nFrom the data container that you just created (rancher-data-<DATE>), create a backup tarball (rancher-data-backup-<RANCHER_VERSION>-<DATE>.tar.gz). Use the following command, replacing each placeholder.\n\ndocker run  --volumes-from rancher-data-<DATE> -v $PWD:/backup:z busybox tar pzcvf /backup/rancher-data-backup-<RANCHER_VERSION>-<DATE>.tar.gz /var/lib/rancher\n\n\nStep Result: A stream of commands runs on the screen.\n\nEnter the ls command to confirm that the backup tarball was created. It will have a name similar to rancher-data-backup-<RANCHER_VERSION>-<DATE>.tar.gz.\n\nMove your backup tarball to a safe location external to your Rancher Server. Then delete the rancher-data-<DATE> container from your Rancher Server.\n\nRestart Rancher Server. Replace <RANCHER_CONTAINER_NAME> with the name of your Rancher container.\n\ndocker start <RANCHER_CONTAINER_NAME>\n\nResult: A backup tarball of your Rancher Server data is created. See Restoring Backups: Docker Installs if you need to restore backup data.During the creation of your backup, you’ll enter a series of commands, replacing placeholders with data from your environment. These placeholders are denoted with angled brackets and all capital letters (<EXAMPLE>). Here’s an example of a command with a placeholder:docker run  --volumes-from rancher-data-<DATE> -v $PWD:/backup busybox tar pzcvf /backup/rancher-data-backup-<RANCHER_VERSION>-<DATE>.tar.gz /var/lib/rancher\nIn this command, <DATE> is a placeholder for the date that the data container and backup were created. 9-27-18 for example.Cross reference the image and reference table below to learn how to obtain this placeholder data. Write down or copy this information before starting the procedure below.Terminal docker ps Command, Displaying Where to Find <RANCHER_CONTAINER_TAG> and <RANCHER_CONTAINER_NAME>\n\n\n\nPlaceholder\nExample\nDescription\n\n\n\n\n\n<RANCHER_CONTAINER_TAG>\nv2.0.5\nThe rancher/rancher image you pulled for initial install.\n\n\n\n<RANCHER_CONTAINER_NAME>\nfestive_mestorf\nThe name of your Rancher container.\n\n\n\n<RANCHER_VERSION>\nv2.0.5\nThe version of Rancher that you’re creating a backup for.\n\n\n\n<DATE>\n9-27-18\nThe date that the data container or backup was created.\n\n\nYou can obtain <RANCHER_CONTAINER_TAG> and <RANCHER_CONTAINER_NAME> by logging into your Rancher Server by remote connection and entering the command to view the containers that are running: docker ps. You can also view containers that are stopped with docker ps -a. Use these commands for help anytime while creating backups.","postref":"2f9fee7e4eb61f9b20044a4e03735ccd","objectID":"2cb2382352eb7d126d5dd95b0b39e1a2","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/backups/backups/single-node-backups/"},{"anchor":"#","title":"Network Options","content":"\nNote: Please reference the Networking page for information about CoreDNS, Traefik, and the Service LB.\n\n\nBy default, K3s will run with flannel as the CNI, using VXLAN as the default backend. To change the CNI, refer to the section on configuring a custom CNI. To change the flannel backend, refer to the flannel options section.\n\nFlannel Options\n\nThe default backend for flannel is VXLAN. To enable encryption, pass the IPSec (Internet Protocol Security) or WireGuard options below.\n\nIf you wish to use WireGuard as your flannel backend it may require additional kernel modules. Please see the WireGuard Install Guide for details. The WireGuard install steps will ensure the appropriate kernel modules are installed for your operating system. You need to install WireGuard on every node, both server and agents before attempting to leverage the WireGuard flannel backend option.\n\n\n\n\nCLI Flag and Value\nDescription\n\n\n\n\n\n--flannel-backend=vxlan\n(Default) Uses the VXLAN backend.\n\n\n\n--flannel-backend=ipsec\nUses the IPSEC backend which encrypts network traffic.\n\n\n\n--flannel-backend=host-gw\nUses the host-gw backend.\n\n\n\n--flannel-backend=wireguard\nUses the WireGuard backend which encrypts network traffic. May require additional kernel modules and configuration.\n\n\n\n\nCustom CNI\n\nRun K3s with --flannel-backend=none and install your CNI of choice. IP Forwarding should be enabled for Canal and Calico. Please reference the steps below.\n\n\n  \n  \n  Visit the Project Calico Docs website. Follow the steps to install Canal. Modify the Canal YAML so that IP forwarding is allowed in the container_settings section, for example:\n\n\"container_settings\": {\n              \"allow_ip_forwarding\": true\n          }\n\n\nApply the Canal YAML.\n\nEnsure the settings were applied by running the following command on the host:\n\ncat /etc/cni/net.d/10-canal.conflist\n\n\nYou should see that IP forwarding is set to true.\n\n\n\n\n  Follow the Calico CNI Plugins Guide. Modify the Calico YAML so that IP forwarding is allowed in the container_settings section, for example:\n\n\"container_settings\": {\n              \"allow_ip_forwarding\": true\n          }\n\n\nApply the Calico YAML.\n\nEnsure the settings were applied by running the following command on the host:\n\ncat /etc/cni/net.d/10-calico.conflist\n\n\nYou should see that IP forwarding is set to true.\n\n\n\n\n\n","postref":"e14d4e3c55635cdd279b31acf91073ca","objectID":"8d03b11fa7fcda00c66b9876984afff4","permalink":"http://jijeesh.github.io/docs/k3s/latest/en/installation/network-options/"},{"anchor":"#","title":"Rancher Deployment Quick Start Guides","content":"\nNote: The intent of these guides is to quickly launch a sandbox that you can use to evaluate Rancher. These guides are not intended for production environments. For comprehensive setup instructions, see Installation.\n\n\nHowdy buckaroos! Use this section of the docs to jump start your deployment and testing of Rancher 2.x! It contains instructions for a simple Rancher setup and some common use cases. We plan on adding more content to this section in the future.\n\nWe have Quick Start Guides for:\n\n\nDeploying Rancher Server: Get started running Rancher using the method most convenient for you.\n\nDeploying Workloads: Deploy a simple workload and expose it, letting you access it from outside the cluster.\n\nUsing the CLI: Use kubectl or Rancher command line interface (CLI) to interact with your Rancher instance.\n\n","postref":"0a95c8a6e8dbe56b71a2ca455083ecf6","objectID":"28eebb3091e881a4c4c7574c4920410c","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/quick-start-guide/"},{"anchor":"#","title":"Upgrades","content":"This section describes how to upgrade your K3s cluster.\n\nUpgrade basics describes several techniques for upgrading your cluster manually. It can also be used as a basis for upgrading through third-party Infrastructure-as-Code tools like Terraform.\n\nAutomated upgrades describes how to perform Kubernetes-native automated upgrades using Rancher’s system-upgrade-controller.\n","postref":"92acfae1d08fc25594df5efc7b5bed07","objectID":"70d05eb3cfa061b7514c1eab2c3841c2","permalink":"http://jijeesh.github.io/docs/k3s/latest/en/upgrades/"},{"anchor":"#","title":"404 - default backend","content":"\nImportant: RKE add-on install is only supported up to Rancher v2.0.8\n\nPlease use the Rancher Helm chart to install Rancher on a Kubernetes cluster. For details, see the Kubernetes Install - Installation Outline.\n\nIf you are currently using the RKE add-on install method, see Migrating from a Kubernetes Install with an RKE Add-on for details on how to move to using the helm chart.\n\n\nTo debug issues around this error, you will need to download the command-line tool kubectl. See Install and Set Up kubectl how to download kubectl for your platform.\n\nWhen you have made changes to rancher-cluster.yml, you will have to run rke remove --config rancher-cluster.yml to clean the nodes, so it cannot conflict with previous configuration errors.\n\nPossible causes\n\nThe nginx ingress controller is not able to serve the configured host in rancher-cluster.yml. This should be the FQDN you configured to access Rancher. You can check if it is properly configured by viewing the ingress that is created by running the following command:\n\nkubectl --kubeconfig kube_config_rancher-cluster.yml get ingress -n cattle-system -o wide\n\n\nCheck if the HOSTS column is displaying the FQDN you configured in the template, and that the used nodes are listed in the ADDRESS column. If that is configured correctly, we can check the logging of the nginx ingress controller.\n\nThe logging of the nginx ingress controller will show why it cannot serve the requested host. To view the logs, you can run the following command\n\nkubectl --kubeconfig kube_config_rancher-cluster.yml logs -l app=ingress-nginx -n ingress-nginx\n\n\nErrors\n\n\nx509: certificate is valid for fqdn, not your_configured_fqdn\n\n\nThe used certificates do not contain the correct hostname. Generate new certificates that contain the chosen FQDN to access Rancher and redeploy.\n\n\nPort 80 is already in use. Please check the flag --http-port\n\n\nThere is a process on the node occupying port 80, this port is needed for the nginx ingress controller to route requests to Rancher. You can find the process by running the command: netstat -plant | grep \\:80.\n\nStop/kill the process and redeploy.\n\n\nunexpected error creating pem file: no valid PEM formatted block found\n\n\nThe base64 encoded string configured in the template is not valid. Please check if you can decode the configured string using base64 -D STRING, this should return the same output as the content of the file you used to generate the string. If this is correct, please check if the base64 encoded string is placed directly after the key, without any newlines before, in between or after. (For example: tls.crt: LS01..)\n","postref":"451ccd641fb8f493da8406bb77d720d0","objectID":"e49f7701877eb612c9867a4ee7dc82c1","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/helm2/rke-add-on/troubleshooting/404-default-backend/"},{"anchor":"#","title":"404 - default backend","content":"\nImportant: RKE add-on install is only supported up to Rancher v2.0.8\n\nPlease use the Rancher Helm chart to install Rancher on a Kubernetes cluster. For details, see the Kubernetes Install - Installation Outline.\n\nIf you are currently using the RKE add-on install method, see Migrating from a Kubernetes Install with an RKE Add-on for details on how to move to using the helm chart.\n\n\nTo debug issues around this error, you will need to download the command-line tool kubectl. See Install and Set Up kubectl how to download kubectl for your platform.\n\nWhen you have made changes to rancher-cluster.yml, you will have to run rke remove --config rancher-cluster.yml to clean the nodes, so it cannot conflict with previous configuration errors.\n\nPossible causes\n\nThe nginx ingress controller is not able to serve the configured host in rancher-cluster.yml. This should be the FQDN you configured to access Rancher. You can check if it is properly configured by viewing the ingress that is created by running the following command:\n\nkubectl --kubeconfig kube_config_rancher-cluster.yml get ingress -n cattle-system -o wide\n\n\nCheck if the HOSTS column is displaying the FQDN you configured in the template, and that the used nodes are listed in the ADDRESS column. If that is configured correctly, we can check the logging of the nginx ingress controller.\n\nThe logging of the nginx ingress controller will show why it cannot serve the requested host. To view the logs, you can run the following command\n\nkubectl --kubeconfig kube_config_rancher-cluster.yml logs -l app=ingress-nginx -n ingress-nginx\n\n\nErrors\n\n\nx509: certificate is valid for fqdn, not your_configured_fqdn\n\n\nThe used certificates do not contain the correct hostname. Generate new certificates that contain the chosen FQDN to access Rancher and redeploy.\n\n\nPort 80 is already in use. Please check the flag --http-port\n\n\nThere is a process on the node occupying port 80, this port is needed for the nginx ingress controller to route requests to Rancher. You can find the process by running the command: netstat -plant | grep \\:80.\n\nStop/kill the process and redeploy.\n\n\nunexpected error creating pem file: no valid PEM formatted block found\n\n\nThe base64 encoded string configured in the template is not valid. Please check if you can decode the configured string using base64 -D STRING, this should return the same output as the content of the file you used to generate the string. If this is correct, please check if the base64 encoded string is placed directly after the key, without any newlines before, in between or after. (For example: tls.crt: LS01..)\n","postref":"b7c0402f42c893f0bd62c266bd2f15b6","objectID":"953c63e46a95daae2ca0a4e7de9200ed","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/rke-add-on/troubleshooting/404-default-backend/"},{"anchor":"#","title":"High Availability with an External DB","content":"\nNote: Official support for installing Rancher on a Kubernetes cluster was introduced in our v1.0.0 release.\n\n\nThis section describes how to install a high-availability K3s cluster with an external database.\n\nSingle server clusters can meet a variety of use cases, but for environments where uptime of the Kubernetes control plane is critical, you can run K3s in an HA configuration. An HA K3s cluster is comprised of:\n\n\nTwo or more server nodes that will serve the Kubernetes API and run other control plane services\nZero or more agent nodes that are designated to run your apps and services\nAn external datastore (as opposed to the embedded SQLite datastore used in single-server setups)\nA fixed registration address that is placed in front of the server nodes to allow agent nodes to register with the cluster\n\n\nFor more details on how these components work together, refer to the architecture section.\n\nAgents register through the fixed registration address, but after registration they establish a connection directly to one of the server nodes. This is a websocket connection initiated by the k3s agent process and it is maintained by a client-side load balancer running as part of the agent process.\n\nInstallation Outline\n\nSetting up an HA cluster requires the following steps:\n\n\nCreate an external datastore\nLaunch server nodes\nConfigure the fixed registration address\nJoin agent nodes\n\n\n1. Create an External Datastore\n\nYou will first need to create an external datastore for the cluster. See the Cluster Datastore Options documentation for more details.\n\n2. Launch Server Nodes\n\nK3s requires two or more server nodes for this HA configuration. See the Installation Requirements for minimum machine requirements.\n\nWhen running the k3s server command on these nodes, you must set the datastore-endpoint parameter so that K3s knows how to connect to the external datastore.\n\nFor example, a command like the following could be used to install the K3s server with a MySQL database as the external datastore:\n\ncurl -sfL https://get.k3s.io | sh -s - server \\\n  --datastore-endpoint=\"mysql://username:password@tcp(hostname:3306)/database-name\"\n\n\nThe datastore endpoint format differs based on the database type. For details, refer to the section on datastore endpoint formats.\n\nTo configure TLS certificates when launching server nodes, refer to the datastore configuration guide.\n\n\nNote: The same installation options available to single-server installs are also available for high-availability installs. For more details, see the Installation and Configuration Options documentation.\n\n\nBy default, server nodes will be schedulable and thus your workloads can get launched on them. If you wish to have a dedicated control plane where no user workloads will run, you can use taints. The node-taint parameter will allow you to configure nodes with taints, for example --node-taint k3s-controlplane=true:NoExecute.\n\nOnce you’ve launched the k3s server process on all server nodes, ensure that the cluster has come up properly with k3s kubectl get nodes. You should see your server nodes in the Ready state.\n\n3. Configure the Fixed Registration Address\n\nAgent nodes need a URL to register against. This can be the IP or hostname of any of the server nodes, but in many cases those may change over time. For example, if you are running your cluster in a cloud that supports scaling groups, you may scale the server node group up and down over time, causing nodes to be created and destroyed and thus having different IPs from the initial set of server nodes. Therefore, you should have a stable endpoint in front of the server nodes that will not change over time. This endpoint can be set up using any number approaches, such as:\n\n\nA layer-4 (TCP) load balancer\nRound-robin DNS\nVirtual or elastic IP addresses\n\n\nThis endpoint can also be used for accessing the Kubernetes API. So you can, for example, modify your kubeconfig file to point to it instead of a specific node.\n\n4. Optional: Join Agent Nodes\n\nBecause K3s server nodes are schedulable by default, the minimum number of nodes for an HA K3s server cluster is two server nodes and zero agent nodes. To add nodes designated to run your apps and services, join agent nodes to your cluster.\n\nJoining agent nodes in an HA cluster is the same as joining agent nodes in a single server cluster. You just need to specify the URL the agent should register to and the token it should use.\n\nK3S_TOKEN=SECRET k3s agent --server https://fixed-registration-address:6443\n\n","postref":"3bf58308a1e073fdf9b09f50e3dbe7e7","objectID":"6f0a06d2dba597108f3c20ac87a3e32b","permalink":"http://jijeesh.github.io/docs/k3s/latest/en/installation/ha/"},{"anchor":"#","title":"Volumes and Storage","content":"When deploying an application that needs to retain data, you’ll need to create persistent storage. Persistent storage allows you to store application data external from the pod running your application. This storage practice allows you to maintain application data, even if the application’s pod fails.\n\nA persistent volume (PV) is a piece of storage in the Kubernetes cluster, while a persistent volume claim (PVC) is a request for storage. For details on how PVs and PVCs work, refer to the official Kubernetes documentation on storage.\n\nThis page describes how to set up persistent storage with a local storage provider, or with Longhorn.\n\nSetting up the Local Storage Provider\n\nK3s comes with Rancher’s Local Path Provisioner and this enables the ability to create persistent volume claims out of the box using local storage on the respective node. Below we cover a simple example. For more information please reference the official documentation here.\n\nCreate a hostPath backed persistent volume claim and a pod to utilize it:\n\npvc.yaml\n\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: local-path-pvc\n  namespace: default\nspec:\n  accessModes:\n    - ReadWriteOnce\n  storageClassName: local-path\n  resources:\n    requests:\n      storage: 2Gi\n\n\npod.yaml\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: volume-test\n  namespace: default\nspec:\n  containers:\n  - name: volume-test\n    image: nginx:stable-alpine\n    imagePullPolicy: IfNotPresent\n    volumeMounts:\n    - name: volv\n      mountPath: /data\n    ports:\n    - containerPort: 80\n  volumes:\n  - name: volv\n    persistentVolumeClaim:\n      claimName: local-path-pvc\n\n\nApply the yaml:\n\nkubectl create -f pvc.yaml\nkubectl create -f pod.yaml\n\n\nConfirm the PV and PVC are created:\n\nkubectl get pv\nkubectl get pvc\n\n\nThe status should be Bound for each.\n\nSetting up Longhorn\n\n\nNote: At this time Longhorn only supports amd64.\n\n\nK3s supports Longhorn. Longhorn is an open-source distributed block storage system for Kubernetes.\n\nBelow we cover a simple example. For more information, refer to the official documentation here.\n\nApply the longhorn.yaml to install Longhorn:\n\nkubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/master/deploy/longhorn.yaml\n\n\nLonghorn will be installed in the namespace longhorn-system.\n\nBefore we create a PVC, we will create a storage class for Longhorn with this yaml:\n\nkubectl create -f https://raw.githubusercontent.com/longhorn/longhorn/master/examples/storageclass.yaml\n\n\nApply the yaml to create the PVC and pod:\n\nkubectl create -f pvc.yaml\nkubectl create -f pod.yaml\n\n\npvc.yaml\n\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: longhorn-volv-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  storageClassName: longhorn\n  resources:\n    requests:\n      storage: 2Gi\n\n\npod.yaml\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: volume-test\n  namespace: default\nspec:\n  containers:\n  - name: volume-test\n    image: nginx:stable-alpine\n    imagePullPolicy: IfNotPresent\n    volumeMounts:\n    - name: volv\n      mountPath: /data\n    ports:\n    - containerPort: 80\n  volumes:\n  - name: volv\n    persistentVolumeClaim:\n      claimName: longhorn-volv-pvc\n\n\nConfirm the PV and PVC are created:\n\nkubectl get pv\nkubectl get pvc\n\n\nThe status should be Bound for each.\n","postref":"2d3e9810bcde87a4fd9d0503deceb75a","objectID":"efe5889d0ccf3b9b596ea77659d64e7e","permalink":"http://jijeesh.github.io/docs/k3s/latest/en/storage/"},{"anchor":"#","title":"Access and Sharing","content":"If you are an RKE template owner, you can share it with users or groups of users, who can then use the template to create clusters.\n\nSince RKE templates are specifically shared with users and groups, owners can share different RKE templates with different sets of users.\n\nWhen you share a template, each user can have one of two access levels:\n\n\nOwner: This user can update, delete, and share the templates that they own. The owner can also share the template with other users.\nUser: These users can create clusters using the template. They can also upgrade those clusters to new revisions of the same template. When you share a template as Make Public (read-only), all users in your Rancher setup have the User access level for the template.\n\n\nIf you create a template, you automatically become an owner of that template.\n\nIf you want to delegate responsibility for updating the template, you can share ownership of the template. For details on how owners can modify templates, refer to the documentation about revising templates.\n\nThere are several ways to share templates:\n\n\nAdd users to a new RKE template during template creation\nAdd users to an existing RKE template\nMake the RKE template public, sharing it with all users in the Rancher setup\nShare template ownership with users who are trusted to modify the template\n\n\nSharing Templates with Specific Users or Groups\n\nTo allow users or groups to create clusters using your template, you can give them the basic User access level for the template.\n\n\nFrom the Global view, click Tools > RKE Templates.\nGo to the template that you want to share and click the Vertical Ellipsis (…) > Edit.\nIn the Share Template section, click on Add Member.\nSearch in the Name field for the user or group you want to share the template with.\nChoose the User access type.\nClick Save.\n\n\nResult: The user or group can create clusters using the template.\n\nSharing Templates with All Users\n\n\nFrom the Global view, click Tools > RKE Templates.\nGo to the template that you want to share and click the Vertical Ellipsis (…) > Edit.\nUnder Share Template, click Make Public (read-only). Then click Save.\n\n\nResult: All users in the Rancher setup can create clusters using the template.\n\nSharing Ownership of Templates\n\nIf you are the creator of a template, you might want to delegate responsibility for maintaining and updating a template to another user or group.\n\nIn that case, you can give users the Owner access type, which allows another user to update your template, delete it, or share access to it with other users.\n\nTo give Owner access to a user or group,\n\n\nFrom the Global view, click Tools > RKE Templates.\nGo to the RKE template that you want to share and click the Vertical Ellipsis (…) > Edit.\nUnder Share Template, click on Add Member and search in the Name field for the user or group you want to share the template with.\nIn the Access Type field, click Owner.\nClick Save.\n\n\nResult: The user or group has the Owner access type, and can modify, share, or delete the template.\n","postref":"b0ab071c3acda58123b3932e95bb0ab3","objectID":"bbc1a0952af42469475fe692f1489da9","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/rke-templates/template-access-and-sharing/"},{"anchor":"#","title":"Creating and Revising Templates","content":"This section describes how to manage RKE templates and revisions. You an create, share, update, and delete templates from the Global view under Tools > RKE Templates.\n\nTemplate updates are handled through a revision system. When template owners want to change or update a template, they create a new revision of the template. Individual revisions cannot be edited. However, if you want to prevent a revision from being used to create a new cluster, you can disable it.\n\nTemplate revisions can be used in two ways: to create a new cluster, or to upgrade a cluster that was created with an earlier version of the template. The template creator can choose a default revision, but when end users create a cluster, they can choose any template and any template revision that is available to them. After the cluster is created from a specific revision, it cannot change to another template, but the cluster can be upgraded to a newer available revision of the same template.\n\nThe template owner has full control over template revisions, and can create new revisions to update the template, delete or disable revisions that should not be used to create clusters, and choose which template revision is the default.\n\nThis section covers the following topics:\n\n\nPrerequisites\nCreating a template\nUpdating a template\nDeleting a template\nCreating a revision based on the default revision\nCreating a revision based on a cloned revision\nDisabling a template revision\nRe-enabling a disabled template revision\nSetting a template revision as default\nDeleting a template revision\nUpgrading a cluster to use a new template revision\nExporting a running cluster to a new RKE template and revision\n\n\nPrerequisites\n\nYou can create RKE templates if you have the Create RKE Templates permission, which can be given by an administrator.\n\nYou can revise, share, and delete a template if you are an owner of the template. For details on how to become an owner of a template, refer to the documentation on sharing template ownership.\n\nCreating a Template\n\n\nFrom the Global view, click Tools > RKE Templates.\nClick Add Template.\nProvide a name for the template. An auto-generated name is already provided for the template’ first version, which is created along with this template.\nOptional: Share the template with other users or groups by adding them as members. You can also make the template public to share with everyone in the Rancher setup.\nThen follow the form on screen to save the cluster configuration parameters as part of the template’s revision. The revision can be marked as default for this template.\n\n\nResult: An RKE template with one revision is configured. You can use this RKE template revision later when you provision a Rancher-launched cluster.\n\nUpdating a Template\n\nWhen you update an RKE template, you are creating a revision of the existing template. Clusters that were created with an older version of the template can be updated to match the new revision.\n\nYou can’t edit individual revisions. Since you can’t edit individual revisions of a template, in order to prevent a revision from being used, you can disable it.\n\nWhen new template revisions are created, clusters using an older revision of the template are unaffected.\n\n\nFrom the Global view, click Tools > RKE Templates.\nGo to the template that you want to edit and click the Vertical Ellipsis (…) > Edit.\nEdit the required information and click Save.\nOptional: You can change the default revision of this template and also change who it is shared with.\n\n\nResult: The template is updated. To apply it to a cluster using an older version of the template, refer to the section on upgrading a cluster to use a new revision of a template.\n\nDeleting a Template\n\nWhen you no longer use an RKE template for any of your clusters, you can delete it.\n\n\nFrom the Global view, click Tools > RKE Templates.\nGo to the RKE template that you want to delete and click the Vertical Ellipsis (…) > Delete.\nConfirm the deletion when prompted.\n\n\nResult: The template is deleted.\n\nCreating a Revision Based on the Default Revision\n\nYou can clone the default template revision and quickly update its settings rather than creating a new revision from scratch. Cloning templates saves you the hassle of re-entering the access keys and other parameters needed for cluster creation.\n\n\nFrom the Global view, click Tools > RKE Templates.\nGo to the RKE template that you want to clone and click the Vertical Ellipsis (…) > New Revision From Default.\nComplete the rest of the form to create a new revision.\n\n\nResult: The RKE template revision is cloned and configured.\n\nCreating a Revision Based on a Cloned Revision\n\nWhen creating new RKE template revisions from your user settings, you can clone an existing revision and quickly update its settings rather than creating a new one from scratch. Cloning template revisions saves you the hassle of re-entering the cluster parameters.\n\n\nFrom the Global view, click Tools > RKE Templates.\nGo to the template revision you want to clone. Then select Ellipsis > Clone Revision.\nComplete the rest of the form.\n\n\nResult: The RKE template revision is cloned and configured. You can use the RKE template revision later when you provision a cluster. Any existing cluster using this RKE template can be upgraded to this new revision.\n\nDisabling a Template Revision\n\nWhen you no longer want an RKE template revision to be used for creating new clusters, you can disable it. A disabled revision can be re-enabled.\n\nYou can disable the revision if it is not being used by any cluster.\n\n\nFrom the Global view, click Tools > RKE Templates.\nGo to the template revision you want to disable. Then select Ellipsis > Disable.\n\n\nResult: The RKE template revision cannot be used to create a new cluster.\n\nRe-enabling a Disabled Template Revision\n\nIf you decide that a disabled RKE template revision should be used to create new clusters, you can re-enable it.\n\n\nFrom the Global view, click Tools > RKE Templates.\nGo to the template revision you want to re-enable. Then select Ellipsis > Enable.\n\n\nResult: The RKE template revision can be used to create a new cluster.\n\nSetting a Template Revision as Default\n\nWhen end users create a cluster using an RKE template, they can choose which revision to create the cluster with. You can configure which revision is used by default.\n\nTo set an RKE template revision as default,\n\n\nFrom the Global view, click Tools > RKE Templates.\nGo to the RKE template revision that should be default and click the Ellipsis (…) > Set as Default.\n\n\nResult: The RKE template revision will be used as the default option when clusters are created with the template.\n\nDeleting a Template Revision\n\nYou can delete all revisions of a template except for the default revision.\n\nTo permanently delete a revision,\n\n\nFrom the Global view, click Tools > RKE Templates.\nGo to the RKE template revision that should be deleted and click the Ellipsis (…) > Delete.\n\n\nResult: The RKE template revision is deleted.\n\nUpgrading a Cluster to Use a New Template Revision\n\n\nThis section assumes that you already have a cluster that has an RKE template applied.\nThis section also assumes that you have updated the template that the cluster is using so that a new template revision is available.\n\n\nTo upgrade a cluster to use a new template revision,\n\n\nFrom the Global view in Rancher, click the Clusters tab.\nGo to the cluster that you want to upgrade and click Ellipsis (…) > Edit.\nIn the Cluster Options section, click the dropdown menu for the template revision, then select the new template revision.\nClick Save.\n\n\nResult: The cluster is upgraded to use the settings defined in the new template revision.\n\nExporting a Running Cluster to a New RKE Template and Revision\n\nYou can save an existing cluster’s settings as an RKE template.\n\nThis exports the cluster’s settings as a new RKE template, and also binds the cluster to that template. The result is that the cluster can only be changed if the template is updated, and the cluster is upgraded to [use a newer version of the template.]\n\nTo convert an existing cluster to use an RKE template,\n\n\nFrom the Global view in Rancher, click the Clusters tab.\nGo to the cluster that will be converted to use an RKE template. Click Ellipsis (…) > Save as RKE Template.\nEnter a name for the template in the form that appears, and click Create.\n\n\nResults:\n\n\nA new RKE template is created.\nThe cluster is converted to use the new template.\nNew clusters can be created from the new template and revision.\n\n","postref":"c8b45dd29fb4103d7da98933bee2c57a","objectID":"8a083f3475a23d3792de2d9761af1d93","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/rke-templates/creating-and-revising/"},{"anchor":"#","title":"Template Enforcement","content":"This section describes how template administrators can enforce templates in Rancher, restricting the ability of users to create clusters without a template.\n\nBy default, any standard user in Rancher can create clusters. But when RKE template enforcement is turned on,\n\n\nOnly an administrator has the ability to create clusters without a template.\nAll standard users must use an RKE template to create a new cluster.\nStandard users cannot create a cluster without using a template.\n\n\nUsers can only create new templates if the administrator gives them permission.\n\nAfter a cluster is created with an RKE template, the cluster creator cannot edit settings that are defined in the template. The only way to change those settings after the cluster is created is to upgrade the cluster to a new revision of the same template. If cluster creators want to change template-defined settings, they would need to contact the template owner to get a new revision of the template. For details on how template revisions work, refer to the documentation on revising templates.\n\nRequiring New Clusters to Use an RKE Template\n\nYou might want to require new clusters to use a template to ensure that any cluster launched by a standard user will use the Kubernetes and/or Rancher settings that are vetted by administrators.\n\nTo require new clusters to use an RKE template, administrators can turn on RKE template enforcement with the following steps:\n\n\nFrom the Global view, click the Settings tab.\nGo to the cluster-template-enforcement setting. Click the vertical Ellipsis (…) and click Edit.\nSet the value to True and click Save.\n\n\nResult: All clusters provisioned by Rancher must use a template, unless the creator is an administrator.\n\nDisabling RKE Template Enforcement\n\nTo allow new clusters to be created without an RKE template, administrators can turn off RKE template enforcement with the following steps:\n\n\nFrom the Global view, click the Settings tab.\nGo to the cluster-template-enforcement setting. Click the vertical Ellipsis (…) and click Edit.\nSet the value to False and click Save.\n\n\nResult: When clusters are provisioned by Rancher, they don’t need to use a template.\n","postref":"e0b3b85aefcf33d914854fa6748df8db","objectID":"b608f1c41c1c4a2c43076af01c372af8","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/rke-templates/enforcement/"},{"anchor":"#","title":"Overriding Template Settings","content":"When a user creates an RKE template, each setting in the template has a switch in the Rancher UI that indicates if users can override the setting. This switch marks those settings as Allow User Override.\n\nAfter a cluster is created with a template, end users can’t update any of the settings defined in the template unless the template owner marked them as Allow User Override. However, if the template is updated to a new revision that changes the settings or allows end users to change them, the cluster can be upgraded to a new revision of the template and the changes in the new revision will be applied to the cluster.\n\nWhen any parameter is set as Allow User Override on the RKE template, it means that end users have to fill out those fields during cluster creation and they can edit those settings afterward at any time.\n\nThe Allow User Override model of the RKE template is useful for situations such as:\n\n\nAdministrators know that some settings will need the flexibility to be frequently updated over time\nEnd users will need to enter their own access keys or secret keys, for example, cloud credentials or credentials for backup snapshots\n\n","postref":"d2376eaadb3d2f9e067a2b5ea5a64288","objectID":"f6e97e9568011d7071a5bb0d647ac070","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/rke-templates/overrides/"},{"anchor":"#open-ports","title":"Open Ports","content":"K3s includes a basic service load balancer that uses available host ports. If you try to create a load balancer that listens on port 80, for example, it will try to find a free host in the cluster for port 80. If no port is available, the load balancer will stay in Pending.To disable the embedded load balancer, run the server with the --no-deploy servicelb option. This is necessary if you wish to run a different load balancer, such as MetalLB.Traefik is a modern HTTP reverse proxy and load balancer made to deploy microservices with ease. It simplifies networking complexity while designing, deploying, and running applications.Traefik is deployed by default when starting the server. For more information see Auto Deploying Manifests. The default config file is found in /var/lib/rancher/k3s/server/manifests/traefik.yaml and any changes made to this file will automatically be deployed to Kubernetes in a manner similar to kubectl apply.The Traefik ingress controller will use ports 80, 443, and 8080 on the host (i.e. these will not be usable for HostPort or NodePort).You can tweak traefik to meet your needs by setting options in the traefik.yaml file. Refer to the official Traefik for Helm Configuration Parameters readme for more information.To disable it, start each server with the --no-deploy traefik option.CoreDNS is deployed on start of the agent. To disable, run each server with the --no-deploy coredns option.If you don’t install CoreDNS, you will need to install a cluster DNS provider yourself.Please reference the Installation Requirements page for port information.","postref":"26b85b431b9b78b003c4fe2211ec5fbd","objectID":"38dd390e64f261d5173074dc3c0bef48","permalink":"http://jijeesh.github.io/docs/k3s/latest/en/networking/"},{"anchor":"#","title":"High Availability with Embedded DB (Experimental)","content":"As of v1.0.0, K3s is previewing support for running a highly available control plane without the need for an external database. This means there is no need to manage an external etcd or SQL datastore in order to run a reliable production-grade setup. While this feature is currently experimental, we expect it to be the primary architecture for running HA K3s clusters in the future.\n\nThis architecture is achieved by embedding a dqlite database within the K3s server process. DQLite is short for “distributed SQLite.” According to https://dqlite.io, it is “a fast, embedded, persistent SQL database with Raft consensus that is perfect for fault-tolerant IoT and Edge devices.” This makes it a natural fit for K3s.\n\nTo run K3s in this mode, you must have an odd number of server nodes. We recommend starting with three nodes.\n\nTo get started, first launch a server node with the cluster-init flag to enable clustering and a token that will be used as a shared secret to join additional servers to the cluster.\n\nK3S_TOKEN=SECRET k3s server --cluster-init\n\n\nAfter launching the first server, join the second and third servers to the cluster using the shared secret:\n\nK3S_TOKEN=SECRET k3s server --server https://<ip or hostname of server1>:6443\n\n\nNow you have a highly available control plane. Joining additional worker nodes to the cluster follows the same procedure as a single server cluster.\n","postref":"a4354bf686f4aaf800b0a4654dac8ce2","objectID":"353040ca21cd271886a8cd5e019f7cee","permalink":"http://jijeesh.github.io/docs/k3s/latest/en/installation/ha-embedded/"},{"anchor":"#","title":"Helm","content":"K3s release v1.17.0+k3s.1 added support for Helm 3. You can access the Helm 3 documentation here.\n\nHelm is the package management tool of choice for Kubernetes. Helm charts provide templating syntax for Kubernetes YAML manifest documents. With Helm we can create configurable deployments instead of just using static files. For more information about creating your own catalog of deployments, check out the docs at https://helm.sh/.\n\nK3s does not require any special configuration to start using Helm 3. Just be sure you have properly set up your kubeconfig as per the section about cluster access.\n\nThis section covers the following topics:\n\n\nUpgrading Helm\nDeploying manifests and Helm charts\nUsing the Helm CRD\n\n\nUpgrading Helm\n\nIf you were using Helm v2 in previous versions of K3s, you may upgrade to v1.17.0+k3s.1 or newer and Helm 2 will still function. If you wish to migrate to Helm 3, this blog post by Helm explains how to use a plugin to successfully migrate. Refer to the official Helm 3 documentation here for more information. K3s will handle either Helm v2 or Helm v3 as of v1.17.0+k3s.1. Just be sure you have properly set your kubeconfig as per the examples in the section about cluster access.\n\nNote that Helm 3 no longer requires Tiller and the helm init command. Refer to the official documentation for details.\n\nDeploying Manifests and Helm Charts\n\nAny file found in /var/lib/rancher/k3s/server/manifests will automatically be deployed to Kubernetes in a manner similar to kubectl apply.\n\nIt is also possible to deploy Helm charts. K3s supports a CRD controller for installing charts. A YAML file specification can look as following (example taken from /var/lib/rancher/k3s/server/manifests/traefik.yaml):\napiVersion: helm.cattle.io/v1\nkind: HelmChart\nmetadata:\n  name: traefik\n  namespace: kube-system\nspec:\n  chart: stable/traefik\n  set:\n    rbac.enabled: \"true\"\n    ssl.enabled: \"true\"\nKeep in mind that namespace in your HelmChart resource metadata section should always be kube-system, because the K3s deploy controller is configured to watch this namespace for new HelmChart resources. If you want to specify the namespace for the actual Helm release, you can do that using targetNamespace key under the spec directive, as shown in the configuration example below.\n\n\nNote: In order for the Helm Controller to know which version of Helm to use to Auto-Deploy a helm app, please specify the helmVersion in the spec of your YAML file.\n\n\nAlso note that besides set, you can use valuesContent under the spec directive. And it’s okay to use both of them:\napiVersion: helm.cattle.io/v1\nkind: HelmChart\nmetadata:\n  name: grafana\n  namespace: kube-system\nspec:\n  chart: stable/grafana\n  targetNamespace: monitoring\n  set:\n    adminPassword: \"NotVerySafePassword\"\n  valuesContent: |-\n    image:\n      tag: master\n    env:\n      GF_EXPLORE_ENABLED: true\n    adminUser: admin\n    sidecar:\n      datasources:\n        enabled: true\nK3s versions <= v0.5.0 used k3s.cattle.io for the API group of HelmCharts. This has been changed to helm.cattle.io for later versions.\n\nUsing the Helm CRD\n\nYou can deploy a third-party Helm chart using an example like this:\napiVersion: helm.cattle.io/v1\nkind: HelmChart\nmetadata:\n  name: nginx\n  namespace: kube-system\nspec:\n  chart: nginx\n  repo: https://charts.bitnami.com/bitnami\n  targetNamespace: default\nYou can install a specific version of a Helm chart using an example like this:\napiVersion: helm.cattle.io/v1\nkind: HelmChart\nmetadata:\n  name: stable/nginx-ingress\n  namespace: kube-system\nspec:\n  chart: nginx-ingress\n  version: 1.24.4\n  targetNamespace: default","postref":"d0f0148077fcbcae254336178ae124a5","objectID":"103dbd51b18623d852d765daae1766cb","permalink":"http://jijeesh.github.io/docs/k3s/latest/en/helm/"},{"anchor":"#","title":"Advanced Options and Configuration","content":"This section contains advanced information describing the different ways you can run and manage K3s:\n\n\nAuto-deploying manifests\nUsing Docker as the container runtime\nSecrets Encryption Config (Experimental)\nRunning K3s with RootlessKit (Experimental)\nNode labels and taints\nStarting the server with the installation script\nAdditional preparation for Alpine Linux setup\nRunning K3d (K3s in Docker) and docker-compose\nRaspbian Buster - Enable legacy iptables\n\n\nAuto-Deploying Manifests\n\nAny file found in /var/lib/rancher/k3s/server/manifests will automatically be deployed to Kubernetes in a manner similar to kubectl apply.\n\nFor information about deploying Helm charts, refer to the section about Helm.\n\nUsing Docker as the Container Runtime\n\nK3s includes and defaults to containerd, an industry-standard container runtime. If you want to use Docker instead of containerd then you simply need to run the agent with the --docker flag.\n\nK3s will generate config.toml for containerd in /var/lib/rancher/k3s/agent/etc/containerd/config.toml. For advanced customization for this file you can create another file called config.toml.tmpl in the same directory and it will be used instead.\n\nThe config.toml.tmpl will be treated as a Golang template file, and the config.Node structure is being passed to the template, the following is an example on how to use the structure to customize the configuration file https://github.com/rancher/k3s/blob/master/pkg/agent/templates/templates.go#L16-L32\n\nSecrets Encryption Config (Experimental)\n\nAs of v1.17.4+k3s1, K3s added the experimental feature of enabling secrets encryption at rest by passing the flag --secrets-encryption on a server, this flag will do the following automatically:\n\n\nGenerate an AES-CBC key\nGenerate an encryption config file with the generated key\n\n\n{\n  \"kind\": \"EncryptionConfiguration\",\n  \"apiVersion\": \"apiserver.config.k8s.io/v1\",\n  \"resources\": [\n    {\n      \"resources\": [\n        \"secrets\"\n      ],\n      \"providers\": [\n        {\n          \"aescbc\": {\n            \"keys\": [\n              {\n                \"name\": \"aescbckey\",\n                \"secret\": \"xxxxxxxxxxxxxxxxxxx\"\n              }\n            ]\n          }\n        },\n        {\n          \"identity\": {}\n        }\n      ]\n    }\n  ]\n}\n\n\n\nPass the config to the KubeAPI as encryption-provider-config\n\n\nOnce enabled any created secret will be encrypted with this key. Note that if you disable encryption then any encrypted secrets will not be readable until you enable encryption again.\n\nRunning K3s with RootlessKit (Experimental)\n\n\nWarning: This feature is experimental.\n\n\nRootlessKit is a kind of Linux-native “fake root” utility, made for mainly running Docker and Kubernetes as an unprivileged user, so as to protect the real root on the host from potential container-breakout attacks.\n\nInitial rootless support has been added but there are a series of significant usability issues surrounding it.\n\nWe are releasing the initial support for those interested in rootless and hopefully some people can help to improve the usability.  First, ensure you have a proper setup and support for user namespaces.  Refer to the requirements section in RootlessKit for instructions.\nIn short, latest Ubuntu is your best bet for this to work.\n\nKnown Issues with RootlessKit\n\n\nPorts\n\nWhen running rootless a new network namespace is created.  This means that K3s instance is running with networking fairly detached from the host.  The only way to access services run in K3s from the host is to set up port forwards to the K3s network namespace. We have a controller that will automatically bind 6443 and service port below 1024 to the host with an offset of 10000.\n\nThat means service port 80 will become 10080 on the host, but 8080 will become 8080 without any offset.\n\nCurrently, only LoadBalancer services are automatically bound.\n\nDaemon lifecycle\n\nOnce you kill K3s and then start a new instance of K3s it will create a new network namespace, but it doesn’t kill the old pods.  So you are left\nwith a fairly broken setup.  This is the main issue at the moment, how to deal with the network namespace.\n\nThe issue is tracked in https://github.com/rootless-containers/rootlesskit/issues/65\n\nCgroups\n\nCgroups are not supported.\n\n\nRunning Servers and Agents with Rootless\n\nJust add --rootless flag to either server or agent. So run k3s server --rootless and then look for the message Wrote kubeconfig [SOME PATH] for where your kubeconfig file is.\n\nFor more information about setting up the kubeconfig file, refer to the section about cluster access.\n\n\nBe careful, if you use -o to write the kubeconfig to a different directory it will probably not work. This is because the K3s instance in running in a different mount namespace.\n\n\nNode Labels and Taints\n\nK3s agents can be configured with the options --node-label and --node-taint which adds a label and taint to the kubelet. The two options only add labels and/or taints at registration time, so they can only be added once and not changed after that again by running K3s commands.\n\nIf you want to change node labels and taints after node registration you should use kubectl. Refer to the official Kubernetes documentation for details on how to add taints and node labels.\n\nStarting the Server with the Installation Script\n\nThe installation script will auto-detect if your OS is using systemd or openrc and start the service.\nWhen running with openrc, logs will be created at /var/log/k3s.log.\n\nWhen running with systemd, logs will be created in /var/log/syslog and viewed using journalctl -u k3s.\n\nAn example of installing and auto-starting with the install script:\ncurl -sfL https://get.k3s.io | sh -\nWhen running the server manually you should get an output similar to the following:\n\n$ k3s server\nINFO[2019-01-22T15:16:19.908493986-07:00] Starting k3s dev                             \nINFO[2019-01-22T15:16:19.908934479-07:00] Running kube-apiserver --allow-privileged=true --authorization-mode Node,RBAC --service-account-signing-key-file /var/lib/rancher/k3s/server/tls/service.key --service-cluster-ip-range 10.43.0.0/16 --advertise-port 6445 --advertise-address 127.0.0.1 --insecure-port 0 --secure-port 6444 --bind-address 127.0.0.1 --tls-cert-file /var/lib/rancher/k3s/server/tls/localhost.crt --tls-private-key-file /var/lib/rancher/k3s/server/tls/localhost.key --service-account-key-file /var/lib/rancher/k3s/server/tls/service.key --service-account-issuer k3s --api-audiences unknown --basic-auth-file /var/lib/rancher/k3s/server/cred/passwd --kubelet-client-certificate /var/lib/rancher/k3s/server/tls/token-node.crt --kubelet-client-key /var/lib/rancher/k3s/server/tls/token-node.key \nFlag --insecure-port has been deprecated, This flag will be removed in a future version.\nINFO[2019-01-22T15:16:20.196766005-07:00] Running kube-scheduler --kubeconfig /var/lib/rancher/k3s/server/cred/kubeconfig-system.yaml --port 0 --secure-port 0 --leader-elect=false \nINFO[2019-01-22T15:16:20.196880841-07:00] Running kube-controller-manager --kubeconfig /var/lib/rancher/k3s/server/cred/kubeconfig-system.yaml --service-account-private-key-file /var/lib/rancher/k3s/server/tls/service.key --allocate-node-cidrs --cluster-cidr 10.42.0.0/16 --root-ca-file /var/lib/rancher/k3s/server/tls/token-ca.crt --port 0 --secure-port 0 --leader-elect=false \nFlag --port has been deprecated, see --secure-port instead.\nINFO[2019-01-22T15:16:20.273441984-07:00] Listening on :6443                           \nINFO[2019-01-22T15:16:20.278383446-07:00] Writing manifest: /var/lib/rancher/k3s/server/manifests/coredns.yaml \nINFO[2019-01-22T15:16:20.474454524-07:00] Node token is available at /var/lib/rancher/k3s/server/node-token \nINFO[2019-01-22T15:16:20.474471391-07:00] To join node to cluster: k3s agent -s https://10.20.0.3:6443 -t ${NODE_TOKEN} \nINFO[2019-01-22T15:16:20.541027133-07:00] Wrote kubeconfig /etc/rancher/k3s/k3s.yaml\nINFO[2019-01-22T15:16:20.541049100-07:00] Run: k3s kubectl                             \n\n\nThe output will likely be much longer as the agent will create a lot of logs. By default the server\nwill register itself as a node (run the agent).\n\nAdditional Preparation for Alpine Linux Setup\n\nIn order to set up Alpine Linux, you have to go through the following preparation:\n\nUpdate /etc/update-extlinux.conf by adding:\n\ndefault_kernel_opts=\"...  cgroup_enable=cpuset cgroup_memory=1 cgroup_enable=memory\"\n\n\nThen update the config and reboot:\nupdate-extlinux\nreboot\nRunning K3d (K3s in Docker) and docker-compose\n\nk3d is a utility designed to easily run K3s in Docker.\n\nIt can be installed via the the brew utility on MacOS:\n\nbrew install k3d\n\n\nrancher/k3s images are also available to run the K3s server and agent from Docker.\n\nA docker-compose.yml is in the root of the K3s repo that serves as an example of how to run K3s from Docker. To run from docker-compose from this repo, run:\n\ndocker-compose up --scale agent=3\n# kubeconfig is written to current dir\n\nkubectl --kubeconfig kubeconfig.yaml get node\n\nNAME    ","postref":"0d162d4a5dfd0038ae8334e9a7ea0085","objectID":"539970904c75dadf7147dc39f054d190","permalink":"http://jijeesh.github.io/docs/k3s/latest/en/advanced/"},{"anchor":"#","title":"Applying Templates","content":"You can create a cluster from an RKE template that you created, or from a template that has been shared with you.\n\nRKE templates can be applied to new clusters.\n\nAs of Rancher v2.3.3, you can save the configuration of an existing cluster as an RKE template. Then the cluster’s settings can only be changed if the template is updated.\n\nYou can’t change a cluster to use a different RKE template. You can only update the cluster to a new revision of the same template.\n\nThis section covers the following topics:\n\n\nCreating a cluster from an RKE template\nUpdating a cluster created with an RKE template\nConverting an existing cluster to use an RKE template\n\n\nCreating a Cluster from an RKE Template\n\nTo add a cluster hosted by an infrastructure provider using an RKE template, use these steps:\n\n\nFrom the Global view, go to the Clusters tab.\nClick Add Cluster and choose the infrastructure provider.\nProvide the cluster name and node template details as usual.\nTo use an RKE template, under the Cluster Options, check the box for Use an existing RKE template and revision.\nChoose an existing template and revision from the dropdown menu.\nOptional: You can edit any settings that the RKE template owner marked as Allow User Override when the template was created. If there are settings that you want to change, but don’t have the option to, you will need to contact the template owner to get a new revision of the template. Then you will need to edit the cluster to upgrade it to the new revision.\nClick Save to launch the cluster.\n\n\nUpdating a Cluster Created with an RKE Template\n\nWhen the template owner creates a template, each setting has a switch in the Rancher UI that indicates if users can override the setting.\n\n\nIf the setting allows a user override, you can update these settings in the cluster by editing the cluster.\nIf the switch is turned off, you cannot change these settings unless the cluster owner creates a template revision that lets you override them. If there are settings that you want to change, but don’t have the option to, you will need to contact the template owner to get a new revision of the template.\n\n\nIf a cluster was created from an RKE template, you can edit the cluster to update the cluster to a new revision of the template.\n\nAs of Rancher v2.3.3, an existing cluster’s settings can be saved as an RKE template. In that situation, you can also edit the cluster to update the cluster to a new revision of the template.\n\n\nNote: You can’t change the cluster to use a different RKE template. You can only update the cluster to a new revision of the same template.\n\n\nConverting an Existing Cluster to Use an RKE Template\n\nAvailable as of v2.3.3\n\nThis section describes how to create an RKE template from an existing cluster.\n\nRKE templates cannot be applied to existing clusters, except if you save an existing cluster’s settings as an RKE template. This exports the cluster’s settings as a new RKE template, and also binds the cluster to that template. The result is that the cluster can only be changed if the template is updated, and the cluster is upgraded to use a newer version of the template.\n\nTo convert an existing cluster to use an RKE template,\n\n\nFrom the Global view in Rancher, click the Clusters tab.\nGo to the cluster that will be converted to use an RKE template. Click Ellipsis (…) > Save as RKE Template.\nEnter a name for the template in the form that appears, and click Create.\n\n\nResults:\n\n\nA new RKE template is created.\nThe cluster is converted to use the new template.\nNew clusters can be created from the new template.\n\n","postref":"16562207b3943a8e2e795540a1a4040a","objectID":"15653040603e2c42858cfafbffa0ef3b","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/rke-templates/applying-templates/"},{"anchor":"#","title":"Backups","content":"This section contains information about how to create backups of your Rancher data and how to restore them in a disaster scenario.\n\n\nDocker Install Backups\nKubernetes Install Backups\n\n\nIf you are looking to back up your Rancher launched Kubernetes cluster, please refer here.\n","postref":"f4263d99ee181b8c9a36d0b3e81999c0","objectID":"fc6a3be0ea35c1efb077e534377015a8","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/backups/backups/"},{"anchor":"#","title":"Cluster Datastore Options","content":"The ability to run Kubernetes using a datastore other than etcd sets K3s apart from other Kubernetes distributions. This feature provides flexibility to Kubernetes operators. The available datastore options allow you to select a datastore that best fits your use case. For example:\n\n\nIf your team doesn’t have expertise in operating etcd, you can choose an enterprise-grade SQL database like MySQL or PostgreSQL\nIf you need to run a simple, short-lived cluster in your CI/CD environment, you can use the embedded SQLite database\nIf you wish to deploy Kubernetes on the edge and require a highly available solution but can’t afford the operational overhead of managing a database at the edge, you can use K3s’s embedded HA datastore built on top of DQLite (currently experimental)\n\n\nK3s supports the following datastore options:\n\n\nEmbedded SQLite\nPostgreSQL (certified against versions 10.7 and 11.5)\nMySQL (certified against version 5.7)\nMariaDB (certified against version 10.3.20)\netcd (certified against version 3.3.15)\nEmbedded DQLite for High Availability (experimental)\n\n\nExternal Datastore Configuration Parameters\n\nIf you wish to use an external datastore such as PostgreSQL, MySQL, or etcd you must set the datastore-endpoint parameter so that K3s knows how to connect to it. You may also specify parameters to configure the authentication and encryption of the connection. The below table summarizes these parameters, which can be passed as either CLI flags or environment variables.\n\n\n\n\nCLI Flag\nEnvironment Variable\nDescription\n\n\n\n\n\n--datastore-endpoint\nK3S_DATASTORE_ENDPOINT\nSpecify a PostgresSQL, MySQL, or etcd connection string. This is a string used to describe the connection to the datastore. The structure of this string is specific to each backend and is detailed below.\n\n\n\n--datastore-cafile\nK3S_DATASTORE_CAFILE\nTLS Certificate Authority (CA) file used to help secure communication with the datastore. If your datastore serves requests over TLS using a certificate signed by a custom certificate authority, you can specify that CA using this parameter so that the K3s client can properly verify the certificate.\n\n\n\n--datastore-certfile\nK3S_DATASTORE_CERTFILE\nTLS certificate file used for client certificate based authentication to your datastore. To use this feature, your datastore must be configured to support client certificate based authentication. If you specify this parameter, you must also specify the datastore-keyfile parameter.\n\n\n\n--datastore-keyfile\nK3S_DATASTORE_KEYFILE\nTLS key file used for client certificate based authentication to your datastore. See the previous datastore-certfile parameter for more details.\n\n\n\n\nAs a best practice we recommend setting these parameters as environment variables rather than command line arguments so that your database credentials or other sensitive information aren’t exposed as part of the process info.\n\nDatastore Endpoint Format and Functionality\n\nAs mentioned, the format of the value passed to the datastore-endpoint parameter is dependent upon the datastore backend. The following details this format and functionality for each supported external datastore.\n\n\n  \n  \n  In its most common form, the datastore-endpoint parameter for PostgreSQL has the following format:\n\npostgres://username:password@hostname:port/database-name\n\nMore advanced configuration parameters are available. For more information on these, please see https://godoc.org/github.com/lib/pq.\n\nIf you specify a database name and it does not exist, the server will attempt to create it.\n\nIf you only supply postgres://  as the endpoint, K3s will attempt to do the following:\n\n\nConnect to localhost using postgres as the username and password\nCreate a database named kubernetes\n\n\n\n\n\n  In its most common form, the datastore-endpoint parameter for MySQL and MariaDB has the following format:\n\nmysql://username:password@tcp(hostname:3306)/database-name\n\nMore advanced configuration parameters are available. For more information on these, please see https://github.com/go-sql-driver/mysql#dsn-data-source-name\n\nNote that due to a known issue in K3s, you cannot set the tls parameter. TLS communication is supported, but you cannot, for example, set this parameter to “skip-verify” to cause K3s to skip certificate verification.\n\nIf you specify a database name and it does not exist, the server will attempt to create it.\n\nIf you only supply mysql:// as the endpoint, K3s will attempt to do the following:\n\n\nConnect to the MySQL socket at /var/run/mysqld/mysqld.sock using the root user and no password\nCreate a database with the name kubernetes\n\n\n\n\n\n  In its most common form, the datastore-endpoint parameter for etcd has the following format:\n\nhttps://etcd-host-1:2379,https://etcd-host-2:2379,https://etcd-host-3:2379\n\nThe above assumes a typical three node etcd cluster. The parameter can accept one more comma separated etcd URLs.\n\n\n\n\n\n\nBased on the above, the following example command could be used to launch a server instance that connects to a PostgresSQL database named k3s:\n\nK3S_DATASTORE_ENDPOINT='postgres://username:password@hostname:5432/k3s' k3s server\n\n\nAnd the following example could be used to connect to a MySQL database using client certificate authentication:\n\nK3S_DATASTORE_ENDPOINT='mysql://username:password@tcp(hostname:3306)/k3s' \\\nK3S_DATASTORE_CERTFILE='/path/to/client.crt' \\\nK3S_DATASTORE_KEYFILE='/path/to/client.key' \\\nk3s server\n\n\nEmbedded DQLite for HA (Experimental)\n\nK3s’s use of DQLite is similar to its use of SQLite. It is simple to set up and manage. As such, there is no external configuration or additional steps to take in order to use this option. Please see High Availability with Embedded DB (Experimental) for instructions on how to run with this option.\n","postref":"b73388dde213064b66184ebbc0935882","objectID":"a0d98ea76a312dc6e72dc273008191d6","permalink":"http://jijeesh.github.io/docs/k3s/latest/en/installation/datastore/"},{"anchor":"#backup-outline","title":"Backup Outline","content":"Backing up your high-availability Rancher cluster is process that involves completing multiple tasks.\nTake Snapshots of the etcd Database\n\nTake snapshots of your current etcd database using Rancher Kubernetes Engine (RKE).\n\nStore Snapshot(s) Externally\n\nAfter taking your snapshots, export them to a safe location that won’t be affected if your cluster encounters issues.\n1. Take Snapshots of the etcd DatabaseTake snapshots of your etcd database. You can use these snapshots later to recover from a disaster scenario. There are two ways to take snapshots: recurringly, or as a one-off.  Each option is better suited to a specific use case. Read the short description below each link to know when to use each option.\nOption A: Recurring Snapshots\n\nAfter you stand up a high-availability Rancher install, we recommend configuring RKE to automatically take recurring snapshots so that you always have a safe restoration point available.\n\nOption B: One-Time Snapshots\n\nWe advise taking one-time snapshots before events like upgrades or restoration of another snapshot.\nOption A: Recurring SnapshotsFor all high-availability Rancher installs, we recommend taking recurring snapshots so that you always have a safe restoration point available.To take recurring snapshots, enable the etcd-snapshot service, which is a service that’s included with RKE. This service runs in a service container alongside the etcd container. You can enable this service by adding some code to rancher-cluster.yml.To Enable Recurring Snapshots:\nOpen rancher-cluster.yml with your favorite text editor.\n\nEdit the code for the etcd service to enable recurring snapshots. As of RKE v0.2.0, snapshots can be saved in a S3 compatible backend.\n\nUsing RKE v0.2.0+\n\nservices:\n  etcd:\n    backup_config:\n      enabled: true     # enables recurring etcd snapshots\n      interval_hours: 6 # time increment between snapshots\n      retention: 60     # time in days before snapshot purge\n      # Optional S3\n      s3backupconfig:\n        access_key: \"myaccesskey\"\n        secret_key:  \"myaccesssecret\"\n        bucket_name: \"my-backup-bucket\"\n        folder: \"folder-name\" # Available as of v2.3.0\n        endpoint: \"s3.eu-west-1.amazonaws.com\"\n        region: \"eu-west-1\"\n\n\nUsing RKE v0.1.x\n\nservices:\n  etcd:\n    snapshot: true # enables recurring etcd snapshots\n    creation: 6h0s # time increment between snapshots\n    retention: 24h # time increment before snapshot purge\n\n\nSave and close rancher-cluster.yml.\n\nOpen Terminal and change directory to the location of the RKE binary. Your rancher-cluster.yml file must reside in the same directory.\n\nRun the following command:\n\nrke up --config rancher-cluster.yml\n\nResult: RKE is configured to take recurring snapshots of etcd on all nodes running the etcd role. Snapshots are saved locally to the following directory: /opt/rke/etcd-snapshots/. If configured, the snapshots are also uploaded to your S3 compatible backend.Option B: One-Time SnapshotsWhen you’re about to upgrade Rancher or restore it to a previous snapshot, you should snapshot your live image so that you have a backup of etcd in its last known state.To Take a One-Time Local Snapshot:\nOpen Terminal and change directory to the location of the RKE binary. Your rancher-cluster.yml file must reside in the same directory.\n\nEnter the following command. Replace <SNAPSHOT.db> with any name that you want to use for the snapshot (e.g. upgrade.db).\n\nrke etcd snapshot-save --name <SNAPSHOT.db> --config rancher-cluster.yml\n\nResult: RKE takes a snapshot of etcd running on each etcd node. The file is saved to /opt/rke/etcd-snapshots.To Take a One-Time S3 Snapshot:Available as of RKE v0.2.0\nOpen Terminal and change directory to the location of the RKE binary. Your rancher-cluster.yml file must reside in the same directory.\n\nEnter the following command. Replace <SNAPSHOT.db> with any name that you want to use for the snapshot (e.g. upgrade.db).\nrke etcd snapshot-save --config rancher-cluster.yml --name snapshot-name  \\\n--s3 --access-key S3_ACCESS_KEY --secret-key S3_SECRET_KEY \\\n--bucket-name s3-bucket-name  --s3-endpoint  s3.amazonaws.com \\\n--folder folder-name # Available as of v2.3.0\nResult: RKE takes a snapshot of etcd running on each etcd node. The file is saved to /opt/rke/etcd-snapshots. It is also uploaded to the S3 compatible backend.2. Backup Local Snapshots to a Safe Location\nNote: If you are using RKE v0.2.0, you can enable saving the backups to a S3 compatible backend directly and skip this step.\nAfter taking the etcd snapshots, save them to a safe location so that they’re unaffected if your cluster experiences a disaster scenario. This location should be persistent.In this documentation, as an example, we’re using Amazon S3 as our safe location, and S3cmd as our tool to create the backups. The backup location and tool that you use are ultimately your decision.Example:root@node:~# s3cmd mb s3://rke-etcd-snapshots\nroot@node:~# s3cmd put /opt/rke/etcd-snapshots/snapshot.db s3://rke-etcd-snapshots/\n","postref":"be996e2b4a135c2290ede1785f2e6ba6","objectID":"b340a96032bb4563b606bf99537e7fc5","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/backups/backups/ha-backups/"},{"anchor":"#","title":"Installing Rancher","content":"This section provides an overview of the architecture options of installing Rancher, describing advantages of each option.\n\nTerminology\n\nIn this section,\n\nThe Rancher server manages and provisions Kubernetes clusters. You can interact with downstream Kubernetes clusters through the Rancher server’s user interface.\n\nRKE (Rancher Kubernetes Engine) is a certified Kubernetes distribution and CLI/library which creates and manages a Kubernetes cluster. When you create a cluster in the Rancher UI, it calls RKE as a library to provision Rancher-launched Kubernetes clusters.\n\nOverview of Installation Options\n\nIf you use Rancher to deploy Kubernetes clusters, it is important to ensure that the Rancher server doesn’t fail, because if it goes down, you could lose access to the Kubernetes clusters that are managed by Rancher. For that reason, we recommend that for a production-grade architecture, you should set up a Kubernetes cluster with RKE, then install Rancher on it. After Rancher is installed, you can use Rancher to deploy and manage Kubernetes clusters.\n\nFor testing or demonstration purposes, you can install Rancher in single Docker container. In this installation, you can use Rancher to set up Kubernetes clusters out-of-the-box.\n\nOur instructions for installing Rancher on Kubernetes describe how to first use RKE to create and manage a cluster, then install Rancher onto that cluster. For this type of architecture, you will need to deploy three nodes - typically virtual machines - in the infrastructure provider of your choice. You will also need to configure a load balancer to direct front-end traffic to the three nodes. When the nodes are running and fulfill the node requirements, you can use RKE to deploy Kubernetes onto them, then use Helm to deploy Rancher onto Kubernetes.\n\nFor a longer discussion of Rancher architecture, refer to the architecture overview, recommendations for production-grade architecture, or our best practices guide.\n\nRancher can be installed on these main architectures:\n\n\nHigh-availability Kubernetes Install: We recommend using Helm, a Kubernetes package manager, to install Rancher on a dedicated Kubernetes cluster. We recommend using three nodes in the cluster because increased availability is achieved by running Rancher on multiple nodes.\nSingle-node Kubernetes Install: Another option is to install Rancher with Helm on a Kubernetes cluster, but to only use a single node in the cluster. In this case, the Rancher server doesn’t have high availability, which is important for running Rancher in production. However, this option is useful if you want to save resources by using a single node in the short term, while preserving a high-availability migration path. In the future, you can add nodes to the cluster to get a high-availability Rancher server.\nDocker Install: For test and demonstration purposes, Rancher can be installed with Docker on a single node. This installation works out-of-the-box, but there is no migration path from a Docker installation to a high-availability installation on a Kubernetes cluster. Therefore, you may want to use a Kubernetes installation from the start.\n\n\nThe single-node Kubernetes install is achieved by describing only one node in the cluster.yml when provisioning the Kubernetes cluster with RKE. The single node should have all three roles: etcd, controlplane, and worker. Then Rancher can be installed with Helm on the cluster in the same way that it would be installed on any other cluster.\n\nThere are also separate instructions for installing Rancher in an air gap environment or behind an HTTP proxy:\n\n\n\n\nLevel of Internet Access\nKubernetes Installation - Strongly Recommended\nDocker Installation\n\n\n\n\n\nWith direct access to the Internet\nDocs\nDocs\n\n\n\nBehind an HTTP proxy\nThese docs, plus this configuration\nThese docs, plus this configuration\n\n\n\nIn an air gap environment\nDocs\nDocs\n\n\n\n\nPrerequisites\n\nBefore installing Rancher, make sure that your nodes fulfill all of the installation requirements.\n\nArchitecture Tip\n\nFor the best performance and greater security, we recommend a separate, dedicated Kubernetes cluster for the Rancher management server. Running user workloads on this cluster is not advised. After deploying Rancher, you can create or import clusters for running your workloads.\n\nFor more architecture recommendations, refer to this page.\n\nMore Options for Installations on a Kubernetes Cluster\n\nRefer to the Helm chart options for details on installing Rancher on a Kubernetes cluster with other configurations, including:\n\n\nWith API auditing to record all transactions\nWith TLS termination on a load balancer\nWith a custom Ingress\n\n\nIn the Rancher installation instructions, we recommend using RKE (Rancher Kubernetes Engine) to set up a Kubernetes cluster before installing Rancher on the cluster. RKE has many configuration options for customizing the Kubernetes cluster to suit your specific environment. Please see the RKE Documentation for the full list of options and capabilities.\n\nMore Options for Installations with Docker\n\nRefer to the Docker installation docs for details other configurations including:\n\n\nWith API auditing to record all transactions\nWith an external load balancer\nWith a persistent data store\n\n","postref":"55edcc8f62a5d15ac6265f037f123c82","objectID":"2d4ee4ab56c964fc42debe46ab30f3d8","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/"},{"anchor":"#download-the-rke-binary","title":"Download the RKE binary","content":"After your cluster is up and running, you can start using the generated kubeconfig file to start interacting with your Kubernetes cluster using kubectl.After installation, there are several maintenance items that might arise:\nCertificate Management\nAdding and Removing Nodes in the cluster\n\nImportant\nThe files mentioned below are needed to maintain, troubleshoot and upgrade your cluster.\nSave a copy of the following files in a secure location:\ncluster.yml: The RKE cluster configuration file.\nkube_config_cluster.yml: The Kubeconfig file for the cluster, this file contains credentials for full access to the cluster.\ncluster.rkestate: The Kubernetes Cluster State file, this file contains credentials for full access to the cluster.The Kubernetes Cluster State file is only created when using RKE v0.2.0 or higher.\n\nNote: The “rancher-cluster” parts of the two latter file names are dependent on how you name the RKE cluster configuration file.\nKubernetes Cluster StateThe Kubernetes cluster state, which consists of the cluster configuration file cluster.yml and components certificates in Kubernetes cluster, is saved by RKE, but depending on your RKE version, the cluster state is saved differently.As of v0.2.0, RKE creates a .rkestate file in the same directory that has the cluster configuration file cluster.yml. The .rkestate file contains the current state of the cluster including the RKE configuration and the certificates. It is required to keep this file in order to update the cluster or perform any operation on it through RKE.Prior to v0.2.0, RKE saved the Kubernetes cluster state as a secret. When updating the state, RKE pulls the secret, updates/changes the state and saves a new secret.After you’ve created your cluster.yml, you can deploy your cluster with a simple command. This command assumes the cluster.yml file is in the same directory as where you are running the command.rke up\n\nINFO[0000] Building Kubernetes cluster\nINFO[0000] [dialer] Setup tunnel for host [10.0.0.1]\nINFO[0000] [network] Deploying port listener containers\nINFO[0000] [network] Pulling image [alpine:latest] on host [10.0.0.1]\n...\nINFO[0101] Finished building Kubernetes cluster successfully\nThe last line should read Finished building Kubernetes cluster successfully to indicate that your cluster is ready to use. As part of the Kubernetes creation process, a kubeconfig file has been created and written at kube_config_cluster.yml, which can be used to start interacting with your Kubernetes cluster.\nNote: If you have used a different file name from cluster.yml, then the kube config file will be named kube_config_<FILE_NAME>.yml.\nRKE uses a cluster configuration file, referred to as cluster.yml to determine what nodes will be in the cluster and how to deploy Kubernetes. There are many configuration options that can be set in the cluster.yml. In our example, we will be assuming the minimum of one node for your Kubernetes cluster.There are two easy ways to create a cluster.yml:\nUsing our minimal cluster.yml and updating it based on the node that you will be using.\nUsing rke config to query for all the information needed.\nUsing rke configRun rke config to create a new cluster.yml in the current directory. This command will prompt you for all the information needed to build a cluster. See cluster configuration options for details on the various options.rke config --name cluster.yml\nOther RKE Configuration OptionsYou can create an empty template cluster.yml file by specifying the --empty flag.rke config --empty --name cluster.yml\nInstead of creating a file, you can print the generated configuration to stdout using the --print flag.rke config --print\nHigh AvailabilityRKE is HA ready, you can specify more than one controlplane node in the cluster.yml file. RKE will deploy master components on all of these nodes and the kubelets are configured to connect to 127.0.0.1:6443 by default which is the address of nginx-proxy service that proxy requests to all master nodes.To create an HA cluster, specify more than one host with role controlplane.CertificatesAvailable as of v0.2.0By default, Kubernetes clusters require certificates and RKE auto-generates the certificates for all cluster components. You can also use custom certificates. After the Kubernetes cluster is deployed, you can manage these auto-generated certificates.The Kubernetes cluster components are launched using Docker on a Linux distro. You can use any Linux you want, as long as you can install Docker on it.Review the OS requirements and configure each node appropriately.\nFrom your workstation, open a web browser and navigate to our RKE Releases page. Download the latest RKE installer applicable to your Operating System and Architecture:\n\n\nMacOS: rke_darwin-amd64\nLinux (Intel/AMD): rke_linux-amd64\nLinux (ARM 32-bit): rke_linux-arm\nLinux (ARM 64-bit): rke_linux-arm64\nWindows (32-bit): rke_windows-386.exe\nWindows (64-bit): rke_windows-amd64.exe\n\n\nCopy the RKE binary to a folder in your $PATH and rename it rke (or rke.exe for Windows)\n\n# MacOS\n$ mv rke_darwin-amd64 rke\n# Linux\n$ mv rke_linux-amd64 rke\n# Windows PowerShell\n> mv rke_windows-amd64.exe rke.exe\n\n\nMake the RKE binary that you just downloaded executable. Open Terminal, change directory to the location of the RKE binary, and then run one of the commands below.\n\n\nUsing Windows?\nThe file is already an executable. Skip to Prepare the Nodes for the Kubernetes Cluster.\n\n\n$ chmod +x rke\n\n\nConfirm that RKE is now executable by running the following command:\n\n$ rke --version\n\nAlternative RKE MacOS X Install - HomebrewRKE can also be installed and updated using Homebrew, a package manager for MacOS X.\nInstall Homebrew. See https://brew.sh/ for instructions.\n\nUsing brew, install RKE by running the following command in a Terminal window:\n\n$ brew install rke\n\nIf you have already installed RKE using brew, you can upgrade RKE by running:$ brew upgrade rke\n","postref":"1895d83200b04af55aa5817f1ec94763","objectID":"8d4e9150ab7b6b3616695e510edb1ed3","permalink":"http://jijeesh.github.io/docs/rke/latest/en/installation/"},{"anchor":"#","title":"Private Registry Configuration","content":"Available as of v1.0.0\n\nContainerd can be configured to connect to private registries and use them to pull private images on the node.\n\nUpon startup, K3s will check to see if a registries.yaml file exists at /etc/rancher/k3s/ and instruct containerd to use any registries defined in the file. If you wish to use a private registry, then you will need to create this file as root on each node that will be using the registry.\n\nNote that server nodes are schedulable by default. If you have not tainted the server nodes and will be running workloads on them, please ensure you also create the registries.yaml file on each server as well.\n\nConfiguration in containerd can be used to connect to a private registry with a TLS connection and with registries that enable authentication as well. The following section will explain the registries.yaml file and give different examples of using private registry configuration in K3s.\n\nRegistries Configuration File\n\nThe file consists of two main sections:\n\n\nmirrors\nconfigs\n\n\nMirrors\n\nMirrors is a directive that defines the names and endpoints of the private registries, for example:\n\nmirrors:\n  docker.io:\n    endpoint:\n      - \"https://mycustomreg.com:5000\"\n\n\nEach mirror must have a name and set of endpoints. When pulling an image from a registry, containerd will try these endpoint URLs one by one, and use the first working one.\n\nConfigs\n\nThe configs section defines the TLS and credential configuration for each mirror. For each mirror you can define auth and/or tls. The TLS part consists of:\n\n\n\n\nDirective\nDescription\n\n\n\n\n\ncert_file\nThe client certificate path that will be used to authenticate with the registry\n\n\n\nkey_file\nThe client key path that will be used to authenticate with the registry\n\n\n\nca_file\nDefines the CA certificate path to be used to verify the registry’s server cert file\n\n\n\n\nThe credentials consist of either username/password or authentication token:\n\n\nusername: user name of the private registry basic auth\npassword: user password of the private registry basic auth\nauth: authentication token of the private registry basic auth\n\n\nBelow are basic examples of using private registries in different modes:\n\nWith TLS\n\nBelow are examples showing how you may configure /etc/rancher/k3s/registries.yaml on each node when using TLS.\n\n\n  \n  \n  mirrors:\n  docker.io:\n    endpoint:\n      - \"https://mycustomreg.com:5000\"\nconfigs:\n  \"mycustomreg:5000\":\n    auth:\n      username: xxxxxx # this is the registry username\n      password: xxxxxx # this is the registry password\n    tls:\n      cert_file: # path to the cert file used in the registry\n      key_file:  # path to the key file used in the registry\n      ca_file:   # path to the ca file used in the registry\n\n\n\n\n\n  mirrors:\n  docker.io:\n    endpoint:\n      - \"https://mycustomreg.com:5000\"\nconfigs:\n  \"mycustomreg:5000\":\n    tls:\n      cert_file: # path to the cert file used in the registry\n      key_file:  # path to the key file used in the registry\n      ca_file:   # path to the ca file used in the registry\n\n\n\n\n\n\n\nWithout TLS\n\nBelow are examples showing how you may configure /etc/rancher/k3s/registries.yaml on each node when not using TLS.\n\n\n  \n  \n  mirrors:\n  docker.io:\n    endpoint:\n      - \"http://mycustomreg.com:5000\"\nconfigs:\n  \"mycustomreg:5000\":\n    auth:\n      username: xxxxxx # this is the registry username\n      password: xxxxxx # this is the registry password\n\n\n\n\n\n  mirrors:\n  docker.io:\n    endpoint:\n      - \"http://mycustomreg.com:5000\"\n\n\n\n\n\n\n\n\nIn case of no TLS communication, you need to specify http:// for the endpoints, otherwise it will default to https.\n\n\nIn order for the registry changes to take effect, you need to restart K3s on each node.\n\nAdding Images to the Private Registry\n\nFirst, obtain the k3s-images.txt file from GitHub for the release you are working with.\nPull the K3s images listed on the k3s-images.txt file from docker.io\n\nExample: docker pull docker.io/rancher/coredns-coredns:1.6.3\n\nThen, retag the images to the private registry.\n\nExample: docker tag coredns-coredns:1.6.3 mycustomreg:5000/coredns-coredns\n\nLast, push the images to the private registry.\n\nExample: docker push mycustomreg:5000/coredns-coredns\n","postref":"def87dee9e0677ac3c236a8a0cc489a3","objectID":"08af9a10299b9b6eba5f14406ceecbbc","permalink":"http://jijeesh.github.io/docs/k3s/latest/en/installation/private-registry/"},{"anchor":"#","title":"Air-Gap Install","content":"You can install K3s in an air-gapped environment using two different methods. You can either deploy a private registry and mirror docker.io or you can manually deploy images such as for small clusters.\n\nPrivate Registry Method\n\nThis document assumes you have already created your nodes in your air-gap environment and have a secure Docker private registry on your bastion host.\nIf you have not yet set up a private Docker registry, refer to the official documentation here.\n\nCreate the Registry YAML\n\nFollow the Private Registry Configuration guide to create and configure the registry.yaml file.\n\nOnce you have completed this, you may now go to the Install K3s section below.\n\nManually Deploy Images Method\n\nWe are assuming you have created your nodes in your air-gap environment.\nThis method requires you to manually deploy the necessary images to each node and is appropriate for edge deployments where running a private registry is not practical.\n\nPrepare the Images Directory and K3s Binary\n\nObtain the images tar file for your architecture from the releases page for the version of K3s you will be running.\n\nPlace the tar file in the images directory, for example:\nsudo mkdir -p /var/lib/rancher/k3s/agent/images/\nsudo cp ./k3s-airgap-images-$ARCH.tar /var/lib/rancher/k3s/agent/images/\nPlace the k3s binary at /usr/local/bin/k3s and ensure it is executable.\n\nFollow the steps in the next section to install K3s.\n\nInstall K3s\n\nOnly after you have completed either the Private Registry Method or the Manually Deploy Images Method above should you install K3s.\n\nObtain the K3s binary from the releases page, matching the same version used to get the airgap images.\nObtain the K3s install script at https://get.k3s.io\n\nPlace the binary in /usr/local/bin on each node and ensure it is executable.\nPlace the install script anywhere on each node, and name it install.sh.\n\nInstall Options\n\nYou can install K3s on one or more servers as described below.\n\n\n  \n  \n  To install K3s on a single server simply do the following on the server node.\n\nINSTALL_K3S_SKIP_DOWNLOAD=true ./install.sh\n\n\nThen, to optionally add additional agents do the following on each agent node. Take care to ensure you replace myserver with the IP or valid DNS of the server and replace mynodetoken with the node token from the server typically at /var/lib/rancher/k3s/server/node-token\n\nINSTALL_K3S_SKIP_DOWNLOAD=true K3S_URL=https://myserver:6443 K3S_TOKEN=mynodetoken ./install.sh\n\n\n\n\n\n  Reference the High Availability with an External DB or High Availability with Embedded DB (Experimental) guides. You will be tweaking install commands so you specify INSTALL_K3S_SKIP_DOWNLOAD=true and run your install script locally instead of via curl. You will also utilize INSTALL_K3S_EXEC='args' to supply any arguments to k3s.\n\nFor example, step two of the High Availability with an External DB guide mentions the following:\n\ncurl -sfL https://get.k3s.io | sh -s - server \\\n  --datastore-endpoint=\"mysql://username:password@tcp(hostname:3306)/database-name\"\n\n\nInstead, you would modify such examples like below:\n\nINSTALL_K3S_SKIP_DOWNLOAD=true INSTALL_K3S_EXEC='server --datastore-endpoint=\"mysql://username:password@tcp(hostname:3306)/database-name\"' ./install.sh\n\n\n\n\n\n\n\n\nNote: K3s additionally provides a --resolv-conf flag for kubelets, which may help with configuring DNS in air-gap networks.\n\n\nUpgrading\n\nInstall Script Method\n\nUpgrading an air-gap environment can be accomplished in the following manner:\n\n\nDownload the new air-gap images (tar file) from the releases page for the version of K3s you will be upgrading to. Place the tar in the /var/lib/rancher/k3s/agent/images/ directory on each\nnode. Delete the old tar file.\nCopy and replace the old K3s binary in /usr/local/bin on each node. Copy over the install script at https://get.k3s.io (as it is possible it has changed since the last release). Run the script again just as you had done in the past\nwith the same environment variables.\nRestart the K3s service (if not restarted automatically by installer).\n\n\nAutomated Upgrades Method\n\nAs of v1.17.4+k3s1 K3s supports automated upgrades. To enable this in air-gapped environments, you must ensure the required images are available in your private registry.\n\nYou will need the version of rancher/k3s-upgrade that corresponds to the version of K3s you intend to upgrade to. Note, the image tag replaces the + in the K3s release with a - because Docker images do not support +.\n\nYou will also need the versions of system-upgrade-controller and kubectl that are specified in the system-upgrade-controller manifest YAML that you will deploy. Check for the latest release of the system-upgrade-controller here and download the system-upgrade-controller.yaml to determine the versions you need to push to your private registry. For example, in release v0.4.0 of the system-upgrade-controller, these images are specified in the manifest YAML:\n\nrancher/system-upgrade-controller:v0.4.0\nrancher/kubectl:v0.17.0\n\n\nOnce you have added the necessary rancher/k3s-upgrade, rancher/system-upgrade-controller, and rancher/kubectl images to your private registry, follow the automated upgrades guide.\n","postref":"e07cc1b7438ca0df9cc66153c3b994b8","objectID":"9f22689975aa72774f4b15da269d8895","permalink":"http://jijeesh.github.io/docs/k3s/latest/en/installation/airgap/"},{"anchor":"#","title":"Example YAML","content":"Below is an example RKE template configuration file for reference.\n\nThe YAML in the RKE template uses the same customization that is used when you create an RKE cluster. However, since the YAML is within the context of a Rancher provisioned RKE cluster, the customization from the RKE docs needs to be nested under the rancher_kubernetes_engine directive.\n# \n# Cluster Config\n# \ndocker_root_dir: /var/lib/docker\n\nenable_cluster_alerting: false\n# This setting is not enforced. Clusters\n# created with this sample template\n# would have alerting turned off by default,\n# but end users could still turn alerting\n# on or off.\n\nenable_cluster_monitoring: true \n# This setting is not enforced. Clusters\n# created with this sample template\n# would have monitoring turned on\n# by default, but end users could still\n# turn monitoring on or off.\n\nenable_network_policy: false\nlocal_cluster_auth_endpoint:\n  enabled: true\n# \n# Rancher Config\n# \nrancher_kubernetes_engine_config: # Your RKE template config goes here.\n  addon_job_timeout: 30\n  authentication:\n    strategy: x509\n  ignore_docker_version: true\n# \n# # Currently only nginx ingress provider is supported.\n# # To disable ingress controller, set `provider: none`\n# # To enable ingress on specific nodes, use the node_selector, eg:\n#    provider: nginx\n#    node_selector:\n#      app: ingress\n# \n  ingress:\n    provider: nginx\n  kubernetes_version: v1.15.3-rancher3-1\n  monitoring:\n    provider: metrics-server\n# \n#   If you are using calico on AWS\n# \n#    network:\n#      plugin: calico\n#      calico_network_provider:\n#        cloud_provider: aws\n# \n# # To specify flannel interface\n# \n#    network:\n#      plugin: flannel\n#      flannel_network_provider:\n#      iface: eth1\n# \n# # To specify flannel interface for canal plugin\n# \n#    network:\n#      plugin: canal\n#      canal_network_provider:\n#        iface: eth1\n# \n  network:\n    options:\n      flannel_backend_type: vxlan\n    plugin: canal\n# \n#    services:\n#      kube-api:\n#        service_cluster_ip_range: 10.43.0.0/16\n#      kube-controller:\n#        cluster_cidr: 10.42.0.0/16\n#        service_cluster_ip_range: 10.43.0.0/16\n#      kubelet:\n#        cluster_domain: cluster.local\n#        cluster_dns_server: 10.43.0.10\n# \n  services:\n    etcd:\n      backup_config:\n        enabled: true\n        interval_hours: 12\n        retention: 6\n        safe_timestamp: false\n      creation: 12h\n      extra_args:\n        election-timeout: 5000\n        heartbeat-interval: 500\n      gid: 0\n      retention: 72h\n      snapshot: false\n      uid: 0\n    kube_api:\n      always_pull_images: false\n      pod_security_policy: false\n      service_node_port_range: 30000-32767\n  ssh_agent_auth: false\nwindows_prefered_cluster: false","postref":"cf7d2333d0baeae29f8f7050b704aa50","objectID":"d3d22f29a87d2b6866c8dbd6173a99e7","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/rke-templates/example-yaml/"},{"anchor":"#","title":"FAQ","content":"The FAQ is updated periodically and designed to answer the questions our users most frequently ask about K3s.\n\nIs K3s a suitable replacement for k8s?\n\nK3s is capable of nearly everything k8s can do. It is just a more lightweight version. See the main docs page for more details.\n\nHow can I use my own Ingress instead of Traefik?\n\nSimply start K3s server with --no-deploy=traefik and deploy your ingress.\n\nDoes K3s support Windows?\n\nAt this time K3s does not natively support Windows, however we are open to the idea in the future.\n\nHow can I build from source?\n\nPlease reference the K3s BUILDING.md with instructions.\n","postref":"328183b3677e253cdce1bbecac989cde","objectID":"5d4a7c72ffb38f4212cb03344250ce13","permalink":"http://jijeesh.github.io/docs/k3s/latest/en/faq/"},{"anchor":"#","title":"Kubernetes Dashboard","content":"This installation guide will help you to deploy and configure the Kubernetes Dashboard on K3s.\n\nDeploying the Kubernetes Dashboard\nGITHUB_URL=https://github.com/kubernetes/dashboard/releases\nVERSION_KUBE_DASHBOARD=$(curl -w '%{url_effective}' -I -L -s -S ${GITHUB_URL}/latest -o /dev/null | sed -e 's|.*/||')\nsudo k3s kubectl create -f https://raw.githubusercontent.com/kubernetes/dashboard/${VERSION_KUBE_DASHBOARD}/aio/deploy/recommended.yaml\nDashboard RBAC Configuration\n\n\nImportant: The admin-user created in this guide will have administrative privileges in the Dashboard.\n\n\nCreate the following resource manifest files:\n\ndashboard.admin-user.yml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: admin-user\n  namespace: kubernetes-dashboard\ndashboard.admin-user-role.yml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: admin-user\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n- kind: ServiceAccount\n  name: admin-user\n  namespace: kubernetes-dashboard\nDeploy the admin-user configuration:\nsudo k3s kubectl create -f dashboard.admin-user.yml -f dashboard.admin-user-role.yml\nObtain the Bearer Token\nsudo k3s kubectl -n kubernetes-dashboard describe secret admin-user-token | grep ^token\nLocal Access to the Dashboard\n\nTo access the Dashboard you must create a secure channel to your K3s cluster:\nsudo k3s kubectl proxy\nThe Dashboard is now accessible at:\n\n\nhttp://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/\nSign In with the admin-user Bearer Token\n\n\nAdvanced: Remote Access to the Dashboard\n\nPlease see the Dashboard documentation: Using Port Forwarding to Access Applications in a Cluster.\n\nUpgrading the Dashboard\nsudo k3s kubectl delete ns kubernetes-dashboard\nGITHUB_URL=https://github.com/kubernetes/dashboard/releases\nVERSION_KUBE_DASHBOARD=$(curl -w '%{url_effective}' -I -L -s -S ${GITHUB_URL}/latest -o /dev/null | sed -e 's|.*/||')\nsudo k3s kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/${VERSION_KUBE_DASHBOARD}/aio/deploy/recommended.yaml -f dashboard.admin-user.yml -f dashboard.admin-user-role.yml\nDeleting the Dashboard and admin-user configuration\nsudo k3s kubectl delete ns kubernetes-dashboard","postref":"67020eeb879d7133683940a864366c4b","objectID":"45ced0903a55d368204e43e850550aa6","permalink":"http://jijeesh.github.io/docs/k3s/latest/en/installation/kube-dashboard/"},{"anchor":"#","title":"Uninstalling K3s","content":"If you installed K3s using the installation script, a script to uninstall K3s was generated during installation.\n\nTo uninstall K3s from a server node, run:\n\n/usr/local/bin/k3s-uninstall.sh\n\n\nTo uninstall K3s from an agent node, run:\n\n/usr/local/bin/k3s-agent-uninstall.sh\n\n","postref":"a507757de6a4cbcbd5bd41a797611382","objectID":"c2d3baf549a2e9ff0314375e70e998f3","permalink":"http://jijeesh.github.io/docs/k3s/latest/en/installation/uninstall/"},{"anchor":"#","title":"Known Issues","content":"The Known Issues are updated periodically and designed to inform you about any issues that may not be immediately addressed in the next upcoming release.\n\nSnap Docker\n\nIf you plan to use K3s with docker, Docker installed via a snap package is not recommended as it has been known to cause issues running K3s.\n\nIptables\n\nIf you are running iptables in nftables mode instead of legacy you might encounter issues. We recommend utilizing newer iptables (such as 1.6.1+) to avoid issues.\n\nRootlessKit\n\nRunning K3s with RootlessKit is experimental and has several known issues.\n","postref":"cc1e15f1bb896a023b40480547d652bc","objectID":"4a45d1619cd163c62a0de75dadd37b85","permalink":"http://jijeesh.github.io/docs/k3s/latest/en/known-issues/"},{"anchor":"#","title":"Upgrading Kubernetes","content":"\nPrerequisite: The options below are available only for clusters that are launched using RKE.\n\n\nFollowing an upgrade to the latest version of Rancher, you can update your existing clusters to use the latest supported version of Kubernetes.\n\nBefore a new version of Rancher is released, it’s tested with the latest minor versions of Kubernetes to ensure compatibility. For example, Rancher v2.3.0 is was tested with Kubernetes v1.15.4, v1.14.7, and v1.13.11. For details on which versions of Kubernetes were tested on each Rancher version, refer to the support maintenance terms.\n\nAs of Rancher v2.3.0, the Kubernetes metadata feature was added, which allows Rancher to ship Kubernetes patch versions without upgrading Rancher. For details, refer to the section on Kubernetes metadata.\n\n\nRecommended: Before upgrading Kubernetes, backup your cluster.\n\n\n\nFrom the Global view, find the cluster for which you want to upgrade Kubernetes. Select Vertical Ellipsis (…) > Edit.\n\nExpand Cluster Options.\n\nFrom the Kubernetes Version drop-down, choose the version of Kubernetes that you want to use for the cluster.\n\nClick Save.\n\n\nResult: Kubernetes begins upgrading for the cluster. During the upgrade, your cluster is unavailable.\n","postref":"06abf7da620d25049ed3c00a22b5024f","objectID":"d41228b6cc3a62428ef69cfb0edc40a5","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/upgrading-kubernetes/"},{"anchor":"#","title":"Adding a Pod Security Policy","content":"\nPrerequisite: The options below are available only for clusters that are launched using RKE.\n\n\nWhen your cluster is running pods with security-sensitive configurations, assign it a pod security policy, which is a set of rules that monitors the conditions and settings in your pods. If a pod doesn’t meet the rules specified in your policy, the policy stops it from running.\n\nYou can assign a pod security policy when you provision a cluster. However, if you need to relax or restrict security for your pods later, you can update the policy while editing your cluster.\n\n\nFrom the Global view, find the cluster to which you want to apply a pod security policy. Select Vertical Ellipsis (…) > Edit.\n\nExpand Cluster Options.\n\nFrom Pod Security Policy Support, select Enabled.\n\n\nNote: This option is only available for clusters provisioned by RKE.\n\n\nFrom the Default Pod Security Policy drop-down, select the policy you want to apply to the cluster.\n\nRancher ships with policies of restricted and unrestricted, although you can create custom policies as well.\n\nClick Save.\n\n\nResult: The pod security policy is applied to the cluster and any projects within the cluster.\n\n\nNote: Workloads already running before assignment of a pod security policy are grandfathered in. Even if they don’t meet your pod security policy, workloads running before assignment of the policy continue to run.\n\nTo check if a running workload passes your pod security policy, clone or upgrade it.\n\n","postref":"f228ba163562e53596eb993d8c551da3","objectID":"d51cca89d285e48027766a37a679a3b3","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/pod-security-policy/"},{"anchor":"#","title":"RKE Templates and Infrastructure","content":"In Rancher, RKE templates are used to provision Kubernetes and define Rancher settings, while node templates are used to provision nodes.\n\nTherefore, even if RKE template enforcement is turned on, the end user still has flexibility when picking the underlying hardware when creating a Rancher cluster. The end users of an RKE template can still choose an infrastructure provider and the nodes they want to use.\n\nIf you want to standardize the hardware in your clusters, use RKE templates conjunction with node templates or with a server provisioning tool such as Terraform.\n\nNode Templates\n\nNode templates are responsible for node configuration and node provisioning in Rancher. From your user profile, you can set up node templates to define which templates are used in each of your node pools. With node pools enabled, you can make sure you have the required number of nodes in each node pool, and ensure that all nodes in the pool are the same.\n\nTerraform\n\nTerraform is a server provisioning tool. It uses infrastructure-as-code that lets you create almost every aspect of your infrastructure with Terraform configuration files. It can automate the process of server provisioning in a way that is self-documenting and easy to track in version control.\n\nThis section focuses on how to use Terraform with the Rancher 2 Terraform provider, which is a recommended option to standardize the hardware for your Kubernetes clusters. If you use the Rancher Terraform provider to provision hardware, and then use an RKE template to provision a Kubernetes cluster on that hardware, you can quickly create a comprehensive, production-ready cluster.\n\nTerraform allows you to:\n\n\nDefine almost any kind of infrastructure-as-code, including servers, databases, load balancers, monitoring, firewall settings, and SSL certificates\nLeverage catalog apps and multi-cluster apps\nCodify infrastructure across many platforms, including Rancher and major cloud providers\nCommit infrastructure-as-code to version control\nEasily repeat configuration and setup of infrastructure\nIncorporate infrastructure changes into standard development practices\nPrevent configuration drift, in which some servers become configured differently than others\n\n\nHow Does Terraform Work?\n\nTerraform is written in files with the extension .tf. It is written in HashiCorp Configuration Language, which is a declarative language that lets you define the infrastructure you want in your cluster, the cloud provider you are using, and your credentials for the provider. Then Terraform makes API calls to the provider in order to efficiently create that infrastructure.\n\nTo create a Rancher-provisioned cluster with Terraform, go to your Terraform configuration file and define the provider as Rancher 2. You can set up your Rancher 2 provider with a Rancher API key. Note: The API key has the same permissions and access level as the user it is associated with.\n\nThen Terraform calls the Rancher API to provision your infrastructure, and Rancher calls the infrastructure provider. As an example, if you wanted to use Rancher to provision infrastructure on AWS, you would provide both your Rancher API key and your AWS credentials in the Terraform configuration file or in environment variables so that they could be used to provision the infrastructure.\n\nWhen you need to make changes to your infrastructure, instead of manually updating the servers, you can make changes in the Terraform configuration files. Then those files can be committed to version control, validated, and reviewed as necessary. Then when you run terraform apply, the changes would be deployed.\n\nTips for Working with Terraform\n\n\nThere are examples of how to provide most aspects of a cluster in the documentation for the Rancher 2 provider.\n\nIn the Terraform settings, you can install Docker Machine by using the Docker Machine node driver.\n\nYou can also modify auth in the Terraform provider.\n\nYou can reverse engineer how to do define a setting in Terraform by changing the setting in Rancher, then going back and checking your Terraform state file to see how it maps to the current state of your infrastructure.\n\nIf you want to manage Kubernetes cluster settings, Rancher settings, and hardware settings all in one place, use Terraform modules. You can pass a cluster configuration YAML file or an RKE template configuration file to a Terraform module so that the Terraform module will create it. In that case, you could use your infrastructure-as-code to manage the version control and revision history of both your Kubernetes cluster and its underlying hardware.\n\n\nTip for Creating CIS Benchmark Compliant Clusters\n\nThis section describes one way that you can make security and compliance-related config files standard in your clusters.\n\nWhen you create a CIS benchmark compliant cluster, you have an encryption config file and an audit log config file.\n\nYour infrastructure provisioning system can write those files to disk. Then in your RKE template, you would specify where those files will be, then add your encryption config file and audit log config file as extra mounts to the kube-api-server.\n\nThen you would make sure that the kube-api-server flag in your RKE template uses your CIS-compliant config files.\n\nIn this way, you can create flags that comply with the CIS benchmark.\n\nResources\n\n\nTerraform documentation\nRancher2 Terraform provider documentation\nThe RanchCast - Episode 1: Rancher 2 Terraform Provider: In this demo, Director of Community Jason van Brackel walks through using the Rancher 2 Terraform Provider to provision nodes and create a custom cluster.\n\n","postref":"4695f3f9a9717fd3375d2219f0c666b3","objectID":"d725578895a54e594153ee1c51dec73a","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/rke-templates/rke-templates-and-hardware/"},{"anchor":"#","title":"1. Prepare your Node(s)","content":"This section is about how to prepare your node(s) to install Rancher for your air gapped environment. An air gapped environment could be where Rancher server will be installed offline, behind a firewall, or behind a proxy. There are tabs for either a high availability (recommended) or a Docker installation.\n\nPrerequisites\n\n\n  \n  \n  OS, Docker, Hardware, and Networking\n\nMake sure that your node(s) fulfill the general installation requirements.\n\nPrivate Registry\n\nRancher supports air gap installs using a private registry. You must have your own private registry or other means of distributing Docker images to your machines.\n\nIf you need help with creating a private registry, please refer to the Docker documentation.\n\nCLI Tools\n\nThe following CLI tools are required for the Kubernetes Install. Make sure these tools are installed on your workstation and available in your $PATH.\n\n\nkubectl - Kubernetes command-line tool.\nrke - Rancher Kubernetes Engine, cli for building Kubernetes clusters.\nhelm - Package management for Kubernetes. Refer to the Helm version requirements to choose a version of Helm to install Rancher.\n\n\n\n\n\n  OS, Docker, Hardware, and Networking\n\nMake sure that your node(s) fulfill the general installation requirements.\n\nPrivate Registry\n\nRancher supports air gap installs using a private registry. You must have your own private registry or other means of distributing Docker images to your machines.\n\nIf you need help with creating a private registry, please refer to the Docker documentation.\n\n\n\n\n\n\nSet up Infrastructure\n\n\n  \n  \n  Rancher recommends installing Rancher on a Kubernetes cluster. A highly available Kubernetes install is comprised of three nodes running the Rancher server components on a Kubernetes cluster. The persistence layer (etcd) is also replicated on these three nodes, providing redundancy and data duplication in case one of the nodes fails.\n\nRecommended Architecture\n\n\nDNS for Rancher should resolve to a layer 4 load balancer\nThe Load Balancer should forward port TCP/80 and TCP/443 to all 3 nodes in the Kubernetes cluster.\nThe Ingress controller will redirect HTTP to HTTPS and terminate SSL/TLS on port TCP/443.\nThe Ingress controller will forward traffic to port TCP/80 on the pod in the Rancher deployment.\n\n\nRancher installed on a Kubernetes cluster with layer 4 load balancer, depicting SSL termination at ingress controllers\n\n\n\nA. Provision three air gapped Linux hosts according to our requirements\n\nThese hosts will be disconnected from the internet, but require being able to connect with your private registry.\n\nView hardware and software requirements for each of your cluster nodes in Requirements.\n\nB. Set up your Load Balancer\n\nWhen setting up the Kubernetes cluster that will run the Rancher server components, an Ingress controller pod will be deployed on each of your nodes. The Ingress controller pods are bound to ports TCP/80 and TCP/443 on the host network and are the entry point for HTTPS traffic to the Rancher server.\n\nYou will need to configure a load balancer as a basic Layer 4 TCP forwarder to direct traffic to these ingress controller pods. The exact configuration will vary depending on your environment.\n\n\nImportant:\nOnly use this load balancer (i.e, the local cluster Ingress) to load balance the Rancher server. Sharing this Ingress with other applications may result in websocket errors to Rancher following Ingress configuration reloads for other apps.\n\n\nLoad Balancer Configuration Samples:\n\n\nFor an example showing how to set up an NGINX load balancer, refer to this page.\nFor an example showing how to set up an Amazon NLB load balancer, refer to this page.\n\n\n\n\n\n  The Docker installation is for Rancher users that are wanting to test out Rancher. Instead of running on a Kubernetes cluster, you install the Rancher server component on a single node using a docker run command. Since there is only one node and a single Docker container, if the node goes down, there is no copy of the etcd data available on other nodes and you will lose all the data of your Rancher server.\n\n\nImportant: If you install Rancher following the Docker installation guide, there is no upgrade path to transition your Docker installation to a Kubernetes Installation.\n\n\nInstead of running the Docker installation, you have the option to follow the Kubernetes Install guide, but only use one node to install Rancher. Afterwards, you can scale up the etcd nodes in your Kubernetes cluster to make it a Kubernetes Installation.\n\nA. Provision a single, air gapped Linux host according to our Requirements\n\nThese hosts will be disconnected from the internet, but require being able to connect with your private registry.\n\nView hardware and software requirements for each of your cluster nodes in Requirements.\n\n\n\n\n\n\nNext: Collect and Publish Images to your Private Registry\n","postref":"2f45169d3ebcf8c7ec3d119762d1f5c9","objectID":"9bf054c36a40a4fde9363e466ff651f3","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/air-gap-helm2/prepare-nodes/"},{"anchor":"#","title":"1. Prepare your Node(s)","content":"This section is about how to prepare your node(s) to install Rancher for your air gapped environment. An air gapped environment could be where Rancher server will be installed offline, behind a firewall, or behind a proxy. There are tabs for either a high availability (recommended) or a Docker installation.\n\nPrerequisites\n\n\n  \n  \n  OS, Docker, Hardware, and Networking\n\nMake sure that your node(s) fulfill the general installation requirements.\n\nPrivate Registry\n\nRancher supports air gap installs using a private registry. You must have your own private registry or other means of distributing Docker images to your machines.\n\nIf you need help with creating a private registry, please refer to the Docker documentation.\n\nCLI Tools\n\nThe following CLI tools are required for the Kubernetes Install. Make sure these tools are installed on your workstation and available in your $PATH.\n\n\nkubectl - Kubernetes command-line tool.\nrke - Rancher Kubernetes Engine, cli for building Kubernetes clusters.\nhelm - Package management for Kubernetes. Refer to the Helm version requirements to choose a version of Helm to install Rancher.\n\n\n\n\n\n  OS, Docker, Hardware, and Networking\n\nMake sure that your node(s) fulfill the general installation requirements.\n\nPrivate Registry\n\nRancher supports air gap installs using a private registry. You must have your own private registry or other means of distributing Docker images to your machines.\n\nIf you need help with creating a private registry, please refer to the Docker documentation.\n\n\n\n\n\n\nSet up Infrastructure\n\n\n  \n  \n  Rancher recommends installing Rancher on a Kubernetes cluster. A highly available Kubernetes install is comprised of three nodes running the Rancher server components on a Kubernetes cluster. The persistence layer (etcd) is also replicated on these three nodes, providing redundancy and data duplication in case one of the nodes fails.\n\nRecommended Architecture\n\n\nDNS for Rancher should resolve to a layer 4 load balancer\nThe Load Balancer should forward port TCP/80 and TCP/443 to all 3 nodes in the Kubernetes cluster.\nThe Ingress controller will redirect HTTP to HTTPS and terminate SSL/TLS on port TCP/443.\nThe Ingress controller will forward traffic to port TCP/80 on the pod in the Rancher deployment.\n\n\nRancher installed on a Kubernetes cluster with layer 4 load balancer, depicting SSL termination at ingress controllers\n\n\n\nA. Provision three air gapped Linux hosts according to our requirements\n\nThese hosts will be disconnected from the internet, but require being able to connect with your private registry.\n\nView hardware and software requirements for each of your cluster nodes in Requirements.\n\nB. Set up your Load Balancer\n\nWhen setting up the Kubernetes cluster that will run the Rancher server components, an Ingress controller pod will be deployed on each of your nodes. The Ingress controller pods are bound to ports TCP/80 and TCP/443 on the host network and are the entry point for HTTPS traffic to the Rancher server.\n\nYou will need to configure a load balancer as a basic Layer 4 TCP forwarder to direct traffic to these ingress controller pods. The exact configuration will vary depending on your environment.\n\n\nImportant:\nOnly use this load balancer (i.e, the local cluster Ingress) to load balance the Rancher server. Sharing this Ingress with other applications may result in websocket errors to Rancher following Ingress configuration reloads for other apps.\n\n\nLoad Balancer Configuration Samples:\n\n\nFor an example showing how to set up an NGINX load balancer, refer to this page.\nFor an example showing how to set up an Amazon NLB load balancer, refer to this page.\n\n\n\n\n\n  The Docker installation is for Rancher users that are wanting to test out Rancher. Instead of running on a Kubernetes cluster, you install the Rancher server component on a single node using a docker run command. Since there is only one node and a single Docker container, if the node goes down, there is no copy of the etcd data available on other nodes and you will lose all the data of your Rancher server.\n\n\nImportant: If you install Rancher following the Docker installation guide, there is no upgrade path to transition your Docker installation to a Kubernetes Installation.\n\n\nInstead of running the Docker installation, you have the option to follow the Kubernetes Install guide, but only use one node to install Rancher. Afterwards, you can scale up the etcd nodes in your Kubernetes cluster to make it a Kubernetes Installation.\n\nA. Provision a single, air gapped Linux host according to our Requirements\n\nThese hosts will be disconnected from the internet, but require being able to connect with your private registry.\n\nView hardware and software requirements for each of your cluster nodes in Requirements.\n\n\n\n\n\n\nNext: Collect and Publish Images to your Private Registry\n","postref":"d88626c3863c7eb50d4ecfc7ad14d282","objectID":"75744d3568fa7207f7e7d1c91804a41a","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/other-installation-methods/air-gap/prepare-nodes/"},{"anchor":"#outline","title":"Outline","content":"Although the migration-tool CLI parses your Rancher v1.6 Compose files to Kubernetes manifests, there are discrepancies between v1.6 and v2.x that you must address by manually editing your parsed Kubernetes manifests. In other words, you need to edit each workload and service imported into Rancher v2.x, as displayed below.Edit Migrated ServicesAs mentioned in Migration Tools CLI Output, the output.txt files generated during parsing lists the manual steps you must make for each deployment. Review the upcoming topics for more information on manually editing your Kubernetes specs.Open your output.txt file and take a look at its contents. When you parsed your Compose files into Kubernetes manifests, migration-tools CLI output a manifest for each workload that it creates for Kubernetes. For example, our when our Migration Example Files are parsed into Kubernetes manifests, output.txt lists each resultant parsed Kubernetes manifest file (i.e., workloads). Each workload features a list of action items to restore operations for the workload in v2.x.Output.txt ExampleThe following table lists possible directives that may appear in output.txt, what they mean, and links on how to resolve them.\n\n\nDirective\nInstructions\n\n\n\n\n\nports\nRancher v1.6 Port Mappings cannot be migrated to v2.x. Instead, you must manually declare either a HostPort or NodePort, which are similar to Port Mappings.\n\n\n\nhealth_check\nThe Rancher v1.6 health check microservice has been replaced with native Kubernetes health checks, called probes. Recreate your v1.6 health checks in v2.0 using probes.\n\n\n\nlabels\nRancher v1.6 uses labels to implement a variety of features in v1.6. In v2.x, Kubernetes uses different mechanisms to implement these features. Click through on the links here for instructions on how to address each label.io.rancher.container.pull_image: In v1.6, this label instructed deployed containers to pull a new version of the image upon restart. In v2.x, this functionality is replaced by the  imagePullPolicy directive.io.rancher.scheduler.global: In v1.6, this label scheduled a container replica on every cluster host. In v2.x, this functionality is replaced by Daemon Sets.io.rancher.scheduler.affinity: In v2.x, affinity is applied in a different way.\n\n\n\nlinks\nDuring migration, you must create links between your Kubernetes workloads and services for them to function properly in v2.x.\n\n\n\nscale\nIn v1.6, scale refers to the number of container replicas running on a single node. In v2.x, this feature is replaced by replica sets.\n\n\n\nstart_on_create\nNo Kubernetes equivalent. No action is required from you.\n\n\nNext: Expose Your Services\nNote: Although these instructions deploy your v1.6 services in Rancher v2.x, they will not work correctly until you adjust their Kubernetes manifests.\n\n  \n  \n  You can deploy the Kubernetes manifests created by migration-tools by importing them into Rancher v2.x.\n\n\nReceiving an ImportYaml Error?\n\nDelete the YAML directive listed in the error message. These are YAML directives from your v1.6 services that Kubernetes can’t read.\n\n\nDeploy Services: Import Kubernetes Manifest\n\n\n\n\n\n\n  \nPrerequisite: Install Rancher CLI for Rancher v2.x.\n\n\nUse the following Rancher CLI commands to deploy your application using Rancher v2.x. For each Kubernetes manifest output by migration-tools CLI, enter one of the commands below to import it into Rancher v2.x.\n\n./rancher kubectl create -f <DEPLOYMENT_YAML_FILE> # DEPLOY THE DEPLOYMENT YAML\n\n./rancher kubectl create -f <SERVICE_YAML_FILE> # DEPLOY THE SERVICE YAML\n\n\n\n\nFollowing importation, you can view your v1.6 services in the v2.x UI as Kubernetes manifests by using the context menu to select <CLUSTER> > <PROJECT> that contains your services. The imported manifests will display on the Resources > Workloads and on the tab at Resources > Workloads > Service Discovery. (In Rancher v2.x prior to v2.3.0, these are on the Workloads and Service Discovery tabs in the top navigation bar.)Imported ServicesNext, use the migration-tools CLI to export all stacks in all of the Cattle environments into Compose files. Then, for stacks that you want to migrate to Rancher v2.x, convert the Compose files into Kubernetes manifest.\nPrerequisite: Create an Account API Key to authenticate with Rancher v1.6 when using the migration-tools CLI.\n\nExport the Docker Compose files for your Cattle environments and stacks from Rancher v1.6.\n\nIn the terminal window, execute the following command, replacing each placeholder with your values.\n\nmigration-tools export --url http://<RANCHER_URL:PORT> --access-key <RANCHER_ACCESS_KEY> --secret-key <RANCHER_SECRET_KEY> --export-dir <EXPORT_DIR> --all\n\n\nStep Result: migration-tools exports Compose files (docker-compose.yml and rancher-compose.yml) for each stack in the --export-dir directory. If you omitted this option, Compose files are output to your current directory.\n\nA unique directory is created for each environment and stack. For example, if we export each environment/stack from Rancher v1.6, the following directory structure is created:\n\nexport/                            # migration-tools --export-dir\n|--<ENVIRONMENT>/                  # Rancher v1.6 ENVIRONMENT\n    |--<STACK>/                    # Rancher v1.6 STACK\n         |--docker-compose.yml     # STANDARD DOCKER DIRECTIVES FOR ALL STACK SERVICES\n         |--rancher-compose.yml    # RANCHER-SPECIFIC DIRECTIVES FOR ALL STACK SERVICES\n         |--README.md              # README OF CHANGES FROM v1.6 to v2.x\n\n\nConvert the exported Compose files to Kubernetes manifest.\n\nExecute the following command, replacing each placeholder with the absolute path to your Stack’s Compose files. If you want to migrate multiple stacks, you’ll have to re-run the command for each pair of Compose files that you exported.\n\nmigration-tools parse --docker-file <DOCKER_COMPOSE_ABSOLUTE_PATH> --rancher-file <RANCHER_COMPOSE_ABSOLUTE_PATH>\n\n\n\nNote: If you omit the --docker-file and --rancher-file options from your command, migration-tools uses the current working directory to find Compose files.\n\n\nWant full usage and options for the migration-tools CLI? See the Migration Tools CLI Reference.\nmigration-tools CLI OutputAfter you run the migration-tools parse command, the following files are output to your target directory.\n\n\nOutput\nDescription\n\n\n\n\n\noutput.txt\nThis file lists how to recreate your Rancher v1.6-specific functionality in Kubernetes. Each listing links to the relevant blog articles on how to implement it in Rancher v2.x.\n\n\n\nKubernetes manifest specs\nMigration-tools internally invokes Kompose to generate a Kubernetes manifest for each service you’re migrating to v2.x. Each YAML spec file is named for the service you’re migrating.\n\n\nWhy are There Separate Deployment and Service Manifests?To make an application publicly accessible by URL, a Kubernetes service is required in support of the deployment. A Kubernetes service is a REST object that abstracts access to the pods in the workload. In other words, a service provides a static endpoint to the pods by mapping a URL to pod(s) Therefore, even if the pods change IP address, the public endpoint remains unchanged. A service object points to its corresponding deployment (workload) by using selector labels.When a you export a service from Rancher v1.6 that exposes public ports, migration-tools CLI parses those ports to a Kubernetes service spec that links to a deployment YAML spec.Migration Example File OutputIf we parse the two example files from Migration Example Files, docker-compose.yml and rancher-compose.yml, the following files are output:\n\n\nFile\nDescription\n\n\n\n\n\nweb-deployment.yaml\nA file containing Kubernetes container specs for a Let’s Chat deployment.\n\n\n\nweb-service.yaml\nA file containing specs for the Let’s Chat service.\n\n\n\ndatabase-deployment.yaml\nA file containing container specs for the MongoDB deployment in support of Let’s Chat.\n\n\n\nwebLB-deployment.yaml\nA file containing container specs for an HAProxy deployment that’s serving as a load balancer.1\n\n\n\nwebLB-service.yaml\nA file containing specs for the HAProxy service.1\n\n\n\n1 Because Rancher v2.x uses Ingress for load balancing, we won’t be migrating our Rancher v1.6 load balancer to v2.x.\nAfter you download migration-tools CLI, rename it and make it executable.\nOpen a terminal window and change to the directory that contains the migration-tool file.\n\nRename the file to migration-tools so that it no longer includes the platform name.\n\nEnter the following command to make migration-tools executable:\n\nchmod +x migration-tools\n\nThe migration-tools CLI for your platform can be downloaded from our GitHub releases page. The tools are available for Linux, Mac, and Windows platforms.\nA. Download the migration-tools CLI\nB. Configure the migration-tools CLI\nC. Run the migration-tools CLI\nD. Deploy Services Using Rancher CLI\nWhat Now?\n","postref":"a1d8396fcfb3c2bc9f887ab6c7b0bade","objectID":"f79c25160e436e53fb25a5298d64182b","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/v1.6-migration/run-migration-tool/"},{"anchor":"#rancher-cli","title":"Rancher CLI","content":"Install the kubectl utility. See install kubectl.Configure kubectl by visiting your cluster in the Rancher Web UI then clicking on Kubeconfig, copying contents and putting into your ~/.kube/config file.Run kubectl cluster-info or kubectl get pods successfully.Follow the steps in rancher cli.Ensure you can run rancher kubectl get pods successfully.","postref":"5af5ca69f876444015d3350a7405915c","objectID":"a13cd2271199bcdc43c1ab19860b3839","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/quick-start-guide/cli/"},{"anchor":"#","title":"Deploying Rancher Server","content":"Use one of the following guides to deploy and provision Rancher and a Kubernetes cluster in the provider of your choice.\n\n\nDigitalOcean (uses Terraform)\nAWS (uses Terraform)\nAzure (uses Terraform)\nGCP (uses Terraform)\nVagrant\n\n\nIf you prefer, the following guide will take you through the same process in individual steps. Use this if you want to run Rancher in a different provider, on prem, or if you would just like to see how easy it is.\n\n\nManual Install\n\n","postref":"a1869e675d7d3fcb29bf389790b8686a","objectID":"356b31bd33cc5db8ccf9488bb2a95b77","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/quick-start-guide/deployment/"},{"anchor":"#","title":"Hardening Guide v2.3.5","content":"This document provides prescriptive guidance for hardening a production installation of Rancher v2.3.5. It outlines the configurations and controls required to address Kubernetes benchmark controls from the Center for Information Security (CIS).\n\n\nThis hardening guide describes how to secure the nodes in your cluster, and it is recommended to follow this guide before installing Kubernetes.\n\n\nThis hardening guide is intended to be used with specific versions of the CIS Kubernetes Benchmark, Kubernetes, and Rancher:\n\n\n\n\nHardening Guide Version\nRancher Version\nCIS Benchmark Version\nKubernetes Version\n\n\n\n\n\nHardening Guide v2.3.5\nRancher v2.3.5\nBenchmark v1.5\nKubernetes 1.15\n\n\n\n\nClick here to download a PDF version of this document\n\nOverview\n\nThis document provides prescriptive guidance for hardening a production installation of Rancher v2.3.5 with Kubernetes v1.15. It outlines the configurations required to address Kubernetes benchmark controls from the Center for Information Security (CIS).\n\nFor more detail about evaluating a hardened cluster against the official CIS benchmark, refer to the CIS Benchmark Rancher Self-Assessment Guide - Rancher v2.3.5.\n\nKnown Issues\n\nRancher exec shell and view logs for pods are not functional in a cis 1.5 hardened setup when only public ip is provided when registering custom nodes.\n\nConfigure Kernel Runtime Parameters\n\nThe following sysctl configuration is recommended for all nodes type in the cluster. Set the following parameters in /etc/sysctl.d/90-kubelet.conf:\n\nvm.overcommit_memory=1\nvm.panic_on_oom=0\nkernel.panic=10\nkernel.panic_on_oops=1\nkernel.keys.root_maxbytes=25000000\n\n\nRun sysctl -p /etc/sysctl.d/90-kubelet.conf to enable the settings.\n\nConfigure etcd user and group\n\nA user account and group for the etcd service is required to be setup prior to installing RKE. The uid and gid for the etcd user will be used in the RKE config.yml to set the proper permissions for files and directories during installation time.\n\ncreate etcd user and group\n\nTo create the etcd group run the following console commands.\n\ngroupadd --gid 52034 etcd\nuseradd --comment \"etcd service account\" --uid 52034 --gid 52034 etcd\n\n\nUpdate the RKE config.yml with the uid and gid of the etcd user:\nservices:\n  etcd:\n    gid: 52034\n    uid: 52034\nSet automountServiceAccountToken to false for default service accounts\n\nKubernetes provides a default service account which is used by cluster workloads where no specific service account is assigned to the pod. Where access to the Kubernetes API from a pod is required, a specific service account should be created for that pod, and rights granted to that service account. The default service account should be configured such that it does not provide a service account token and does not have any explicit rights assignments.\n\nFor each namespace the default service account must include this value:\n\nautomountServiceAccountToken: false\n\n\nSave the following yaml to a file called account_update.yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: default\nautomountServiceAccountToken: false\nCreate a bash script file called account_update.sh. Be sure to chmod +x account_update.sh so the script has execute permissions.\n\n#!/bin/bash -e\n\nfor namespace in $(kubectl get namespaces -A -o json | jq -r '.items[].metadata.name'); do\n  kubectl patch serviceaccount default -n ${namespace} -p \"$(cat account_update.yaml)\"\ndone\n\n\nEnsure that all Namespaces have Network Policies defined\n\nRunning different applications on the same Kubernetes cluster creates a risk of one\ncompromised application attacking a neighboring application. Network segmentation is\nimportant to ensure that containers can communicate only with those they are supposed\nto. A network policy is a specification of how selections of pods are allowed to\ncommunicate with each other and other network endpoints.\n\nNetwork Policies are namespace scoped. When a network policy is introduced to a given\nnamespace, all traffic not allowed by the policy is denied. However, if there are no network\npolicies in a namespace all traffic will be allowed into and out of the pods in that\nnamespace. To enforce network policies, a CNI (container network interface) plugin must be enabled.\nThis guide uses canal to provide the policy enforcement.\nAdditional information about CNI providers can be found\nhere\n\nOnce a CNI provider is enabled on a cluster a default network policy can be applied. For reference purposes a\npermissive example is provide below. If you want to allow all traffic to all pods in a namespace\n(even if policies are added that cause some pods to be treated as “isolated”),\nyou can create a policy that explicitly allows all traffic in that namespace. Save the following yaml as\ndefault-allow-all.yaml. Additional documentation\nabout network policies can be found on the Kubernetes site.\n\n\nThis NetworkPolicy is not recommended for production use\n\n---\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-allow-all\nspec:\n  podSelector: {}\n  ingress:\n  - {}\n  egress:\n  - {}\n  policyTypes:\n  - Ingress\n  - Egress\nCreate a bash script file called apply_networkPolicy_to_all_ns.sh. Be sure to\nchmod +x apply_networkPolicy_to_all_ns.sh so the script has execute permissions.\n\n#!/bin/bash -e\n\nfor namespace in $(kubectl get namespaces -A -o json | jq -r '.items[].metadata.name'); do\n  kubectl apply -f default-allow-all.yaml -n ${namespace}\ndone\n\n\nExecute this script to apply the default-allow-all.yaml the permissive NetworkPolicy to all namespaces.\n\nReference Hardened RKE cluster.yml configuration\n\nThe reference cluster.yml is used by the RKE CLI that provides the configuration needed to achieve a hardened install\nof Rancher Kubernetes Engine (RKE). Install documentation is\nprovided with additional details about the configuration items.\n# If you intend to deploy Kubernetes in an air-gapped environment,\n# please consult the documentation on how to configure custom RKE images.\nkubernetes_version: \"v1.15.9-rancher1-1\"\nenable_network_policy: true\ndefault_pod_security_policy_template_id: \"restricted\"\nservices:\n  etcd:\n    uid: 52034\n    gid: 52034\n  kube-api:\n    pod_security_policy: true\n    secrets_encryption_config:\n      enabled: true\n    audit_log:\n      enabled: true\n    admission_configuration:\n    event_rate_limit:\n      enabled: true\n  kube-controller:\n    extra_args:\n      feature-gates: \"RotateKubeletServerCertificate=true\"\n  scheduler:\n    image: \"\"\n    extra_args: {}\n    extra_binds: []\n    extra_env: []\n  kubelet:\n    generate_serving_certificate: true\n    extra_args:\n      feature-gates: \"RotateKubeletServerCertificate=true\"\n      protect-kernel-defaults: \"true\"\n      tls-cipher-suites: \"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256\"\n    extra_binds: []\n    extra_env: []\n    cluster_domain: \"\"\n    infra_container_image: \"\"\n    cluster_dns_server: \"\"\n    fail_swap_on: false\n  kubeproxy:\n    image: \"\"\n    extra_args: {}\n    extra_binds: []\n    extra_env: []\nnetwork:\n  plugin: \"\"\n  options: {}\n  mtu: 0\n  node_selector: {}\nauthentication:\n  strategy: \"\"\n  sans: []\n  webhook: null\naddons: |\n  ---\n  apiVersion: v1\n  kind: Namespace\n  metadata:\n    name: ingress-nginx\n  ---\n  apiVersion: rbac.authorization.k8s.io/v1\n  kind: Role\n  metadata:\n    name: default-psp-role\n    namespace: ingress-nginx\n  rules:\n  - apiGroups:\n    - extensions\n    resourceNames:\n    - default-psp\n    resources:\n    - podsecuritypolicies\n    verbs:\n    - use\n  ---\n  apiVersion: rbac.authorization.k8s.io/v1\n  kind: RoleBinding\n  metadata:\n    name: default-psp-rolebinding\n    namespace: ingress-nginx\n  roleRef:\n    apiGroup: rbac.authorization.k8s.io\n    kind: Role\n    name: default-psp-role\n  subjects:\n  - apiGroup: rbac.authorization.k8s.io\n    kind: Group\n    name: system:serviceaccounts\n  - apiGroup: rbac.authorization.k8s.io\n    kind: Group\n    name: system:authenticated\n  ---\n  apiVersion: v1\n  kind: Namespace\n  metadata:\n    name: cattle-system\n  ---\n  apiVersion: rbac.authorization.k8s.io/v1\n  kind: Role\n  metadata:\n    name: default-psp-role\n    namespace: cattle-system\n  rules:\n  - apiGroups:\n    - extensions\n    resourceNames:\n    - default-psp\n    resources:\n    - podsecuritypolicies\n    verbs:\n    - use\n  ---\n  apiVersion: rbac.authorization.k8s.io/v1\n  kind: RoleBinding\n  metadata:\n    name: default-psp-rolebinding\n    namespace: cattle-system\n  roleRef:\n    apiGroup: rbac.authorization.k8s.io\n    kind: Role\n    name: default-psp-role\n  subjects:\n  - apiGroup: rbac.authorization.k8s.io\n    kind: Group\n    name: system:serviceaccounts\n  - apiGroup: rbac.authorization.k8s.io\n    kind: Group\n    name: system:authenticated\n  ---\n  apiVersion: policy/v1beta1\n  kind: PodSecurityPolicy\n  metadata:\n    name: restricte","postref":"875ee26936aca404e31c6ab88bdb70c0","objectID":"f462c79f82f944abae01fdee28ed9198","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/security/hardening-2.3.5/"},{"anchor":"#","title":"Installing and Configuring kubectl","content":"kubectl is a CLI utility for running commands against Kubernetes clusters. It’s required for many maintenance and administrative tasks in Rancher 2.x.\n\nInstallation\n\nSee kubectl Installation for installation on your operating system.\n\nConfiguration\n\nWhen you create a Kubernetes cluster with RKE, RKE creates a kube_config_rancher-cluster.yml in the local directory that contains credentials to connect to your new cluster with tools like kubectl or helm.\n\nYou can copy this file to $HOME/.kube/config or if you are working with multiple Kubernetes clusters, set the KUBECONFIG environmental variable to the path of kube_config_rancher-cluster.yml.\n\nexport KUBECONFIG=$(pwd)/kube_config_rancher-cluster.yml\n\n\nTest your connectivity with kubectl and see if you can get the list of nodes back.\n\nkubectl get nodes\n NAME                          STATUS    ROLES                      AGE       VERSION\n165.227.114.63                Ready     controlplane,etcd,worker   11m       v1.10.1\n165.227.116.167               Ready     controlplane,etcd,worker   11m       v1.10.1\n165.227.127.226               Ready     controlplane,etcd,worker   11m       v1.10.1\n\n","postref":"5ae0859db21cd059c447a7ddeecb6299","objectID":"88932276681855da7bd94ecc7e4f946e","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/faq/kubectl/"},{"anchor":"#","title":"Kubernetes Components","content":"The commands and steps listed in this section apply to the core Kubernetes components on Rancher Launched Kubernetes clusters.\n\nThis section includes troubleshooting tips in the following categories:\n\n\nTroubleshooting etcd Nodes\nTroubleshooting Controlplane Nodes\nTroubleshooting nginx-proxy Nodes\nTroubleshooting Worker Nodes and Generic Components\n\n\nKubernetes Component Diagram\n\n\nLines show the traffic flow between components. Colors are used purely for visual aid\n","postref":"625cf40771dcef51be3d0515b4fc4904","objectID":"9432866ffcf2b33dddfde049260a5ca4","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/troubleshooting/kubernetes-components/"},{"anchor":"#download","title":"Download","content":"Migration-Tools Export ReferenceThe migration-tools export command exports all stacks from your Rancher v1.6 server into Compose files.Options\n\n\nOption\nRequired?\nDescription\n\n\n\n\n\n--url <VALUE>\n✓\nRancher API endpoint URL (<RANCHER_URL>).\n\n\n\n--access-key <VALUE>\n✓\nRancher API access key. Using an account API key exports all stacks from all cattle environments (<RANCHER_ACCESS_KEY>).\n\n\n\n--secret-key <VALUE>\n✓\nRancher API secret key associated with the access key. (<RANCHER_SECRET_KEY>).\n\n\n\n--export-dir <VALUE>\n\nBase directory that Compose files export to under sub-directories created for each environment/stack (default: Export).\n\n\n\n--all, --a\n\nExport all stacks. Using this flag exports any stack in a state of inactive, stopped, or removing.\n\n\n\n--system, --s\n\nExport system and infrastructure stacks.\n\n\nUsageExecute the following command, replacing each placeholder with your values. The access key and secret key are Account API keys, which will allow you to export from all Cattle environments.migration-tools export --url <RANCHER_URL> --access-key <RANCHER_ACCESS_KEY> --secret-key <RANCHER_SECRET_KEY> --export-dir <EXPORT_DIR>\nResult: The migration-tools CLI exports Compose files for each stack in every Cattle environments in the --export-dir directory. If you omitted this option, the files are saved to your current directory.Migration-Tools Parse ReferenceThe migration-tools parse command parses the Compose files for a stack and uses Kompose to generate an equivalent Kubernetes manifest. It also outputs an output.txt file, which lists all the constructs that will need manual intervention in order to be converted to Kubernetes.Options\n\n\nOption\nRequired?\nDescription\n\n\n\n\n\n--docker-file <VALUE>\n\nParses Docker Compose file to output Kubernetes manifest(default: docker-compose.yml)\n\n\n\n--output-file <VALUE>\n\nName of file that outputs listing checks and advice for conversion (default: output.txt).\n\n\n\n--rancher-file <VALUE>\n\nParses Rancher Compose file to output Kubernetes manifest(default: rancher-compose.yml)\n\n\nSubcommands\n\n\nSubcommand\nDescription\n\n\n\n\n\nhelp, h\nShows a list of options available for use with preceding command.\n\n\nUsageExecute the following command, replacing each placeholder with the absolute path to your Stack’s Compose files. For each stack, you’ll have to re-run the command for each pair of Compose files that was exported.migration-tools parse --docker-file <DOCKER_COMPOSE_ABSOLUTE_PATH> --rancher-file <RANCHER_COMPOSE_ABSOLUTE_PATH>\n\nNote: If you omit the --docker-file and --rancher-file options from your command, the migration-tools CLI checks its home directory for these Compose files.\nResult: The migration-tools CLI parses your Compose files and outputs Kubernetes manifest specs as well as an output.txt file. For each service in the stack, a Kubernetes manifest is created and named the same as your service. The output.txt file lists all constructs for each service in docker-compose.yml that requires special handling to be successfully migrated to Rancher v2.x. Each construct links to the relevant blog articles on how to implement it in Rancher v2.x.The migration-tools CLI includes a handful of global options.\n\n\nGlobal Option\nDescription\n\n\n\n\n\n--debug\nEnables debug logging.\n\n\n\n--log <VALUE>\nOutputs logs to the path you enter.\n\n\n\n--help, -h\nDisplays a list of all commands available.\n\n\n\n--version, -v\nPrints the version of migration-tools CLI in use.\n\n\nmigration-tools [global options] command [command options] [arguments...]\nThe migration-tools CLI for your platform can be downloaded from our GitHub releases page. The tool is available for Linux, Mac, and Windows platforms.","postref":"9a5f2bdfeb113306471147a44340e16c","objectID":"f539fcbc89279c0fcc8cdcd96a86c539","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/v1.6-migration/run-migration-tool/migration-tools-ref/"},{"anchor":"#prerequisites","title":"Prerequisites","content":"\nFrom the quickstart/aws folder, execute terraform destroy --auto-approve.\n\nWait for confirmation that all resources have been destroyed.\n\nClone Rancher Quickstart to a folder using git clone https://github.com/rancher/quickstart.\n\nGo into the AWS folder containing the terraform files by executing cd quickstart/aws.\n\nRename the terraform.tfvars.example file to terraform.tfvars.\n\nEdit terraform.tfvars and customize the following variables:\n\n\naws_access_key - Amazon AWS Access Key\naws_secret_key - Amazon AWS Secret Key\nrancher_server_admin_password - Admin password for created Rancher server\n\n\nOptional: Modify optional variables within terraform.tfvars.\nSee the Quickstart Readme and the AWS Quickstart Readme for more information.\nSuggestions include:\n\n\naws_region - Amazon AWS region, choose the closest instead of the default\nprefix - Prefix for all created resources\ninstance_type - EC2 instance size used, minimum is t3a.medium but t3a.large or t3a.xlarge could be used if within budget\nssh_key_file_name - Use a specific SSH key instead of ~/.ssh/id_rsa (public key is assumed to be ${ssh_key_file_name}.pub)\n\n\nRun terraform init.\n\nInstall the RKE terraform provider, see installation instructions.\n\nTo initiate the creation of the environment, run terraform apply --auto-approve. Then wait for output similar to the following:\n\nApply complete! Resources: 16 added, 0 changed, 0 destroyed.\n\nOutputs:\n\nrancher_node_ip = xx.xx.xx.xx\nrancher_server_url = https://ec2-xx-xx-xx-xx.compute-1.amazonaws.com\nworkload_node_ip = yy.yy.yy.yy\n\n\nPaste the rancher_server_url from the output above into the browser. Log in when prompted (default username is admin, use the password set in rancher_server_admin_password).\nResultTwo Kubernetes clusters are deployed into your AWS account, one running Rancher Server and the other ready for experimentation deployments.What’s Next?Use Rancher to create a deployment. For more information, see Creating Deployments.\nNote\nDeploying to Amazon AWS will incur charges.\n\nAmazon AWS Account: An Amazon AWS Account is required to create resources for deploying Rancher and Kubernetes.\nAmazon AWS Access Key: Use this link to follow a tutorial to create an Amazon AWS Access Key if you don’t have one yet.\nAmazon AWS Key Pair Use this link and follow instructions to create a Key Pair.\nTerraform: Used to provision the server and cluster in Amazon AWS.\n","postref":"8b8fce7f4244f74212af6a0500dc7e5b","objectID":"e33cc9771f50a9aca05a62c548aba0d0","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/quick-start-guide/deployment/amazon-aws-qs/"},{"anchor":"#prerequisites","title":"Prerequisites","content":"\nFrom the quickstart/azure folder, execute terraform destroy --auto-approve.\n\nWait for confirmation that all resources have been destroyed.\n\nClone Rancher Quickstart to a folder using git clone https://github.com/rancher/quickstart.\n\nGo into the Azure folder containing the terraform files by executing cd quickstart/azure.\n\nRename the terraform.tfvars.example file to terraform.tfvars.\n\nEdit terraform.tfvars and customize the following variables:\n\n\nazure_subscription_id - Microsoft Azure Subscription ID\nazure_client_id - Microsoft Azure Client ID\nazure_client_secret - Microsoft Azure Client Secret\nazure_tenant_id - Microsoft Azure Tenant ID\nrancher_server_admin_password - Admin password for created Rancher server\n\n\nOptional: Modify optional variables within terraform.tfvars.\nSee the Quickstart Readme and the Azure Quickstart Readme for more information.\nSuggestions include:\n\n\nazure_location - Microsoft Azure region, choose the closest instead of the default\nprefix - Prefix for all created resources\ninstance_type - Compute instance size used, minimum is Standard_DS2_v2 but Standard_DS2_v3 or Standard_DS3_v2 could be used if within budget\nssh_key_file_name - Use a specific SSH key instead of ~/.ssh/id_rsa (public key is assumed to be ${ssh_key_file_name}.pub)\n\n\nRun terraform init.\n\nInstall the RKE terraform provider, see installation instructions.\n\nTo initiate the creation of the environment, run terraform apply --auto-approve. Then wait for output similar to the following:\n\nApply complete! Resources: 16 added, 0 changed, 0 destroyed.\n\nOutputs:\n\nrancher_node_ip = xx.xx.xx.xx\nrancher_server_url = https://xx-xx-xx-xx.nip.io\nworkload_node_ip = yy.yy.yy.yy\n\n\nPaste the rancher_server_url from the output above into the browser. Log in when prompted (default username is admin, use the password set in rancher_server_admin_password).\nResultTwo Kubernetes clusters are deployed into your Azure account, one running Rancher Server and the other ready for experimentation deployments.What’s Next?Use Rancher to create a deployment. For more information, see Creating Deployments.\nNote\nDeploying to Microsoft Azure will incur charges.\n\nMicrosoft Azure Account: A Microsoft Azure Account is required to create resources for deploying Rancher and Kubernetes.\nMicrosoft Azure Subscription: Use this link to follow a tutorial to create a Microsoft Azure subscription if you don’t have one yet.\nMicsoroft Azure Tenant: Use this link and follow instructions to create a Microsoft Azure tenant.\nMicrosoft Azure Client ID/Secret: Use this link and follow instructions to create a Microsoft Azure client and secret.\nTerraform: Used to provision the server and cluster in Microsoft Azure.\n","postref":"ed3e427133d8675b11b2aa7212e6f563","objectID":"7115c25bb73b83f01ceb971c82e3f073","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/quick-start-guide/deployment/microsoft-azure-qs/"},{"anchor":"#","title":"Rancher Deployment Strategies","content":"There are two recommended deployment strategies. Each one has its own pros and cons. Read more about which one would fit best for your use case:\n\n\nHub and Spoke\nRegional\n\n\nHub & Spoke Strategy\n\n\n\nIn this deployment scenario, there is a single Rancher control plane managing Kubernetes clusters across the globe. The control plane would be run on a high-availability Kubernetes cluster, and there would be impact due to latencies.\n\n\n\n\n    \n    \n    \n    \n    \n    \n\n\n\nPros\n\n\nEnvironments could have nodes and network connectivity across regions.\nSingle control plane interface to view/see all regions and environments.\nKubernetes does not require Rancher to operate and can tolerate losing connectivity to the Rancher control plane.\n\n\nCons\n\n\nSubject to network latencies.\nIf the control plane goes out, global provisioning of new services is unavailable until it is restored. However, each Kubernetes cluster can continue to be managed individually.\n\n\nRegional Strategy\n\n\n\nIn the regional deployment model a control plane is deployed in close proximity to the compute nodes.\n\n\n\n\n    \n    \n    \n    \n    \n    \n\n\n\nPros\n\n\nRancher functionality in regions stay operational if a control plane in another region goes down.\nNetwork latency is greatly reduced, improving the performance of functionality in Rancher.\nUpgrades of the Rancher control plane can be done independently per region.\n\n\nCons\n\n\nOverhead of managing multiple Rancher installations.\nVisibility across global Kubernetes clusters requires multiple interfaces/panes of glass.\nDeploying multi-cluster apps in Rancher requires repeating the process for each Rancher server.\n\n","postref":"df7ff611828d744f47c6d3f9dfbea5d2","objectID":"49a628f6a4bdfd423fd4730150d61e2a","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/best-practices/deployment-strategies/"},{"anchor":"#prerequisites","title":"Prerequisites","content":"\nFrom the quickstart/aws folder, execute terraform destroy --auto-approve.\n\nWait for confirmation that all resources have been destroyed.\n\nClone Rancher Quickstart to a folder using git clone https://github.com/rancher/quickstart.\n\nGo into the DigitalOcean folder containing the terraform files by executing cd quickstart/do.\n\nRename the terraform.tfvars.example file to terraform.tfvars.\n\nEdit terraform.tfvars and customize the following variables:\n\n\ndo_token - DigitalOcean access key\nrancher_server_admin_password - Admin password for created Rancher server\n\n\nOptional: Modify optional variables within terraform.tfvars.\nSee the Quickstart Readme and the DO Quickstart Readme for more information.\nSuggestions include:\n\n\ndo_region - DigitalOcean region, choose the closest instead of the default\nprefix - Prefix for all created resources\ndroplet_size - Droplet size used, minimum is s-2vcpu-4gb but s-4vcpu-8g could be used if within budget\nssh_key_file_name - Use a specific SSH key instead of ~/.ssh/id_rsa (public key is assumed to be ${ssh_key_file_name}.pub)\n\n\nRun terraform init.\n\nInstall the RKE terraform provider, see installation instructions.\n\nTo initiate the creation of the environment, run terraform apply --auto-approve. Then wait for output similar to the following:\n\nApply complete! Resources: 15 added, 0 changed, 0 destroyed.\n\nOutputs:\n\nrancher_node_ip = xx.xx.xx.xx\nrancher_server_url = https://rancher.xx.xx.xx.xx.xip.io\nworkload_node_ip = yy.yy.yy.yy\n\n\nPaste the rancher_server_url from the output above into the browser. Log in when prompted (default username is admin, use the password set in rancher_server_admin_password).\nResultTwo Kubernetes clusters are deployed into your DigitalOcean account, one running Rancher Server and the other ready for experimentation deployments.What’s Next?Use Rancher to create a deployment. For more information, see Creating Deployments.\nNote\nDeploying to DigitalOcean will incur charges.\n\nDigitalOcean Account: You will require an account on DigitalOcean as this is where the server and cluster will run.\nDigitalOcean Access Key: Use this link to create a DigitalOcean Access Key if you don’t have one.\nTerraform: Used to provision the server and cluster to DigitalOcean.\n","postref":"a367a7ee1e39cd55accc5a21fd979bb0","objectID":"f4517d4a7a3009b9a4dda2673fa37c65","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/quick-start-guide/deployment/digital-ocean-qs/"},{"anchor":"#prerequisites","title":"Prerequisites","content":"\nFrom the quickstart/gcp folder, execute terraform destroy --auto-approve.\n\nWait for confirmation that all resources have been destroyed.\n\nClone Rancher Quickstart to a folder using git clone https://github.com/rancher/quickstart.\n\nGo into the GCP folder containing the terraform files by executing cd quickstart/gcp.\n\nRename the terraform.tfvars.example file to terraform.tfvars.\n\nEdit terraform.tfvars and customize the following variables:\n\n\ngcp_account_json - GCP service account file path and file name\nrancher_server_admin_password - Admin password for created Rancher server\n\n\nOptional: Modify optional variables within terraform.tfvars.\nSee the Quickstart Readme and the GCP Quickstart Readme for more information.\nSuggestions include:\n\n\ngcp_region - Google GCP region, choose the closest instead of the default\nprefix - Prefix for all created resources\nmachine_type - Compute instance size used, minimum is n1-standard-1 but n1-standard-2 or n1-standard-4 could be used if within budget\nssh_key_file_name - Use a specific SSH key instead of ~/.ssh/id_rsa (public key is assumed to be ${ssh_key_file_name}.pub)\n\n\nRun terraform init.\n\nInstall the RKE terraform provider, see installation instructions.\n\nTo initiate the creation of the environment, run terraform apply --auto-approve. Then wait for output similar to the following:\n\nApply complete! Resources: 16 added, 0 changed, 0 destroyed.\n\nOutputs:\n\nrancher_node_ip = xx.xx.xx.xx\nrancher_server_url = https://xx-xx-xx-xx.nip.io\nworkload_node_ip = yy.yy.yy.yy\n\n\nPaste the rancher_server_url from the output above into the browser. Log in when prompted (default username is admin, use the password set in rancher_server_admin_password).\nResultTwo Kubernetes clusters are deployed into your GCP account, one running Rancher Server and the other ready for experimentation deployments.What’s Next?Use Rancher to create a deployment. For more information, see Creating Deployments.\nNote\nDeploying to Google GCP will incur charges.\n\nGoogle GCP Account: A Google GCP Account is required to create resources for deploying Rancher and Kubernetes.\nGoogle GCP Project: Use this link to follow a tutorial to create a GCP Project if you don’t have one yet.\nGoogle GCP Service Account: Use this link and follow instructions to create a GCP service account and token file.\nTerraform: Used to provision the server and cluster in Google GCP.\n","postref":"b8e409a57ac79319ff70550c2bebe3a3","objectID":"fff223c3c2aaf7a8ea47921ea514bb43","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/quick-start-guide/deployment/google-gcp-qs/"},{"anchor":"#","title":"Running RancherOS","content":"RancherOS runs on virtualization platforms, cloud providers and bare metal servers. We also support running a local VM on your laptop. To start running RancherOS as quickly as possible, follow our Quick Start Guide.\n\nPlatforms\n\nWorkstation\n\nDocker Machine\n\nBoot from ISO\n\nCloud\n\nAmazon EC2\n\nGoogle Compute Engine\n\nDigitalOcean\n\nAzure\n\nOpenStack\n\nVMware ESXi\n\nAliyun\n\nBare Metal & Virtual Servers\n\nPXE\n\nInstall to Hard Disk\n\nRaspberry Pi\n","postref":"31e4584d466386e6fad84c9667a1a88a","objectID":"8b8ae5410ebfad8b4fd43babf8a822d2","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/running-rancheros/"},{"anchor":"#","title":"SSH Connectivity Errors","content":"Failed to set up SSH tunneling for host [xxx.xxx.xxx.xxx]: Can’t retrieve Docker Info\n\nFailed to dial to /var/run/docker.sock: ssh: rejected: administratively prohibited (open failed)\n\n\nUser specified to connect with does not have permission to access the Docker socket. This can be checked by logging into the host and running the command docker ps:\n\n\n$ ssh -i ssh_privatekey_file user@server\nuser@server$ docker ps\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                    NAMES\n\n\nSee Manage Docker as a non-root user how to set this up properly.\n\n\nWhen using RedHat/CentOS as operating system, you cannot use the user root to connect to the nodes because of Bugzilla #1527565. You will need to add a separate user and configure it to access the Docker socket. See RKE OS Requirements for more on how to set this up.\n\nSSH server version is not version 6.7 or higher. This is needed for socket forwarding to work, which is used to connect to the Docker socket over SSH. This can be checked using sshd -V on the host you are connecting to, or using netcat:\n\n$ nc xxx.xxx.xxx.xxx 22\nSSH-2.0-OpenSSH_6.6.1p1 Ubuntu-2ubuntu2.10\n\n\n\nFailed to dial ssh using address [xxx.xxx.xxx.xxx:xx]: Error configuring SSH: ssh: no key found\n\n\nThe key file specified as ssh_key_path cannot be accessed. Make sure that you specified the private key file (not the public key, .pub), and that the user that is running the rke command can access the private key file.\nThe key file specified as ssh_key_path is malformed. Check if the key is valid by running ssh-keygen -y -e -f private_key_file. This will print the public key of the private key, which will fail if the private key file is not valid.\n\n\nFailed to dial ssh using address [xxx.xxx.xxx.xxx:xx]: ssh: handshake failed: ssh: unable to authenticate, attempted methods [none publickey], no supported methods remain\n\n\nThe key file specified as ssh_key_path is not correct for accessing the node. Double-check if you specified the correct ssh_key_path for the node and if you specified the correct user to connect with.\n\n\nFailed to dial ssh using address [xxx.xxx.xxx.xxx:xx]: Error configuring SSH: ssh: cannot decode encrypted private keys\n\n\nIf you want to use encrypted private keys, you should use ssh-agent to load your keys with your passphrase. You can configure RKE to use that agent by specifying --ssh-agent-auth on the command-line, it will use the SSH_AUTH_SOCK environment variable in the environment where the rke command is run.\n\n\nCannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\n\n\nThe node is not reachable on the configured address and port.\n\n","postref":"e6799618dfe960a5e1d3081fd61a2b90","objectID":"3fef90bc17d92c703f3d03d77e6ce638","permalink":"http://jijeesh.github.io/docs/rke/latest/en/troubleshooting/ssh-connectivity-errors/"},{"anchor":"#","title":"Tips for Running Rancher","content":"A high-availability Kubernetes installation, defined as an installation of Rancher on a Kubernetes cluster with at least three nodes, should be used in any production installation of Rancher, as well as any installation deemed “important.” Multiple Rancher instances running on multiple nodes ensure high availability that cannot be accomplished with a single node environment.\n\nWhen you set up your high-availability Rancher installation, consider the following:\n\nRun Rancher on a Separate Cluster\n\nDon’t run other workloads or microservices in the Kubernetes cluster that Rancher is installed on.\n\nDon’t Run Rancher on a Hosted Kubernetes Environment\n\nWhen the Rancher server is installed on a Kubernetes cluster, it should not be run in a hosted Kubernetes environment such as Google’s GKE, Amazon’s EKS, or Microsoft’s AKS. These hosted Kubernetes solutions do not expose etcd to a degree that is manageable for Rancher, and their customizations can interfere with Rancher operations.\n\nIt is strongly recommended to use hosted infrastructure such as Amazon’s EC2 or Google’s GCE instead. When you create a cluster using RKE on an infrastructure provider, you can configure the cluster to create etcd snapshots as a backup. You can then use RKE or Rancher to restore your cluster from one of these snapshots. In a hosted Kubernetes environment, this backup and restore functionality is not supported.\n\nMake sure nodes are configured correctly for Kubernetes\n\nIt’s important to follow K8s and etcd best practices when deploying your nodes, including disabling swap, double checking you have full network connectivity between all machines in the cluster, using unique hostnames, MAC addresses, and product_uuids for every node, checking that all correct ports are opened, and deploying with ssd backed etcd.  More details can be found in the kubernetes docs and etcd’s performance op guide\n\nWhen using RKE: Backup the Statefile\n\nRKE keeps record of the cluster state in a file called cluster.rkestate. This file is important for the recovery of a cluster and/or the continued maintenance of the cluster through RKE. Because this file contains certificate material, we strongly recommend encrypting this file before backing up. After each run of rke up you should backup the state file.\n\nRun All Nodes in the Cluster in the Same Datacenter\n\nFor best performance, run all three of your nodes in the same geographic datacenter. If you are running nodes in the cloud, such as AWS, run each node in a separate Availability Zone. For example, launch node 1 in us-west-2a, node 2 in us-west-2b, and node 3 in us-west-2c.\n\nDevelopment and Production Environments Should be Similar\n\nIt’s strongly recommended to have a “staging” or “pre-production” environment of the Kubernetes cluster that Rancher runs on. This environment should mirror your production environment as closely as possible in terms of software and hardware configuration.\n\nMonitor Your Clusters to Plan Capacity\n\nThe Rancher server’s Kubernetes cluster should run within the system and hardware requirements as closely as possible. The more you deviate from the system and hardware requirements, the more risk you take.\n\nHowever, metrics-driven capacity planning analysis should be the ultimate guidance for scaling Rancher, because the published requirements take into account a variety of workload types.\n\nUsing Rancher, you can monitor the state and processes of your cluster nodes, Kubernetes components, and software deployments through integration with Prometheus, a leading open-source monitoring solution, and Grafana, which lets you visualize the metrics from Prometheus.\n\nAfter you enable monitoring in the cluster, you can set up a notification channel and cluster alerts to let you know if your cluster is approaching its capacity. You can also use the Prometheus and Grafana monitoring framework to establish a baseline for key metrics as you scale.\n","postref":"779caa1a58dd4d4a8d6b32018f04a60e","objectID":"ad4307720dc58fd30086c9b4da7d4caf","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/best-practices/deployment-types/"},{"anchor":"#","title":"Tips for Setting Up Containers","content":"Running well built containers can greatly impact the overall performance and security of your environment.\n\nBelow are a few tips for setting up your containers.\n\nFor a more detailed discussion of security for containers, you can also refer to Rancher’s Guide to Container Security.\n\nUse a Common Container OS\n\nWhen possible, you should try to standardize on a common container base OS.\n\nSmaller distributions such as Alpine and BusyBox reduce container image size and generally have a smaller attack/vulnerability surface.\n\nPopular distributions such as Ubuntu, Fedora, and CentOS are more field-tested and offer more functionality.\n\nStart with a FROM scratch container\n\nIf your microservice is a standalone static binary, you should use a FROM scratch container.\n\nThe FROM scratch container is an official Docker image that is empty so that you can use it to design minimal images.\n\nThis will have the smallest attack surface and smallest image size.\n\nRun Container Processes as Unprivileged\n\nWhen possible, use a non-privileged user when running processes within your container. While container runtimes provide isolation, vulnerabilities and attacks are still possible. Inadvertent or accidental host mounts can also be impacted if the container is running as root. For details on configuring a security context for a pod or container, refer to the Kubernetes docs.\n\nDefine Resource Limits\n\nApply CPU and memory limits to your pods. This can help manage the resources on your worker nodes and avoid a malfunctioning microservice from impacting other microservices.\n\nIn standard Kubernetes, you can set resource limits on the namespace level. In Rancher, you can set resource limits on the project level and they will propagate to all the namespaces within the project. For details, refer to the Rancher docs.\n\nWhen setting resource quotas, if you set anything related to CPU or Memory (i.e. limits or reservations) on a project or namespace, all containers will require a respective CPU or Memory field set during creation. To avoid setting these limits on each and every container during workload creation, a default container resource limit can be specified on the namespace.\n\nThe Kubernetes docs have more information on how resource limits can be set at the container level and the namespace level.\n\nDefine Resource Requirements\n\nYou should apply CPU and memory requirements to your pods. This is crucial for informing the scheduler which type of compute node your pod needs to be placed on, and ensuring it does not over-provision that node. In Kubernetes, you can set a resource requirement by defining resources.requests in the resource requests field in a pod’s container spec. For details, refer to the Kubernetes docs.\n\n\nNote: If you set a resource limit for the namespace that the pod is deployed in, and the container doesn’t have a specific resource request, the pod will not be allowed to start. To avoid setting these fields on each and every container during workload creation, a default container resource limit can be specified on the namespace.\n\n\nIt is recommended to define resource requirements on the container level because otherwise, the scheduler makes assumptions that will likely not be helpful to your application when the cluster experiences load.\n\nLiveness and Readiness Probes\n\nSet up liveness and readiness probes for your container. Unless your container completely crashes, Kubernetes will not know it’s unhealthy unless you create an endpoint or mechanism that can report container status. Alternatively, make sure your container halts and crashes if unhealthy.\n\nThe Kubernetes docs show how to configure liveness and readiness probes for containers.\n","postref":"490461ee8eb2c67437722971967815a7","objectID":"1157ca04e791b2ae17663cdb6c92c1b8","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/best-practices/containers/"},{"anchor":"#","title":"Upgrades","content":"After RKE has deployed Kubernetes, you can upgrade the versions of the components in your Kubernetes cluster, the definition of the Kubernetes services or the add-ons.\n\nThe default Kubernetes version for each RKE version can be found in the RKE release notes.\n\nYou can also select a newer version of Kubernetes to install for your cluster. Downgrading Kubernetes is not supported.\n\nEach version of RKE has a specific list of supported Kubernetes versions.\n\nIn case the Kubernetes version is defined in the kubernetes_version directive and under the system-images directive are defined, the system-images configuration will take precedence over kubernetes_version.\n\nThis page covers the following topics:\n\n\nPrerequisites\nUpgrading Kubernetes\nListing supported Kubernetes versions\nKubernetes version precedence\nUsing an unsupported Kubernetes version\nMapping the Kubernetes version to services\nService upgrades\nAdd-ons upgrades\n\n\nPrerequisites\n\n\nEnsure that any system_images configuration is absent from the cluster.yml. The Kubernetes version should only be listed under the system_images directive if an unsupported version is being used. Refer to Kubernetes version precedence for more information.\nEnsure that the correct files to manage Kubernetes cluster state are present in the working directory. Refer to the tabs below for the required files, which differ based on the RKE version.\n\n\n\n  \n  \n  The cluster.rkestate file contains the current state of the cluster including the RKE configuration and the certificates.\n\nThis file is created in the same directory that has the cluster configuration file cluster.yml.\n\nIt is required to keep the cluster.rkestate file to perform any operation on the cluster through RKE, or when upgrading a cluster last managed via RKE v0.2.0 or later.\n\n\n\n\n  Ensure that the kube_config_cluster.yml file is present in the working directory.\n\nRKE saves the Kubernetes cluster state as a secret. When updating the state, RKE pulls the secret, updates or changes the state, and saves a new secret. The kube_config_cluster.yml file is required for upgrading a cluster last managed via RKE v0.1.x.\n\n\n\n\n\n\nUpgrading Kubernetes\n\n\nNote: RKE does not support rolling back to previous versions.\n\n\nTo upgrade the Kubernetes version of an RKE-provisioned cluster, set the kubernetes_version string in the cluster.yml to the desired version from the list of supported Kubernetes versions for the specific version of RKE:\nkubernetes_version: \"v1.15.5-rancher1-1\"\nThen invoke rke up:\n\n$ rke up --config cluster.yml\n\n\nListing Supported Kubernetes Versions\n\nPlease refer to the release notes of the RKE version that you are running, to find the list of supported Kubernetes versions as well as the default Kubernetes version.\n\nYou can also list the supported versions and system images of specific version of RKE release with a quick command.\n\n$ rke config --list-version --all\nv1.15.3-rancher2-1\nv1.13.10-rancher1-2\nv1.14.6-rancher2-1\nv1.16.0-beta.1-rancher1-1\n\n\nKubernetes Version Precedence\n\nIn case both kubernetes_version and system_images are defined, the system_images configuration will take precedence over kubernetes_version.\n\nIn addition, if neither kubernetes_version nor system_images are configured in the cluster.yml, RKE will apply the default Kubernetes version for the specific version of RKE used to invoke rke up.\n\nUsing an Unsupported Kubernetes Version\n\nAs of v0.2.0, if a version is defined in kubernetes_version and is not found in the specific list of supported Kubernetes versions, then RKE will error out.\n\nPrior to v0.2.0, if a version is defined in kubernetes_version and is not found in the specific list of supported Kubernetes versions,  the default version from the supported list is used.\n\nIf you want to use a different version from the supported list, please use the system images option.\n\nMapping the Kubernetes Version to Services\n\nIn RKE, kubernetes_version is used to map the version of Kubernetes to the default services, parameters, and options.\n\nFor RKE v0.3.0+, the service defaults are located here.\n\nFor RKE prior to v0.3.0, the service defaults are located here. Note: The version in the path of the service defaults file corresponds to a Rancher version. Therefore, for Rancher v2.1.x, this file should be used.\n\nService Upgrades\n\nServices can be upgraded by changing any of the services arguments or extra_args and running rke up again with the updated configuration file.\n\n\nNote: The following arguments, service_cluster_ip_range or cluster_cidr, cannot be changed as any changes to these arguments will result in a broken cluster. Currently, network pods are not automatically upgraded.\n\n\nAdd-Ons Upgrades\n\nAs of v0.1.8, upgrades to add-ons are supported.\n\nAdd-ons can also be upgraded by changing any of the add-ons and running rke up again with the updated configuration file.\n","postref":"f1695b120b0f3bd1dfacffb481ee616b","objectID":"fc875970b72a54233446f9b64dd767ce","permalink":"http://jijeesh.github.io/docs/rke/latest/en/upgrades/"},{"anchor":"#","title":"Workload with Ingress Quick Start","content":"Prerequisite\n\nYou have a running cluster with at least 1 node.\n\n1. Deploying a Workload\n\nYou’re ready to create your first workload. A workload is an object that includes pods along with other files and info needed to deploy your application.\n\nFor this workload, you’ll be deploying the application Rancher Hello-World.\n\n\nFrom the Clusters page, open the cluster that you just created.\n\nFrom the main menu of the Dashboard, select Projects/Namespaces.\n\nOpen the Project: Default project.\n\nClick Resources > Workloads. In versions prior to v2.3.0, click Workloads > Workloads.\n\nClick Deploy.\n\nStep Result: The Deploy Workload page opens.\n\nEnter a Name for your workload.\n\nFrom the Docker Image field, enter rancher/hello-world. This field is case-sensitive.\n\nLeave the remaining options on their default setting. We’ll tell you about them later.\n\nClick Launch.\n\n\nResult:\n\n\nYour workload is deployed. This process might take a few minutes to complete.\nWhen your workload completes deployment, it’s assigned a state of Active. You can view this status from the project’s Workloads page.\n\n\n\n\n2. Expose The Application Via An Ingress\n\nNow that the application is up and running it needs to be exposed so that other services can connect.\n\n\nFrom the Clusters page, open the cluster that you just created.\n\nFrom the main menu of the Dashboard, select Projects.\n\nOpen the Default project.\n\nClick Resources > Workloads > Load Balancing. In versions prior to v2.3.0, click the Workloads tab. Click on the Load Balancing tab.\n\nClick Add Ingress.\n\nEnter a name i.e. hello.\n\nIn the Target field, drop down the list and choose the name that you set for your service.\n\nEnter 80 in the Port field.\n\nLeave everything else as default and click Save.\n\n\nResult:  The application is assigned a xip.io address and exposed. It may take a minute or two to populate.\n\nView Your Application\n\nFrom the Load Balancing page, click the target link, which will look something like hello.default.xxx.xxx.xxx.xxx.xip.io > hello-world.\n\nYour application will open in a separate window.\n\nFinished\n\nCongratulations! You have successfully deployed a workload exposed via an ingress.\n\nWhat’s Next?\n\nWhen you’re done using your sandbox, destroy the Rancher Server and your cluster. See one of the following:\n\n\nAmazon AWS: Destroying the Environment\nDigitalOcean: Destroying the Environment\nVagrant: Destroying the Environment\n\n","postref":"5b0c1a57874212c089cb848df2941123","objectID":"fa875a236aaa43558f5ebf5e30509572","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/quick-start-guide/workload/quickstart-deploy-workload-ingress/"},{"anchor":"#1-1-rancher-rke-kubernetes-cluster-host-configuration","title":"1.1 - Rancher RKE Kubernetes cluster host configuration","content":"Before apply, replace rancher_kubernetes_engine_config.services.etcd.gid and rancher_kubernetes_engine_config.services.etcd.uid with the proper etcd group and user ids that were created on etcd nodes.\n  \n  RKE template for k8s 1.14\n  \n    # \n# Cluster Config\n# \nanswers: {}\ndefault_pod_security_policy_template_id: restricted\ndocker_root_dir: /var/lib/docker\nenable_cluster_alerting: false\nenable_cluster_monitoring: false\nenable_network_policy: false\nlocal_cluster_auth_endpoint:\n  enabled: false\nname: test-35378\n# \n# Rancher Config\n# \nrancher_kubernetes_engine_config:\n  addon_job_timeout: 30\n  authentication:\n    strategy: x509\n  authorization: {}\n  bastion_host:\n    ssh_agent_auth: false\n  cloud_provider: {}\n  ignore_docker_version: true\n# \n# # Currently only nginx ingress provider is supported.\n# # To disable ingress controller, set `provider: none`\n# # To enable ingress on specific nodes, use the node_selector, eg:\n#    provider: nginx\n#    node_selector:\n#      app: ingress\n# \n  ingress:\n    provider: nginx\n  kubernetes_version: v1.14.9-rancher1-1\n  monitoring:\n    provider: metrics-server\n# \n#   If you are using calico on AWS\n# \n#    network:\n#      plugin: calico\n#      calico_network_provider:\n#        cloud_provider: aws\n# \n# # To specify flannel interface\n# \n#    network:\n#      plugin: flannel\n#      flannel_network_provider:\n#      iface: eth1\n# \n# # To specify flannel interface for canal plugin\n# \n#    network:\n#      plugin: canal\n#      canal_network_provider:\n#        iface: eth1\n# \n  network:\n    options:\n      flannel_backend_type: vxlan\n    plugin: canal\n  restore:\n    restore: false\n# \n#    services:\n#      kube-api:\n#        service_cluster_ip_range: 10.43.0.0/16\n#      kube-controller:\n#        cluster_cidr: 10.42.0.0/16\n#        service_cluster_ip_range: 10.43.0.0/16\n#      kubelet:\n#        cluster_domain: cluster.local\n#        cluster_dns_server: 10.43.0.10\n# \n  services:\n    etcd:\n      backup_config:\n        enabled: true\n        interval_hours: 12\n        retention: 6\n        safe_timestamp: false\n      creation: 12h\n      extra_args:\n        election-timeout: '5000'\n        heartbeat-interval: '500'\n      gid: 1000\n      retention: 72h\n      snapshot: false\n      uid: 1000\n    kube-api:\n      always_pull_images: true\n      audit_log:\n        enabled: true\n      event_rate_limit:\n        enabled: true\n      extra_args:\n        anonymous-auth: 'false'\n        enable-admission-plugins: >-\n          ServiceAccount,NamespaceLifecycle,LimitRanger,PersistentVolumeLabel,DefaultStorageClass,ResourceQuota,DefaultTolerationSeconds,AlwaysPullImages,DenyEscalatingExec,NodeRestriction,PodSecurityPolicy,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,Priority,EventRateLimit\n        profiling: 'false'\n        service-account-lookup: 'true'\n        tls-cipher-suites: >-\n          TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256\n      extra_binds:\n        - '/opt/kubernetes:/opt/kubernetes'\n      pod_security_policy: true\n      secrets_encryption_config:\n        enabled: true\n      service_node_port_range: 30000-32767\n    kube-controller:\n      extra_args:\n        address: 127.0.0.1\n        feature-gates: RotateKubeletServerCertificate=true\n        profiling: 'false'\n        terminated-pod-gc-threshold: '1000'\n    kubelet:\n      extra_args:\n        protect-kernel-defaults: 'true'\n      fail_swap_on: false\n      generate_serving_certificate: true\n    kubeproxy: {}\n    scheduler:\n      extra_args:\n        address: 127.0.0.1\n        profiling: 'false'\n  ssh_agent_auth: false\nwindows_prefered_cluster: false\n  \n\n  \n  RKE template for k8s 1.15\n  \n    # \n# Cluster Config\n# \ndefault_pod_security_policy_template_id: restricted\ndocker_root_dir: /var/lib/docker\nenable_cluster_alerting: false\nenable_cluster_monitoring: false\nenable_network_policy: false\nlocal_cluster_auth_endpoint:\n  enabled: true\n# \n# Rancher Config\n# \nrancher_kubernetes_engine_config:\n  addon_job_timeout: 30\n  authentication:\n    strategy: x509\n  ignore_docker_version: true\n# \n# # Currently only nginx ingress provider is supported.\n# # To disable ingress controller, set `provider: none`\n# # To enable ingress on specific nodes, use the node_selector, eg:\n#    provider: nginx\n#    node_selector:\n#      app: ingress\n# \n  ingress:\n    provider: nginx\n  kubernetes_version: v1.15.6-rancher1-2\n  monitoring:\n    provider: metrics-server\n# \n#   If you are using calico on AWS\n# \n#    network:\n#      plugin: calico\n#      calico_network_provider:\n#        cloud_provider: aws\n# \n# # To specify flannel interface\n# \n#    network:\n#      plugin: flannel\n#      flannel_network_provider:\n#      iface: eth1\n# \n# # To specify flannel interface for canal plugin\n# \n#    network:\n#      plugin: canal\n#      canal_network_provider:\n#        iface: eth1\n# \n  network:\n    options:\n      flannel_backend_type: vxlan\n    plugin: canal\n# \n#    services:\n#      kube-api:\n#        service_cluster_ip_range: 10.43.0.0/16\n#      kube-controller:\n#        cluster_cidr: 10.42.0.0/16\n#        service_cluster_ip_range: 10.43.0.0/16\n#      kubelet:\n#        cluster_domain: cluster.local\n#        cluster_dns_server: 10.43.0.10\n# \n  services:\n    etcd:\n      backup_config:\n        enabled: true\n        interval_hours: 12\n        retention: 6\n        safe_timestamp: false\n      creation: 12h\n      extra_args:\n        election-timeout: 5000\n        heartbeat-interval: 500\n      gid: 1000\n      retention: 72h\n      snapshot: false\n      uid: 1000\n    kube_api:\n      always_pull_images: true\n      pod_security_policy: true\n      service_node_port_range: 30000-32767\n      event_rate_limit:\n        enabled: true\n      audit_log:\n        enabled: true\n      secrets_encryption_config:\n        enabled: true\n      extra_args:\n        anonymous-auth: \"false\"\n        enable-admission-plugins: \"ServiceAccount,NamespaceLifecycle,LimitRanger,PersistentVolumeLabel,DefaultStorageClass,ResourceQuota,DefaultTolerationSeconds,AlwaysPullImages,DenyEscalatingExec,NodeRestriction,EventRateLimit,PodSecurityPolicy\"\n        profiling: \"false\"\n        service-account-lookup: \"true\"\n        tls-cipher-suites: \"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256\"\n      extra_binds:\n        - \"/opt/kubernetes:/opt/kubernetes\"\n    kubelet:\n      generate_serving_certificate: true\n      extra_args:\n        feature-gates: \"RotateKubeletServerCertificate=true\"\n        protect-kernel-defaults: \"true\"\n        tls-cipher-suites: \"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256\"\n    kube-controller:\n      extra_args:\n        profiling: \"false\"\n        address: \"127.0.0.1\"\n        terminated-pod-gc-threshold: \"1000\"\n        feature-gates: \"RotateKubeletServerCertificate=true\"\n    scheduler:\n      extra_args:\n        profiling: \"false\"\n        address: \"127.0.0.1\"\n  ssh_agent_auth: false\nwindows_prefered_cluster: false\n  \n\n  \n  RKE template for k8s 1.16\n  \n    # \n# Cluster Config\n# \ndefault_pod_security_policy_template_id: restricted\ndocker_root_dir: /var/lib/docker\nenable_cluster_alerting: false\nenable_cluster_monitoring: false\nenable_network_policy: false\nlocal_cluster_auth_endpoint:\n  enabled: true\n# \n# Rancher Config\n# \nrancher_kubernetes_engine_config:\n  addon_job_timeout: 30\n  authentication:\n    strategy: x509\n  ignore_docker_version: true\n# \n# # Currently only nginx ingress provider is supported.\n# # To disable ingress controller, set `provider: none`\n# # To enable ingress on specific nodes, use the node_selector, eg:\n#    provider: nginx\n#    node_selector:\n#      app: ingress\n# \n  ingress:\n    provider: nginx\n  kubernetes_version: v1.16.3-rancher1-1\n  monitoring:\n    provider: metrics-server\n# \n#   If you are using calico on AWS\n# \n#    network:\n#      plugin: calico\n#      calico_network_provider:\n#        cloud_provider: aws\n# \n# # To specify flannel interface\n# \n#    network:\n#      plugin: flannel\n#      flannel_network_provider:\n#      iface: eth1\n# \n# # To specify flannel interface for canal plugin\n# \n#    network:\n#      plugin: canal\n#      canal_network_provider:\n#        iface: eth1\n# \n  network:\n    options:\n      flannel_backend_type: vxlan\n    plugin: canal\n# \n#    services:\n#      kube-api:\n#        service_cluster_ip_range: 10.43.0.0/16\n#      kube-controller:\n#        cluster_cidr: 10.42.0","postref":"85d8db5b677021564029eb47002fd54d","objectID":"c27790fbe3b94e5429bd8fc3454a1170","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/security/hardening-2.3.3/"},{"anchor":"#","title":"Kubernetes resources","content":"The commands/steps listed on this page can be used to check the most important Kubernetes resources and apply to Rancher Launched Kubernetes clusters.\n\nMake sure you configured the correct kubeconfig (for example, export KUBECONFIG=$PWD/kube_config_rancher-cluster.yml for Rancher HA) or are using the embedded kubectl via the UI.\n\nNodes\n\nGet nodes\n\nRun the command below and check the following:\n\n\nAll nodes in your cluster should be listed, make sure there is not one missing.\nAll nodes should have the Ready status (if not in Ready state, check the kubelet container logs on that node using docker logs kubelet)\nCheck if all nodes report the correct version.\nCheck if OS/Kernel/Docker values are shown as expected (possibly you can relate issues due to upgraded OS/Kernel/Docker)\n\n\nkubectl get nodes -o wide\n\n\nExample output:\n\nNAME             STATUS   ROLES          AGE   VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME\ncontrolplane-0   Ready    controlplane   31m   v1.13.5   138.68.188.91    <none>        Ubuntu 18.04.2 LTS   4.15.0-47-generic   docker://18.9.5\netcd-0           Ready    etcd           31m   v1.13.5   138.68.180.33    <none>        Ubuntu 18.04.2 LTS   4.15.0-47-generic   docker://18.9.5\nworker-0         Ready    worker         30m   v1.13.5   139.59.179.88    <none>        Ubuntu 18.04.2 LTS   4.15.0-47-generic   docker://18.9.5\n\n\nGet node conditions\n\nRun the command below to list nodes with Node Conditions\n\nkubectl get nodes -o go-template='{{range .items}}{{$node := .}}{{range .status.conditions}}{{$node.metadata.name}}{{\": \"}}{{.type}}{{\":\"}}{{.status}}{{\"\\n\"}}{{end}}{{end}}'\n\n\nRun the command below to list nodes with Node Conditions that are active that could prevent normal operation.\n\nkubectl get nodes -o go-template='{{range .items}}{{$node := .}}{{range .status.conditions}}{{if ne .type \"Ready\"}}{{if eq .status \"True\"}}{{$node.metadata.name}}{{\": \"}}{{.type}}{{\":\"}}{{.status}}{{\"\\n\"}}{{end}}{{else}}{{if ne .status \"True\"}}{{$node.metadata.name}}{{\": \"}}{{.type}}{{\": \"}}{{.status}}{{\"\\n\"}}{{end}}{{end}}{{end}}{{end}}'\n\n\nExample output:\n\nworker-0: DiskPressure:True\n\n\nKubernetes leader election\n\nKubernetes Controller Manager leader\n\nThe leader is determined by a leader election process. After the leader has been determined, the leader (holderIdentity) is saved in the kube-controller-manager endpoint (in this example, controlplane-0).\n\nkubectl -n kube-system get endpoints kube-controller-manager -o jsonpath='{.metadata.annotations.control-plane\\.alpha\\.kubernetes\\.io/leader}'\n{\"holderIdentity\":\"controlplane-0_xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\"leaseDurationSeconds\":15,\"acquireTime\":\"2018-12-27T08:59:45Z\",\"renewTime\":\"2018-12-27T09:44:57Z\",\"leaderTransitions\":0}>\n\n\nKubernetes Scheduler leader\n\nThe leader is determined by a leader election process. After the leader has been determined, the leader (holderIdentity) is saved in the kube-scheduler endpoint (in this example, controlplane-0).\n\nkubectl -n kube-system get endpoints kube-scheduler -o jsonpath='{.metadata.annotations.control-plane\\.alpha\\.kubernetes\\.io/leader}'\n{\"holderIdentity\":\"controlplane-0_xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\"leaseDurationSeconds\":15,\"acquireTime\":\"2018-12-27T08:59:45Z\",\"renewTime\":\"2018-12-27T09:44:57Z\",\"leaderTransitions\":0}>\n\n\nIngress Controller\n\nThe default Ingress Controller is NGINX and is deployed as a DaemonSet in the ingress-nginx namespace. The pods are only scheduled to nodes with the worker role.\n\nCheck if the pods are running on all nodes:\n\nkubectl -n ingress-nginx get pods -o wide\n\n\nExample output:\n\nkubectl -n ingress-nginx get pods -o wide\nNAME                                    READY     STATUS    RESTARTS   AGE       IP               NODE\ndefault-http-backend-797c5bc547-kwwlq   1/1       Running   0          17m       x.x.x.x          worker-1\nnginx-ingress-controller-4qd64          1/1       Running   0          14m       x.x.x.x          worker-1\nnginx-ingress-controller-8wxhm          1/1       Running   0          13m       x.x.x.x          worker-0\n\n\nIf a pod is unable to run (Status is not Running, Ready status is not showing 1/1 or you see a high count of Restarts), check the pod details, logs and namespace events.\n\nPod details\n\nkubectl -n ingress-nginx describe pods -l app=ingress-nginx\n\n\nPod container logs\n\nkubectl -n ingress-nginx logs -l app=ingress-nginx\n\n\nNamespace events\n\nkubectl -n ingress-nginx get events\n\n\nDebug logging\n\nTo enable debug logging:\n\nkubectl -n ingress-nginx patch ds nginx-ingress-controller --type='json' -p='[{\"op\": \"add\", \"path\": \"/spec/template/spec/containers/0/args/-\", \"value\": \"--v=5\"}]'\n\n\nCheck configuration\n\nRetrieve generated configuration in each pod:\n\nkubectl -n ingress-nginx get pods -l app=ingress-nginx --no-headers -o custom-columns=.NAME:.metadata.name | while read pod; do kubectl -n ingress-nginx exec $pod -- cat /etc/nginx/nginx.conf; done\n\n\nRancher agents\n\nCommunication to the cluster (Kubernetes API via cattle-cluster-agent) and communication to the nodes (cluster provisioning via cattle-node-agent) is done through Rancher agents.\n\ncattle-node-agent\n\nCheck if the cattle-node-agent pods are present on each node, have status Running and don’t have a high count of Restarts:\n\nkubectl -n cattle-system get pods -l app=cattle-agent -o wide\n\n\nExample output:\n\nNAME                      READY     STATUS    RESTARTS   AGE       IP                NODE\ncattle-node-agent-4gc2p   1/1       Running   0          2h        x.x.x.x           worker-1\ncattle-node-agent-8cxkk   1/1       Running   0          2h        x.x.x.x           etcd-1\ncattle-node-agent-kzrlg   1/1       Running   0          2h        x.x.x.x           etcd-0\ncattle-node-agent-nclz9   1/1       Running   0          2h        x.x.x.x           controlplane-0\ncattle-node-agent-pwxp7   1/1       Running   0          2h        x.x.x.x           worker-0\ncattle-node-agent-t5484   1/1       Running   0          2h        x.x.x.x           controlplane-1\ncattle-node-agent-t8mtz   1/1       Running   0          2h        x.x.x.x           etcd-2\n\n\nCheck logging of a specific cattle-node-agent pod or all cattle-node-agent pods:\n\nkubectl -n cattle-system logs -l app=cattle-agent\n\n\ncattle-cluster-agent\n\nCheck if the cattle-cluster-agent pod is present in the cluster, has status Running and doesn’t have a high count of Restarts:\n\nkubectl -n cattle-system get pods -l app=cattle-cluster-agent -o wide\n\n\nExample output:\n\nNAME                                    READY     STATUS    RESTARTS   AGE       IP           NODE\ncattle-cluster-agent-54d7c6c54d-ht9h4   1/1       Running   0          2h        x.x.x.x      worker-1\n\n\nCheck logging of cattle-cluster-agent pod:\n\nkubectl -n cattle-system logs -l app=cattle-cluster-agent\n\n\nGeneric\n\nAll pods/jobs should have status Running/Completed\n\nTo check, run the command:\n\nkubectl get pods --all-namespaces\n\n\nIf a pod is not in Running state, you can dig into the root cause by running:\n\nDescribe pod\n\nkubectl describe pod POD_NAME -n NAMESPACE\n\n\nPod container logs\n\nkubectl logs POD_NAME -n NAMESPACE\n\n\nIf a job is not in Completed state, you can dig into the root cause by running:\n\nDescribe job\n\nkubectl describe job JOB_NAME -n NAMESPACE\n\n\nLogs from the containers of pods of the job\n\nkubectl logs -l job-name=JOB_NAME -n NAMESPACE\n\n\nEvicted pods\n\nPods can be evicted based on eviction signals.\n\nRetrieve a list of evicted pods (podname and namespace):\n\nkubectl get pods --all-namespaces -o go-template='{{range .items}}{{if eq .status.phase \"Failed\"}}{{if eq .status.reason \"Evicted\"}}{{.metadata.name}}{{\" \"}}{{.metadata.namespace}}{{\"\\n\"}}{{end}}{{end}}{{end}}'\n\n\nTo delete all evicted pods:\n\nkubectl get pods --all-namespaces -o go-template='{{range .items}}{{if eq .status.phase \"Failed\"}}{{if eq .status.reason \"Evicted\"}}{{.metadata.name}}{{\" \"}}{{.metadata.namespace}}{{\"\\n\"}}{{end}}{{end}}{{end}}' | while read epod enamespace; do kubectl -n $enamespace delete pod $epod; done\n\n\nRetrieve a list of evicted pods, scheduled node and the reason:\n\nkubectl get pods --all-namespaces -o go-template='{{range .items}}{{if eq .status.phase \"Failed\"}}{{if eq .status.reason \"Evicted\"}}{{.metadata.name}}{{\" \"}}{{.metadata.namespace}}{{\"\\n\"}}{{end}}{{end}}{{end}}' | while read epod enamespace; do kubectl -n $enamespace get pod $epod -o=custom-columns=NAME:.metadata.name,NODE:.spec.nodeName,MSG:.status.message; done\n\n","postref":"79ca814011b9031695e4a8b94e92eea0","objectID":"44981d53fb54d893da1563a21e0f3ed8","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/troubleshooting/kubernetes-resources/"},{"anchor":"#","title":"Tips for Scaling, Security and Reliability","content":"Rancher allows you to set up numerous combinations of configurations. Some configurations are more appropriate for development and testing, while there are other best practices for production environments for maximum availability and fault tolerance. The following best practices should be followed for production.\n\nTips for Preventing and Handling Problems\n\nThese tips can help you solve problems before they happen.\n\nRun Rancher on a Supported OS and Supported Docker Version\n\nRancher is container-based and can potentially run on any Linux-based operating system. However, only operating systems listed in the requirements documentation should be used for running Rancher, along with a supported version of Docker. These versions have been most thoroughly tested and can be properly supported by the Rancher Support team.\n\nUpgrade Your Kubernetes Version\n\nKeep your Kubernetes cluster up to date with a recent and supported version. Typically the Kubernetes community will support the current version and previous three minor releases (for example, 1.14.x, 1.13.x, 1.12.x, and 1.11.x). After a new version is released, the third-oldest supported version reaches EOL (End of Life) status. Running on an EOL release can be a risk if a security issues are found and patches are not available. The community typically makes minor releases every quarter (every three months).\n\nRancher’s SLAs are not community dependent, but as Kubernetes is a community-driven software, the quality of experience will degrade as you get farther away from the community’s supported target.\n\nKill Pods Randomly During Testing\n\nRun chaoskube or a similar mechanism to randomly kill pods in your test environment. This will test the resiliency of your infrastructure and the ability of Kubernetes to self-heal. It’s not recommended to run this in your production environment.\n\nDeploy Complicated Clusters with Terraform\n\nRancher’s “Add Cluster” UI is preferable for getting started with Kubernetes cluster orchestration or for simple use cases. However, for more complex or demanding use cases, it is recommended to use a CLI/API driven approach. Terraform is recommended as the tooling to implement this. When you use Terraform with version control and a CI/CD environment, you can have high assurances of consistency and reliability when deploying Kubernetes clusters. This approach also gives you the most customization options.\n\nRancher maintains a Terraform provider for working with Rancher 2.0 Kubernetes. It is called the Rancher2 Provider.\n\nUpgrade Rancher in a Staging Environment\n\nAll upgrades, both patch and feature upgrades, should be first tested on a staging environment before production is upgraded. The more closely the staging environment mirrors production, the higher chance your production upgrade will be successful.\n\nRenew Certificates Before they Expire\n\nMultiple people in your organization should set up calendar reminders for certificate renewal. Consider renewing the certificate two weeks to one month in advance. If you have multiple certificates to track, consider using monitoring and alerting mechanisms to track certificate expiration.\n\nRancher-provisioned Kubernetes clusters will use certificates that expire in one year. Clusters provisioned by other means may have a longer or shorter expiration.\n\nCertificates can be renewed for Rancher-provisioned clusters through the Rancher user interface.\n\nEnable Recurring Snapshots for Backing up and Restoring the Cluster\n\nMake sure etcd recurring snapshots are enabled. Extend the snapshot retention to a period of time that meets your business needs. In the event of a catastrophic failure or deletion of data, this may be your only recourse for recovery. For details about configuring snapshots, refer to the RKE documentation or the Rancher documentation on backups.\n\nProvision Clusters with Rancher\n\nWhen possible, use Rancher to provision your Kubernetes cluster rather than importing a cluster. This will ensure the best compatibility and supportability.\n\nUse Stable and Supported Rancher Versions for Production\n\nDo not upgrade production environments to alpha, beta, release candidate (rc), or “latest” versions. These early releases are often not stable and may not have a future upgrade path.\n\nWhen installing or upgrading a non-production environment to an early release, anticipate problems such as features not working, data loss, outages, and inability to upgrade without a reinstall.\n\nMake sure the feature version you are upgrading to is considered “stable” as determined by Rancher. Use the beta, release candidate, and “latest” versions in a testing, development, or demo environment to try out new features. Feature version upgrades, for example 2.1.x to 2.2.x, should be considered as and when they are released. Some bug fixes and most features are not back ported into older versions.\n\nKeep in mind that Rancher does End of Life support for old versions, so you will eventually want to upgrade if you want to continue to receive patches.\n\nFor more detail on what happens during the Rancher product lifecycle, refer to the Support Maintenance Terms.\n\nNetwork Topology\n\nThese tips can help Rancher work more smoothly with your network.\n\nUse Low-latency Networks for Communication Within Clusters\n\nKubernetes clusters are best served by low-latency networks. This is especially true for the control plane components and etcd, where lots of coordination and leader election traffic occurs. Networking between Rancher server and the Kubernetes clusters it manages are more tolerant of latency.\n\nAllow Rancher to Communicate Directly with Clusters\n\nLimit the use of proxies or load balancers between Rancher server and Kubernetes clusters. As Rancher is maintaining a long-lived web sockets connection, these intermediaries can interfere with the connection lifecycle as they often weren’t configured with this use case in mind.\n\nTips for Scaling and Reliability\n\nThese tips can help you scale your cluster more easily.\n\nUse One Kubernetes Role Per Host\n\nSeparate the etcd, control plane, and worker roles onto different hosts. Don’t assign multiple roles to the same host, such as a worker and control plane. This will give you maximum scalability.\n\nRun the Control Plane and etcd on Virtual Machines\n\nRun your etcd and control plane nodes on virtual machines where you can scale vCPU and memory easily if needed in the future.\n\nUse at Least Three etcd Nodes\n\nProvision 3 or 5 etcd nodes. Etcd requires a quorum to determine a leader by the majority of nodes, therefore it is not recommended to have clusters of even numbers. Three etcd nodes is generally sufficient for smaller clusters and five etcd nodes for large clusters.\n\nUse at Least Two Control Plane Nodes\n\nProvision two or more control plane nodes. Some control plane components, such as the kube-apiserver, run in active-active mode and will give you more scalability. Other components such as kube-scheduler and kube-controller run in active-passive mode (leader elect) and give you more fault tolerance.\n\nMonitor Your Cluster\n\nClosely monitor and scale your nodes as needed. You should enable cluster monitoring and use the Prometheus metrics and Grafana visualization options as a starting point.\n\nTips for Security\n\nBelow are some basic tips for increasing security in Rancher. For more detailed information about securing your cluster, you can refer to these resources:\n\n\nRancher’s security documentation and Kubernetes cluster hardening guide\n101 More Security Best Practices for Kubernetes\n\n\nUpdate Rancher with Security Patches\n\nKeep your Rancher installation up to date with the latest patches. Patch updates have important software fixes and sometimes have security fixes. When patches with security fixes are released, customers with Rancher licenses are notified by e-mail. These updates are also posted on Rancher’s forum.\n\nReport Security Issues Directly to Rancher\n\nIf you believe you have uncovered a security-related problem in Rancher, please communicate this immediately and discretely to the Rancher team (security@rancher.com). Posting security issues on public forums such as Twitter, Rancher Slack, GitHub, etc. can potentially compromise security for all Rancher customers. Reporting security issues discretely allows Rancher to assess and mitigate the problem. Security patches are typically given high priority and released as quickly as possible.\n\nOnly Upgrade One Component at a Time\n\nIn addition to Rancher software updates, closely monitor security fixes for related software, such as Docker, Linux, and any libraries used by your workloads. For production environments, try to avoid upgrading too many entities during a single maintenance window. Upgrading multiple components can make it difficult to root cause an issue in the event of a failure. As business requirements allow, upgrade one component at a time.\n\nTips for Multi-Tenant Clusters\n\nNamespaces\n\nEach tenant should have their own unique namespaces within the cluster","postref":"fd069a8ecf19f8e9569c8e4eefa6b675","objectID":"40ab23e8ff391e7129e9bcd151d3e37e","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/best-practices/management/"},{"anchor":"#","title":"Using Docker Machine","content":"Before we get started, you’ll need to make sure that you have docker machine installed. Download it directly from the docker machine releases.\nYou also need to know the memory requirements.\n\n\nNote: If you create a RancherOS instance using Docker Machine, you will not be able to upgrade your version of RancherOS.\n\n\nDownloading RancherOS\n\nGet the latest ISO artifact from the RancherOS releases.\n\n\n\n\nMachine Driver\nRecommended RancherOS version\nISO File\n\n\n\n\n\nVirtualBox\n>=v1.0.0\nrancheros.iso\n\n\n\nVMWare VSphere\n>=v1.4.0\nrancheros-autoformat.iso\n\n\n\nVMWare Fusion\n>=v1.4.0\nrancheros-autoformat.iso\n\n\n\nHyper-V\n>=v1.5.0\nrancheros.iso\n\n\n\nProxmox VE\n>=v1.5.1\nrancheros-autoformat.iso\n\n\n\n\nUsing Docker Machine\n\nYou can use Docker Machine to launch VMs for various providers. Currently VirtualBox and VMWare(VMWare VSphere, VMWare Fusion) and AWS are supported.\n\nUsing Docker Machine with VirtualBox\n\nBefore moving forward, you’ll need to have VirtualBox installed. Download it directly from VirtualBox. Once you have VirtualBox and Docker Machine installed, it’s just one command to get RancherOS running.\n\nHere is an example about using the RancherOS latest link:\n\n$ docker-machine create -d virtualbox \\\n        --virtualbox-boot2docker-url https://releases.rancher.com/os/latest/rancheros.iso \\\n        --virtualbox-memory <MEMORY-SIZE> \\\n        <MACHINE-NAME>\n\n\n\nNote: Instead of downloading the ISO, you can directly use the URL for the rancheros.iso.\n\n\nThat’s it! You should now have a RancherOS host running on VirtualBox. You can verify that you have a VirtualBox VM running on your host.\n\n\nNote: After the machine is created, Docker Machine may display some errors regarding creation, but if the VirtualBox VM is running, you should be able to log in.\n\n\n$ VBoxManage list runningvms | grep <MACHINE-NAME>\n\n\nThis command will print out the newly created machine. If not, something went wrong with the provisioning step.\n\nUsing Docker Machine with VMWare VSphere\n\nAvailable as of v1.4\n\nBefore moving forward, you’ll need to have VMWare VSphere installed. Once you have VMWare VSphere and Docker Machine installed, it’s just one command to get RancherOS running.\n\nHere is an example about using the RancherOS latest link:\n\n$ docker-machine create -d vmwarevsphere \\\n        --vmwarevsphere-username <USERNAME> \\\n        --vmwarevsphere-password <PASSWORD> \\\n        --vmwarevsphere-memory-size <MEMORY-SIZE> \\\n        --vmwarevsphere-boot2docker-url https://releases.rancher.com/os/latest/vmware/rancheros-autoformat.iso \\\n        --vmwarevsphere-vcenter <IP-ADDRESS> \\\n        --vmwarevsphere-vcenter-port <PORT> \\\n        --vmwarevsphere-disk-size <DISK-SIZE> \\\n        <MACHINE-NAME>\n\n\nThat’s it! You should now have a RancherOS host running on VMWare VSphere. You can verify that you have a VMWare(ESXi) VM running on your host.\n\nUsing Docker Machine with VMWare Fusion\n\nAvailable as of v1.4\n\nBefore moving forward, you’ll need to have VMWare Fusion installed. Once you have VMWare Fusion and Docker Machine installed, it’s just one command to get RancherOS running.\n\nHere is an example about using the RancherOS latest link:\n\n$ docker-machine create -d vmwarefusion \\\n        --vmwarefusion-no-share \\\n        --vmwarefusion-memory-size <MEMORY> \\\n        --vmwarefusion-boot2docker-url https://releases.rancher.com/os/latest/vmware/rancheros-autoformat.iso \\\n        <MACHINE_NAME>\n\n\nThat’s it! You should now have a RancherOS host running on VMWare Fusion. You can verify that you have a VMWare Fusion VM running on your host.\n\nUsing Docker Machine with Hyper-V\n\nAvailable as of v1.5\n\nYou should refer to the documentation of Hyper-V driver, here is an example of using the latest RancherOS URL. We recommend using a specific version so you know which version of RancherOS that you are installing.\n\n$ docker-machine.exe create -d hyperv \\\n        --hyperv-memory 2048 \\\n        --hyperv-boot2docker-url https://releases.rancher.com/os/latest/hyperv/rancheros.iso\n        --hyperv-virtual-switch <SWITCH_NAME> \\\n        <MACHINE_NAME>\n\n\nUsing Docker Machine with Proxmox VE\n\nAvailable as of v1.5.1\n\nThere is currently no official Proxmox VE driver, but there is a choice that you can refer to.\n\nLogging into RancherOS\n\nLogging into RancherOS follows the standard Docker Machine commands. To login into your newly provisioned RancherOS VM.\n\n$ docker-machine ssh <MACHINE-NAME>\n\n\nYou’ll be logged into RancherOS and can start exploring the OS, This will log you into the RancherOS VM. You’ll then be able to explore the OS by adding system services, customizing the configuration, and launching containers.\n\nIf you want to exit out of RancherOS, you can exit by pressing Ctrl+D.\n\nDocker Machine Benefits\n\nWith Docker Machine, you can point the docker client on your host to the docker daemon running inside of the VM. This allows you to run your docker commands as if you had installed docker on your host.\n\nTo point your docker client to the docker daemon inside the VM, use the following command:\n\n$ eval $(docker-machine env <MACHINE-NAME>)\n\n\nAfter setting this up, you can run any docker command in your host, and it will execute the command in your RancherOS VM.\n\n$ docker run -p 80:80 -p 443:443 -d nginx\n\n\nIn your VM, a nginx container will start on your VM. To access the container, you will need the IP address of the VM.\n\n$ docker-machine ip <MACHINE-NAME>\n\n\nOnce you obtain the IP address, paste it in a browser and a Welcome Page for nginx will be displayed.\n","postref":"b2a66f864c35bf098da5ff25657be028","objectID":"1c8c4ea2d27ace321124035f526ccebc","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/running-rancheros/workstation/docker-machine/"},{"anchor":"#","title":"Booting from ISO","content":"The RancherOS ISO file can be used to create a fresh RancherOS install on KVM, VMware, VirtualBox, Hyper-V, Proxmox VE, or bare metal servers. You can download the rancheros.iso file from our releases page.\n\nSome hypervisors may require a built-in agent to communicate with the guest, for this, RancherOS precompiles some ISO files.\n\n\n\n\nHypervisor\nISO\n\n\n\n\n\nVMware\nrancheros-vmware.iso\n\n\n\nHyper-V\nrancheros-hyperv.iso\n\n\n\nProxmox VE\nrancheros-proxmoxve.iso\n\n\n\n\nYou must boot with enough memory which you can refer to here. If you boot with the ISO, you will automatically be logged in as the rancher user. Only the ISO is set to use autologin by default. If you run from a cloud or install to disk, SSH keys or a password of your choice is expected to be used.\n\nInstall to Disk\n\nAfter you boot RancherOS from ISO, you can follow the instructions here to install RancherOS to a hard disk.\n","postref":"d85762c6c2d76f05e6a60216be0c6dc9","objectID":"1b3b672139bc0bf136cc9c41e3cdee07","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/running-rancheros/workstation/boot-from-iso/"},{"anchor":"#1-1-rancher-ha-kubernetes-cluster-host-configuration","title":"1.1 - Rancher HA Kubernetes cluster host configuration","content":"#\n# Cluster Config\n#\ndocker_root_dir: /var/lib/docker\nenable_cluster_alerting: false\nenable_cluster_monitoring: false\nenable_network_policy: false\n#\n# Rancher Config\n#\nrancher_kubernetes_engine_config:\n  addon_job_timeout: 30\n  ignore_docker_version: true\n#\n#   If you are using calico on AWS\n#\n#    network:\n#      plugin: calico\n#      calico_network_provider:\n#        cloud_provider: aws\n#\n# # To specify flannel interface\n#\n#    network:\n#      plugin: flannel\n#      flannel_network_provider:\n#      iface: eth1\n#\n# # To specify flannel interface for canal plugin\n#\n#    network:\n#      plugin: canal\n#      canal_network_provider:\n#        iface: eth1\n#\n  network:\n    plugin: canal\n#\n#    services:\n#      kube-api:\n#        service_cluster_ip_range: 10.43.0.0/16\n#      kube-controller:\n#        cluster_cidr: 10.42.0.0/16\n#        service_cluster_ip_range: 10.43.0.0/16\n#      kubelet:\n#        cluster_domain: cluster.local\n#        cluster_dns_server: 10.43.0.10\n#\n  services:\n    etcd:\n      backup_config:\n        enabled: false\n        interval_hours: 12\n        retention: 6\n        safe_timestamp: false\n      creation: 12h\n      extra_args:\n        election-timeout: '5000'\n        heartbeat-interval: '500'\n      gid: 1001\n      retention: 72h\n      snapshot: false\n      uid: 1001\n    kube_api:\n      always_pull_images: false\n      extra_args:\n        admission-control-config-file: /opt/kubernetes/admission.yaml\n        anonymous-auth: 'false'\n        audit-log-format: json\n        audit-log-maxage: '5'\n        audit-log-maxbackup: '5'\n        audit-log-maxsize: '100'\n        audit-log-path: /var/log/kube-audit/audit-log.json\n        audit-policy-file: /opt/kubernetes/audit.yaml\n        enable-admission-plugins: >-\n          ServiceAccount,NamespaceLifecycle,LimitRanger,PersistentVolumeLabel,DefaultStorageClass,ResourceQuota,DefaultTolerationSeconds,AlwaysPullImages,DenyEscalatingExec,NodeRestriction,EventRateLimit,PodSecurityPolicy\n        encryption-provider-config: /opt/kubernetes/encryption.yaml\n        profiling: 'false'\n        service-account-lookup: 'true'\n        tls-cipher-suites: >-\n          TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256\n      extra_binds:\n        - '/var/log/kube-audit:/var/log/kube-audit'\n        - '/opt/kubernetes:/opt/kubernetes'\n      pod_security_policy: true\n      service_node_port_range: 30000-32767\n    kube_controller:\n      extra_args:\n        address: 127.0.0.1\n        feature-gates: RotateKubeletServerCertificate=true\n        profiling: 'false'\n        terminated-pod-gc-threshold: '1000'\n    kubelet:\n      extra_args:\n        anonymous-auth: 'false'\n        event-qps: '0'\n        feature-gates: RotateKubeletServerCertificate=true\n        make-iptables-util-chains: 'true'\n        protect-kernel-defaults: 'true'\n        streaming-connection-idle-timeout: 1800s\n        tls-cipher-suites: >-\n          TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256\n      fail_swap_on: false\n    scheduler:\n      extra_args:\n        address: 127.0.0.1\n        profiling: 'false'\n  ssh_agent_auth: false\nwindows_prefered_cluster: falsenodes:\n- address: 18.191.190.205\n  internal_address: 172.31.24.213\n  user: ubuntu\n  role: [ \"controlplane\", \"etcd\", \"worker\" ]\n- address: 18.191.190.203\n  internal_address: 172.31.24.203\n  user: ubuntu\n  role: [ \"controlplane\", \"etcd\", \"worker\" ]\n- address: 18.191.190.10\n  internal_address: 172.31.24.244\n  user: ubuntu\n  role: [ \"controlplane\", \"etcd\", \"worker\" ]\n\nservices:\n  kubelet:\n    extra_args:\n      streaming-connection-idle-timeout: \"1800s\"\n      authorization-mode: \"Webhook\"\n      protect-kernel-defaults: \"true\"\n      make-iptables-util-chains: \"true\"\n      event-qps: \"0\"\n      anonymous-auth: \"false\"\n      feature-gates: \"RotateKubeletServerCertificate=true\"\n      tls-cipher-suites: \"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256\"\n      generate_serving_certificate: true\n  kube-api:\n    pod_security_policy: true\n    extra_args:\n      anonymous-auth: \"false\"\n      profiling: \"false\"\n      service-account-lookup: \"true\"\n      enable-admission-plugins: \"ServiceAccount,NamespaceLifecycle,LimitRanger,PersistentVolumeLabel,DefaultStorageClass,ResourceQuota,DefaultTolerationSeconds,AlwaysPullImages,DenyEscalatingExec,NodeRestriction,EventRateLimit,PodSecurityPolicy\"\n      encryption-provider-config: /opt/kubernetes/encryption.yaml\n      admission-control-config-file: \"/opt/kubernetes/admission.yaml\"\n      audit-log-path: \"/var/log/kube-audit/audit-log.json\"\n      audit-log-maxage: \"5\"\n      audit-log-maxbackup: \"5\"\n      audit-log-maxsize: \"100\"\n      audit-log-format: \"json\"\n      audit-policy-file: /opt/kubernetes/audit.yaml\n      tls-cipher-suites: \"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256\"\n    extra_binds:\n      - \"/var/log/kube-audit:/var/log/kube-audit\"\n      - \"/opt/kubernetes:/opt/kubernetes\"\n  scheduler:\n    extra_args:\n      profiling: \"false\"\n      address: \"127.0.0.1\"\n  kube-controller:\n    extra_args:\n      profiling: \"false\"\n      address: \"127.0.0.1\"\n      terminated-pod-gc-threshold: \"1000\"\n      feature-gates: \"RotateKubeletServerCertificate=true\"\n  services:\n    etcd:\n      uid: 1001\n      gid: 1001\naddons: |\n  apiVersion: v1\n  kind: Namespace\n  metadata:\n    name: ingress-nginx\n  ---\n  apiVersion: rbac.authorization.k8s.io/v1\n  kind: Role\n  metadata:\n    name: default-psp-role\n    namespace: ingress-nginx\n  rules:\n  - apiGroups:\n    - extensions\n    resourceNames:\n    - default-psp\n    resources:\n    - podsecuritypolicies\n    verbs:\n    - use\n  ---\n  apiVersion: rbac.authorization.k8s.io/v1\n  kind: RoleBinding\n  metadata:\n    name: default-psp-rolebinding\n    namespace: ingress-nginx\n  roleRef:\n    apiGroup: rbac.authorization.k8s.io\n    kind: Role\n    name: default-psp-role\n  subjects:\n  - apiGroup: rbac.authorization.k8s.io\n    kind: Group\n    name: system:serviceaccounts\n  - apiGroup: rbac.authorization.k8s.io\n    kind: Group\n    name: system:authenticated\n  ---\n  apiVersion: v1\n  kind: Namespace\n  metadata:\n    name: cattle-system\n  ---\n  apiVersion: rbac.authorization.k8s.io/v1\n  kind: Role\n  metadata:\n    name: default-psp-role\n    namespace: cattle-system\n  rules:\n  - apiGroups:\n    - extensions\n    resourceNames:\n    - default-psp\n    resources:\n    - podsecuritypolicies\n    verbs:\n    - use\n  ---\n  apiVersion: rbac.authorization.k8s.io/v1\n  kind: RoleBinding\n  metadata:\n    name: default-psp-rolebinding\n    namespace: cattle-system\n  roleRef:\n    apiGroup: rbac.authorization.k8s.io\n    kind: Role\n    name: default-psp-role\n  subjects:\n  - apiGroup: rbac.authorization.k8s.io\n    kind: Group\n    name: system:serviceaccounts\n  - apiGroup: rbac.authorization.k8s.io\n    kind: Group\n    name: system:authenticated\n  ---\n  apiVersion: extensions/v1beta1\n  kind: PodSecurityPolicy\n  metadata:\n    name: restricted\n  spec:\n    requiredDropCapabilities:\n    - NET_RAW\n    privileged: false\n    allowPrivilegeEscalation: false\n    defaultAllowPrivilegeEscalation: false\n    fsGroup:\n      rule: RunAsAny\n    runAsUser:\n      rule: MustRunAsNonRoot\n    seLinux:\n      rule: RunAsAny\n    supplementalGroups:\n      rule: RunAsAny\n    volumes:\n    - emptyDir\n    - secret\n    - persistentVolumeClaim\n    - downwardAPI\n    - configMap\n    - projected\n  ---\n  apiVersion: rbac.authorization.k8s.io/v1\n  kind: ClusterRole\n  metadata:\n    name: psp:restricted\n  rules:\n  - apiGroups:\n    - extensions\n    resourceNames:\n    - restricted\n    resources:\n    - podsecuritypolicies\n    verbs:\n    - use\n  ---\n  apiVersion: rbac.authorization.k8s.io/v1\n  kind: ClusterRoleBinding\n  metadata:\n    name: psp:restricted\n  roleRef:\n    apiGroup: rbac.authorization.k8s.io\n    kind: ClusterRole\n    name: psp:restricted\n  subjects:\n  - apiGroup: rbac.authorization.k8s.io\n    kind: Group\n    name: system:serviceaccounts\n  - apiGroup: rbac.authorization.k8s.io\n    kind: Group\n    name: system:authenticatedcloud-config file to automate hardening manual steps on nodes deployment.#cloud-config\nbootcmd:\n- apt-get update\n- apt-g","postref":"857069ab01ba30fa7098945271305fba","objectID":"b16718eb5b26020fa30029f7f1950362","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/security/hardening-2.3/"},{"anchor":"#","title":"Networking","content":"The commands/steps listed on this page can be used to check networking related issues in your cluster.\n\nMake sure you configured the correct kubeconfig (for example, export KUBECONFIG=$PWD/kube_config_rancher-cluster.yml for Rancher HA) or are using the embedded kubectl via the UI.\n\nDouble check if all the required ports are opened in your (host) firewall\n\nDouble check if all the required ports are opened in your (host) firewall. The overlay network uses UDP in comparison to all other required ports which are TCP.\n\nCheck if overlay network is functioning correctly\n\nThe pod can be scheduled to any of the hosts you used for your cluster, but that means that the NGINX ingress controller needs to be able to route the request from NODE_1 to NODE_2. This happens over the overlay network. If the overlay network is not functioning, you will experience intermittent TCP/HTTP connection failures due to the NGINX ingress controller not being able to route to the pod.\n\nTo test the overlay network, you can launch the following DaemonSet definition. This will run a busybox container on every host, which we will use to run a ping test between containers on all hosts.\n\n\nSave the following file as ds-overlaytest.yml\n\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: overlaytest\nspec:\n  selector:\n      matchLabels:\n        name: overlaytest\n  template:\n    metadata:\n      labels:\n        name: overlaytest\n    spec:\n      tolerations:\n      - operator: Exists\n      containers:\n      - image: busybox:1.28\n        imagePullPolicy: Always\n        name: busybox\n        command: [\"sh\", \"-c\", \"tail -f /dev/null\"]\n        terminationMessagePath: /dev/termination-log\n\n\nLaunch it using kubectl create -f ds-overlaytest.yml\n\nWait until kubectl rollout status ds/overlaytest -w returns: daemon set \"overlaytest\" successfully rolled out.\n\nRun the following command, from the same location, to let each container on every host ping each other (it’s a single line bash command).\n\necho \"=> Start network overlay test\"; kubectl get pods -l name=overlaytest -o jsonpath='{range .items[*]}{@.metadata.name}{\" \"}{@.spec.nodeName}{\"\\n\"}{end}' | while read spod shost; do kubectl get pods -l name=overlaytest -o jsonpath='{range .items[*]}{@.status.podIP}{\" \"}{@.spec.nodeName}{\"\\n\"}{end}' | while read tip thost; do kubectl --request-timeout='10s' exec $spod -- /bin/sh -c \"ping -c2 $tip > /dev/null 2>&1\"; RC=$?; if [ $RC -ne 0 ]; then echo $shost cannot reach $thost; fi; done; done; echo \"=> End network overlay test\"\n\n\nWhen this command has finished running, the output indicating everything is correct is:\n\n=> Start network overlay test\n=> End network overlay test\n\n\n\nIf you see error in the output, that means that the required ports for overlay networking are not opened between the hosts indicated.\n\nExample error output of a situation where NODE1 had the UDP ports blocked.\n\n=> Start network overlay test\ncommand terminated with exit code 1\nNODE2 cannot reach NODE1\ncommand terminated with exit code 1\nNODE3 cannot reach NODE1\ncommand terminated with exit code 1\nNODE1 cannot reach NODE2\ncommand terminated with exit code 1\nNODE1 cannot reach NODE3\n=> End network overlay test\n\n\nCleanup the busybox DaemonSet by running kubectl delete ds/overlaytest.\n\nCheck if MTU is correctly configured on hosts and on peering/tunnel appliances/devices\n\nWhen the MTU is incorrectly configured (either on hosts running Rancher, nodes in created/imported clusters or on appliances/devices in between), error messages will be logged in Rancher and in the agents, similar to:\n\n\nwebsocket: bad handshake\nFailed to connect to proxy\nread tcp: i/o timeout\n\n\nSee Google Cloud VPN: MTU Considerations for an example how to configure MTU correctly when using Google Cloud VPN between Rancher and cluster nodes.\n\nResolved issues\n\nOverlay network broken when using Canal/Flannel due to missing node annotations\n\n\n\n\n\n\n\n\n\n\n\nGitHub issue\n#13644\n\n\n\nResolved in\nv2.1.2\n\n\n\n\nTo check if your cluster is affected, the following command will list nodes that are broken (this command requires jq to be installed):\n\nkubectl get nodes -o json | jq '.items[].metadata | select(.annotations[\"flannel.alpha.coreos.com/public-ip\"] == null or .annotations[\"flannel.alpha.coreos.com/kube-subnet-manager\"] == null or .annotations[\"flannel.alpha.coreos.com/backend-type\"] == null or .annotations[\"flannel.alpha.coreos.com/backend-data\"] == null) | .name'\n\n\nIf there is no output, the cluster is not affected.\n\nSystem namespace pods network connectivity broken\n\n\nNote: This applies only to Rancher upgrades from v2.0.6 or earlier to v2.0.7 or later. Upgrades from v2.0.7 to later version are unaffected.\n\n\n\n\n\n\n\n\n\n\n\n\nGitHub issue\n#15146\n\n\n\n\nIf pods in system namespaces cannot communicate with pods in other system namespaces, you will need to follow the instructions in Upgrading to v2.0.7+ — Namespace Migration to restore connectivity. Symptoms include:\n\n\nNGINX ingress controller showing 504 Gateway Time-out when accessed.\nNGINX ingress controller logging upstream timed out (110: Connection timed out) while connecting to upstream when accessed.\n\n","postref":"7a97501d67e939493ae5d7d12dba85ce","objectID":"3510e28b9cfcd45806a89522585a6697","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/troubleshooting/networking/"},{"anchor":"#","title":"DNS","content":"The commands/steps listed on this page can be used to check name resolution issues in your cluster.\n\nMake sure you configured the correct kubeconfig (for example, export KUBECONFIG=$PWD/kube_config_rancher-cluster.yml for Rancher HA) or are using the embedded kubectl via the UI.\n\nBefore running the DNS checks, check the default DNS provider for your cluster and make sure that the overlay network is functioning correctly as this can also be the reason why DNS resolution (partly) fails.\n\nCheck if DNS pods are running\n\nkubectl -n kube-system get pods -l k8s-app=kube-dns\n\n\nExample output when using CoreDNS:\n\nNAME                       READY   STATUS    RESTARTS   AGE\ncoredns-799dffd9c4-6jhlz   1/1     Running   0          76m\n\n\nExample output when using kube-dns:\n\nNAME                        READY   STATUS    RESTARTS   AGE\nkube-dns-5fd74c7488-h6f7n   3/3     Running   0          4m13s\n\n\nCheck if the DNS service is present with the correct cluster-ip\n\nkubectl -n kube-system get svc -l k8s-app=kube-dns\n\n\nNAME               TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)         AGE\nservice/kube-dns   ClusterIP   10.43.0.10   <none>        53/UDP,53/TCP   4m13s\n\n\nCheck if domain names are resolving\n\nCheck if internal cluster names are resolving (in this example, kubernetes.default), the IP shown after Server: should be the same as the CLUSTER-IP from the kube-dns service.\n\nkubectl run -it --rm --restart=Never busybox --image=busybox:1.28 -- nslookup kubernetes.default\n\n\nExample output:\n\nServer:    10.43.0.10\nAddress 1: 10.43.0.10 kube-dns.kube-system.svc.cluster.local\n\nName:      kubernetes.default\nAddress 1: 10.43.0.1 kubernetes.default.svc.cluster.local\npod \"busybox\" deleted\n\n\nCheck if external names are resolving (in this example, www.google.com)\n\nkubectl run -it --rm --restart=Never busybox --image=busybox:1.28 -- nslookup www.google.com\n\n\nExample output:\n\nServer:    10.43.0.10\nAddress 1: 10.43.0.10 kube-dns.kube-system.svc.cluster.local\n\nName:      www.google.com\nAddress 1: 2a00:1450:4009:80b::2004 lhr35s04-in-x04.1e100.net\nAddress 2: 216.58.211.100 ams15s32-in-f4.1e100.net\npod \"busybox\" deleted\n\n\nIf you want to check resolving of domain names on all of the hosts, execute the following steps:\n\n\nSave the following file as ds-dnstest.yml\n\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: dnstest\nspec:\n  selector:\n      matchLabels:\n        name: dnstest\n  template:\n    metadata:\n      labels:\n        name: dnstest\n    spec:\n      tolerations:\n      - operator: Exists\n      containers:\n      - image: busybox:1.28\n        imagePullPolicy: Always\n        name: alpine\n        command: [\"sh\", \"-c\", \"tail -f /dev/null\"]\n        terminationMessagePath: /dev/termination-log\n\n\nLaunch it using kubectl create -f ds-dnstest.yml\n\nWait until kubectl rollout status ds/dnstest -w returns: daemon set \"dnstest\" successfully rolled out.\n\nConfigure the environment variable DOMAIN to a fully qualified domain name (FQDN) that the host should be able to resolve (www.google.com is used as an example) and run the following command to let each container on every host resolve the configured domain name (it’s a single line command).\n\nexport DOMAIN=www.google.com; echo \"=> Start DNS resolve test\"; kubectl get pods -l name=dnstest --no-headers -o custom-columns=NAME:.metadata.name,HOSTIP:.status.hostIP | while read pod host; do kubectl exec $pod -- /bin/sh -c \"nslookup $DOMAIN > /dev/null 2>&1\"; RC=$?; if [ $RC -ne 0 ]; then echo $host cannot resolve $DOMAIN; fi; done; echo \"=> End DNS resolve test\"\n\n\nWhen this command has finished running, the output indicating everything is correct is:\n\n=> Start DNS resolve test\n=> End DNS resolve test\n\n\n\nIf you see error in the output, that means that the mentioned host(s) is/are not able to resolve the given FQDN.\n\nExample error output of a situation where host with IP 209.97.182.150 had the UDP ports blocked.\n\n=> Start DNS resolve test\ncommand terminated with exit code 1\n209.97.182.150 cannot resolve www.google.com\n=> End DNS resolve test\n\n\nCleanup the alpine DaemonSet by running kubectl delete ds/dnstest.\n\nCoreDNS specific\n\nCheck CoreDNS logging\n\nkubectl -n kube-system logs -l k8s-app=kube-dns\n\n\nCheck configuration\n\nCoreDNS configuration is stored in the configmap coredns in the kube-system namespace.\n\nkubectl -n kube-system get configmap coredns -o go-template={{.data.Corefile}}\n\n\nCheck upstream nameservers in resolv.conf\n\nBy default, the configured nameservers on the host (in /etc/resolv.conf) will be used as upstream nameservers for CoreDNS. You can check this file on the host or run the following Pod with dnsPolicy set to Default, which will inherit the /etc/resolv.conf from the host it is running on.\n\nkubectl run -i --restart=Never --rm test-${RANDOM} --image=ubuntu --overrides='{\"kind\":\"Pod\", \"apiVersion\":\"v1\", \"spec\": {\"dnsPolicy\":\"Default\"}}' -- sh -c 'cat /etc/resolv.conf'\n\n\nEnable query logging\n\nEnabling query logging can be done by enabling the log plugin in the Corefile configuration in the configmap coredns. You can do so by using kubectl -n kube-system edit configmap coredns or use the command below to replace the configuration in place:\n\nkubectl get configmap -n kube-system coredns -o json |  kubectl get configmap -n kube-system coredns -o json | sed -e 's_loadbalance_log\\\\n    loadbalance_g' | kubectl apply -f -\n\n\nAll queries will now be logged and can be checked using the command in Check CoreDNS logging.\n\nkube-dns specific\n\nCheck upstream nameservers in kubedns container\n\nBy default, the configured nameservers on the host (in /etc/resolv.conf) will be used as upstream nameservers for kube-dns. Sometimes the host will run a local caching DNS nameserver, which means the address in /etc/resolv.conf will point to an address in the loopback range (127.0.0.0/8) which will be unreachable by the container. In case of Ubuntu 18.04, this is done by systemd-resolved. Since Rancher v2.0.7, we detect if systemd-resolved is running, and will automatically use the /etc/resolv.conf file with the correct upstream nameservers (which is located at /run/systemd/resolve/resolv.conf).\n\nUse the following command to check the upstream nameservers used by the kubedns container:\n\nkubectl -n kube-system get pods -l k8s-app=kube-dns --no-headers -o custom-columns=NAME:.metadata.name,HOSTIP:.status.hostIP | while read pod host; do echo \"Pod ${pod} on host ${host}\"; kubectl -n kube-system exec $pod -c kubedns cat /etc/resolv.conf; done\n\n\nExample output:\n\nPod kube-dns-667c7cb9dd-z4dsf on host x.x.x.x\nnameserver 1.1.1.1\nnameserver 8.8.4.4\n\n\nIf the output shows an address in the loopback range (127.0.0.0/8), you can correct this in two ways:\n\n\nMake sure the correct nameservers are listed in /etc/resolv.conf on your nodes in the cluster, please consult your operating system documentation on how to do this. Make sure you execute this before provisioning a cluster, or reboot the nodes after making the modification.\nConfigure the kubelet to use a different file for resolving names, by using extra_args as shown below (where /run/resolvconf/resolv.conf is the file with the correct nameservers):\n\n\nservices:\n  kubelet:\n    extra_args:\n      resolv-conf: \"/run/resolvconf/resolv.conf\"\n\n\n\nNote: As the kubelet is running inside a container, the path for files located in /etc and /usr are in /host/etc and /host/usr inside the kubelet container.\n\n\nSee Editing Cluster as YAML how to apply this change. When the provisioning of the cluster has finished, you have to remove the kube-dns pod to activate the new setting in the pod:\n\nkubectl delete pods -n kube-system -l k8s-app=kube-dns\npod \"kube-dns-5fd74c7488-6pwsf\" deleted\n\n\nTry to resolve name again using Check if domain names are resolving.\n\nIf you want to check the kube-dns configuration in your cluster (for example, to check if there are different upstream nameservers configured), you can run the following command to list the kube-dns configuration:\n\nkubectl -n kube-system get configmap kube-dns -o go-template='{{range $key, $value := .data}}{{ $key }}{{\":\"}}{{ $value }}{{\"\\n\"}}{{end}}'\n\n\nExample output:\n\nupstreamNameservers:[\"1.1.1.1\"]\n\n","postref":"5c3544bf95424dcc6ebeb5701517e45d","objectID":"ab33a6286cfcb91e2b599c438ba1f680","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/troubleshooting/dns/"},{"anchor":"#1-1-rancher-ha-kubernetes-cluster-host-configuration","title":"1.1 - Rancher HA Kubernetes cluster host configuration","content":"nodes:\n- address: 18.191.190.205\n  internal_address: 172.31.24.213\n  user: ubuntu\n  role: [ \"controlplane\", \"etcd\", \"worker\" ]\n- address: 18.191.190.203\n  internal_address: 172.31.24.203\n  user: ubuntu\n  role: [ \"controlplane\", \"etcd\", \"worker\" ]\n- address: 18.191.190.10\n  internal_address: 172.31.24.244\n  user: ubuntu\n  role: [ \"controlplane\", \"etcd\", \"worker\" ]\n\nservices:\n  kubelet:\n    extra_args:\n      streaming-connection-idle-timeout: \"1800s\"\n      authorization-mode: \"Webhook\"\n      protect-kernel-defaults: \"true\"\n      make-iptables-util-chains: \"true\"\n      event-qps: \"0\"\n      anonymous-auth: \"false\"\n      feature-gates: \"RotateKubeletServerCertificate=true\"\n      tls-cipher-suites: \"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256\"\n  kube-api:\n    pod_security_policy: true\n    extra_args:\n      anonymous-auth: \"false\"\n      profiling: \"false\"\n      service-account-lookup: \"true\"\n      enable-admission-plugins: \"ServiceAccount,NamespaceLifecycle,LimitRanger,PersistentVolumeLabel,DefaultStorageClass,ResourceQuota,DefaultTolerationSeconds,AlwaysPullImages,DenyEscalatingExec,NodeRestriction,EventRateLimit,PodSecurityPolicy\"\n      encryption-provider-config: /opt/kubernetes/encryption.yaml\n      admission-control-config-file: \"/opt/kubernetes/admission.yaml\"\n      audit-log-path: \"/var/log/kube-audit/audit-log.json\"\n      audit-log-maxage: \"5\"\n      audit-log-maxbackup: \"5\"\n      audit-log-maxsize: \"100\"\n      audit-log-format: \"json\"\n      audit-policy-file: /opt/kubernetes/audit.yaml\n      tls-cipher-suites: \"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256\"\n    extra_binds:\n      - \"/var/log/kube-audit:/var/log/kube-audit\"\n      - \"/opt/kubernetes:/opt/kubernetes\"\n  scheduler:\n    extra_args:\n      profiling: \"false\"\n      address: \"127.0.0.1\"\n  kube-controller:\n    extra_args:\n      profiling: \"false\"\n      address: \"127.0.0.1\"\n      terminated-pod-gc-threshold: \"1000\"\n      feature-gates: \"RotateKubeletServerCertificate=true\"\naddons: |\n  apiVersion: v1\n  kind: Namespace\n  metadata:\n    name: ingress-nginx\n  ---\n  apiVersion: rbac.authorization.k8s.io/v1\n  kind: Role\n  metadata:\n    name: default-psp-role\n    namespace: ingress-nginx\n  rules:\n  - apiGroups:\n    - extensions\n    resourceNames:\n    - default-psp\n    resources:\n    - podsecuritypolicies\n    verbs:\n    - use\n  ---\n  apiVersion: rbac.authorization.k8s.io/v1\n  kind: RoleBinding\n  metadata:\n    name: default-psp-rolebinding\n    namespace: ingress-nginx\n  roleRef:\n    apiGroup: rbac.authorization.k8s.io\n    kind: Role\n    name: default-psp-role\n  subjects:\n  - apiGroup: rbac.authorization.k8s.io\n    kind: Group\n    name: system:serviceaccounts\n  - apiGroup: rbac.authorization.k8s.io\n    kind: Group\n    name: system:authenticated\n  ---\n  apiVersion: v1\n  kind: Namespace\n  metadata:\n    name: cattle-system\n  ---\n  apiVersion: rbac.authorization.k8s.io/v1\n  kind: Role\n  metadata:\n    name: default-psp-role\n    namespace: cattle-system\n  rules:\n  - apiGroups:\n    - extensions\n    resourceNames:\n    - default-psp\n    resources:\n    - podsecuritypolicies\n    verbs:\n    - use\n  ---\n  apiVersion: rbac.authorization.k8s.io/v1\n  kind: RoleBinding\n  metadata:\n    name: default-psp-rolebinding\n    namespace: cattle-system\n  roleRef:\n    apiGroup: rbac.authorization.k8s.io\n    kind: Role\n    name: default-psp-role\n  subjects:\n  - apiGroup: rbac.authorization.k8s.io\n    kind: Group\n    name: system:serviceaccounts\n  - apiGroup: rbac.authorization.k8s.io\n    kind: Group\n    name: system:authenticated\n  ---\n  apiVersion: extensions/v1beta1\n  kind: PodSecurityPolicy\n  metadata:\n    name: restricted\n  spec:\n    requiredDropCapabilities:\n    - NET_RAW\n    privileged: false\n    allowPrivilegeEscalation: false\n    defaultAllowPrivilegeEscalation: false\n    fsGroup:\n      rule: RunAsAny\n    runAsUser:\n      rule: MustRunAsNonRoot\n    seLinux:\n      rule: RunAsAny\n    supplementalGroups:\n      rule: RunAsAny\n    volumes:\n    - emptyDir\n    - secret\n    - persistentVolumeClaim\n    - downwardAPI\n    - configMap\n    - projected\n  ---\n  apiVersion: rbac.authorization.k8s.io/v1\n  kind: ClusterRole\n  metadata:\n    name: psp:restricted\n  rules:\n  - apiGroups:\n    - extensions\n    resourceNames:\n    - restricted\n    resources:\n    - podsecuritypolicies\n    verbs:\n    - use\n  ---\n  apiVersion: rbac.authorization.k8s.io/v1\n  kind: ClusterRoleBinding\n  metadata:\n    name: psp:restricted\n  roleRef:\n    apiGroup: rbac.authorization.k8s.io\n    kind: ClusterRole\n    name: psp:restricted\n  subjects:\n  - apiGroup: rbac.authorization.k8s.io\n    kind: Group\n    name: system:serviceaccounts\n  - apiGroup: rbac.authorization.k8s.io\n    kind: Group\n    name: system:authenticated3.4.1 - Ensure only approved node drivers are activeProfile Applicability\nLevel 1\nDescriptionEnsure that node drivers that are not needed or approved are not active in the Rancher console.RationaleNode drivers are used to provision compute nodes in various cloud providers and local IaaS infrastructure. For convenience, popular cloud providers are enabled by default. If the organization does not intend to use these or does not allow users to provision resources in certain providers, the drivers should be disabled. This will prevent users from using Rancher resources to provision the nodes.Audit\nIn the Rancher UI select Global\nSelect Node Drivers\nReview the list of node drivers that are in an Active state.\nRemediationIf a disallowed node driver is active, visit the Node Drivers page under Global and disable it.3.3.1 - Ensure that administrator privileges are only granted to those who require themProfile Applicability\nLevel 1\nDescriptionRestrict administrator access to only those responsible for managing and operating the Rancher server.RationaleThe admin  privilege level gives the user the highest level of access to the Rancher server and all attached clusters. This privilege should only be granted to a few people who are responsible for the availability and support of Rancher and the clusters that it manages.AuditThe following script uses the Rancher API to show users with administrator privileges:#!/bin/bash\nfor i in $(curl -sk -u 'token-<id>:<secret>' https://<RANCHER_URL>/v3/users|jq -r .data[].links.globalRoleBindings); do\n\ncurl -sk -u 'token-<id>:<secret>' $i| jq '.data[] | \"\\(.userId) \\(.globalRoleId)\"'\n\ndoneThe admin role should only be assigned to users that require administrative privileges. Any role that is not admin or user should be audited in the RBAC section of the UI to ensure that the privileges adhere to policies for global access.The Rancher server permits customization of the default global permissions. We recommend that auditors also review the policies of any custom global roles.RemediationRemove the admin role from any user that does not require administrative privileges.3.2.1 - Change the local administrator password from the default valueProfile Applicability\nLevel 1\nDescriptionThe local administrator password should be changed from the default.RationaleThe default administrator password is common across all Rancher installations and should be changed immediately upon startup.AuditAttempt to login into the UI with the following credentials:\n  - Username: admin\n  - Password: adminThe login attempt must not succeed.RemediationChange the password from admin to a password that meets the recommended password standards for your organization.3.2.2 - Configure an Identity Provider for AuthenticationProfile Applicability\nLevel 1\nDescriptionWhen running Rancher in a production environment, configure an identity provider for authentication.RationaleRancher supports several authentication backends that are common in enterprises. It is recommended to tie Rancher into an external authentication system to simplify user and group access in the Rancher cluster. Doing so assures that access control follows the organization’s change management process for user accounts.Audit\nIn the Rancher UI, select Global\nSelect Security\nSelect Authentication\nEnsure the authentication provider for your environment is active and configured correctly\nRemediationConfigure the appropriate authentication provider for your Rancher installation according to the documentation found at the link in the reference section below.Reference\nhttps://rancher.com/docs/rancher/v2.x/en/admin-settings/authentication/\n3.1.1 - Disable the local cluster optionProfile Applicability\nLevel 2\nDescriptionWhen deploying Rancher, disable the local cluster option on the Rancher Server.NOTE","postref":"04797ddb336c137c2e1b4b6fb6d99680","objectID":"e4fbf229456e6046913d867e24e0d6d7","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/security/hardening-2.2/"},{"anchor":"#1-1-rancher-ha-kubernetes-cluster-host-configuration","title":"1.1 - Rancher HA Kubernetes cluster host configuration","content":"nodes:\n- address: 18.191.190.205\n  internal_address: 172.31.24.213\n  user: ubuntu\n  role: [ \"controlplane\", \"etcd\", \"worker\" ]\n- address: 18.191.190.203\n  internal_address: 172.31.24.203\n  user: ubuntu\n  role: [ \"controlplane\", \"etcd\", \"worker\" ]\n- address: 18.191.190.10\n  internal_address: 172.31.24.244\n  user: ubuntu\n  role: [ \"controlplane\", \"etcd\", \"worker\" ]\n\nservices:\n  kubelet:\n    extra_args:\n      streaming-connection-idle-timeout: \"1800s\"\n      protect-kernel-defaults: \"true\"\n      make-iptables-util-chains: \"true\"\n      event-qps: \"0\"\n  kube-api:\n    pod_security_policy: true\n    extra_args:\n      anonymous-auth: \"false\"\n      profiling: \"false\"\n      repair-malformed-updates: \"false\"\n      service-account-lookup: \"true\"\n      enable-admission-plugins: \"ServiceAccount,NamespaceLifecycle,LimitRanger,PersistentVolumeLabel,DefaultStorageClass,ResourceQuota,DefaultTolerationSeconds,AlwaysPullImages,DenyEscalatingExec,NodeRestriction,EventRateLimit,PodSecurityPolicy\"\n      experimental-encryption-provider-config: /etc/kubernetes/encryption.yaml\n      admission-control-config-file: \"/etc/kubernetes/admission.yaml\"\n      audit-log-path: \"/var/log/kube-audit/audit-log.json\"\n      audit-log-maxage: \"5\"\n      audit-log-maxbackup: \"5\"\n      audit-log-maxsize: \"100\"\n      audit-log-format: \"json\"\n      audit-policy-file: /etc/kubernetes/audit.yaml\n    extra_binds:\n      - \"/var/log/kube-audit:/var/log/kube-audit\"\n  scheduler:\n    extra_args:\n      profiling: \"false\"\n      address: \"127.0.0.1\"\n  kube-controller:\n    extra_args:\n      profiling: \"false\"\n      address: \"127.0.0.1\"\n      terminated-pod-gc-threshold: \"1000\"\naddons: |\n  apiVersion: rbac.authorization.k8s.io/v1\n  kind: Role\n  metadata:\n    name: default-psp-role\n    namespace: ingress-nginx\n  rules:\n  - apiGroups:\n    - extensions\n    resourceNames:\n    - default-psp\n    resources:\n    - podsecuritypolicies\n    verbs:\n    - use\n  ---\n  apiVersion: rbac.authorization.k8s.io/v1\n  kind: RoleBinding\n  metadata:\n    name: default-psp-rolebinding\n    namespace: ingress-nginx\n  roleRef:\n    apiGroup: rbac.authorization.k8s.io\n    kind: Role\n    name: default-psp-role\n  subjects:\n  - apiGroup: rbac.authorization.k8s.io\n    kind: Group\n    name: system:serviceaccounts\n  - apiGroup: rbac.authorization.k8s.io\n    kind: Group\n    name: system:authenticated\n  ---\n  apiVersion: v1\n  kind: Namespace\n  metadata:\n    name: cattle-system\n  ---\n  apiVersion: rbac.authorization.k8s.io/v1\n  kind: Role\n  metadata:\n    name: default-psp-role\n    namespace: cattle-system\n  rules:\n  - apiGroups:\n    - extensions\n    resourceNames:\n    - default-psp\n    resources:\n    - podsecuritypolicies\n    verbs:\n    - use\n  ---\n  apiVersion: rbac.authorization.k8s.io/v1\n  kind: RoleBinding\n  metadata:\n    name: default-psp-rolebinding\n    namespace: cattle-system\n  roleRef:\n    apiGroup: rbac.authorization.k8s.io\n    kind: Role\n    name: default-psp-role\n  subjects:\n  - apiGroup: rbac.authorization.k8s.io\n    kind: Group\n    name: system:serviceaccounts\n  - apiGroup: rbac.authorization.k8s.io\n    kind: Group\n    name: system:authenticated\n  ---\n  apiVersion: extensions/v1beta1\n  kind: PodSecurityPolicy\n  metadata:\n    name: restricted\n  spec:\n    requiredDropCapabilities:\n    - NET_RAW\n    privileged: false\n    allowPrivilegeEscalation: false\n    defaultAllowPrivilegeEscalation: false\n    fsGroup:\n      rule: RunAsAny\n    runAsUser:\n      rule: MustRunAsNonRoot\n    seLinux:\n      rule: RunAsAny\n    supplementalGroups:\n      rule: RunAsAny\n    volumes:\n    - emptyDir\n    - secret\n    - persistentVolumeClaim\n    - downwardAPI\n    - configMap\n    - projected\n  ---\n  apiVersion: rbac.authorization.k8s.io/v1\n  kind: ClusterRole\n  metadata:\n    name: psp:restricted\n  rules:\n  - apiGroups:\n    - extensions\n    resourceNames:\n    - restricted\n    resources:\n    - podsecuritypolicies\n    verbs:\n    - use\n  ---\n  apiVersion: rbac.authorization.k8s.io/v1\n  kind: ClusterRoleBinding\n  metadata:\n    name: psp:restricted\n  roleRef:\n    apiGroup: rbac.authorization.k8s.io\n    kind: ClusterRole\n    name: psp:restricted\n  subjects:\n  - apiGroup: rbac.authorization.k8s.io\n    kind: Group\n    name: system:serviceaccounts\n  - apiGroup: rbac.authorization.k8s.io\n    kind: Group\n    name: system:authenticated3.4.1 - Ensure only approved node drivers are activeProfile Applicability\nLevel 1\nDescriptionEnsure that node drivers that are not needed or approved are not active in the Rancher console.RationaleNode drivers are used to provision compute nodes in various cloud providers and local IaaS infrastructure. For convenience, popular cloud providers are enabled by default. If the organization does not intend to use these or does not allow users to provision resources in certain providers, the drivers should be disabled. This will prevent users from using Rancher resources to provision the nodes.Audit\nIn the Rancher UI select Global\nSelect Node Drivers\nReview the list of node drivers that are in an Active state.\nRemediationIf a disallowed node driver is active, visit the Node Drivers page under Global and disable it.3.3.1 - Ensure that administrator privileges are only granted to those who require themProfile Applicability\nLevel 1\nDescriptionRestrict administrator access to only those responsible for managing and operating the Rancher server.RationaleThe admin  privilege level gives the user the highest level of access to the Rancher server and all attached clusters. This privilege should only be granted to a few people who are responsible for the availability and support of Rancher and the clusters that it manages.AuditThe following script uses the Rancher API to show users with administrator privileges:#!/bin/bash\nfor i in $(curl -sk -u 'token-<id>:<secret>' https://<RANCHER_URL>/v3/users|jq -r .data[].links.globalRoleBindings); do\n\ncurl -sk -u 'token-<id>:<secret>' $i| jq '.data[] | \"\\(.userId) \\(.globalRoleId)\"'\n\ndoneThe admin role should only be assigned to users that require administrative privileges. Any role that is not admin or user should be audited in the RBAC section of the UI to ensure that the privileges adhere to policies for global access.The Rancher server permits customization of the default global permissions. We  recommend that auditors also review the policies of any custom global roles.RemediationRemove the admin role from any user that does not require administrative privileges.3.2.1 - Change the local administrator password from the default valueProfile Applicability\nLevel 1\nDescriptionThe local administrator password should be changed from the default.RationaleThe default administrator password is common across all Rancher installations and should be changed immediately upon startup.AuditAttempt to login into the UI with the following credentials:\n  - Username: admin\n  - Password: adminThe login attempt must not succeed.RemediationChange the password from admin to a password that meets the recommended password standards for your organization.3.2.2 - Configure an Identity Provider for AuthenticationProfile Applicability\nLevel 1\nDescriptionWhen running Rancher in a production environment, configure an identity provider for authentication.RationaleRancher supports several authentication backends that are common in enterprises. It is recommended to tie Rancher into an external authentication system to simplify user and group access in the Rancher cluster. Doing so assures that access control follows the organization’s change management process for user accounts.Audit\nIn the Rancher UI, select Global\nSelect Security\nSelect Authentication\nEnsure the authentication provider for your environment is active and configured correctly\nRemediationConfigure the appropriate authentication provider for your Rancher installation according to the documentation found at the link in the reference section below.Reference\nhttps://rancher.com/docs/rancher/v2.x/en/admin-settings/authentication/\n3.1.1 - Disable the local cluster optionProfile Applicability\nLevel 2\nDescriptionWhen deploying Rancher, disable the local cluster option on the Rancher Server.NOTE: This requires Rancher v2.1.2 or above.RationaleHaving access to the local cluster from the Rancher UI is convenient for troubleshooting and debugging; however, if the local cluster is enabled in the Rancher UI, a user has access to all elements of the system, including the Rancher management server itself. Disabling the local cluster is a defense in depth measure and removes the possible attack vector from the Rancher UI and API.Audit\nVerify the Rancher deployment has the --add-local=false option set.\nkubectl get deployment rancher -n cattle-system -o yaml |grep 'add-local'\nIn the Rancher UI go to Clusters in the Global view and verify that no local cluster is present.\nRemediation\nUpgrade to Rancher v2.1.2 via the Helm chart. While performing the upgrade, provide the following installation flag:\n--set addLocal=\"false\"3.1.2 - Enable Rancher Audit loggingProfile Applicability\nLevel ","postref":"9f0b1529071cb8b815164bdd76f57d42","objectID":"8cb861de2f84ce499e004b924a80b485","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/security/hardening-2.1/"},{"anchor":"#","title":"Rancher HA","content":"The commands/steps listed on this page can be used to check your Rancher Kubernetes Installation.\n\nMake sure you configured the correct kubeconfig (for example, export KUBECONFIG=$PWD/kube_config_rancher-cluster.yml).\n\nCheck Rancher pods\n\nRancher pods are deployed as a Deployment in the cattle-system namespace.\n\nCheck if the pods are running on all nodes:\n\nkubectl -n cattle-system get pods -l app=rancher -o wide\n\n\nExample output:\n\nNAME                       READY   STATUS    RESTARTS   AGE   IP          NODE\nrancher-7dbd7875f7-n6t5t   1/1     Running   0          8m    x.x.x.x     x.x.x.x\nrancher-7dbd7875f7-qbj5k   1/1     Running   0          8m    x.x.x.x     x.x.x.x\nrancher-7dbd7875f7-qw7wb   1/1     Running   0          8m    x.x.x.x     x.x.x.x\n\n\nIf a pod is unable to run (Status is not Running, Ready status is not showing 1/1 or you see a high count of Restarts), check the pod details, logs and namespace events.\n\nPod details\n\nkubectl -n cattle-system describe pods -l app=rancher\n\n\nPod container logs\n\nkubectl -n cattle-system logs -l app=rancher\n\n\nNamespace events\n\nkubectl -n cattle-system get events\n\n\nCheck ingress\n\nIngress should have the correct HOSTS (showing the configured FQDN) and ADDRESS (host address(es) it will be routed to).\n\nkubectl -n cattle-system get ingress\n\n\nExample output:\n\nNAME      HOSTS                    ADDRESS                   PORTS     AGE\nrancher   rancher.yourdomain.com   x.x.x.x,x.x.x.x,x.x.x.x   80, 443   2m\n\n\nCheck ingress controller logs\n\nWhen accessing your configured Rancher FQDN does not show you the UI, check the ingress controller logging to see what happens when you try to access Rancher:\n\nkubectl -n ingress-nginx logs -l app=ingress-nginx\n\n\nLeader election\n\nThe leader is determined by a leader election process. After the leader has been determined, the leader (holderIdentity) is saved in the cattle-controllers ConfigMap (in this example, rancher-7dbd7875f7-qbj5k).\n\nkubectl -n kube-system get configmap cattle-controllers -o jsonpath='{.metadata.annotations.control-plane\\.alpha\\.kubernetes\\.io/leader}'\n{\"holderIdentity\":\"rancher-7dbd7875f7-qbj5k\",\"leaseDurationSeconds\":45,\"acquireTime\":\"2019-04-04T11:53:12Z\",\"renewTime\":\"2019-04-04T12:24:08Z\",\"leaderTransitions\":0}\n\n","postref":"19a0a58c2874459db34bc2613208a43a","objectID":"0652665154a44103c61c1687397809ee","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/troubleshooting/rancherha/"},{"anchor":"#logging-into-rancheros","title":"Logging into RancherOS","content":"Please check the README in our RancherOS repository for our latest AMIs.From a command line, log into the EC2 Instance. If you added ssh keys using a cloud-config,\nboth those keys, and the one you selected in the AWS UI will be installed.$ ssh -i /Directory/of/MySSHKeyName.pem rancher@<ip-of-ec2-instance>\nIf you have issues logging into RancherOS, try using this command to help debug the issue.$ ssh -v -i /Directory/of/MySSHKeyName.pem rancher@<ip-of-ec2-instance>\n","postref":"8e4405bc4f06bcc6c117f99c68c19687","objectID":"86f8fd378237346aec7e5467f53df6b2","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/running-rancheros/cloud/aws/"},{"anchor":"#1-master-node-security-configuration","title":"1 Master Node Security Configuration","content":"5.1 RBAC and Service Accounts5.1.5 Ensure that default service accounts are not actively used. (Scored)Result: PASSRemediation:\nCreate explicit service accounts wherever a Kubernetes workload requires specific access\nto the Kubernetes API server.\nModify the configuration of each default service account to include this valueautomountServiceAccountToken: falseAudit Script: 5.1.5.sh#!/bin/bash\n\nexport KUBECONFIG=${KUBECONFIG:-/root/.kube/config}\n\nkubectl version > /dev/null\nif [ $? -ne 0 ]; then\n  echo \"fail: kubectl failed\"\n  exit 1\nfi\n\naccounts=\"$(kubectl --kubeconfig=${KUBECONFIG} get serviceaccounts -A -o json | jq -r '.items[] | select(.metadata.name==\"default\") | select((.automountServiceAccountToken == null) or (.automountServiceAccountToken == true)) | \"fail \\(.metadata.name) \\(.metadata.namespace)\"')\"\n\nif [[ \"${accounts}\" == \"\" ]]; then\n  echo \"--pass\"\n  exit 0\nfi\n\necho ${accounts}\nexit 1\nAudit Execution:./5.1.5.sh \nExpected result:'--pass' is present\n5.2 Pod Security Policies5.2.2 Minimize the admission of containers wishing to share the host process ID namespace (Scored)Result: PASSRemediation:\nCreate a PSP as described in the Kubernetes documentation, ensuring that the\n.spec.hostPID field is omitted or set to false.Audit:kubectl --kubeconfig=/root/.kube/config get psp -o json | jq .items[] | jq -r 'select((.spec.hostPID == null) or (.spec.hostPID == false))' | jq .metadata.name | wc -l | xargs -I {} echo '--count={}'\nExpected result:1 is greater than 0\n5.2.3 Minimize the admission of containers wishing to share the host IPC namespace (Scored)Result: PASSRemediation:\nCreate a PSP as described in the Kubernetes documentation, ensuring that the\n.spec.hostIPC field is omitted or set to false.Audit:kubectl --kubeconfig=/root/.kube/config get psp -o json | jq .items[] | jq -r 'select((.spec.hostIPC == null) or (.spec.hostIPC == false))' | jq .metadata.name | wc -l | xargs -I {} echo '--count={}'\nExpected result:1 is greater than 0\n5.2.4 Minimize the admission of containers wishing to share the host network namespace (Scored)Result: PASSRemediation:\nCreate a PSP as described in the Kubernetes documentation, ensuring that the\n.spec.hostNetwork field is omitted or set to false.Audit:kubectl --kubeconfig=/root/.kube/config get psp -o json | jq .items[] | jq -r 'select((.spec.hostNetwork == null) or (.spec.hostNetwork == false))' | jq .metadata.name | wc -l | xargs -I {} echo '--count={}'\nExpected result:1 is greater than 0\n5.2.5 Minimize the admission of containers with allowPrivilegeEscalation (Scored)Result: PASSRemediation:\nCreate a PSP as described in the Kubernetes documentation, ensuring that the\n.spec.allowPrivilegeEscalation field is omitted or set to false.Audit:kubectl --kubeconfig=/root/.kube/config get psp -o json | jq .items[] | jq -r 'select((.spec.allowPrivilegeEscalation == null) or (.spec.allowPrivilegeEscalation == false))' | jq .metadata.name | wc -l | xargs -I {} echo '--count={}'\nExpected result:1 is greater than 0\n5.3 Network Policies and CNI5.3.2 Ensure that all Namespaces have Network Policies defined (Scored)Result: PASSRemediation:\nFollow the documentation and create NetworkPolicy objects as you need them.Audit Script: 5.3.2.sh#!/bin/bash -e\n\nexport KUBECONFIG=${KUBECONFIG:-\"/root/.kube/config\"}\n\nkubectl version > /dev/null\nif [ $? -ne 0 ]; then\n  echo \"fail: kubectl failed\"\n  exit 1\nfi\n\nfor namespace in $(kubectl get namespaces -A -o json | jq -r '.items[].metadata.name'); do\n  policy_count=$(kubectl get networkpolicy -n ${namespace} -o json | jq '.items | length')\n  if [ ${policy_count} -eq 0 ]; then\n    echo \"fail: ${namespace}\"\n    exit 1\n  fi\ndone\n\necho \"pass\"\nAudit Execution:./5.3.2.sh \nExpected result:'pass' is present\n5.6 General Policies5.6.4 The default namespace should not be used (Scored)Result: PASSRemediation:\nEnsure that namespaces are created to allow for appropriate segregation of Kubernetes\nresources and that all new resources are created in a specific namespace.Audit Script: 5.6.4.sh#!/bin/bash -e\n\nexport KUBECONFIG=${KUBECONFIG:-/root/.kube/config}\n\nkubectl version > /dev/null\nif [[ $? -gt 0 ]]; then\n  echo \"fail: kubectl failed\"\n  exit 1\nfi\n\ndefault_resources=$(kubectl get all -o json | jq --compact-output '.items[] | select((.kind == \"Service\") and (.metadata.name == \"kubernetes\") and (.metadata.namespace == \"default\") | not)' | wc -l)\n\necho \"--count=${default_resources}\"\nAudit Execution:./5.6.4.sh \nExpected result:'0' is equal to '0'\n4.1 Worker Node Configuration Files4.1.1 Ensure that the kubelet service file permissions are set to 644 or more restrictive (Scored)Result: Not ApplicableRemediation:\nRKE doesn’t require or maintain a configuration file for the kubelet service. All configuration is passed in as arguments at container run time.4.1.2 Ensure that the kubelet service file ownership is set to root:root (Scored)Result: Not ApplicableRemediation:\nRKE doesn’t require or maintain a configuration file for the kubelet service. All configuration is passed in as arguments at container run time.4.1.3 Ensure that the proxy kubeconfig file permissions are set to 644 or more restrictive (Scored)Result: Not ApplicableRemediation:\nRKE doesn’t require or maintain a configuration file for the proxy service. All configuration is passed in as arguments at container run time.4.1.4 Ensure that the proxy kubeconfig file ownership is set to root:root (Scored)Result: Not ApplicableRemediation:\nRKE doesn’t require or maintain a configuration file for the proxy service. All configuration is passed in as arguments at container run time.4.1.5 Ensure that the kubelet.conf file permissions are set to 644 or more restrictive (Scored)Result: Not ApplicableRemediation:\nRKE doesn’t require or maintain a configuration file for the kubelet service. All configuration is passed in as arguments at container run time.4.1.6 Ensure that the kubelet.conf file ownership is set to root:root (Scored)Result: Not ApplicableRemediation:\nRKE doesn’t require or maintain a configuration file for the kubelet service. All configuration is passed in as arguments at container run time.4.1.7 Ensure that the certificate authorities file permissions are set to 644 or more restrictive (Scored)Result: PASSRemediation:\nRun the following command to modify the file permissions of the--client-ca-file chmod 644 <filename>Audit:stat -c %a /etc/kubernetes/ssl/kube-ca.pem\nExpected result:'644' is equal to '644' OR '640' is present OR '600' is present\n4.1.8 Ensure that the client certificate authorities file ownership is set to root:root (Scored)Result: PASSRemediation:\nRun the following command to modify the ownership of the --client-ca-file.chown root:root <filename>Audit:/bin/sh -c 'if test -e /etc/kubernetes/ssl/kube-ca.pem; then stat -c %U:%G /etc/kubernetes/ssl/kube-ca.pem; fi' \nExpected result:'root:root' is equal to 'root:root'\n4.1.9 Ensure that the kubelet configuration file has permissions set to 644 or more restrictive (Scored)Result: Not ApplicableRemediation:\nRKE doesn’t require or maintain a configuration file for the kubelet service. All configuration is passed in as arguments at container run time.4.1.10 Ensure that the kubelet configuration file ownership is set to root:root (Scored)Result: Not ApplicableRemediation:\nRKE doesn’t require or maintain a configuration file for the kubelet service. All configuration is passed in as arguments at container run time.4.2 Kubelet4.2.1 Ensure that the --anonymous-auth argument is set to false (Scored)Result: PASSRemediation:\nIf using a Kubelet config file, edit the file to set authentication: anonymous: enabled to\nfalse.\nIf using executable arguments, edit the kubelet service file\n/etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and\nset the below parameter in KUBELET_SYSTEM_PODS_ARGS variable.--anonymous-auth=falseBased on your system, restart the kubelet service. For example:systemctl daemon-reload\nsystemctl restart kubelet.serviceAudit:/bin/ps -fC kubelet\nAudit Config:/bin/cat /var/lib/kubelet/config.yaml\nExpected result:'false' is equal to 'false'\n4.2.2 Ensure that the --authorization-mode argument is not set to AlwaysAllow (Scored)Result: PASSRemediation:\nIf using a Kubelet config file, edit the file to set authorization: mode to Webhook. If\nusing executable arguments, edit the kubelet service file\n/etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and\nset the below parameter in KUBELET_AUTHZ_ARGS variable.--authorization-mode=WebhookBased on your system, restart the kubelet service. For example:systemctl daemon-reload\nsystemctl restart kubelet.serviceAudit:/bin/ps -fC kubelet\nAudit Config:/bin/cat /var/lib/kubelet/config.yaml\nExpected result:'Webhook' not have 'AlwaysAllow'\n4.2.3 Ensure that the --client-ca-file argument is set as appropriate (Scored)Result: PASSRemediation:\nIf using a Kubelet config file, edit the file to set authentication: x509: clientCAFile to\nthe location of the client CA file.\nIf using command line arguments, ","postref":"209f5510e9c78daffe5a39cd15288f62","objectID":"35a65989a7e1f06bcdf62e0ec2681766","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/security/benchmark-2.3.5/"},{"anchor":"#","title":"Imported clusters","content":"The commands/steps listed on this page can be used to check clusters that you are importing or that are imported in Rancher.\n\nMake sure you configured the correct kubeconfig (for example, export KUBECONFIG=$PWD/kubeconfig_from_imported_cluster.yml)\n\nRancher agents\n\nCommunication to the cluster (Kubernetes API via cattle-cluster-agent) and communication to the nodes is done through Rancher agents.\n\nIf the cattle-cluster-agent cannot connect to the configured server-url, the cluster will remain in Pending state, showing Waiting for full cluster configuration.\n\ncattle-node-agent\n\nCheck if the cattle-node-agent pods are present on each node, have status Running and don’t have a high count of Restarts:\n\nkubectl -n cattle-system get pods -l app=cattle-agent -o wide\n\n\nExample output:\n\nNAME                      READY     STATUS    RESTARTS   AGE       IP                NODE\ncattle-node-agent-4gc2p   1/1       Running   0          2h        x.x.x.x           worker-1\ncattle-node-agent-8cxkk   1/1       Running   0          2h        x.x.x.x           etcd-1\ncattle-node-agent-kzrlg   1/1       Running   0          2h        x.x.x.x           etcd-0\ncattle-node-agent-nclz9   1/1       Running   0          2h        x.x.x.x           controlplane-0\ncattle-node-agent-pwxp7   1/1       Running   0          2h        x.x.x.x           worker-0\ncattle-node-agent-t5484   1/1       Running   0          2h        x.x.x.x           controlplane-1\ncattle-node-agent-t8mtz   1/1       Running   0          2h        x.x.x.x           etcd-2\n\n\nCheck logging of a specific cattle-node-agent pod or all cattle-node-agent pods:\n\nkubectl -n cattle-system logs -l app=cattle-agent\n\n\ncattle-cluster-agent\n\nCheck if the cattle-cluster-agent pod is present in the cluster, has status Running and doesn’t have a high count of Restarts:\n\nkubectl -n cattle-system get pods -l app=cattle-cluster-agent -o wide\n\n\nExample output:\n\nNAME                                    READY     STATUS    RESTARTS   AGE       IP           NODE\ncattle-cluster-agent-54d7c6c54d-ht9h4   1/1       Running   0          2h        x.x.x.x      worker-1\n\n\nCheck logging of cattle-cluster-agent pod:\n\nkubectl -n cattle-system logs -l app=cattle-cluster-agent\n\n","postref":"1bdd6bb14c90e4a5edc0e56d3f55f0ef","objectID":"2cf24d70b1b4b70f3fc8b64e956bccdf","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/troubleshooting/imported-clusters/"},{"anchor":"#1-master-node-security-configuration","title":"1 - Master Node Security Configuration","content":"2.1 - Kubelet2.1.1 - Ensure that the --anonymous-auth argument is set to false (Scored)Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--anonymous-auth=false\").string'Returned Value: --anonymous-auth=falseResult: Pass2.1.2 - Ensure that the --authorization-mode argument is not set to AlwaysAllow (Scored)Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--authorization-mode=Webhook\").string'Returned Value: --authorization-mode=WebhookResult: Pass2.1.3 - Ensure that the --client-ca-file argument is set as appropriate (Scored)Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--client-ca-file=.*\").string'Returned Value: --client-ca-file=/etc/kubernetes/ssl/kube-ca.pemResult: Pass2.1.4 - Ensure that the --read-only-port argument is set to 0 (Scored)Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--read-only-port=0\").string'Returned Value: --read-only-port=0Result: Pass2.1.5 - Ensure that the --streaming-connection-idle-timeout argument is not set to 0 (Scored)Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--streaming-connection-idle-timeout=.*\").string'Returned Value: --streaming-connection-idle-timeout=30mResult: Pass2.1.6 - Ensure that the --protect-kernel-defaults argument is set to true (Scored)Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--protect-kernel-defaults=true\").string'Returned Value: --protect-kernel-defaults=trueResult: Pass2.1.7 - Ensure that the --make-iptables-util-chains argument is set to true (Scored)Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--make-iptables-util-chains=true\").string'Returned Value: --make-iptables-util-chains=trueResult: Pass2.1.8 - Ensure that the --hostname-override argument is not set (Scored)Notes\nThis is used by most cloud providers. Not setting this is not practical in most cases.Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--hostname-override=.*\").string'Returned Value: --hostname-override=<ipv4 address>Result: Fail2.1.9 - Ensure that the --event-qps argument is set to 0 (Scored)Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--event-qps=0\").string'Returned Value: --event-qps=0Result: Pass2.1.10 - Ensure that the --tls-cert-file and --tls-private-key-file arguments are set as appropriate (Scored)NotesRKE does not set these options and uses the kubelet’s self generated certificates for TLS communication. These files are located in the default directory (/var/lib/kubelet/pki).Audit (--tls-cert-file)docker inspect kubelet | jq -e '.[0].Args[] | match(\"--tls-cert-file=.*\").string'Returned Value: --tls-cert-file=/etc/kubernetes/ssl/kube-kubelet-172-31-40-84.pemAudit (--tls-private-key-file)docker inspect kubelet | jq -e '.[0].Args[] | match(\"--tls-private-key-file=.*\").string'Returned Value: --tls-private-key-file=/etc/kubernetes/ssl/kube-kubelet-172-31-40-84-key.pemResult: Pass2.1.11 - Ensure that the --cadvisor-port argument is set to 0 (Scored)Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--cadvisor-port=0\").string'Returned Value: nullResult: Pass2.1.12 - Ensure that the --rotate-certificates argument is not set to false (Scored)NotesRKE handles certificate rotation through an external process.Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--rotate-certificates=true\").string'Returned Value: nullResult: Pass (Not Applicable)2.1.13 - Ensure that the RotateKubeletServerCertificate argument is set to true (Scored)Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--feature-gates=.*(RotateKubeletServerCertificate=true).*\").captures[].string'Returned Value: RotateKubeletServerCertificate=trueResult: Pass2.1.14 - Ensure that the kubelet only makes use of strong cryptographic ciphers (Not Scored)Audit (Allowed Ciphers)docker inspect kubelet | jq -e '.[0].Args[] | match(\"--tls-cipher-suites=.*(TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256).*\").captures[].string'Returned Value: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--tls-cipher-suites=.*(TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256).*\").captures[].string'Returned Value: TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--tls-cipher-suites=.*(TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305).*\").captures[].string'Returned Value: TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--tls-cipher-suites=.*(TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384).*\").captures[].string'Returned Value: TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--tls-cipher-suites=.*(TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305).*\").captures[].string'Returned Value: TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--tls-cipher-suites=.*(TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384).*\").captures[].string'Returned Value: TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--tls-cipher-suites=.*(TLS_RSA_WITH_AES_256_GCM_SHA384).*\").captures[].string'Returned Value: TLS_RSA_WITH_AES_256_GCM_SHA384Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--tls-cipher-suites=.*(TLS_RSA_WITH_AES_128_GCM_SHA256).*\").captures[].string'Returned Value: TLS_RSA_WITH_AES_128_GCM_SHA256Audit (Disallowed Ciphers)docker inspect kubelet | jq -e '.[0].Args[] | match(\"--tls-cipher-suites=.*(CBC).*\").captures[].string'Returned Value: nullAuditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--tls-cipher-suites=.*(RC4).*\").captures[].string'Returned Value: nullResult: Pass2.2 - Configuration Files2.2.1 - Ensure that the permissions for kubelet.conf are set to 644 or more restrictive (Scored)NotesThis is the value of the --kubeconfig option.Auditstat -c %a /etc/kubernetes/ssl/kubecfg-kube-node.yamlReturned Value: 640Result: Pass2.2.2 - Ensure that the kubelet.conf file ownership is set to root:root (Scored)NotesThis is the value of the --kubeconfig option.Auditstat -c %U:%G /etc/kubernetes/ssl/kubecfg-kube-node.yamlReturned Value: root:rootResult: Pass2.2.3 - Ensure that the kubelet service file permissions are set to 644 or more restrictive (Scored)NotesRKE doesn’t require or maintain a configuration file for kubelet. All configuration is passed in as arguments at container run time.Result: Pass (Not Applicable)2.2.4 - Ensure that the kubelet service file ownership is set to root:root (Scored)NotesRKE doesn’t require or maintain a configuration file for kubelet. All configuration is passed in as arguments at container run time.Result: Pass (Not Applicable)2.2.5 - Ensure that the proxy kubeconfig file permissions are set to 644 or more restrictive (Scored)Auditstat -c %a /etc/kubernetes/ssl/kubecfg-kube-proxy.yamlReturned Value: 640Result: Pass2.2.6 - Ensure that the proxy kubeconfig file ownership is set to root:root (Scored)Auditstat -c %U:%G /etc/kubernetes/ssl/kubecfg-kube-proxy.yamlReturned Value: root:rootResult: Pass2.2.7 - Ensure that the certificate authorities file permissions are set to 644 or more restrictive (Scored)Auditstat -c %a /etc/kubernetes/ssl/kube-ca.pemReturned Value: 640Result: Pass2.2.8 - Ensure that the client certificate authorities file ownership is set to root:root (Scored)Auditstat -c %U:%G /etc/kubernetes/ssl/kube-ca.pemReturned Value: root:rootResult: Pass2.2.9 - Ensure that the kubelet configuration file ownership is set to root:root (Scored)NotesRKE doesn’t require or maintain a configuration file for kubelet. All configuration is passed in as arguments at container run time.Result: Pass (Not Applicable)2.2.10 - Ensure that the kubelet configuration file permissions are set to 644 or more restrictive (Scored)NotesRKE doesn’t require or maintain a configuration file for kubelet. All configuration is passed in as arguments at container run time.Result: Pass (Not Applicable)1.1 - API Server1.1.1 - Ensure that the --anonymous-auth argument is set to false (Scored)Auditdocker inspect kube-apiserver | jq -e '.[0].Args[] | match(\"--anonymous-auth=false\").string'Returned Value: --anonymous-auth=falseResult: Pass1.1.2 - Ensure that the --basic-auth-file argument is not set (Scored)Auditdocker inspect kube-apiserver | jq -e '.[0].Args[] | match(\"--basic-auth-file=.*\").string'Returned Value:  nullResult: Pass1.1.3 - Ensure that the --insecure-allow-any-token argument is not set (Scored)Auditdocker inspect kube-apiserver | jq -e '.[0].Args[] | match(\"--insecure-allow-any-token\").string'Returned Value:  nullResult: Pass1.1.4 - Ensure that the --kubelet-https argument is set to true (Scored)Auditdocker inspect kube-apiserver | jq -e '.[0].Args[] | match(\"--kubelet-https=false\").string'Returned Value: nullResult: Pass1.1.5 - Ensure that the --insecure-bind-address argument is not set (Scored)NotesFlag not set or --insecure-bind-address=127.0.0.1. RKE sets this flag to --insecure-bind-address=127.0.0.1Auditdocker inspect kube-apiserver | jq -e '.[0].Args[] | match(\"--insecure-bind-address=(?:(?!127\\\\.0\\\\.0\\\\.1).)+\")'R","postref":"fedb85fe9674fdd74112f98a0d91f3a0","objectID":"ff43c353c032e70aafdd82f7cc4d3133","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/security/benchmark-2.3.3/"},{"anchor":"#logging-into-rancheros","title":"Logging into RancherOS","content":"Remember, the SSH keys are passed to the rancher user. The SSH keys can be passed from the project level, the instance level or through the cloud config file. If you add any of these SSH keys after the instance has been created, the instance will need to be reset before the SSH keys are passed through.$ gcloud compute ssh rancher@<INSTANCE_NAME> --project <PROJECT_ID> --zone <ZONE_OF_INSTANCE>\nIf you have issues logging into RancherOS, try using this command to help debug the instance.$ gcloud compute instances get-serial-port-output <INSTANCE_NAME> --zone <ZONE_OF_INSTANCE> --project <PROJECT_ID>\n","postref":"9a5b2e4c0b2d99d5227efb4823b18aec","objectID":"cca865948b26259c1cfb25f382ce4335","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/running-rancheros/cloud/gce/"},{"anchor":"#1-master-node-security-configuration","title":"1 - Master Node Security Configuration","content":"2.1 - Kubelet2.1.1 - Ensure that the --anonymous-auth argument is set to false (Scored)Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--anonymous-auth=false\").string'Returned Value: --anonymous-auth=falseResult: Pass2.1.2 - Ensure that the --authorization-mode argument is not set to AlwaysAllow (Scored)Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--authorization-mode=Webhook\").string'Returned Value: --authorization-mode=WebhookResult: Pass2.1.3 - Ensure that the --client-ca-file argument is set as appropriate (Scored)Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--client-ca-file=.*\").string'Returned Value: --client-ca-file=/etc/kubernetes/ssl/kube-ca.pemResult: Pass2.1.4 - Ensure that the --read-only-port argument is set to 0 (Scored)Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--read-only-port=0\").string'Returned Value: --read-only-port=0Result: Pass2.1.5 - Ensure that the --streaming-connection-idle-timeout argument is not set to 0 (Scored)Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--streaming-connection-idle-timeout=.*\").string'Returned Value: --streaming-connection-idle-timeout=1800sResult: Pass2.1.6 - Ensure that the --protect-kernel-defaults argument is set to true (Scored)Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--protect-kernel-defaults=true\").string'Returned Value: --protect-kernel-defaults=trueResult: Pass2.1.7 - Ensure that the --make-iptables-util-chains argument is set to true (Scored)Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--make-iptables-util-chains=true\").string'Returned Value: --make-iptables-util-chains=trueResult: Pass2.1.8 - Ensure that the --hostname-override argument is not set (Scored)Notes\nThis is used by most cloud providers. Not setting this is not practical in most cases.Result: Not Applicable2.1.9 - Ensure that the --event-qps argument is set to 0 (Scored)Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--event-qps=0\").string'Returned Value: --event-qps=0Result: Pass2.1.10 - Ensure that the --tls-cert-file and --tls-private-key-file arguments are set as appropriate (Scored)NotesRKE does not set these options and uses the kubelet’s self generated certificates for TLS communication. These files are located in the default directory (/var/lib/kubelet/pki).Audit (--tls-cert-file)docker inspect kubelet | jq -e '.[0].Args[] | match(\"--tls-cert-file=.*\").string'Returned Value: nullAudit (--tls-private-key-file)docker inspect kubelet | jq -e '.[0].Args[] | match(\"--tls-private-key-file=.*\").string'Returned Value: nullResult: Pass2.1.11 - Ensure that the --cadvisor-port argument is set to 0 (Scored)Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--cadvisor-port=0\").string'Returned Value: nullResult: Pass2.1.12 - Ensure that the --rotate-certificates argument is not set to false (Scored)NotesRKE handles certificate rotation through an external process.Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--rotate-certificates=true\").string'Returned Value: nullResult: Not Applicable2.1.13 - Ensure that the RotateKubeletServerCertificate argument is set to true (Scored)NotesRKE handles certificate rotation through an external process.Result: Not Applicable2.1.14 - Ensure that the kubelet only makes use of strong cryptographic ciphers (Not Scored)Audit (Allowed Ciphers)docker inspect kubelet | jq -e '.[0].Args[] | match(\"--tls-cipher-suites=.*(TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256).*\").captures[].string'Returned Value: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--tls-cipher-suites=.*(TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256).*\").captures[].string'Returned Value: TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--tls-cipher-suites=.*(TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305).*\").captures[].string'Returned Value: TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--tls-cipher-suites=.*(TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384).*\").captures[].string'Returned Value: TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--tls-cipher-suites=.*(TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305).*\").captures[].string'Returned Value: TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--tls-cipher-suites=.*(TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384).*\").captures[].string'Returned Value: TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--tls-cipher-suites=.*(TLS_RSA_WITH_AES_256_GCM_SHA384).*\").captures[].string'Returned Value: TLS_RSA_WITH_AES_256_GCM_SHA384Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--tls-cipher-suites=.*(TLS_RSA_WITH_AES_128_GCM_SHA256).*\").captures[].string'Returned Value: TLS_RSA_WITH_AES_128_GCM_SHA256Audit (Disallowed Ciphers)docker inspect kubelet | jq -e '.[0].Args[] | match(\"--tls-cipher-suites=.*(CBC).*\").captures[].string'Returned Value: nullAuditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--tls-cipher-suites=.*(RC4).*\").captures[].string'Returned Value: nullResult: Pass2.2 - Configuration Files2.2.1 - Ensure that the permissions for kubelet.conf are set to 644 or more restrictive (Scored)NotesThis is the value of the --kubeconfig option.Auditstat -c %a /etc/kubernetes/ssl/kubecfg-kube-node.yamlReturned Value: 644Result: Pass2.2.2 - Ensure that the kubelet.conf file ownership is set to root:root (Scored)NotesThis is the value of the --kubeconfig option.Auditstat -c %U:%G /etc/kubernetes/ssl/kubecfg-kube-node.yamlReturned Value: root:rootResult: Pass2.2.3 - Ensure that the kubelet service file permissions are set to 644 or more restrictive (Scored)NotesRKE doesn’t require or maintain a configuration file for kubelet. All configuration is passed in as arguments at container run time.Result: Not Applicable2.2.4 - Ensure that the kubelet service file ownership is set to root:root (Scored)NotesRKE doesn’t require or maintain a configuration file for kubelet. All configuration is passed in as arguments at container run time.Result: Not Applicable2.2.5 - Ensure that the proxy kubeconfig file permissions are set to 644 or more restrictive (Scored)Auditstat -c %a /etc/kubernetes/ssl/kubecfg-kube-proxy.yamlReturned Value: 644Result: Pass2.2.6 - Ensure that the proxy kubeconfig file ownership is set to root:root (Scored)Auditstat -c %U:%G /etc/kubernetes/ssl/kubecfg-kube-proxy.yamlReturned Value: root:rootResult: Pass2.2.7 - Ensure that the certificate authorities file permissions are set to 644 or more restrictive (Scored)Auditstat -c %a /etc/kubernetes/ssl/kube-ca.pemReturned Value: 644Result: Pass2.2.8 - Ensure that the client certificate authorities file ownership is set to root:root (Scored)Auditstat -c %U:%G /etc/kubernetes/ssl/kube-ca.pemReturned Value: root:rootResult: Pass2.2.9 - Ensure that the kubelet configuration file ownership is set to root:root (Scored)NotesRKE doesn’t require or maintain a configuration file for kubelet. All configuration is passed in as arguments at container run time.Result: Not Applicable2.2.10 - Ensure that the kubelet configuration file permissions are set to 644 or more restrictive (Scored)NotesRKE doesn’t require or maintain a configuration file for kubelet. All configuration is passed in as arguments at container run time.Result: Not Applicable1.1 - API Server1.1.1 - Ensure that the --anonymous-auth argument is set to false (Scored)Auditdocker inspect kube-apiserver | jq -e '.[0].Args[] | match(\"--anonymous-auth=false\").string'Returned Value: --anonymous-auth=falseResult: Pass1.1.2 - Ensure that the --basic-auth-file argument is not set (Scored)Auditdocker inspect kube-apiserver | jq -e '.[0].Args[] | match(\"--basic-auth-file=.*\").string'Returned Value:  nullResult: Pass1.1.3 - Ensure that the --insecure-allow-any-token argument is not set (Scored)Auditdocker inspect kube-apiserver | jq -e '.[0].Args[] | match(\"--insecure-allow-any-token\").string'Returned Value:  nullResult: Pass1.1.4 - Ensure that the --kubelet-https argument is set to true (Scored)Auditdocker inspect kube-apiserver | jq -e '.[0].Args[] | match(\"--kubelet-https=false\").string'Returned Value: nullResult: Pass1.1.5 - Ensure that the --insecure-bind-address argument is not set (Scored)NotesFlag not set or --insecure-bind-address=127.0.0.1. RKE sets this flag to --insecure-bind-address=127.0.0.1Auditdocker inspect kube-apiserver | jq -e '.[0].Args[] | match(\"--insecure-bind-address=(?:(?!127\\\\.0\\\\.0\\\\.1).)+\")'Returned Value: nullResult: Pass1.1.6 - Ensure that the --insecure-port argument is set to 0 (Scored)Auditdocker inspect kube-apiserver | jq -e '.[0].Args[] | match(\"--insecure-port=0\").string'Returned Value: --insecure-port=0Result: Pass1.1.7 - Ensure that the --secure-port argument is not set to 0 (Scored)Auditdocker inspect kube-apiserver | jq -e '.[0].Args[] | match(\"--secure-port=6443\").string'Return","postref":"03ba243ca5fb93e295ce8f9f67856dc5","objectID":"e48715a8aea7cbe0ddf10b98048c8278","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/security/benchmark-2.3/"},{"anchor":"#","title":"Digital Ocean","content":"RancherOS is available in the Digital Ocean portal. RancherOS is a member of container distributions and you can find it easily.\n\n\nNote\nDeploying to Digital Ocean will incur charges.\n\n\nTo start a RancherOS Droplet on Digital Ocean:\n\n\nIn the Digital Ocean portal, go to the project view.\nClick New Droplet.\nClick Create Droplet.\nClick the Container distributions tab.\nClick RancherOS.\nChoose a plan. Make sure your Droplet has the minimum hardware requirements for RancherOS.\nChoose any options for backups, block storage, and datacenter region.\nOptional: In the Select additional options section, you can check the User data box and enter a cloud-config file in the text box that appears. The cloud-config file is used to provide a script to be run on the first boot. An example is below.\nChoose an SSH key that you have access to, or generate a new SSH key.\nChoose your project.\nClick Create.\n\n\nYou can access the host via SSH after the Droplet is booted. The default user is rancher.\n\nBelow is an example cloud-config file that you can use to initialize the Droplet with user data, such as deploying Rancher:\n\n#cloud-config\n\nwrite_files:\n  - path: /etc/rc.local\n    permissions: \"0755\"\n    owner: root\n    content: |\n      #!/bin/bash\n      wait-for-docker\n\n      export curlimage=appropriate/curl\n      export jqimage=stedolan/jq\n      export rancher_version=v2.2.2\n\n      for image in $curlimage $jqimage \"rancher/rancher:${rancher_version}\"; do\n        until docker inspect $image > /dev/null 2>&1; do\n          docker pull $image\n          sleep 2\n        done\n      done\n\n      docker run -d --restart=unless-stopped -p 80:80 -p 443:443 -v /opt/rancher:/var/lib/rancher rancher/rancher:${rancher_version}\n\n","postref":"a6c04f8a605e25e524a8fa020f59c48d","objectID":"cc6c5833cc5753b30549b64add9a1eaf","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/running-rancheros/cloud/do/"},{"anchor":"#1-master-node-security-configuration","title":"1 - Master Node Security Configuration","content":"2.1 - Kubelet2.1.1 - Ensure that the --anonymous-auth argument is set to false (Scored)Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--anonymous-auth=false\").string'Returned Value: --anonymous-auth=falseResult: Pass2.1.2 - Ensure that the --authorization-mode argument is not set to AlwaysAllow (Scored)Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--authorization-mode=Webhook\").string'Returned Value: --authorization-mode=WebhookResult: Pass2.1.3 - Ensure that the --client-ca-file argument is set as appropriate (Scored)Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--client-ca-file=.*\").string'Returned Value: --client-ca-file=/etc/kubernetes/ssl/kube-ca.pemResult: Pass2.1.4 - Ensure that the --read-only-port argument is set to 0 (Scored)Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--read-only-port=0\").string'Returned Value: --read-only-port=0Result: Pass2.1.5 - Ensure that the --streaming-connection-idle-timeout argument is not set to 0 (Scored)Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--streaming-connection-idle-timeout=.*\").string'Returned Value: --streaming-connection-idle-timeout=1800sResult: Pass2.1.6 - Ensure that the --protect-kernel-defaults argument is set to true (Scored)Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--protect-kernel-defaults=true\").string'Returned Value: --protect-kernel-defaults=trueResult: Pass2.1.7 - Ensure that the --make-iptables-util-chains argument is set to true (Scored)Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--make-iptables-util-chains=true\").string'Returned Value: --make-iptables-util-chains=trueResult: Pass2.1.8 - Ensure that the --hostname-override argument is not set (Scored)Notes\nThis is used by most cloud providers. Not setting this is not practical in most cases.Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--hostname-override=.*\").string'Returned Value: --hostname-override=<ipv4 address>Result: Fail2.1.9 - Ensure that the --event-qps argument is set to 0 (Scored)Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--event-qps=0\").string'Returned Value: --event-qps=0Result: Pass2.1.10 - Ensure that the --tls-cert-file and --tls-private-key-file arguments are set as appropriate (Scored)NotesRKE does not set these options and uses the kubelet’s self generated certificates for TLS communication. These files are located in the default directory (/var/lib/kubelet/pki).Audit (--tls-cert-file)docker inspect kubelet | jq -e '.[0].Args[] | match(\"--tls-cert-file=.*\").string'Returned Value: nullAudit (--tls-private-key-file)docker inspect kubelet | jq -e '.[0].Args[] | match(\"--tls-private-key-file=.*\").string'Returned Value: nullResult: Pass2.1.11 - Ensure that the --cadvisor-port argument is set to 0 (Scored)Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--cadvisor-port=0\").string'Returned Value: nullResult: Pass2.1.12 - Ensure that the --rotate-certificates argument is not set to false (Scored)NotesRKE handles certificate rotation through an external process.Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--rotate-certificates=true\").string'Returned Value: nullResult: Pass (Not Applicable)2.1.13 - Ensure that the RotateKubeletServerCertificate argument is set to true (Scored)Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--feature-gates=.*(RotateKubeletServerCertificate=true).*\").captures[].string'Returned Value: RotateKubeletServerCertificate=trueResult: Pass2.1.14 - Ensure that the kubelet only makes use of strong cryptographic ciphers (Not Scored)Audit (Allowed Ciphers)docker inspect kubelet | jq -e '.[0].Args[] | match(\"--tls-cipher-suites=.*(TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256).*\").captures[].string'Returned Value: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--tls-cipher-suites=.*(TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256).*\").captures[].string'Returned Value: TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--tls-cipher-suites=.*(TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305).*\").captures[].string'Returned Value: TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--tls-cipher-suites=.*(TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384).*\").captures[].string'Returned Value: TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--tls-cipher-suites=.*(TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305).*\").captures[].string'Returned Value: TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--tls-cipher-suites=.*(TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384).*\").captures[].string'Returned Value: TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--tls-cipher-suites=.*(TLS_RSA_WITH_AES_256_GCM_SHA384).*\").captures[].string'Returned Value: TLS_RSA_WITH_AES_256_GCM_SHA384Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--tls-cipher-suites=.*(TLS_RSA_WITH_AES_128_GCM_SHA256).*\").captures[].string'Returned Value: TLS_RSA_WITH_AES_128_GCM_SHA256Audit (Disallowed Ciphers)docker inspect kubelet | jq -e '.[0].Args[] | match(\"--tls-cipher-suites=.*(CBC).*\").captures[].string'Returned Value: nullAuditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--tls-cipher-suites=.*(RC4).*\").captures[].string'Returned Value: nullResult: Pass2.2 - Configuration Files2.2.1 - Ensure that the permissions for kubelet.conf are set to 644 or more restrictive (Scored)NotesThis is the value of the --kubeconfig option.Auditstat -c %a /etc/kubernetes/ssl/kubecfg-kube-node.yamlReturned Value: 644Result: Pass2.2.2 - Ensure that the kubelet.conf file ownership is set to root:root (Scored)NotesThis is the value of the --kubeconfig option.Auditstat -c %U:%G /etc/kubernetes/ssl/kubecfg-kube-node.yamlReturned Value: root:rootResult: Pass2.2.3 - Ensure that the kubelet service file permissions are set to 644 or more restrictive (Scored)NotesRKE doesn’t require or maintain a configuration file for kubelet. All configuration is passed in as arguments at container run time.Result: Pass (Not Applicable)2.2.4 - Ensure that the kubelet service file ownership is set to root:root (Scored)NotesRKE doesn’t require or maintain a configuration file for kubelet. All configuration is passed in as arguments at container run time.Result: Pass (Not Applicable)2.2.5 - Ensure that the proxy kubeconfig file permissions are set to 644 or more restrictive (Scored)Auditstat -c %a /etc/kubernetes/ssl/kubecfg-kube-proxy.yamlReturned Value: 644Result: Pass2.2.6 - Ensure that the proxy kubeconfig file ownership is set to root:root (Scored)Auditstat -c %U:%G /etc/kubernetes/ssl/kubecfg-kube-proxy.yamlReturned Value: root:rootResult: Pass2.2.7 - Ensure that the certificate authorities file permissions are set to 644 or more restrictive (Scored)Auditstat -c %a /etc/kubernetes/ssl/kube-ca.pemReturned Value: 644Result: Pass2.2.8 - Ensure that the client certificate authorities file ownership is set to root:root (Scored)Auditstat -c %U:%G /etc/kubernetes/ssl/kube-ca.pemReturned Value: root:rootResult: Pass2.2.9 - Ensure that the kubelet configuration file ownership is set to root:root (Scored)NotesRKE doesn’t require or maintain a configuration file for kubelet. All configuration is passed in as arguments at container run time.Result: Pass (Not Applicable)2.2.10 - Ensure that the kubelet configuration file permissions are set to 644 or more restrictive (Scored)NotesRKE doesn’t require or maintain a configuration file for kubelet. All configuration is passed in as arguments at container run time.Result: Pass (Not Applicable)1.1 - API Server1.1.1 - Ensure that the --anonymous-auth argument is set to false (Scored)Auditdocker inspect kube-apiserver | jq -e '.[0].Args[] | match(\"--anonymous-auth=false\").string'Returned Value: --anonymous-auth=falseResult: Pass1.1.2 - Ensure that the --basic-auth-file argument is not set (Scored)Auditdocker inspect kube-apiserver | jq -e '.[0].Args[] | match(\"--basic-auth-file=.*\").string'Returned Value:  nullResult: Pass1.1.3 - Ensure that the --insecure-allow-any-token argument is not set (Scored)Auditdocker inspect kube-apiserver | jq -e '.[0].Args[] | match(\"--insecure-allow-any-token\").string'Returned Value:  nullResult: Pass1.1.4 - Ensure that the --kubelet-https argument is set to true (Scored)Auditdocker inspect kube-apiserver | jq -e '.[0].Args[] | match(\"--kubelet-https=false\").string'Returned Value: nullResult: Pass1.1.5 - Ensure that the --insecure-bind-address argument is not set (Scored)NotesFlag not set or --insecure-bind-address=127.0.0.1. RKE sets this flag to --insecure-bind-address=127.0.0.1Auditdocker inspect kube-apiserver | jq -e '.[0].Args[] | match(\"--insecure-bind-address=(?:(?!127\\\\.0\\\\.0\\\\.1).)+\")'Returned Value: nullResult: Pass1.1.6 - Ensure that the --insecure-port argument is set to 0 (Scored)Auditdocker inspect kube-apiser","postref":"d23dc73fbd40406b6025b3007cc56bd5","objectID":"0e0a756bed2273edbe47671add6211fb","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/security/benchmark-2.2/"},{"anchor":"#","title":"VMware ESXi","content":"As of v1.1.0, RancherOS automatically detects that it is running on VMware ESXi, and automatically adds the open-vm-tools service to be downloaded and started, and uses guestinfo keys to set the cloud-init data.\n\nAs of v1.5.0, RancherOS releases anything required for VMware, which includes initrd, a standard ISO for VMware, a vmdk image, and a specific ISO to be used with Docker Machine. The open-vm-tools is built in to RancherOS, there is no need to download it.\n\n\n\n\nDescription\nDownload URL\n\n\n\n\n\nBooting from ISO\nhttps://releases.rancher.com/os/latest/vmware/rancheros.iso\n\n\n\nFor docker-machine\nhttps://releases.rancher.com/os/latest/vmware/rancheros-autoformat.iso\n\n\n\nVMDK\nhttps://releases.rancher.com/os/latest/vmware/rancheros.vmdk\n\n\n\nInitrd\nhttps://releases.rancher.com/os/latest/vmware/initrd\n\n\n\n\nVMware Guest Info\n\n\n\n\nVARIABLE\nTYPE\n\n\n\n\n\nhostname\nhostname\n\n\n\ninterface.<n>.name\nstring\n\n\n\ninterface.<n>.mac\nMAC address (is used to match the ethernet device’s MAC address, not to set it)\n\n\n\ninterface.<n>.dhcp\n{“yes”, “no”}\n\n\n\ninterface.<n>.role\n{“public”, “private”}\n\n\n\ninterface.<n>.ip.<m>.address\nCIDR IP address\n\n\n\ninterface.<n>.route.<l>.gateway\nIP address\n\n\n\ninterface.<n>.route.<l>.destination\nCIDR IP address (not available yet)\n\n\n\ndns.server.<x>\nIP address\n\n\n\ndns.domain.<y>\nDNS search domain\n\n\n\ncloud-init.config.data\nstring\n\n\n\ncloud-init.data.encoding\n{“”, “base64”, “gzip+base64”}\n\n\n\ncloud-init.config.url\nURL\n\n\n\n\n\nNote: “n”, “m”, “l”, “x” and “y” are 0-indexed, incrementing integers. The identifier for an interface (<n>) is used in the generation of the default interface name in the form eth<n>.\n\n","postref":"a8b24eff65bdb6d412f6bed79975e9e8","objectID":"5734dd51ba5203f1296c6f26db9b5384","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/running-rancheros/cloud/vmware-esxi/"},{"anchor":"#1-master-node-security-configuration","title":"1 - Master Node Security Configuration","content":"2.1 - Kubelet2.1.1 - Ensure that the --allow-privileged argument is set to false (Scored)NotesThe --allow-privileged argument is deprecated from Kubernetes v1.11, and the default setting is true with the intention that users should use PodSecurityPolicy settings to allow or prevent privileged containers.Our RKE configuration uses PodSecurityPolicy with a default policy to reject privileged containers.Result: Pass (Not Applicable)2.1.2 - Ensure that the --anonymous-auth argument is set to false (Scored)Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--anonymous-auth=false\").string'Returned Value: --anonymous-auth=falseResult: Pass2.1.3 - Ensure that the --authorization-mode argument is not set to AlwaysAllow (Scored)NotesRKE currently runs the kubelet without the --authorization-mode flag.Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--authorization-mode=Webhook\").string'Returned Value: nullResult: Fail2.1.4 - Ensure that the --client-ca-file argument is set as appropriate (Scored)Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--client-ca-file=.*\").string'Returned Value: --client-ca-file=/etc/kubernetes/ssl/kube-ca.pemResult: Pass2.1.5 - Ensure that the --read-only-port argument is set to 0 (Scored)Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--read-only-port=0\").string'Returned Value: --read-only-port=0Result: Pass2.1.6 - Ensure that the --streaming-connection-idle-timeout argument is not set to 0 (Scored)Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--streaming-connection-idle-timeout=.*\").string'Returned Value: --streaming-connection-idle-timeout=1800sResult: Pass2.1.7 - Ensure that the --protect-kernel-defaults argument is set to true (Scored)Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--protect-kernel-defaults=true\").string'Returned Value: --protect-kernel-defaults=trueResult: Pass2.1.8 - Ensure that the --make-iptables-util-chains argument is set to true (Scored)Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--make-iptables-util-chains=true\").string'Returned Value: --make-iptables-util-chains=trueResult: Pass2.1.9 - Ensure that the --hostname-override argument is not set (Scored)Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--hostname-override=.*\").string'Returned Value: --hostname-override=<ipv4 address>Result: Fail2.1.10 - Ensure that the --event-qps argument is set to 0 (Scored)Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--event-qps=0\").string'Returned Value: --event-qps=0Result: Pass2.1.11 - Ensure that the --tls-cert-file and --tls-private-key-file arguments are set as appropriate (Scored)NotesRKE does not set these options and uses the kubelet’s self generated certificates for TLS communication. These files are located in the default directory (/var/lib/kubelet/pki).Audit (--tls-cert-file)docker inspect kubelet | jq -e '.[0].Args[] | match(\"--tls-cert-file=.*\").string'Returned Value: nullAudit (--tls-private-key-file)docker inspect kubelet | jq -e '.[0].Args[] | match(\"--tls-private-key-file=.*\").string'Returned Value: nullResult: Pass2.1.12 - Ensure that the --cadvisor-port argument is set to 0 (Scored)Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--cadvisor-port=0\").string'Returned Value: --cadvisor-port=0Result: Pass2.1.13 - Ensure that the --rotate-certificates argument is not set to false (Scored)NotesRKE will enable certificate rotation in version 0.1.12.Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--rotate-certificates=true\").string'Returned Value: nullResult: Fail2.1.14 - Ensure that the RotateKubeletServerCertificate argument is set to true (Scored)NotesRKE does not yet support certificate rotation. This feature is due for the 0.1.12 release of RKE.Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--feature-gates=.*(RotateKubeletServerCertificate=true).*\").captures[].string'Returned Value: nullResult: Fail2.1.15 - Ensure that the kubelet only makes use of strong cryptographic ciphers (Not Scored)Audit (Allowed Ciphers)docker inspect kubelet | jq -e '.[0].Args[] | match(\"--tls-cipher-suites=.*(TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256).*\").captures[].string'Returned Value: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--tls-cipher-suites=.*(TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384).*\").captures[].string'Returned Value: TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--tls-cipher-suites=.*(TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305).*\").captures[].string'Returned Value: TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--tls-cipher-suites=.*(TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256).*\").captures[].string'Returned Value:Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--tls-cipher-suites=.*(TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384).*\").captures[].string'Returned Value:Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--tls-cipher-suites=.*(TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305).*\").captures[].string'Returned Value:Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--tls-cipher-suites=.*(TLS_RSA_WITH_AES_128_GCM_SHA256).*\").captures[].string'Returned Value: TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305Auditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--tls-cipher-suites=.*(TLS_RSA_WITH_AES_256_GCM_SHA384).*\").captures[].string'Returned Value: TLS_RSA_WITH_AES_256_GCM_SHA384Audit (Disallowed Ciphers)docker inspect kubelet | jq -e '.[0].Args[] | match(\"--tls-cipher-suites=.*(CBC).*\").captures[].string'Returned Value: nullAuditdocker inspect kubelet | jq -e '.[0].Args[] | match(\"--tls-cipher-suites=.*(RC4).*\").captures[].string'Returned Value: nullResult: Pass2.2 - Configuration Files2.2.1 - Ensure that the permissions for kubelet.conf are set to 644 or more restrictive (Scored)NotesThis is the value of the --kubeconfig option.Auditstat -c %a /etc/kubernetes/ssl/kubecfg-kube-node.yamlReturned Value: 644Result: Pass2.2.2 - Ensure that the kubelet.conf file ownership is set to root:root (Scored)NotesThis is the value of the --kubeconfig option.Auditstat -c %U:%G /etc/kubernetes/ssl/kubecfg-kube-node.yamlReturned Value: root:rootResult: Pass2.2.3 - Ensure that the kubelet service file permissions are set to 644 or more restrictive (Scored)NotesRKE doesn’t require or maintain a configuration file for kubelet. All configuration is passed in as arguments at container run time.Result: Pass (Not Applicable)2.2.4 - Ensure that the kubelet service file ownership is set to root:root (Scored)NotesRKE doesn’t require or maintain a configuration file for kubelet. All configuration is passed in as arguments at container run time.Result: Pass (Not Applicable)2.2.5 - Ensure that the proxy kubeconfig file permissions are set to 644 or more restrictive (Scored)Auditstat -c %a /etc/kubernetes/ssl/kubecfg-kube-proxy.yamlReturned Value: 644Result: Pass2.2.6 - Ensure that the proxy kubeconfig file ownership is set to root:root (Scored)Auditstat -c %U:%G /etc/kubernetes/ssl/kubecfg-kube-proxy.yamlReturned Value: root:rootResult: Pass2.2.7 - Ensure that the certificate authorities file permissions are set to 644 or more restrictive (Scored)Auditstat -c %a /etc/kubernetes/ssl/kube-ca.pemReturned Value: 644Result: Pass2.2.8 - Ensure that the client certificate authorities file ownership is set to root:root (Scored)Auditstat -c %U:%G /etc/kubernetes/ssl/kube-ca.pemReturned Value: root:rootResult: Pass2.2.9 - Ensure that the kubelet configuration file ownership is set to root:root (Scored)NotesRKE doesn’t require or maintain a configuration file for kubelet. All configuration is passed in as arguments at container run time.Result: Pass (Not Applicable)2.2.10 - Ensure that the kubelet configuration file permissions are set to 644 or more restrictive (Scored)NotesRKE doesn’t require or maintain a configuration file for kubelet. All configuration is passed in as arguments at container run time.Result: Pass (Not Applicable)1.1 - API Server1.1.1 - Ensure that the --anonymous-auth argument is set to false (Scored)Auditdocker inspect kube-apiserver | jq -e '.[0].Args[] | match(\"--anonymous-auth=false\").string'Returned Value: --anonymous-auth=falseResult: Pass1.1.2 - Ensure that the --basic-auth-file argument is not set (Scored)Auditdocker inspect kube-apiserver | jq -e '.[0].Args[] | match(\"--basic-auth-file=.*\").string'Returned Value:  nullResult: Pass1.1.3 - Ensure that the --insecure-allow-any-token argument is not set (Scored)Auditdocker inspect kube-apiserver | jq -e '.[0].Args[] | match(\"--insecure-allow-any-token\").string'Returned Value:  nullResult: Pass1.1.4 - Ensure that the --kubelet-https argument is set to true (Scored)Auditdocker inspect kube-apiserver | jq -e '.[0].Args[] | match(\"--kubelet-https=false\").string'Returned Value: nullResult: Pass1.1.5 - Ensure that the --insecure-bind-address argument is not set (Scored)NotesFlag not set or --ins","postref":"f2a4757bf2e2a04ab1149390ccf6dcef","objectID":"388b7f335f1e064f672024be5843e783","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/security/benchmark-2.1/"},{"anchor":"#","title":"OpenStack","content":"As of v0.5.0, RancherOS releases include an Openstack image that can be found on our releases page. The image format is QCOW3 that is backward compatible with QCOW2.\n\nWhen launching an instance using the image, you must enable Advanced Options -> Configuration Drive and in order to use a cloud-config file.\n","postref":"a674c29750b37079ebfcde751bce1bdc","objectID":"2d9623d24d591adb1aeb088338afe3ec","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/running-rancheros/cloud/openstack/"},{"anchor":"#","title":"Azure","content":"RancherOS has been published in Azure Marketplace, you can get it from here.\n\nLaunching RancherOS through the Azure Portal\n\nUsing the new Azure Resource Management portal, click on Marketplace. Search for RancherOS. Click on Create.\n\nFollow the steps to create a virtual machine.\n\nIn the Basics step, provide a name for the VM, use rancher as the user name and select the SSH public key option of authenticating. Add your ssh public key into the appropriate field. Select the Resource group that you want to add the VM to or create a new one. Select the location for your VM.\n\nIn the Size step, select a virtual machine that has at least 1GB of memory.\n\nIn the Settings step, you can use all the default settings to get RancherOS running.\n\nReview your VM and buy it so that you can Create your VM.\n\nAfter the VM has been provisioned, click on the VM to find the public IP address. SSH into your VM using the rancher username.\n\n$ ssh rancher@<public_ip_of_vm> -p 22\n\n\nLaunching RancherOS with custom data\n\nAvailable as of v1.5.2\n\nInstance Metadata Service provides the ability for the VM to have access to its custom data. The binary data must be less than 64 KB and is provided to the VM in base64 encoded form.\nYou can get more details from here\n\nFor example, you can add custom data through CLI:\n\n# list images from marketplace\naz vm image list --location westus --publisher Rancher --offer rancheros --sku os --all --output table\n\nOffer      Publisher    Sku    Urn                            Version\n---------  -----------  -----  -----------------------------  ---------\nrancheros  rancher      os     rancher:rancheros:os:1.5.1     1.5.1\nrancheros  rancher      os152  rancher:rancheros:os152:1.5.2  1.5.2\n...\n\n# accept the terms\naz vm image accept-terms --urn rancher:rancheros:os152:1.5.2\n\n# create the vm\nAZURE_ROS_SSH_PUBLIC_KEY=\"xxxxxx\"\naz vm create --resource-group mygroup \\\n             --name myvm \\\n             --image rancher:rancheros:os152:1.5.2 \\\n             --plan-name os152 \\\n             --plan-product rancheros \\\n             --plan-publisher rancher \\\n             --custom-data ./custom_data.txt \\\n             --admin-username rancher \\\n             --size Standard_A1 \\\n             --ssh-key-value \"$AZURE_ROS_SSH_PUBLIC_KEY\"\n\n\nThe custom_data.txt can be the cloud-config format or a shell script, such as:\n\n#cloud-config\nruncmd:\n- [ touch, /home/rancher/test1 ]\n- echo \"test\" > /home/rancher/test2\n\n\n#!/bin/sh\necho \"aaa\" > /home/rancher/aaa.txt\n\n","postref":"27a2420fadd1837376e30d7a506f967e","objectID":"34374857ceec2855ab406835acad4369","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/running-rancheros/cloud/azure/"},{"anchor":"#options","title":"Options","content":"| Option | Description |\n| — | — |\n| Root disk size | The size must be greater than 10GB. Note: When booting the instance, the value must be kept the same. |\n| Platform |  Select Others Linux |\n| Image Format | Select VHD |Launching RancherOS using Aliyun ConsoleAfter the image is uploaded, we can use the Aliyun Console to start a new instance. Currently, RancherOS on Aliyun only supports SSH key access, so it can only be deployed through the UI.Since the image is private, we need to use the Custom Images.After the instance is successfully started, we can login with the rancher user via SSH.","postref":"c94540b4c2994311ca8028bba2ec057c","objectID":"776324ab0275b7fdb6532200d6c76665","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/running-rancheros/cloud/aliyun/"},{"anchor":"#","title":"Installing to Disk","content":"RancherOS comes with a simple installer that will install RancherOS on a given target disk. To install RancherOS on a new disk, you can use the ros install command. Before installing, you’ll need to have already booted RancherOS from ISO. Please be sure to pick the rancheros.iso from our release page.\n\nUsing ros install to Install RancherOS\n\nThe ros install command orchestrates the installation from the rancher/os container. You will need to have already created a cloud-config file and found the target disk.\n\nCloud-Config\n\nThe easiest way to log in is to pass a cloud-config.yml file containing your public SSH keys. To learn more about what’s supported in our cloud-config, please read our documentation.\n\nThe ros install command will process your cloud-config.yml file specified with the -c flag. This file will also be placed onto the disk and installed to /var/lib/rancher/conf/. It will be evaluated on every boot.\n\nCreate a cloud-config file with a SSH key, this allows you to SSH into the box as the rancher user. The yml file would look like this:\n#cloud-config\nssh_authorized_keys:\n  - ssh-rsa AAA...\n\n\nYou can generate a new SSH key for cloud-config.yml file by following this article.\n\nCopy the public SSH key into RancherOS before installing to disk.\n\nNow that our cloud-config.yml contains our public SSH key, we can move on to installing RancherOS to disk!\n\n$ sudo ros install -c cloud-config.yml -d /dev/sda\nINFO[0000] No install type specified...defaulting to generic\nInstalling from rancher/os:v0.5.0\nContinue [y/N]:\n\n\nFor the cloud-config.yml file, you can also specify a remote URL, but you need to make sure you can get it:\n\n$ sudo ros install -c https://link/to/cloud-config.yml\n\n\nYou will be prompted to see if you want to continue. Type y.\n\nUnable to find image 'rancher/os:v0.5.0' locally\nv0.5.0: Pulling from rancher/os\n...\n...\n...\nStatus: Downloaded newer image for rancher/os:v0.5.0\n+ DEVICE=/dev/sda\n...\n...\n...\n+ umount /mnt/new_img\nContinue with reboot [y/N]:\n\n\nAfter installing RancherOS to disk, you will no longer be automatically logged in as the rancher user. You’ll need to have added in SSH keys within your cloud-config file.\n\nInstalling a Different Version\n\nBy default, ros install uses the same installer image version as the ISO it is run from. The -i option specifies the particular image to install from. To keep the ISO as small as possible, the installer image is downloaded from DockerHub and used in System Docker. For example for RancherOS v0.5.0 the default installer image would be rancher/os:v0.5.0.\n\nYou can use ros os list command to find the list of available RancherOS images/versions.\n\n$ sudo ros os list\nrancher/os:v0.4.0 remote\nrancher/os:v0.4.1 remote\nrancher/os:v0.4.2 remote\nrancher/os:v0.4.3 remote\nrancher/os:v0.4.4 remote\nrancher/os:v0.4.5 remote\nrancher/os:v0.5.0 remote\n\n\nAlternatively, you can set the installer image to any image in System Docker to install RancherOS. This is particularly useful for machines that will not have direct access to the internet.\n\nCaching Images\n\nAvailable as of v1.5.3\n\nSome configurations included in cloud-config require images to be downloaded from Docker to start. After installation, these images are downloaded automatically by RancherOS when booting. An example of these configurations are:\n\n\nrancher.services_include\nrancher.console\nrancher.docker\n\n\nIf you want to download and save these images to disk during installation, they will be cached and not need to be downloaded again upon each boot. You can cache these images by adding -s when using ros install:\n\n$ ros install -d <disk> -c <cloud-config.yaml> -s\n\n\nSSH into RancherOS\n\nAfter installing RancherOS, you can ssh into RancherOS using your private key and the rancher user.\n\n$ ssh -i /path/to/private/key rancher@<ip-address>\n\n\nInstalling with no Internet Access\n\nIf you’d like to install RancherOS onto a machine that has no internet access, it is assumed you either have your own private registry or other means of distributing docker images to System Docker of the machine. If you need help with creating a private registry, please refer to the Docker documentation for private registries.\n\nIn the installation command (i.e. sudo ros install), there is an option to pass in a specific image to install. As long as this image is available in System Docker, then RancherOS will use that image to install RancherOS.\n\n$ sudo ros install -c cloud-config.yml -d /dev/sda -i <Image_Name_in_System_Docker>\nINFO[0000] No install type specified...defaulting to generic\nInstalling from <Image_Name_in_System_Docker>\nContinue [y/N]:\n\n","postref":"4a1d60b7e9d1205c7625bbc25de910a6","objectID":"fa7917ae63dd3a70ba4450db8306532c","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/running-rancheros/server/install-to-disk/"},{"anchor":"#","title":"iPXE","content":"#!ipxe\n# Boot a persistent RancherOS to RAM\n\n# Location of Kernel/Initrd images\nset base-url http://releases.rancher.com/os/latest\n\nkernel ${base-url}/vmlinuz rancher.state.dev=LABEL=RANCHER_STATE rancher.state.autoformat=[/dev/sda] rancher.state.wait rancher.cloud_init.datasources=[url:http://example.com/cloud-config]\ninitrd ${base-url}/initrd\nboot\n\n\nIf you want to autoformat the disk when booting by iPXE, you should add the rancher.state.autoformat part to kernel cmdline. However, this does not install the bootloader to disk, so you cannot upgrade RancherOS.\n\nIf you don’t add rancher.state.autoformat, RancherOS will run completely in memory, you can execute ros install to install to disk.\n\nHiding sensitive kernel commandline parameters\n\nFrom RancherOS v0.9.0, secrets can be put on the kernel parameters line afer a -- double dash, and they will be not be shown in any /proc/cmdline. These parameters\nwill be passed to the RancherOS init process and stored in the root accessible /var/lib/rancher/conf/cloud-init.d/init.yml file, and are available to the root user from the ros config commands.\n\nFor example, the kernel line above could be written as:\n\nkernel ${base-url}/vmlinuz rancher.state.dev=LABEL=RANCHER_STATE rancher.state.autoformat=[/dev/sda] -- rancher.cloud_init.datasources=[url:http://example.com/cloud-config]\n\n\nThe hidden part of the command line can be accessed with either sudo ros config get rancher.environment.EXTRA_CMDLINE, or by using a service file’s environment array.\n\nAn example service.yml file:\n\ntest:\n  image: alpine\n  command: echo \"tell me a secret ${EXTRA_CMDLINE}\"\n  labels:\n    io.rancher.os.scope: system\n  environment:\n  - EXTRA_CMDLINE\n\n\nWhen this service is run, the EXTRA_CMDLINE will be set.\n\ncloud-init Datasources\n\nValid cloud-init datasources for RancherOS.\n\n\n\n\ntype\ndefault\n\n\n\n\n\nec2\nDefault metadata address\n\n\n\ndigitalocean\nDefault metadata address\n\n\n\npacket\nDefault metadata address\n\n\n\ncloudstack\nDefault metadata address\n\n\n\naliyun\nDefault metadata address\n\n\n\ngce\nDefault metadata address\n\n\n\nfile\nPath\n\n\n\ncmdline\nKernel command line: cloud-config-url=http://link/user_data\n\n\n\nconfigdrive\n/media/config-2\n\n\n\nurl\nURL address\n\n\n\nvmware\nSet guestinfo cloud-init or interface data as per VMware ESXi\n\n\n\n*\nThis will add [“configdrive”, “vmware”, “ec2”, “digitalocean”, “packet”, “gce”] into the list of datasources to try\n\n\n\n\nThe vmware datasource was added as of v1.1.\n\nCloud-Config\n\nWhen booting via iPXE, RancherOS can be configured using a cloud-config file.\n","postref":"0ffcc26a0a3ef4f9a457ba4a2c03ef49","objectID":"c770f2cf7b69cb21f8b5109409058ea5","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/running-rancheros/server/pxe/"},{"anchor":"#","title":"Raspberry Pi","content":"As of v0.5.0, RancherOS releases include a Raspberry Pi image that can be found on our releases page. The official Raspberry Pi documentation contains instructions on how to install operating system images.\n\nWhen installing, there is no ability to pass in a cloud-config. You will need to boot up, change the configuration and then reboot to apply those changes.\n\nCurrently, only Raspberry Pi 3 is tested and known to work.\n\n\nNote: It is not necessary to run ros install after installing RancherOS to an SD card.\n\n\nUsing the entire SD Card\n\nRancherOS does not currently expand the root partition to fill the remainder of the SD card automatically. Instead, the following workaround can be used to store Docker containers on a larger partition that fills the remainder.\n\n\nsudo fdisk /dev/mmcblk0\nCreate a new partition\nPress [Enter] four (4x) times to accept the defaults\nThen write the table and exit\nsudo reboot to reboot and reload the new partition table\nsudo mkdir /mnt/docker to create the directory to be used as the new Docker root\nsudo ros config set rancher.docker.extra_args [-g,/mnt/docker] to configure Docker to use the new root\nsudo mkfs.ext4 /dev/mmcblk0p3 to format the disk\nsudo ros config set mounts \"[['/dev/mmcblk0p3','/mnt/docker','ext4','']]\" to preserve this mount after reboots\nsudo mount /dev/mmcblk0p3 /mnt/docker to mount the Docker root\nsudo system-docker restart docker to restart Docker using the new root\nIf this is not a new installation, you’ll have to copy over your existing Docker root (/var/lib/docker) to the new root (/mnt/docker).\nsudo cp -R /var/lib/docker/* /mnt/docker to recursively copy all files\nsudo system-docker restart docker to restart Docker using the new root\n\n\nUsing Wi-Fi\n\nAvailable as of v1.5.2\n\nHere are steps about how to enable Wi-Fi on a Raspberry Pi:\n\nmodprobe brcmfmac\nwpa_passphrase <ssid> <psk> > /etc/wpa_supplicant.conf\nwpa_supplicant -iwlan0 -B -c /etc/wpa_supplicant.conf\n# wait a few seconds, then\ndhcpcd -MA4 wlan0\n\n\nYou can also use cloud-config to enable Wi-Fi:\n\n#cloud-config\nrancher:\n  network:\n    interfaces:\n      wlan0:\n        wifi_network: network1\n    wifi_networks:\n      network1:\n        ssid: \"Your wifi ssid\"\n        psk: \"Your wifi password\"\n        scan_ssid: 1\n\n\nRaspberry Pi will automatically drop Wi-Fi connection after a while, this is due to power management. To fix this problem, you can try this:\n\niwconfig wlan0 power off\n\n","postref":"b05372d695ee0bdce3b14d864ca86422","objectID":"cdde48582763e72d3a86ca894cc8daf1","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/running-rancheros/server/raspberry-pi/"},{"anchor":"#","title":"Configuration","content":"There are two ways that RancherOS can be configured.\n\n\nA cloud-config file can be used to provide configuration when first booting RancherOS.\nManually changing configuration with the ros config command.\n\n\nTypically, when you first boot the server, you pass in a cloud-config file to configure the initialization of the server. After the first boot, if you have any changes for the configuration, it’s recommended that you use ros config to set the necessary configuration properties. Any changes will be saved on disk and a reboot will be required for changes to be applied.\n\nCloud-Config\n\nCloud-config is a declarative configuration file format supported by many Linux distributions and is the primary configuration mechanism for RancherOS.\n\nA Linux OS supporting cloud-config will invoke a cloud-init process during startup to parse the cloud-config file and configure the operating system. RancherOS runs its own cloud-init process in a system container. The cloud-init process will attempt to retrieve a cloud-config file from a variety of data sources. Once cloud-init obtains a cloud-config file, it configures the Linux OS according to the content of the cloud-config file.\n\nWhen you create a RancherOS instance on AWS, for example, you can optionally provide cloud-config passed in the user-data field. Inside the RancherOS instance, cloud-init process will retrieve the cloud-config content through its AWS cloud-config data source, which simply extracts the content of user-data received by the VM instance. If the file starts with “#cloud-config”, cloud-init will interpret that file as a cloud-config file. If the file starts with #!<interpreter> (e.g., #!/bin/sh), cloud-init will simply execute that file. You can place any configuration commands in the file as scripts.\n\nA cloud-config file uses the YAML format. YAML is easy to understand and easy to parse. For more information on YAML, please read more at the YAML site. The most important formatting principle is indentation or whitespace. This indentation indicates relationships of the items to one another. If something is indented more than the previous line, it is a sub-item of the top item that is less indented.\n\nExample: Notice how both are indented underneath ssh_authorized_keys.\n#cloud-config\nssh_authorized_keys:\n  - ssh-rsa AAA...ZZZ example1@rancher\n  - ssh-rsa BBB...ZZZ example2@rancher\nIn our example above, we have our #cloud-config line to indicate it’s a cloud-config file. We have 1 top-level property, ssh_authorized_keys. Its value is a list of public keys that are represented as a dashed list under ssh_authorized_keys:.\n\nManually Changing Configuration\n\nTo update RancherOS configuration after booting, the ros config set <key> <value> command can be used.\nFor more complicated settings, like the sysctl settings, you can also create a small YAML file and then run sudo ros config merge -i <your yaml file>.\n\nGetting Values\n\nYou can easily get any value that’s been set in the /var/lib/rancher/conf/cloud-config.yml file. Let’s see how easy it is to get the DNS configuration of the system.\n\n$ sudo ros config get rancher.network.dns.nameservers\n- 8.8.8.8\n- 8.8.4.4\n\n\nSetting Values\n\nYou can set values in the /var/lib/rancher/conf/cloud-config.yml file.\n\nSetting a simple value in the /var/lib/rancher/conf/cloud-config.yml\n\n$ sudo ros config set rancher.docker.tls true\n\n\nSetting a list in the /var/lib/rancher/conf/cloud-config.yml\n\n$ sudo ros config set rancher.network.dns.nameservers \"['8.8.8.8','8.8.4.4']\"\n\n\nExporting the Current Configuration\n\nTo output and review the current configuration state you can use the ros config export command.\n\n$ sudo ros config export\nrancher:\n  docker:\n    tls: true\n  network:\n    dns:\n      nameservers:\n      - 8.8.8.8\n      - 8.8.4.4\n\n\nValidating a Configuration File\n\nTo validate a configuration file you can use the ros config validate command.\n\n$ sudo ros config validate -i cloud-config.yml\n\n","postref":"f02e7306e1100c5dc7c788835e7c7759","objectID":"36f474642f6f2ddd81d0c1ad1962f942","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/configuration/"},{"anchor":"#","title":"Date and time zone","content":"The default console keeps time in the Coordinated Universal Time (UTC) zone and synchronizes clocks with the Network Time Protocol (NTP). The Network Time Protocol daemon (ntpd) is an operating system program that maintains the system time in synchronization with time servers using the NTP.\n\nRancherOS can run ntpd in the System Docker container. You can update its configurations by updating /etc/ntp.conf. For an example of how to update a file such as /etc/ntp.conf within a container, refer to this page.\n\nThe default console cannot support changing the time zone because including tzdata (time zone data) will increase the ISO size. However, you can change the time zone in the container by passing a flag to specify the time zone when you run the container:\n\n$ docker run -e TZ=Europe/Amsterdam debian:jessie date\nTue Aug 20 09:28:19 CEST 2019\n\n\nYou may need to install the tzdata in some images:\n\n$ docker run -e TZ=Asia/Shanghai -e DEBIAN_FRONTEND=noninteractive -it --rm ubuntu /bin/bash -c \"apt-get update && apt-get install -yq tzdata && date”\nThu Aug 29 08:13:02 CST 2019\n\n","postref":"79b2882d457d3e7815ac2e720adbaccf","objectID":"2c89791a8094fd7d75e441104e6a6f78","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/configuration/date-and-timezone/"},{"anchor":"#","title":"Images prefix","content":"Available as of v1.3\n\nWhen you have built your own docker registries, and have cached the rancher/os and other os-services images,\nsomething like a normal docker pull rancher/os can be cached as docker pull dockerhub.mycompanyname.com/docker.io/rancher/os.\n\nHowever, you need a way to inject a prefix into RancherOS for installation or service pulls.\nRancherOS supports a global prefix you can add to force ROS to always use your mirror.\n\nYou can config a global image prefix:\n\nros config set rancher.environment.REGISTRY_DOMAIN xxxx.yyy\n\n\n\nThen you check the os list:\n\n$ ros os list\nxxxx.yyy/rancher/os:v1.3.0 remote latest running\nxxxx.yyy/rancher/os:v1.2.0 remote available\n...\n...\n\n\nAlso you can check consoles:\n\n$ ros console switch ubuntu\nSwitching consoles will\n1. destroy the current console container\n2. log you out\n3. restart Docker\nContinue [y/N]: y\nPulling console (xxxx.yyy/rancher/os-ubuntuconsole:v1.3.0)...\n...\n\n\nIf you want to reset this setting:\n\nros config set rancher.environment.REGISTRY_DOMAIN docker.io\n\n","postref":"6528d43711f938a8862042af597784e7","objectID":"44c2fff3153c95a427611aeafae579c8","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/configuration/images-prefix/"},{"anchor":"#","title":"SSH Settings","content":"RancherOS supports adding SSH keys through the cloud-config file. Within the cloud-config file, you simply add the ssh keys within the ssh_authorized_keys key.\n#cloud-config\nssh_authorized_keys:\n  - ssh-rsa AAA...ZZZ example1@rancher\n  - ssh-rsa BBB...ZZZ example2@rancher\nWhen we pass the cloud-config file during the ros install command, it will allow these ssh keys to be associated with the rancher user. You can ssh into RancherOS using the key.\n\n$ ssh -i /path/to/private/key rancher@<ip-address>\n\n\nPlease note that OpenSSH 7.0 and greater similarly disable the ssh-dss (DSA) public key algorithm. It too is weak and we recommend against its use.\n\nSSHD Port and IP\n\nAvailable as of v1.3\n\nRancherOS supports changing the sshd port and IP, you can use these in the cloud-config file:\n\nrancher:\n  ssh:\n    port: 10022\n    listen_address: 172.22.100.100\n\n\nThese settings are only designed for default console.\nBecause if you change sshd-config, restart the host will restore the default, the new configuration will not take effect.\n\nFor other consoles, all files are persistent, you can modify sshd-config by yourself.\n","postref":"dc13599f15e5df8955d64c52f38d15af","objectID":"e4d674a7af917ef7693c320e5847f788","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/configuration/ssh-keys/"},{"anchor":"#","title":"Writing Files","content":"You can automate writing files to disk using the write_files cloud-config directive.\n#cloud-config\nwrite_files:\n  - path: /etc/rc.local\n    permissions: \"0755\"\n    owner: root\n    content: |\n      #!/bin/bash\n      echo \"I'm doing things on start\"\nWriting Files in Specific System Services\n\nBy default, the write_files directive will create files in the console container. To write files in other system services, the container key can be used. For example, the container key could be used to write to /etc/ntp.conf in the NTP system service.\n#cloud-config\nwrite_files:\n  - container: ntp\n    path: /etc/ntp.conf\n    permissions: \"0644\"\n    owner: root\n    content: |\n      server 0.pool.ntp.org iburst\n      server 1.pool.ntp.org iburst\n      server 2.pool.ntp.org iburst\n      server 3.pool.ntp.org iburst\n\n      # Allow only time queries, at a limited rate, sending KoD when in excess.\n      # Allow all local queries (IPv4, IPv6)\n      restrict default nomodify nopeer noquery limited kod\n      restrict 127.0.0.1\n      restrict [::1]\n\nNote: Currently, writing files to a specific system service is only supported for RancherOS’s built-in services. You are unable to write files to any custom system services.\n\n","postref":"93eb85ae4461c2af95fc8b4af1402699","objectID":"823c1bb86e4d583d9c357a17007f386b","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/configuration/write-files/"},{"anchor":"#","title":"Running Commands","content":"You can automate running commands on boot using the runcmd cloud-config directive. Commands can be specified as either a list or a string. In the latter case, the command is executed with sh.\n#cloud-config\nruncmd:\n- [ touch, /home/rancher/test1 ]\n- echo \"test\" > /home/rancher/test2\nCommands specified using runcmd will be executed within the context of the console container.\n\nRunning Docker commands\n\nWhen using runcmd, RancherOS will wait for all commands to complete before starting Docker. As a result, any docker run command should not be placed under runcmd. Instead, the /etc/rc.local script can be used. RancherOS will not wait for commands in this script to complete, so you can use the wait-for-docker command to ensure that the Docker daemon is running before performing any docker run commands.\n#cloud-config\nrancher:\nwrite_files:\n  - path: /etc/rc.local\n    permissions: \"0755\"\n    owner: root\n    content: |\n      #!/bin/bash\n      wait-for-docker\n      docker run -d nginx\nRunning Docker commands in this manner is useful when pieces of the docker run command are dynamically generated. For services whose configuration is static, adding a system service is recommended.\n","postref":"08d2f6537fb0f3227a78762d1172b1d1","objectID":"97c4e44f1caa7004412cb5b356965ed5","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/configuration/running-commands/"},{"anchor":"#","title":"Setting the Hostname","content":"You can set the hostname of the host using cloud-config. The example below shows how to configure it.\n#cloud-config\nhostname: myhost","postref":"a8501d3bfba34ff5815d7b893d2da654","objectID":"88a44590626c85f478120ec698f26500","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/configuration/hostname/"},{"anchor":"#","title":"Switching Consoles","content":"When booting from the ISO, RancherOS starts with the default console, which is based on busybox.\n\nYou can select which console you want RancherOS to start with using the cloud-config.\n\nEnabling Consoles using Cloud-Config\n\nWhen launching RancherOS with a cloud-config file, you can select which console you want to use.\n\nCurrently, the list of available consoles are:\n\n\ndefault\nalpine\ncentos\ndebian\nfedora\nubuntu\n\n\nHere is an example cloud-config file that can be used to enable the debian console.\n#cloud-config\nrancher:\n  console: debian\nListing Available Consoles\n\nYou can easily list the available consoles in RancherOS and what their status is with sudo ros console list.\n\n$ sudo ros console list\ndisabled alpine\ndisabled centos\ndisabled debian\ncurrent  default\ndisabled fedora\ndisabled ubuntu\n\n\nChanging Consoles after RancherOS has started\n\nYou can view which console is being used by RancherOS by checking which console container is running in System Docker. If you wanted to switch consoles, you just need to run a simple command and select your new console.\n\nFor our example, we’ll switch to the Ubuntu console.\n\n$ sudo ros console switch ubuntu\nSwitching consoles will\n1. destroy the current console container\n2. log you out\n3. restart Docker\nContinue [y/N]:y\nPulling console (rancher/os-ubuntuconsole:v0.5.0-3)...\nv0.5.0-3: Pulling from rancher/os-ubuntuconsole\n6d3a6d998241: Pull complete\n606b08bdd0f3: Pull complete\n1d99b95ffc1c: Pull complete\na3ed95caeb02: Pull complete\n3fc2f42db623: Pull complete\n2fb84911e8d2: Pull complete\nfff5d987b31c: Pull complete\ne7849ae8f782: Pull complete\nde375d40ae05: Pull complete\n8939c16614d1: Pull complete\nDigest: sha256:37224c3964801d633ea8b9629137bc9d4a8db9d37f47901111b119d3e597d15b\nStatus: Downloaded newer image for rancher/os-ubuntuconsole:v0.5.0-3\nswitch-console_1 | time=\"2016-07-02T01:47:14Z\" level=info msg=\"Project [os]: Starting project \"\nswitch-console_1 | time=\"2016-07-02T01:47:14Z\" level=info msg=\"[0/18] [console]: Starting \"\nswitch-console_1 | time=\"2016-07-02T01:47:14Z\" level=info msg=\"Recreating console\"\nConnection to 127.0.0.1 closed by remote host.\n\n\n\n\nAfter logging back, you’ll be in the Ubuntu console.\n\n$ sudo system-docker ps\nCONTAINER ID        IMAGE                                 COMMAND                  CREATED              STATUS              PORTS               NAMES\n6bf33541b2dc        rancher/os-ubuntuconsole:v0.5.0-rc3   \"/usr/sbin/entry.sh /\"   About a minute ago   Up About a minute\n\n\n\n\n\nNote: When switching between consoles, the currently running console container is destroyed, Docker is restarted and you will be logged out.\n\n\nConsole persistence\n\nAll consoles except the default (busybox) console are persistent. Persistent console means that the console container will remain the same and preserves changes made to its filesystem across reboots. If a container is deleted/rebuilt, state in the console will be lost except what is in the persisted directories.\n\n/home\n/opt\n/var/lib/docker\n/var/lib/rancher\n\n\n\n\n\nNote: When using a persistent console and in the current version’s console, rolling back is not supported. For example, rolling back to v0.4.5 when using a v0.5.0 persistent console is not supported.\n\n\nEnabling Consoles\n\nYou can also enable a console that will be changed at the next reboot.\n\nFor our example, we’ll switch to the Debian console.\n\n# Check the console running in System Docker\n$ sudo system-docker ps\nCONTAINER ID        IMAGE                              COMMAND                  CREATED             STATUS              PORTS               NAMES\n95d548689e82        rancher/os-docker:v0.5.0    \"/usr/sbin/entry.sh /\"   About an hour ago   Up About an hour                        docker\n# Enable the Debian console\n$ sudo ros console enable debian\nPulling console (rancher/os-debianconsole:v0.5.0-3)...\nv0.5.0-3: Pulling from rancher/os-debianconsole\n7268d8f794c4: Pull complete\na3ed95caeb02: Pull complete\n21cb8a645d75: Pull complete\n5ee1d288a088: Pull complete\nc09f41c2bd29: Pull complete\n02b48ce40553: Pull complete\n38a4150e7e9c: Pull complete\nDigest: sha256:5dbca5ba6c3b7ba6cd6ac75a1d054145db4b4ea140db732bfcbd06f17059c5d0\nStatus: Downloaded newer image for rancher/os-debianconsole:v0.5.0-3\n\n\n\n\nAt the next reboot, RancherOS will be using the Debian console.\n","postref":"e35b51a8ab94553386c12d70cea3be15","objectID":"521f3b0a421708ce51e3f7acf38a304f","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/configuration/switching-consoles/"},{"anchor":"#","title":"Configuring Docker or System Docker","content":"In RancherOS, you can configure System Docker and Docker daemons by using cloud-config.\n\nConfiguring Docker\n\nIn your cloud-config, Docker configuration is located under the rancher.docker key.\n#cloud-config\nrancher:\n  docker:\n    tls: true\n    tls_args:\n      - \"--tlsverify\"\n      - \"--tlscacert=/etc/docker/tls/ca.pem\"\n      - \"--tlscert=/etc/docker/tls/server-cert.pem\"\n      - \"--tlskey=/etc/docker/tls/server-key.pem\"\n      - \"-H=0.0.0.0:2376\"\n    storage_driver: overlay\nYou can also customize Docker after it’s been started using ros config.\n\n$ sudo ros config set rancher.docker.storage_driver overlay\n\n\nUser Docker settings\n\nMany of the standard Docker daemon arguments can be placed under the rancher.docker key. The command needed to start the Docker daemon will be generated based on these arguments. The following arguments are currently supported.\n\n\n\n\nKey\nValue\n\n\n\n\n\nbridge\nString\n\n\n\nbip\nString\n\n\n\nconfig_file\nString\n\n\n\ncontainerd\nString\n\n\n\ndebug\nBoolean\n\n\n\nexec_root\nString\n\n\n\ngroup\nString\n\n\n\ngraph\nString\n\n\n\nhost\nList\n\n\n\ninsecure_registry\nList\n\n\n\nlive_restore\nBoolean\n\n\n\nlog_driver\nString\n\n\n\nlog_opts\nMap where keys and values are strings\n\n\n\npid_file\nString\n\n\n\nregistry_mirror\nString\n\n\n\nrestart\nBoolean\n\n\n\nselinux_enabled\nBoolean\n\n\n\nstorage_driver\nString\n\n\n\nuserland_proxy\nBoolean\n\n\n\n\nIn addition to the standard daemon arguments, there are a few fields specific to RancherOS.\n\n\n\n\nKey\nValue\nDefault\nDescription\n\n\n\n\n\nextra_args\nList of Strings\n[]\nArbitrary daemon arguments, appended to the generated command\n\n\n\nenvironment\nList of Strings\n[]\n\n\n\n\ntls\nBoolean\nfalse\nWhen setting up TLS, this key needs to be set to true.\n\n\n\ntls_args\nList of Strings (used only if tls: true)\n[]\n\n\n\n\nserver_key\nString (used only if tls: true)\n\"\"\nPEM encoded server TLS key.\n\n\n\nserver_cert\nString (used only if tls: true)\n\"\"\nPEM encoded server TLS certificate.\n\n\n\nca_key\nString (used only if tls: true)\n\"\"\nPEM encoded CA TLS key.\n\n\n\nstorage_context\nString\nconsole\nSpecifies the name of the system container in whose context to run the Docker daemon process.\n\n\n\n\nExample using extra_args for setting MTU\n\nThe following example can be used to set MTU on the Docker daemon:\n#cloud-config\nrancher:\n  docker:\n    extra_args: [--mtu, 1460]\nExample using bip for docker0 bridge\n\nAvailable as of v1.4.x\n\nThe docker0 bridge can be configured with docker args, it will take effect after reboot.\n\n$ ros config set rancher.docker.bip 192.168.0.0/16\n\n\nConfiguring System Docker\n\nIn your cloud-config, System Docker configuration is located under the rancher.system_docker key.\n#cloud-config\nrancher:\n  system_docker:\n    storage_driver: overlay\nSystem Docker settings\n\nAll daemon arguments shown in the first table are also available to System Docker. The following are also supported.\n\n\n\n\nKey\nValue\nDefault\nDescription\n\n\n\n\n\nextra_args\nList of Strings\n[]\nArbitrary daemon arguments, appended to the generated command\n\n\n\nenvironment\nList of Strings (optional)\n[]\n\n\n\n\n\nAvailable as of v1.4.x\n\nThe docker-sys bridge can be configured with system-docker args, it will take effect after reboot.\n\n$ ros config set rancher.system_docker.bip 172.19.0.0/16\n\n\nAvailable as of v1.4.x\n\nThe default path of system-docker logs is /var/log/system-docker.log. If you want to write the system-docker logs to a separate partition,\ne.g. RANCHER_OEM partition, you can try rancher.defaults.system_docker_logs:\n\n#cloud-config\nrancher:\n  defaults:\n    system_docker_logs: /usr/share/ros/oem/system-docker.log\n\n\nUsing a pull through registry mirror\n\nThere are 3 Docker engines that can be configured to use the pull-through Docker Hub registry mirror cache:\n\n#cloud-config\nrancher:\n  bootstrap_docker:\n    registry_mirror: \"http://10.10.10.23:5555\"\n  docker:\n    registry_mirror: \"http://10.10.10.23:5555\"\n  system_docker:\n    registry_mirror: \"http://10.10.10.23:5555\"\n\n\nbootstrap_docker is used to prepare and initial network and pull any cloud-config options that can be used to configure the final network configuration and System-docker - its very unlikely to pull any images.\n\nA successful pull through mirror cache request by System-docker looks like:\n\n[root@rancher-dev rancher]# system-docker pull alpine\nUsing default tag: latest\nDEBU[0201] Calling GET /v1.23/info\n> WARN[0201] Could not get operating system name: Error opening /usr/lib/os-release: open /usr/lib/os-release: no such file or directory\nWARN[0201] Could not get operating system name: Error opening /usr/lib/os-release: open /usr/lib/os-release: no such file or directory\nDEBU[0201] Calling POST /v1.23/images/create?fromImage=alpine%3Alatest\nDEBU[0201] hostDir: /etc/docker/certs.d/10.10.10.23:5555\nDEBU[0201] Trying to pull alpine from http://10.10.10.23:5555/ v2\nDEBU[0204] Pulling ref from V2 registry: alpine:latest\nDEBU[0204] pulling blob \"sha256:2aecc7e1714b6fad58d13aedb0639011b37b86f743ba7b6a52d82bd03014b78e\" latest: Pulling from library/alpine\nDEBU[0204] Downloaded 2aecc7e1714b to tempfile /var/lib/system-docker/tmp/GetImageBlob281102233 2aecc7e1714b: Extracting  1.99 MB/1.99 MB\nDEBU[0204] Untar time: 0.161064213s\nDEBU[0204] Applied tar sha256:3fb66f713c9fa9debcdaa58bb9858bd04c17350d9614b7a250ec0ee527319e59 to 841c99a5995007d7a66b922be9bafdd38f8090af17295b4a44436ef433a2aecc7e1714b: Pull complete\nDigest: sha256:0b94d1d1b5eb130dd0253374552445b39470653fb1a1ec2d81490948876e462c\nStatus: Downloaded newer image for alpine:latest\n\n\nUsing Multiple User Docker Daemons\n\nAvailable as of v1.5.0\n\nWhen RancherOS is booted, you start with a User Docker service that is running in System Docker. With v1.5.0, RancherOS has the ability to create additional User Docker services that can run at the same time.\n\nTerminology\n\nThroughout the rest of this documentation, we may simplify to use these terms when describing Docker.\n\n\n\n\nTerminology\nDefinition\n\n\n\n\n\nDinD\nDocker in docker\n\n\n\nUser Docker\nThe user-docker on RancherOS\n\n\n\nOther User Docker\nThe other user-docker daemons you create, these user-docker daemons are automatically assumed to be Docker in Docker.\n\n\n\n\nPre-Requisites\n\nUser Docker must be set as Docker 17.12.1 or earlier. If it’s a later Docker version, it will produce errors when creating a user defined network in System Docker.\n\n$ ros engine switch docker-17.12.1-ce\n\n\nYou will need to create a user-defined network, which will be used when creating the Other User Docker.\n\n$ system-docker network create --subnet=172.20.0.0/16 dind\n\n\nCreate the Other User Docker\n\nIn order to create another User Docker, you will use ros engine create. Currently, RancherOS only supports Docker 17.12.1 and 18.03.1 for the Other User Docker image.\n\n$ ros engine create otheruserdockername --network=dind --fixed-ip=172.20.0.2\n\n\nAfter the Other User Docker service is created, users can query this service like other services.\n\n$ ros service list\n...\n...\ndisabled volume-efs\ndisabled volume-nfs\nenabled  otheruserdockername\n\n\nYou can use ros service up to start the Other User Docker service.\n\n$ ros service up otheruserdockername\n\n\nAfter the Other User Docker service is running, you can interact with it just like you can use the built-in User Docker. You would need to append -<SERVICE_NAME> to docker.\n\n$ docker-otheruserdockername ps -a\n\n\nSSH into the Other User Docker container\n\nWhen creating the Other User Docker, you can set an external SSH port so you can SSH into the Other User Docker container in System Docker. By using --ssh-port and adding ssh keys with --authorized-keys, you can set up this optional SSH port.\n\n$ ros engine create  --help\n...\n...\nOPTIONS:\n    --ssh-port value\n    --authorized-keys value\n\n\nWhen using --authorized-keys, you will need to put the key file in one of the following directories:\n\n/var/lib/rancher/\n/opt/\n/home/\n\n\nRancherOS will generate a random password for each Other User Docker container, which can be viewed in the container logs. If you do not set any SSH keys, the password can be used.\n\n$ system-docker logs otheruserdockername\n\n======================================\nchpasswd: password for 'root' changed\npassword: xCrw6fEG\n======================================\n\n\nIn System Docker, you can SSH into any Other User Docker Container using ssh.\n\n$ system-docker ps\nCONTAINER ID        IMAGE                              COMMAND                  CREATED             STATUS              PORTS                             NAMES\n2ca07a25799b        rancher/os-dind:17.12.1          \"docker-entrypoint...\"   5 seconds ago       Up 3 seconds        2375/tcp, 0.0.0.0:34791->22/tcp   otheruserdockername\n\n$ ssh -p 34791 root@<HOST_EXTERNAL_IP>\n\n$ ssh root@<OTHERUSERDOCKER_CONTAINER_IP>\n\n\n\nRemoving any Other User Docker Service\n\nWe recommend using ros engine rm to remove any Other User Docker service.\n\n$ ros engine rm otheruserdockername\n\n","postref":"93f80aefc0139661e4be57742b5ef99b","objectID":"bb1139f42b84749a6b9896babfc287f9","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/configuration/docker/"},{"anchor":"#","title":"Setting up Docker TLS","content":"ros tls generate is used to generate both the client and server TLS certificates for Docker.\n\nRemember, all ros commands need to be used with sudo or as a root user.\n\nEnd to end example\n\nEnable TLS for Docker and Generate Server Certificate\n\nTo have docker secured by TLS you need to set rancher.docker.tls to true, and generate a set of server and client keys and certificates:\n\n$ sudo ros config set rancher.docker.tls true\n$ sudo ros tls gen --server -H localhost -H <hostname1> -H <hostname2> ... -H <hostnameN>\n$ sudo system-docker restart docker\n\n\nHere, <hostname*>s are the hostnames that you will be able to use as your docker host names. A <hostname*> can be a wildcard pattern, e.g. “*.*.*.*.*”. It is recommended to have localhost as one of the hostnames, so that you can test docker TLS connectivity locally.\n\nWhen you’ve done that, all the necessary server certificate and key files have been saved to /etc/docker/tls directory, and the docker service has been started with --tlsverify option.\n\nGenerate Client Certificates\n\nYou also need client cert and key to access Docker via a TCP socket now:\n\n$ sudo ros tls gen\n  INFO[0000] Out directory (-d, --dir) not specified, using default: /home/rancher/.docker\n\n\nAll the docker client TLS files are in ~/.docker dir now.\n\nTest docker TLS connection\n\nNow you can use your client cert to check if you can access Docker via TCP:\n\n$ docker --tlsverify version\n\n\nBecause all the necessary files are in the ~/.docker dir, you don’t need to specify them using --tlscacert --tlscert and --tlskey options. You also don’t need -H to access Docker on localhost.\n\nCopy the files from /home/rancher/.docker to $HOME/.docker on your client machine if you need to access Docker on your RancherOS host from there.\n\nOn your client machine, set the Docker host and test out if Docker commands work.\n\n$ export DOCKER_HOST=tcp://<hostname>:2376 DOCKER_TLS_VERIFY=1\n$ docker ps\n\n","postref":"d79f5fc861105683e86dd2fbfcc29a62","objectID":"062b10367da1bcf38318058df46e89b4","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/configuration/setting-up-docker-tls/"},{"anchor":"#","title":"Private Registries","content":"When launching services through a cloud-config, it is sometimes necessary to pull a private image from DockerHub or from a private registry. Authentication for these can be embedded in your cloud-config.\n\nFor example, to add authentication for DockerHub:\n#cloud-config\nrancher:\n  registry_auths:\n    https://index.docker.io/v1/:\n      auth: dXNlcm5hbWU6cGFzc3dvcmQ=\nThe auth key is generated by base64 encoding a string of the form username:password. The docker login command can be used to generate an auth key. After running the command and authenticating successfully, the key can be found in the $HOME/.docker/config.json file.\n{\n\t\"auths\": {\n\t\t\"https://index.docker.io/v1/\": {\n\t\t\t\"auth\": \"dXNlcm5hbWU6cGFzc3dvcmQ=\"\n\t\t}\n\t}\n}\nAlternatively, a username and password can be specified directly.\n#cloud-config\nrancher:\n  registry_auths:\n    https://index.docker.io/v1/:\n      username: username\n      password: password\nDocker Client Authentication\n\nConfiguring authentication for the Docker client is not handled by the registry_auth key. Instead, the write_files directive can be used to write credentials to the standard Docker configuration location.\n\n#cloud-config\nwrite_files:\n  - path: /home/rancher/.docker/config.json\n    permissions: \"0755\"\n    owner: rancher\n    content: |\n      {\n        \"auths\": {\n          \"https://index.docker.io/v1/\": {\n            \"auth\": \"asdf=\",\n            \"email\": \"not@val.id\"\n          }\n        }\n      }\n\n\nCertificates for Private Registries\n\nCertificates can be stored in the standard locations (i.e. /etc/docker/certs.d) following the Docker documentation. By using the write_files directive of the cloud-config, the certificates can be written directly into /etc/docker/certs.d.\n#cloud-config\nwrite_files:\n  - path: /etc/docker/certs.d/myregistrydomain.com:5000/ca.crt\n    permissions: \"0644\"\n    owner: root\n    content: |\n      -----BEGIN CERTIFICATE-----\n      MIIDJjCCAg4CCQDLCSjwGXM72TANBgkqhkiG9w0BAQUFADBVMQswCQYDVQQGEwJB\n      VTETMBEGA1UECBMKU29tZS1TdGF0ZTEhMB8GA1UEChMYSW50ZXJuZXQgV2lkZ2l0\n      cyBQdHkgTHRkMQ4wDAYDVQQDEwVhbGVuYTAeFw0xNTA3MjMwMzUzMDdaFw0xNjA3\n      MjIwMzUzMDdaMFUxCzAJBgNVBAYTAkFVMRMwEQYDVQQIEwpTb21lLVN0YXRlMSEw\n      HwYDVQQKExhJbnRlcm5ldCBXaWRnaXRzIFB0eSBMdGQxDjAMBgNVBAMTBWFsZW5h\n      MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAxdVIDGlAySQmighbfNqb\n      TtqetENPXjNNq1JasIjGGZdOsmFvNciroNBgCps/HPJphICQwtHpNeKv4+ZuL0Yg\n      1FECgW7oo6DOET74swUywtq/2IOeik+i+7skmpu1o9uNC+Fo+twpgHnGAaGk8IFm\n      fP5gDgthrWBWlEPTPY1tmPjI2Hepu2hJ28SzdXi1CpjfFYOiWL8cUlvFBdyNqzqT\n      uo6M2QCgSX3E1kXLnipRT6jUh0HokhFK4htAQ3hTBmzcxRkgTVZ/D0hA5lAocMKX\n      EVP1Tlw0y1ext2ppS1NR9Sg46GP4+ATgT1m3ae7rWjQGuBEB6DyDgyxdEAvmAEH4\n      LQIDAQABMA0GCSqGSIb3DQEBBQUAA4IBAQA45V0bnGPhIIkb54Gzjt9jyPJxPVTW\n      mwTCP+0jtfLxAor5tFuCERVs8+cLw1wASfu4vH/yHJ/N/CW92yYmtqoGLuTsywJt\n      u1+amECJaLyq0pZ5EjHqLjeys9yW728IifDxbQDX0cj7bBjYYzzUXp0DB/dtWb/U\n      KdBmT1zYeKWmSxkXDFFSpL/SGKoqx3YLTdcIbgNHwKNMfTgD+wTZ/fvk0CLxye4P\n      n/1ZWdSeZPAgjkha5MTUw3o1hjo/0H0ekI4erZFrZnG2N3lDaqDPR8djR+x7Gv6E\n      vloANkUoc1pvzvxKoz2HIHUKf+xFT50xppx6wsQZ01pNMSNF0qgc1vvH\n      -----END CERTIFICATE-----","postref":"86f43e4fa9394196218018bbd9b3bc7a","objectID":"aa814b36e6f29ac7f32b28d6323b3188","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/configuration/private-registries/"},{"anchor":"#using-a-custom-version-of-docker","title":"Using a Custom Version of Docker","content":"If you’re using a version of Docker that isn’t available by default or a custom build of Docker then you can create a custom Docker image and service file to distribute it.Docker engine images are built by adding the binaries to a folder named engine and then adding this folder to a FROM scratch image. For example, the following Dockerfile will build a Docker engine image.FROM scratch\nCOPY engine /engine\nOnce the image is built a system service configuration file must be created. An example file can be found in the rancher/os-services repo. Change the image field to point to the Docker engine image you’ve built.All of the previously mentioned methods of switching Docker engines are now available. For example, if your service file is located at https://myservicefile then the following cloud-config file could be used to use your custom Docker engine.#cloud-config\nrancher:\n  docker:\n    engine: https://myservicefile","postref":"6a399dacb71f56f413a7bc5da13d59e7","objectID":"a2a71f8d699e0c9c3867b5032f89a2cd","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/configuration/switching-docker-versions/"},{"anchor":"#","title":"Users","content":"Currently, we don’t support adding other users besides rancher.\n\nYou can add users in the console container, but these users will only exist as long as the console container exists. It only makes sense to add users in a persistent consoles.\n\nIf you want the console user to be able to ssh into RancherOS, you need to add them\nto the docker group.\n","postref":"6957658d4bb71ed971dceb87c74d65aa","objectID":"1af09b9dc6f000d978d812dae14b0705","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/configuration/users/"},{"anchor":"#","title":"Resizing a Device Partition","content":"The resize_device cloud config option can be used to automatically extend the first partition (assuming its ext4) to fill the size of it’s device.\n\nOnce the partition has been resized to fill the device, a /var/lib/rancher/resizefs.done file will be written to prevent the resize tools from being run again. If you need it to run again, delete that file and reboot.\n#cloud-config\nrancher:\n  resize_device: /dev/sda\nThis behavior is the default when launching RancherOS on AWS.\n","postref":"408a84cb67c67f60d4d10fe94030dc55","objectID":"a760316e1022dbddbf4dfe19591eee3d","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/configuration/resizing-device-partition/"},{"anchor":"#","title":"Sysctl Settings","content":"The rancher.sysctl cloud-config key can be used to control sysctl parameters. This works in a manner similar to /etc/sysctl.conf for other Linux distros.\n\n#cloud-config\nrancher:\n  sysctl:\n    net.ipv4.conf.default.rp_filter: 1\n\n\nYou can either add these settings to your cloud-init.yml, or use sudo ros config merge -i somefile.yml to merge settings into your existing system.\n","postref":"e51b02b4aeaa6663f789b49034a95769","objectID":"f6823602a237980b874f87e3b133c571","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/configuration/sysctl/"},{"anchor":"#","title":"Kernel boot parameters","content":"RancherOS parses the Linux kernel boot cmdline to add any keys it understands to its configuration. This allows you to modify what cloud-init sources it will use on boot, to enable rancher.debug logging, or to almost any other configuration setting.\n\nThere are two ways to set or modify persistent kernel parameters, in-place (editing the file and reboot) or during installation to disk.\n\nIn-place editing\n\nAvailable as of v1.1\n\nTo edit the kernel boot parameters of an already installed RancherOS system, use the new sudo ros config syslinux editing command (uses vi).\n\n\nTo activate this setting, you will need to reboot.\n\n\nFor v1.0\n\nFor in-place editing, you will need to run a container with an editor and a mount to access the /boot/global.cfg file containing the kernel parameters.\n\n\nTo activate this setting, you will need to reboot.\n\n$ sudo system-docker run --rm -it -v /:/host alpine vi /host/boot/global.cfg\nDuring installation\n\nIf you want to set the extra kernel parameters when you are Installing RancherOS to Disk please use the --append parameter.\n$ sudo ros install -d /dev/sda --append \"rancheros.autologin=tty1\"\nGraphical boot screen\n\nAvailable as of v1.1\n\nRancherOS v1.1.0 added a Syslinux boot menu, which allows you to temporarily edit the boot parameters, or to select “Debug logging”, “Autologin”, both “Debug logging & Autologin” and “Recovery Console”.\n\nOn desktop systems the Syslinux boot menu can be switched to graphical mode by adding UI vesamenu.c32 to a new line in global.cfg (use sudo ros config syslinux to edit the file).\n\nUseful RancherOS kernel boot parameters\n\nUser password\n\nrancher.password=<passwd...> will set the password for rancher user. If you are not willing to use SSH keys, you can consider this parameter.\n\nRecovery console\n\nrancher.recovery=true will start a single user root bash session as easily in the boot process, with no network, or persistent filesystem mounted. This can be used to fix disk problems, or to debug your system.\n\nEnable/Disable sshd\n\nrancher.ssh.daemon=false (its enabled in the os-config) can be used to start your RancherOS with no sshd daemon. This can be used to further reduce the ports that your system is listening on.\n\nEnable debug logging\n\nrancher.debug=true will log everything to the console for debugging.\n\nAutologin console\n\nrancher.autologin=<tty...> will automatically log in the specified console - common values are tty1, ttyS0 and ttyAMA0 - depending on your platform.\n\nEnable/Disable hypervisor service auto-enable\n\nRancherOS v1.1.0 added detection of Hypervisor, and then will try to download the a service called <hypervisor>-vm-tools. This may cause boot speed issues, and so can be disabled by setting rancher.hypervisor_service=false.\n\nAuto reboot after a kernel panic\n\nAvailable as of v1.3\n\npanic=10 will automatically reboot after a kernel panic, 10 means wait 10 seconds before reboot. This is a common kernel parameter, pointing out that it is because we set this parameter by default.\n","postref":"f8b1bf0eb8375305a89a62cc2f8e6ed2","objectID":"d1d66422f9581d258b631c2308112114","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/configuration/adding-kernel-parameters/"},{"anchor":"#","title":"Loading Kernel Modules","content":"Since RancherOS v0.8, we build our own kernels using an unmodified kernel.org LTS kernel.\nWe provide both loading kernel modules with parameters and loading extra kernel modules for you.\n\nLoading Kernel Modules with parameters\n\nAvailable as of v1.4\n\nThe rancher.modules can help you to set kernel modules or module parameters.\n\nAs an example, I’m going to set a parameter for kernel module ndb\n\nsudo ros config set rancher.modules \"['nbd nbds_max=1024', 'nfs']\"\n\n\nOr\n\n#cloud-config\nrancher:\n  modules: [nbd nbds_max=1024, nfs]\n\n\nAfter rebooting, you can check that ndbs_max parameter has been updated.\n\n# cat /sys/module/nbd/parameters/nbds_max\n1024\n\n\nLoading Extra Kernel Modules\n\nWe also build almost all optional extras as modules - so most in-tree modules are available\nin the kernel-extras service.\n\nIf you do need to build kernel modules for RancherOS, there are 4 options:\n\n\nTry the kernel-extras service\nAsk us to add it into the next release\nIf its out of tree, copy the methods used for the zfs and open-iscsi services\nBuild it yourself.\n\n\nTry the kernel-extras service\n\nWe build the RancherOS kernel with most of the optional drivers as kernel modules, packaged\ninto an optional RancherOS service.\n\nTo install these, run:\n\nsudo ros service enable kernel-extras\nsudo ros service up kernel-extras\n\n\nThe modules should now be available for you to modprobe\n\nAsk us to do it\n\nOpen a GitHub issue in the https://github.com/rancher/os repository - we’ll probably add\nit to the kernel-extras next time we build a kernel. Tell us if you need the module at initial\nconfiguration or boot, and we can add it to the default kernel modules.\n\nCopy the out of tree build method\n\nSee https://github.com/rancher/os-services/blob/master/z/zfs.yml and\nhttps://github.com/rancher/os-services/tree/master/images/20-zfs\n\nThe build container and build.sh script build the source, and then create a tools image, which is used to\n“wonka.sh” import those tools into the console container using docker run\n\nBuild your own.\n\nAs an example I’m going build the intel-ishtp hid driver using the rancher/os-zfs:<version> images to build in, as they should contain the right tools versions for that kernel.\n\nsudo docker run --rm -it --entrypoint bash --privileged -v /lib:/host/lib -v $(pwd):/data -w /data rancher/os-zfs:$(ros -v | cut -d ' ' -f 2)\n\napt-get update\napt-get install -qy libncurses5-dev bc libssh-dev\ncurl -SsL -o src.tgz https://github.com/rancher/os-kernel/releases/download/v$(uname -r)/linux-$(uname -r)-src.tgz\ntar zxvf src.tgz\nzcat /proc/config.gz >.config\n# Yes, ignore the name of the directory :/\ncd v*\n# enable whatever modules you want to add.\nmake menuconfig\n# I finally found an Intel sound hub that wasn't enabled yet\n# CONFIG_INTEL_ISH_HID=m\nmake modules SUBDIRS=drivers/hid/intel-ish-hid\n\n# test it\ninsmod drivers/hid/intel-ish-hid/intel-ishtp.ko\nrmmod intel-ishtp\n\n# install it\nln -s /host/lib/modules/ /lib/\ncp drivers/hid/intel-ish-hid/*.ko /host/lib/modules/$(uname -r)/kernel/drivers/hid/\ndepmod\n\n# done\nexit\n\n\nThen in your console, you should be able to run\n\nmodprobe intel-ishtp\n\n","postref":"f3afe1ced70f1bfe0379ab1091919eff","objectID":"066f41fe64085a8b26d64cf3b241b131","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/configuration/loading-kernel-modules/"},{"anchor":"#","title":"Installing Kernel Modules that require Kernel Headers","content":"To compile any kernel modules, you will need to download the kernel headers. The kernel headers are available in the form of a system service. Since the kernel headers are a system service, they need to be enabled using the ros service command.\n\nInstalling Kernel Headers\n\nThe following commands can be used to install kernel headers for usage by containers in Docker or System Docker.\n\nDocker\n\n$ sudo ros service enable kernel-headers\n$ sudo ros service up kernel-headers\n\n\nSystem Docker\n\n$ sudo ros service enable kernel-headers-system-docker\n$ sudo ros service up kernel-headers-system-docker\n\n\nThe ros service commands will install the kernel headers in /lib/modules/$(uname -r)/build. Based on which service you install, the kernel headers will be available to containers, in Docker or System Docker,  by bind mounting specific volumes. For any containers that compile a kernel module, the Docker command will need to bind mount in /usr/src and /lib/modules.\n\n\nNote: Since both commands install kernel headers in the same location, the only reason for different services is due to the fact that the storage places for System Docker and Docker are different. Either one or both kernel headers can be installed in the same RancherOS services.\n\n\nExample of Launching Containers to use Kernel Headers\n\n# Run a container in Docker and bind mount specific directories\n$ docker run -it -v /usr/src:/usr/src -v /lib/modules:/lib/modules ubuntu:15.10\n# Run a container in System Docker and bind mount specific directories\n$ sudo system-docker run -it -v /usr/src:/usr/src -v /lib/modules:/lib/modules ubuntu:15.10\n\n","postref":"4388eefd47e54d899c58af2e75a3ef82","objectID":"12c5b4d622f2411d45e337b7bfd32e6e","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/configuration/kernel-modules-kernel-headers/"},{"anchor":"#","title":"Disabling Access to RancherOS","content":"Available as of v1.5\n\nIn RancherOS, you can set rancher.password as a kernel parameter and auto-login to be enabled, but there may be some cases where we want to disable both of these options. Both of these options can be disabled in the cloud-config or as part of a ros command.\n\nHow to Disabling Options\n\nIf RancherOS has already been started, you can use ros config set to update that you want to disable\n\n# Disabling the `rancher.password` kernel parameter\n$ sudo ros config set rancher.disable [\"password\"]\n\n# Disabling the `autologin` ability\n$ sudo ros config set rancher.disable [\"autologin\"]\n\n\nAlternatively, you can set it up in your cloud-config so it’s automatically disabled when you boot RancherOS.\n# cloud-config\nrancher:\n  disable:\n  - password\n  - autologin","postref":"37cbbc16783e2004366eb250c9163bc2","objectID":"d1126687712196c8f36f2a3ea4721835","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/configuration/disable-access-to-system/"},{"anchor":"#","title":"Air Gap Configuration","content":"In the air gap environment, the Docker registry, RancherOS repositories URL, and the RancherOS upgrade URL should be configured to ensure the OS can pull images, update OS services, and upgrade the OS.\n\nConfiguring a Private Docker Registry\n\nYou should use a private Docker registry so that user-docker and system-docker can pull images.\n\n\nAdd the private Docker registry domain to the images prefix.\nSet the private registry certificates for user-docker. For details, refer to Certificates for Private Registries\nSet the private registry certificates for system-docker. There are two ways to set the certificates:\n\n\nTo set the private registry certificates before RancherOS starts, you can run a script included with RancherOS. For details, refer to Set Custom Certs in ISO.\nTo set the private registry certificates after RancherOS starts, append your private registry certs to the /etc/ssl/certs/ca-certificates.crt.rancher file. Then reboot to make the certs fully take effect.\n\nThe images used by RancherOS should be pushed to your private registry.\n\n\nSet Custom Certs in ISO\n\nRancherOS provides a script to set your custom certs for an ISO. The following commands show how to use the script:\n$ git clone https://github.com/rancher/os.git\n$ cd os\n$ make shell-bind\n$ cd scripts/tools/\n$ wget http://link/rancheros-xx.iso\n$ wget http://link/custom.crt\n$ ./flush_crt_iso.sh --iso rancheros-xx.iso --cert custom.crt\n$ exit\n\n$ ls ./build/\nConfiguring RancherOS Repositories and Upgrade URL\n\nThe following steps show how to configure RancherOS to update from private repositories.\n\nBy default, RancherOS will update the engine, console, and service list from https://raw.githubusercontent.com/rancher/os-services and update the os list from https://releases.rancher.com/os/releases.yml. So in the air gap environment, you need to change the repository URL and upgrade URL to your own URLs.\n\n1. Clone os-services files\n\nClone github.com/rancher/os-services to local. The repo has many branches named after the RancherOS versions. Please check out the branch that you are using.\n\n$ git clone https://github.com/rancher/os-services.git\n$ cd os-services\n$ git checkout v1.5.2\n\n\n2. Download the OS releases yaml\n\nDownload the releases.yml from https://releases.rancher.com/os/releases.yml.\n\n3. Serve these files by HTTP\n\nUse a HTTP server to serve the cloned os-services directory and download releases.yml.\nMake sure you can access all the files in os-services and releases.yml by URL.\n\n4. Set the URLs\n\nIn your cloud-config, set rancher.repositories.core.url and rancher.upgrade.url to your own os-services and releases URLs:\n#cloud-config\nrancher:\n  repositories:\n    core:\n      url: https://foo.bar.com/os-services\n  upgrade:\n    url: https://foo.bar.com/os/releases.yml\nYou can also customize rancher.repositories.core.url and rancher.upgrade.url after it’s been started using ros config.\n\n$ sudo ros config set rancher.repositories.core.url https://foo.bar.com/os-services\n$ sudo ros config set rancher.upgrade.url https://foo.bar.com/os/releases.yml\n\n\nExample Cloud-config\n\nHere is a total cloud-config example for using RancherOS in an air gap environment.\n\nFor system-docker, see Configuring Private Docker Registry.\n#cloud-config\nwrite_files:\n  - path: /etc/docker/certs.d/myregistrydomain.com:5000/ca.crt\n    permissions: \"0644\"\n    owner: root\n    content: |\n      -----BEGIN CERTIFICATE-----\n      MIIDJjCCAg4CCQDLCSjwGXM72TANBgkqhkiG9w0BAQUFADBVMQswCQYDVQQGEwJB\n      VTETMBEGA1UECBMKU29tZS1TdGF0ZTEhMB8GA1UEChMYSW50ZXJuZXQgV2lkZ2l0\n      cyBQdHkgTHRkMQ4wDAYDVQQDEwVhbGVuYTAeFw0xNTA3MjMwMzUzMDdaFw0xNjA3\n      MjIwMzUzMDdaMFUxCzAJBgNVBAYTAkFVMRMwEQYDVQQIEwpTb21lLVN0YXRlMSEw\n      HwYDVQQKExhJbnRlcm5ldCBXaWRnaXRzIFB0eSBMdGQxDjAMBgNVBAMTBWFsZW5h\n      MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAxdVIDGlAySQmighbfNqb\n      TtqetENPXjNNq1JasIjGGZdOsmFvNciroNBgCps/HPJphICQwtHpNeKv4+ZuL0Yg\n      1FECgW7oo6DOET74swUywtq/2IOeik+i+7skmpu1o9uNC+Fo+twpgHnGAaGk8IFm\n      fP5gDgthrWBWlEPTPY1tmPjI2Hepu2hJ28SzdXi1CpjfFYOiWL8cUlvFBdyNqzqT\n      uo6M2QCgSX3E1kXLnipRT6jUh0HokhFK4htAQ3hTBmzcxRkgTVZ/D0hA5lAocMKX\n      EVP1Tlw0y1ext2ppS1NR9Sg46GP4+ATgT1m3ae7rWjQGuBEB6DyDgyxdEAvmAEH4\n      LQIDAQABMA0GCSqGSIb3DQEBBQUAA4IBAQA45V0bnGPhIIkb54Gzjt9jyPJxPVTW\n      mwTCP+0jtfLxAor5tFuCERVs8+cLw1wASfu4vH/yHJ/N/CW92yYmtqoGLuTsywJt\n      u1+amECJaLyq0pZ5EjHqLjeys9yW728IifDxbQDX0cj7bBjYYzzUXp0DB/dtWb/U\n      KdBmT1zYeKWmSxkXDFFSpL/SGKoqx3YLTdcIbgNHwKNMfTgD+wTZ/fvk0CLxye4P\n      n/1ZWdSeZPAgjkha5MTUw3o1hjo/0H0ekI4erZFrZnG2N3lDaqDPR8djR+x7Gv6E\n      vloANkUoc1pvzvxKoz2HIHUKf+xFT50xppx6wsQZ01pNMSNF0qgc1vvH\n      -----END CERTIFICATE-----\nrancher:\n  environment:\n    REGISTRY_DOMAIN: xxxx.yyy\n  repositories:\n    core:\n      url: https://foo.bar.com/os-services\n  upgrade:\n    url: https://foo.bar.com/os/releases.yml","postref":"01949cf27f78d5448b699d6cf06f82d9","objectID":"70bc35f08fd929544f149594d435c250","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/configuration/airgap-configuration/"},{"anchor":"#","title":"System Services","content":"A system service is a container that can be run in either System Docker or Docker. Rancher provides services that are already available in RancherOS by adding them to the os-services repo. Anything in the index.yml file from the repository for the tagged release will be an available system service when using the ros service list command.\n\nEnabling and Starting System Services\n\nFor any services that are listed from the ros service list, they can be enabled by running a single command. After enabling a service, you will need to run start the service.\n\n# List out available system services\n$ sudo ros service list\ndisabled amazon-ecs-agent\ndisabled kernel-headers\ndisabled kernel-headers-system-docker\ndisabled open-vm-tools\n# Enable a system service\n$ sudo ros service enable kernel-headers\n# Start a system service\n$ sudo ros service up kernel-headers\n\n\nDisabling and Removing System Services\n\nIn order to stop a system service from running, you will need to stop and disable the system service.\n\n# List out available system services\n$ sudo ros service list\ndisabled amazon-ecs-agent\nenabled kernel-headers\ndisabled kernel-headers-system-docker\ndisabled open-vm-tools\n# Disable a system service\n$ sudo ros service disable kernel-headers\n# Stop a system service\n$ sudo ros service stop kernel-headers\n# Remove the containers associated with the system service\n$ sudo ros service down kernel-headers\n\n\n\nIf you want to remove a system service from the list of service, just delete the service.\n\n$ sudo ros service delete <serviceName>\n\n","postref":"c0dfd15f9771ffed75af9c5d66b63ef9","objectID":"2afbc4e9f9a5242020084685a97bd6e8","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/system-services/adding-system-services/"},{"anchor":"#labels","title":"Labels","content":"We use labels to determine how to handle the service containers.\n\n\nKey\nValue\nDescription\n\n\n\n\n\nio.rancher.os.detach\nDefault: true\nEquivalent of docker run -d. If set to false, equivalent of docker run --detach=false\n\n\n\nio.rancher.os.scope\nsystem\nUse this label to have the container deployed in System Docker instead of Docker.\n\n\n\nio.rancher.os.before/io.rancher.os.after\nService Names (Comma separated list is accepted)\nUsed to determine order of when containers should be started.\n\n\n\nio.rancher.os.createonly\nDefault: false\nWhen set to true, only a docker create will be performed and not a docker start.\n\n\n\nio.rancher.os.reloadconfig\nDefault: false\nWhen set to true, it reloads the configuration.\n\n\nRancherOS uses labels to determine if the container should be deployed in System Docker. By default without the label, the container will be deployed in User Docker.labels:\n  - io.rancher.os.scope=systemExample of how to order container deploymentfoo:\n  labels:\n    # Start foo before bar is launched\n    io.rancher.os.before: bar\n    # Start foo after baz has been launched\n    io.rancher.os.after: baz","postref":"6911b64d256a604acb17ca57c75d3bb5","objectID":"f1a8fedebc3e2203d6d0232d441b64e3","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/system-services/custom-system-services/"},{"anchor":"#","title":"System Docker Volumes","content":"A few services are containers in created state. Their purpose is to provide volumes for other services.\n\nuser-volumes\n\nProvides user accessible persistent storage directories, used by console service:\n\n/home\n/opt\n/var/lib/kubelet - Added as of v1.2\n\n\nAvailable as of v1.2\n\nIf you want to change user-volumes, for example, add /etc/kubernetes directory:\n\n$ sudo ros config set rancher.services.user-volumes.volumes  [/home:/home,/opt:/opt,/var/lib/kubelet:/var/lib/kubelet,/etc/kubernetes:/etc/kubernetes]\n$ sudo reboot\n\n\nPlease note that after the restart, the new persistence directory can take effect.\n\ncontainer-data-volumes\n\nProvides docker storage directory, used by console service (and, indirectly, by docker)\n\n/var/lib/docker\n\n\ncommand-volumes\n\nProvides necessary command binaries (read-only), used by system services:\n\n/usr/bin/docker-containerd.dist\n/usr/bin/docker-containerd-shim.dist\n/usr/bin/docker-runc.dist\n/usr/bin/docker.dist\n/usr/bin/dockerlaunch\n/usr/bin/system-docker\n/sbin/poweroff\n/sbin/reboot\n/sbin/halt\n/sbin/shutdown\n/usr/bin/respawn\n/usr/bin/ros\n/usr/bin/cloud-init\n/usr/sbin/netconf\n/usr/sbin/wait-for-docker\n/usr/bin/switch-console\n\n\nsystem-volumes\n\nProvides necessary persistent directories, used by system services:\n\n/host/dev\n/etc/docker\n/etc/hosts\n/etc/resolv.conf\n/etc/ssl/certs/ca-certificates.crt.rancher\n/etc/selinux\n/lib/firmware\n/lib/modules\n/run\n/usr/share/ros\n/var/lib/rancher/cache\n/var/lib/rancher/conf\n/var/lib/rancher\n/var/log\n/var/run\n\n\nall-volumes\n\nCombines all of the above, used by the console service.\n","postref":"2762c16457b9f1f25e00dbc9de397c45","objectID":"ec796bf0e7c6a895b8ab677f6ef6f5bb","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/system-services/system-docker-volumes/"},{"anchor":"#","title":"Environment","content":"The environment key can be used to customize system services. When a value is not assigned, RancherOS looks up the value from the rancher.environment key.\n\nIn the example below, ETCD_DISCOVERY will be set to https://discovery.etcd.io/d1cd18f5ee1c1e2223aed6a1734719f7 for the etcd service.\nrancher:\n  environment:\n    ETCD_DISCOVERY: https://discovery.etcd.io/d1cd18f5ee1c1e2223aed6a1734719f7\n  services:\n    etcd:\n      ...\n      environment:\n      - ETCD_DISCOVERY\nWildcard globbing is also supported. In the example below, ETCD_DISCOVERY will be set as in the previous example, along with any other environment variables beginning with ETCD_.\nrancher:\n  environment:\n    ETCD_DISCOVERY: https://discovery.etcd.io/d1cd18f5ee1c1e2223aed6a1734719f7\n  services:\n    etcd:\n      ...\n      environment:\n      - ETCD_*\nAvailable as of v1.2\n\nThere is also a way to extend PATH environment variable, PATH or path can be set, and multiple values can be comma-separated. Note that need to reboot before taking effect.\nrancher:\n  environment:\n    path: /opt/bin,/home/rancher/bin","postref":"8498e0bdb2c0134adedeb9eb8ef97903","objectID":"5d935513bd75554e3a5b5f355d2f9b3c","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/system-services/environment/"},{"anchor":"#","title":"Kubeconfig File","content":"In order to start interacting with your Kubernetes cluster, you will use a different binary called kubectl. You will need to install kubectl on your local machine.\n\nA kubeconfig file is a file used to configure access to Kubernetes when used in conjunction with the kubectl commandline tool (or other clients).\n\nFor more details on how kubeconfig and kubectl work together, see the Kubernetes documentation.\n\nWhen you deployed Kubernetes, a kubeconfig is automatically generated for your RKE cluster. This file is created and saved as kube_config_cluster.yml.\n\n\nNote: By default, kubectl checks ~/.kube/config for a kubeconfig file, but you can use any directory you want using the --kubeconfig flag. For example:\n\nkubectl --kubeconfig /custom/path/kube.config get pods\n\n\n\nConfirm that kubectl is working by checking the version of your Kubernetes cluster\n\nkubectl --kubeconfig kube_config_cluster.yml version\n\nClient Version: version.Info{Major:\"1\", Minor:\"10\", GitVersion:\"v1.10.0\", GitCommit:\"fc32d2f3698e36b93322a3465f63a14e9f0eaead\", GitTreeState:\"clean\", BuildDate:\"2018-03-27T00:13:02Z\", GoVersion:\"go1.9.4\", Compiler:\"gc\", Platform:\"darwin/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"8+\", GitVersion:\"v1.8.9-rancher1\", GitCommit:\"68595e18f25e24125244e9966b1e5468a98c1cd4\", GitTreeState:\"clean\", BuildDate:\"2018-03-13T04:37:53Z\", GoVersion:\"go1.8.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n\n\nThe client and server version are reported, indicating that you have a local kubectl client and are able to request the server version from the newly built cluster. Now, you can issue any kubectl command to your cluster, like requesting the nodes that are in the cluster.\n\nkubectl --kubeconfig kube_config_cluster.yml get nodes\nNAME            STATUS    ROLES                      AGE       VERSION\n10.0.0.1         Ready     controlplane,etcd,worker   35m       v1.10.3-rancher1\n\n","postref":"91909900c240572db80f9163ac94d227","objectID":"98dc023dc7ceb6650606c4918bd0e0c3","permalink":"http://jijeesh.github.io/docs/rke/latest/en/kubeconfig/"},{"anchor":"#troubleshooting","title":"Troubleshooting","content":"If you have trouble restoring your cluster, you can refer to the troubleshooting page.","postref":"3061f9d2c42187f51ecdc3849d0f523f","objectID":"ab03c926aad739f44ea5d8c986ae5909","permalink":"http://jijeesh.github.io/docs/rke/latest/en/etcd-snapshots/"},{"anchor":"#","title":"Built-in System Services","content":"To launch RancherOS, we have built-in system services. They are defined in the Docker Compose format, and can be found in the default system config file, /usr/share/ros/os-config.yml. You can add your own system services or override services in the cloud-config.\n\npreload-user-images\n\nRead more about image preloading.\n\nnetwork\n\nDuring this service, networking is set up, e.g. hostname, interfaces, and DNS.\n\nIt is configured by hostname and rancher.networksettings in cloud-config.\n\nntp\n\nRuns ntpd in a System Docker container.\n\nconsole\n\nThis service provides the RancherOS user interface by running sshd and getty. It completes the RancherOS configuration on start up:\n\n\nIf the rancher.password=<password> kernel parameter exists, it sets <password> as the password for the rancher user.\nIf there are no host SSH keys, it generates host SSH keys and saves them under rancher.ssh.keys in cloud-config.\nRuns cloud-init -execute, which does the following:\n\n\nUpdates .ssh/authorized_keys in /home/rancher and /home/docker from cloud-config and metadata.\nWrites files specified by the write_files cloud-config setting.\nResizes the device specified by the rancher.resize_device cloud-config setting.\nMount devices specified in the mounts cloud-config setting.\nSet sysctl parameters specified in  therancher.sysctl cloud-config setting.\n\nIf user-data contained a file that started with #!, then a file would be saved at /var/lib/rancher/conf/cloud-config-script during cloud-init and then executed. Any errors are ignored.\nRuns /opt/rancher/bin/start.sh if it exists and is executable. Any errors are ignored.\nRuns /etc/rc.local if it exists and is executable. Any errors are ignored.\n\n\ndocker\n\nThis system service runs the user docker daemon. Normally it runs inside the console system container by running docker-init script which, in turn, looks for docker binaries in /opt/bin, /usr/local/bin and /usr/bin, adds the first found directory with docker binaries to PATH and runs dockerlaunch docker daemon appending the passed arguments.\n\nDocker daemon args are read from rancher.docker.args cloud-config property (followed by rancher.docker.extra_args).\n","postref":"ee3ff854a000286d2edf9c4c8c4826e5","objectID":"6faff277ff3003c68319c481ae8fb57b","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/boot-process/built-in-system-services/"},{"anchor":"#generating-certificate-signing-requests-csrs-and-keys","title":"Generating Certificate Signing Requests (CSRs) and Keys","content":"By default, Kubernetes clusters require certificates and RKE will automatically generate certificates for the clusters. Rotating these certificates are important before the certificates expire as well as if a certificate is compromised.After the certificates are rotated, the Kubernetes components are automatically restarted. Certificates can be rotated for the following services:\netcd\nkubelet\nkube-apiserver\nkube-proxy\nkube-scheduler\nkube-controller-manager\nRKE has the ability to rotate the auto-generated certificates with some simple commands:\nRotating all service certificates while using the same CA\nRotating a certificate on an individual service while using the same CA\nRotating the CA and all service certificates\nWhenever you’re trying to rotate certificates, the cluster.yml that was used to deploy the Kubernetes cluster is required. You can reference a different location for this file by using the --config option when running rke cert rotate.Rotating all Service Certificates while using the same CATo rotate the service certificates for all the Kubernetes services, run the following command, i.e. rke cert rotate. After all the service certificates are rotated, these services will automatically be restarted to start using the new certificate.$ rke cert rotate\nINFO[0000] Initiating Kubernetes cluster                \nINFO[0000] Rotating Kubernetes cluster certificates     \nINFO[0000] [certificates] Generating Kubernetes API server certificates\nINFO[0000] [certificates] Generating Kube Controller certificates\nINFO[0000] [certificates] Generating Kube Scheduler certificates\nINFO[0001] [certificates] Generating Kube Proxy certificates\nINFO[0001] [certificates] Generating Node certificate   \nINFO[0001] [certificates] Generating admin certificates and kubeconfig\nINFO[0001] [certificates] Generating Kubernetes API server proxy client certificates\nINFO[0001] [certificates] Generating etcd-xxxxx certificate and key\nINFO[0001] [certificates] Generating etcd-yyyyy certificate and key\nINFO[0002] [certificates] Generating etcd-zzzzz certificate and key\nINFO[0002] Successfully Deployed state file at [./cluster.rkestate]\nINFO[0002] Rebuilding Kubernetes cluster with rotated certificates\n.....\nINFO[0050] [worker] Successfully restarted Worker Plane..\nRotating a Certificate on an Individual Service while using the same CATo rotate the certificate for an individual Kubernetes service, use the --service option when rotating certificates to specify the service. After the specified Kubernetes service has had its certificate rotated, it is automatically restarted to start using the new certificate.Example of rotating the certificate for only the kubelet:$ rke cert rotate --service kubelet\nINFO[0000] Initiating Kubernetes cluster                \nINFO[0000] Rotating Kubernetes cluster certificates     \nINFO[0000] [certificates] Generating Node certificate   \nINFO[0000] Successfully Deployed state file at [./cluster.rkestate]\nINFO[0000] Rebuilding Kubernetes cluster with rotated certificates\n.....\nINFO[0033] [worker] Successfully restarted Worker Plane..\nRotating the CA and all service certificatesIf the CA certificate needs to be rotated, you are required to rotate all the services certificates as they need to be signed with the newly rotated CA certificate. To include rotating the CA with the service certificates, add the --rotate-ca option. After the CA and all the service certificates are rotated, these services will automatically be restarted to start using the new certificate.Rotating the CA certificate will result in restarting other system pods, that will also use the new CA certificate. This includes:\nNetworking pods (canal, calico, flannel, and weave)\nIngress Controller pods\nKubeDNS pods\n$ rke cert rotate --rotate-ca      \nINFO[0000] Initiating Kubernetes cluster                \nINFO[0000] Rotating Kubernetes cluster certificates     \nINFO[0000] [certificates] Generating CA kubernetes certificates\nINFO[0000] [certificates] Generating Kubernetes API server aggregation layer requestheader client CA certificates\nINFO[0000] [certificates] Generating Kubernetes API server certificates\nINFO[0000] [certificates] Generating Kube Controller certificates\nINFO[0000] [certificates] Generating Kube Scheduler certificates\nINFO[0000] [certificates] Generating Kube Proxy certificates\nINFO[0000] [certificates] Generating Node certificate   \nINFO[0001] [certificates] Generating admin certificates and kubeconfig\nINFO[0001] [certificates] Generating Kubernetes API server proxy client certificates\nINFO[0001] [certificates] Generating etcd-xxxxx certificate and key\nINFO[0001] [certificates] Generating etcd-yyyyy certificate and key\nINFO[0001] [certificates] Generating etcd-zzzzz certificate and key\nINFO[0001] Successfully Deployed state file at [./cluster.rkestate]\nINFO[0001] Rebuilding Kubernetes cluster with rotated certificates\nIf you want to create and sign the certificates by a real Certificate Authority (CA), you can use RKE to generate a set of Certificate Signing Requests (CSRs) and keys.You can use the CSRs and keys to sign the certificates by a real CA. After the certificates are signed, these custom certificates can be used by RKE to as custom certificates for the Kubernetes cluster.","postref":"329b33986c9be98da5e7c4667b4236f5","objectID":"2c683421d3a66b90254a2ed922dffde6","permalink":"http://jijeesh.github.io/docs/rke/latest/en/cert-mgmt/"},{"anchor":"#using-custom-certificates","title":"Using Custom Certificates","content":"If you want to create and sign the certificates by a real Certificate Authority (CA), you can use RKE to generate a set of Certificate Signing Requests (CSRs) and keys. Using the rke cert generate-csr command, you can generate the CSRs and keys.\nSet up your cluster.yml with the node information.\n\nRun rke cert generate-csr to generate certificates for the node(s) in the cluster.yml. By default, the CSRs and keys will be saved in ./cluster_certs. To have them saved in a different directory, use --cert-dir to define what directory to have them saved in.\n\n$ rke cert generate-csr     \nINFO[0000] Generating Kubernetes cluster CSR certificates\nINFO[0000] [certificates] Generating Kubernetes API server csr\nINFO[0000] [certificates] Generating Kube Controller csr\nINFO[0000] [certificates] Generating Kube Scheduler csr\nINFO[0000] [certificates] Generating Kube Proxy csr     \nINFO[0001] [certificates] Generating Node csr and key   \nINFO[0001] [certificates] Generating admin csr and kubeconfig\nINFO[0001] [certificates] Generating Kubernetes API server proxy client csr\nINFO[0001] [certificates] Generating etcd-x.x.x.x csr and key\nINFO[0001] Successfully Deployed certificates at [./cluster_certs]\n\n\nIn addition to the CSRs, you also need to generate the kube-service-account-token-key.pem key. To do this, run the following:\n\n$ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout ./cluster_certs/kube-service-account-token-key.pem -out ./cluster_certs/kube-service-account-token.pem\n\nResult: The CSRs and keys will be deployed in ./cluster_certs directory, assuming you didn’t specify a --cert-dir. The CSR files will contain the right Alternative DNS and IP Names for the certificates. You can use them to sign the certificates by a real CA. After the certificates are signed, those certificates can be used by RKE as custom certificates.$ tree cluster_certs\n\ncluster_certs\n├── kube-admin-csr.pem\n├── kube-admin-key.pem\n├── kube-apiserver-csr.pem\n├── kube-apiserver-key.pem\n├── kube-apiserver-proxy-client-csr.pem\n├── kube-apiserver-proxy-client-key.pem\n├── kube-controller-manager-csr.pem\n├── kube-controller-manager-key.pem\n├── kube-etcd-x-x-x-x-csr.pem\n├── kube-etcd-x-x-x-x-key.pem\n├── kube-node-csr.pem\n├── kube-node-key.pem\n├── kube-proxy-csr.pem\n├── kube-proxy-key.pem\n├── kube-scheduler-csr.pem\n├── kube-service-account-token-key.pem\n├── kube-service-account-token.pem\n└── kube-scheduler-key.pem\n\n0 directories, 18 files\n\nThe following certificates must exist in the certificate directory.\n\n\nName\nCertificate\nKey\n\n\n\n\n\nMaster CA\nkube-ca.pem\n-\n\n\n\nKube API\nkube-apiserver.pem\nkube-apiserver-key.pem\n\n\n\nKube Controller Manager\nkube-controller-manager.pem\nkube-controller-manager-key.pem\n\n\n\nKube Scheduler\nkube-scheduler.pem\nkube-scheduler-key.pem\n\n\n\nKube Proxy\nkube-proxy.pem\nkube-proxy-key.pem\n\n\n\nKube Admin\nkube-admin.pem\nkube-admin-key.pem\n\n\n\nApiserver Proxy Client\nkube-apiserver-proxy-client.pem\nkube-apiserver-proxy-client-key.pem\n\n\n\nEtcd Nodes\nkube-etcd-x-x-x-x.pem\nkube-etcd-x-x-x-x-key.pem\n\n\n\nKube Api Request Header CA\nkube-apiserver-requestheader-ca.pem\nkube-apiserver-requestheader-ca-key.pem\n\n\n\nService Account Token\n-\nkube-service-account-token-key.pem\n\n\n# Use certificates located in the default directory `/cluster_certs`\n$ rke up --custom-certs\n\n# Use certificates located in your own directory\n$ rke up --custom-certs --cert-dir ~/my/own/certs\n","postref":"e2bd8cf0fd5a96e973f37e04e4d38d7e","objectID":"c1cea991cce1e2beb2cfc93e1c10f1fc","permalink":"http://jijeesh.github.io/docs/rke/latest/en/installation/certs/"},{"anchor":"#","title":"Upgrades and Rollbacks","content":"Upgrading Rancher\n\n\nUpgrades\n\n\nRolling Back Unsuccessful Upgrades\n\nIn the event that your Rancher Server does not upgrade successfully, you can rollback to your installation prior to upgrade:\n\n\nRollbacks for Rancher installed with Docker\nRollbacks for Rancher installed on a Kubernetes cluster\n\n\n\nNote: If you are rolling back to versions in either of these scenarios, you must follow some extra instructions in order to get your clusters working.\n\n\nRolling back from v2.1.6+ to any version between v2.1.0 - v2.1.5 or v2.0.0 - v2.0.10.\nRolling back from v2.0.11+ to any version between v2.0.0 - v2.0.10.\n\n\n","postref":"088d737a1b7dfd559c26ca53bd729ce5","objectID":"2136579fb33103446fa5100056e697b2","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/upgrades/"},{"anchor":"#configuration-load-order","title":"Configuration Load Order","content":"Cloud-config is read by system services when they need to get configuration. Each additional file overwrites and extends the previous configuration file.\n/usr/share/ros/os-config.yml - This is the system default configuration, which should not be modified by users.\n/usr/share/ros/oem/oem-config.yml - This will typically exist by OEM, which should not be modified by users.\nFiles in /var/lib/rancher/conf/cloud-config.d/ ordered by filename. If a file is passed in through user-data, it is written by cloud-init and saved as /var/lib/rancher/conf/cloud-config.d/boot.yml.\n/var/lib/rancher/conf/cloud-config.yml - If you set anything with ros config set, the changes are saved in this file.\nKernel parameters with names starting with rancher.\n/var/lib/rancher/conf/metadata - Metadata added by cloud-init.\n","postref":"2988028b06b74a57451401e0c2f0ddbd","objectID":"1c465f81adfab5c1361c4d685435e82a","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/boot-process/cloud-init/"},{"anchor":"#","title":"Image Preloading","content":"On boot, RancherOS scans /var/lib/rancher/preload/docker and /var/lib/rancher/preload/system-docker directories and tries to load container image archives it finds there, with docker load and system-docker load.\n\nThe archives are .tar files, optionally compressed with xz or gzip. These can be produced by docker save command, e.g.:\n\n$ docker save my-image1 my-image2 some-other/image3 | xz > my-images.tar.xz\n\n\nThe resulting files should be placed into /var/lib/rancher/preload/docker or /var/lib/rancher/preload/system-docker (depending on whether you want it preloaded into Docker or System Docker).\n\nPre-loading process only reads each new archive once, so it won’t take time on subsequent boots (<archive>.done files are created to mark the read archives). If you update the archive (place a newer archive with the same name) it’ll get read on the next boot as well.\n\nPre-loading process is asynchronous by default, optionally this can be set to synchronous through the cloud-config file or ros config set command. In the following example, we’ll use the cloud-config file and ros config set command to set RancherOS pre-loading process to synchronous.\n\nAvailable as of v1.4\n\ncloud-config file, e.g.:\n\n#cloud-config\nrancher:\n  preload_wait: true\n\n\nros config set command, e.g.:\n\n$ ros config set rancher.preload_wait true\n\n\nPre-packing docker images is handy when you’re customizing your RancherOS distribution (perhaps, building cloud VM images for your infrastructure).\n","postref":"3fab2b11b5efa17372cf7d8fa74788a0","objectID":"00d4e6059a12ac88da74dc8f04105f4d","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/boot-process/image-preloading/"},{"anchor":"#","title":"System Logging","content":"System services\n\nRancherOS uses containers for its system services. This means the logs for syslog, acipd, system-cron, udev, network, ntp, console and the user Docker are available using sudo ros service logs <service-name>.\n\nBoot logging\n\nSince v1.1.0, the init process’s logs are copied to /var/log/boot after the user-space filesystem is made available. These can be used to diagnose initialisation, network, and cloud-init issues.\n\nRemote Syslog logging\n\nThe Linux kernel has a netconsole logging facility that allows it to send the Kernel level logs to a remote Syslog server.\nWhen you set this kernel boot parameter in RancherOS v1.1.0 and later, the RancherOS debug logs will also be sent to it.\n\nTo set up Linux kernel and RancherOS remote Syslog logging, you need to set both a local, and remote host IP address - even if this address isn’t the final IP address of your system. The kernel setting looks like:\n\n netconsole=[+][src-port]@[src-ip]/[<dev>],[tgt-port]@<tgt-ip>/[tgt-macaddr]\n\n   where\n        +             if present, enable extended console support\n        src-port      source for UDP packets (defaults to 6665)\n        src-ip        source IP to use (interface address)\n        dev           network interface (eth0)\n        tgt-port      port for logging agent (6666)\n        tgt-ip        IP address for logging agent\n        tgt-macaddr   ethernet MAC address for logging agent (broadcast)\n\n\nFor example, on my current test system, I have set the kernel boot line to:\n\nprintk.devkmsg=on console=tty1 rancher.autologin=tty1 console=ttyS0 rancher.autologin=ttyS0    rancher.state.dev=LABEL=RANCHER_STATE rancher.state.autoformat=[/dev/sda,/dev/vda] rancher.rm_usr loglevel=8 netconsole=+9999@10.0.2.14/,514@192.168.42.223/\n\n\nThe kernel boot parameters can be set during installation using sudo ros install --append \"....\", or on an installed RancherOS system,  by running sudo ros config syslinx (which will start vi in a container, editing the global.cfg boot config file.\n","postref":"d5534c32b1605adb8782cce9f3082e73","objectID":"8c3218b45424033236efdc1c234eeed6","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/boot-process/logging/"},{"anchor":"#","title":"Persistent State Partition","content":"RancherOS will store its state in a single partition specified by the dev field.  The field can be a device such as /dev/sda1 or a logical name such LABEL=state or UUID=123124.  The default value is LABEL=RANCHER_STATE.  The file system type of that partition can be set to auto or a specific file system type such as ext4.\n#cloud-config\nrancher:\n  state:\n    fstype: auto\n    dev: LABEL=RANCHER_STATE\nFor other labels such as RANCHER_BOOT and RANCHER_OEM and RANCHER_SWAP, please refer to Custom partition layout.\n\nAutoformat\n\nYou can specify a list of devices to check to format on boot. If the state partition is already found, RancherOS will not try to auto format a partition. By default, auto-formatting is off.\n\nRancherOS will autoformat the partition to ext4 (not what is set in fstype) if the device specified in autoformat:\n\n\nContains a boot2docker magic string\nStarts with 1 megabyte of zeros and rancher.state.formatzero is true\n\n#cloud-config\nrancher:\n  state:\n    autoformat:\n    - /dev/sda\n    - /dev/vda","postref":"f7384c6a8e28b6c35c13a5bf2bb6261f","objectID":"7e8ce02b80f74faa8faef194c96c5313","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/storage/state-partition/"},{"anchor":"#","title":"Additional Mounts","content":"Additional mounts can be specified as part of your cloud-config. These mounts are applied within the console container. Here’s a simple example that mounts /dev/vdb to /mnt/s.\n#cloud-config\nmounts:\n- [\"/dev/vdb\", \"/mnt/s\", \"ext4\", \"\"]\nImportant: Be aware, the 4th parameter is mandatory and cannot be omitted (server crashes). It also yet cannot be defaults\n\nAs you will use the ros cli most probably, it would look like this:\n\nros config set mounts '[[\"/dev/vdb\",\"/mnt/s\",\"ext4\",\"\"]]'\n\n\nhint: You need to pre-format the disks, rancher-os will not do this for you. The mount will not work (silently) until you formatted the disk, e.g. using:\n\nmkfs.ext4 /dev/vdb\n\n\n\n\nThe four arguments for each mount are the same as those given for cloud-init. Only the first four arguments are currently supported. The mount_default_fields key is not yet implemented.\n\nRancherOS uses the mount syscall rather than the mount command behind the scenes. This means that auto cannot be used as the filesystem type (third argument) and defaults cannot be used for the options (forth argument).\n\nWith rancher 1.1.1+ you do no longer need to create the mount-point folder, it will be created automatically.\n\nShared Mounts\n\nBy default, /media and /mnt are mounted as shared in the console container. This means that mounts within these directories will propagate to the host as well as other system services that mount these folders as shared.\n\nSee here for a more detailed overview of shared mounts and their properties.\n","postref":"3d6467a63039dfda7d376e369a7abbb1","objectID":"3defc2da7cfa91b4e9ed87477b4b06ac","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/storage/additional-mounts/"},{"anchor":"#zfs-storage-for-docker-on-rancheros","title":"ZFS storage for Docker on RancherOS","content":"First, you need to stop  thedocker system service and wipe out /var/lib/docker folder:$ sudo system-docker stop docker\n$ sudo rm -rf /var/lib/docker/*\nTo enable ZFS as the storage driver for Docker, you’ll need to create a ZFS filesystem for Docker and make sure it’s mounted.$ sudo zfs create zpool1/docker\n$ sudo zfs list -o name,mountpoint,mounted\nAt this point you’ll have a ZFS filesystem created and mounted at /zpool1/docker. According to Docker ZFS storage docs, if the Docker root dir is a ZFS filesystem, the Docker daemon will automatically use zfs as its storage driver.Now you’ll need to remove -s overlay (or any other storage driver) from the Docker daemon args to allow docker to automatically detect zfs.$ sudo ros config set rancher.docker.storage_driver 'zfs'\n$ sudo ros config set rancher.docker.graph /mnt/zpool1/docker\n# Now that you've changed the Docker daemon args, you'll need to start Docker\n$ sudo system-docker start docker\nAfter customizing the Docker daemon arguments and restarting docker system service, ZFS will be used as Docker storage driver:$ docker info\nContainers: 0\n Running: 0\n Paused: 0\n Stopped: 0\nImages: 0\nServer Version: 1.12.6\nStorage Driver: zfs\n Zpool: error while getting pool information strconv.ParseUint: parsing \"\": invalid syntax\n Zpool Health: not available\n Parent Dataset: zpool1/docker\n Space Used By Parent: 19456\n Space Available: 8256371200\n Parent Quota: no\n Compression: off\nLogging Driver: json-file\nCgroup Driver: cgroupfs\nPlugins:\n Volume: local\n Network: host bridge null overlay\nSwarm: inactive\nRuntimes: runc\nDefault Runtime: runc\nSecurity Options: seccomp\nKernel Version: 4.9.6-rancher\nOperating System: RancherOS v0.8.0-rc8\nOSType: linux\nArchitecture: x86_64\nCPUs: 1\nTotal Memory: 1.953 GiB\nName: ip-172-31-24-201.us-west-1.compute.internal\nID: IEE7:YTUL:Y3F5:L6LF:5WI7:LECX:YDB5:LGWZ:QRPN:4KDI:LD66:KYTC\nDocker Root Dir: /mnt/zpool1/docker\nDebug Mode (client): false\nDebug Mode (server): false\nRegistry: https://index.docker.io/v1/\nInsecure Registries:\n 127.0.0.0/8\n\n","postref":"8dc60762dd28511a14e9e707e448eb34","objectID":"a329ba15c6a21533b132843175b0b538","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/storage/using-zfs/"},{"anchor":"#","title":"Configuring Network Interfaces","content":"Using ros config, you can configure specific interfaces. Wildcard globbing is supported so eth* will match eth1 and eth2.  The available options you can configure are address, gateway, mtu, and dhcp.\n\n$ sudo ros config set rancher.network.interfaces.eth1.address 172.68.1.100/24\n$ sudo ros config set rancher.network.interfaces.eth1.gateway 172.68.1.1\n$ sudo ros config set rancher.network.interfaces.eth1.mtu 1500\n$ sudo ros config set rancher.network.interfaces.eth1.dhcp false\n\n\nIf you wanted to configure the interfaces through the cloud config file, you’ll need to place interface configurations within the rancher key.\n#cloud-config\nrancher:\n  network:\n    interfaces:\n      eth1:\n        address: 172.68.1.100/24\n        gateway: 172.68.1.1\n        mtu: 1500\n        dhcp: false\n\nNote: The address item should be the CIDR format.\n\n\nMultiple NICs\n\nIf you want to configure one of multiple network interfaces, you can specify the MAC address of the interface you want to configure.\n\nUsing ros config, you can specify the MAC address of the NIC you want to configure as follows:\n\n$ sudo ros config set rancher.network.interfaces.”mac=ea:34:71:66:90:12:01”.dhcp true\n\n\nAlternatively, you can place the MAC address selection in your cloud config file as follows:\n#cloud-config\nrancher:\n  network:\n    interfaces:\n      \"mac=ea:34:71:66:90:12:01\":\n         dhcp: true\nNIC bonding\n\nYou can aggregate several network links into one virtual link for redundancy and increased throughput. For example:\n#cloud-config\nrancher:\n  network:\n    interfaces:\n      bond0:\n        addresses:\n        - 192.168.101.33/31\n        - 10.88.23.129/31\n        gateway: 192.168.101.32\n        bond_opts:\n          downdelay: \"200\"\n          lacp_rate: \"1\"\n          miimon: \"100\"\n          mode: \"4\"\n          updelay: \"200\"\n          xmit_hash_policy: layer3+4\n        post_up:\n        - ip route add 10.0.0.0/8 via 10.88.23.128\n      mac=0c:c4:d7:b2:14:d2:\n        bond: bond0\n      mac=0c:c4:d7:b2:14:d3:\n        bond: bond0\nIn this example two physical NICs (with MACs 0c:c4:d7:b2:14:d2 and 0c:c4:d7:b2:14:d3) are aggregated into a virtual one bond0.\n\nDuring the bootup process, RancherOS runs cloud-init. It automatically detects the data sources of cloud-init, but sometimes a data source requires a network connection. By default, in cloud-init, we open rancher.network.interfaces.eth*.dhcp=true, which may affect the bonding NIC. If you do not require the network connection for your data-source, use rancher.network.interfaces.eth*.dhcp=false in the kernel cmdline to disable DHCP for all NICs.\n\nVLANS\n\nIn this example, you can create an interface eth0.100 which is tied to VLAN 100 and an interface foobar that will be tied to VLAN 200.\n\n#cloud-config\nrancher:\n  network:\n    interfaces:\n      eth0:\n        vlans: 100,200:foobar\n\n\nBridging\n\nIn this example, you can create a bridge interface.\n\n#cloud-config\nrancher:\n  network:\n    interfaces:\n      br0:\n        bridge: true\n        dhcp: true\n      eth0:\n        bridge: br0\n\n\nRun custom network configuration commands\n\nAvailable as of v1.1\n\nYou can configure pre and post network configuration commands to run in the network service container by adding pre_cmds and post_cmds array keys to rancher.network, or pre_up andpost_up keys for specific rancher.network.interfaces.\n\nFor example:\n\n#cloud-config\nwrite_files:\n  - container: network\n    path: /var/lib/iptables/rules.sh\n    permissions: \"0755\"\n    owner: root:root\n    content: |\n      #!/bin/bash\n      set -ex\n      echo $@ >> /var/log/net.log\n      # the last line of the file needs to be a blank line or a comment\nrancher:\n  network:\n    dns:\n      nameservers:\n        - 8.8.4.4\n        - 4.2.2.3\n    pre_cmds:\n    - /var/lib/iptables/rules.sh pre_cmds\n    post_cmds:\n    - /var/lib/iptables/rules.sh post_cmds\n    interfaces:\n      lo:\n        pre_up:\n        - /var/lib/iptables/rules.sh pre_up lo\n        post_up:\n        - /var/lib/iptables/rules.sh post_up lo\n      eth0:\n        pre_up:\n        - /var/lib/iptables/rules.sh pre_up eth0\n        post_up:\n        - /var/lib/iptables/rules.sh post_up eth0\n      eth1:\n        dhcp: true\n        pre_up:\n        - /var/lib/iptables/rules.sh pre_up eth1\n        post_up:\n        - /var/lib/iptables/rules.sh post_up eth1\n      eth2:\n        address: 192.168.3.13/16\n        mtu: 1450\n        pre_up:\n        - /var/lib/iptables/rules.sh pre_up eth2\n        post_up:\n        - /var/lib/iptables/rules.sh post_up eth2\n\n\nWiFi\n\nAvailable as of v1.5\n\nIn order to enable WiFi access, update the cloud-config with the WiFi network information. You can use DHCP or STATIC mode.\n\nExample of a wireless adapter using DHCP\n#cloud-config\nrancher:\n  network:\n    interfaces:\n      wlan0:\n        wifi_network: network1\n    wifi_networks:\n      network1:\n        ssid: \"Your wifi ssid\"\n        psk: \"Your wifi password\"\n        scan_ssid: 1\nExample of a wireless adapter using STATIC\nrancher:\n  network:\n    dns:\n      nameservers:\n      - 8.8.8.8\n      - 8.8.4.4\n    interfaces:\n      wlan0:\n        wifi_network: network1\n    wifi_networks:\n      network1:\n        ssid: \"Your wifi ssid\"\n        psk: \"Your wifi password\"\n        scan_ssid: 1\n        address: 192.168.1.78/24\n        gateway: 192.168.1.1\nExample using two wireless adapters with DHCP\nrancher:\n  network:\n    interfaces:\n      wlan0:\n        wifi_network: network1\n      wlan1:\n        wifi_network: network2\n    wifi_networks:\n      network1:\n        ssid: \"Your wifi ssid\"\n        psk: \"Your wifi password\"\n        scan_ssid: 1\n      network2:\n        ssid: \"Your wifi ssid\"\n        psk: \"Your wifi password\"\n        scan_ssid: 1\nWhen adding in WiFi access, you do not need a system reboot, you only need to restart the network service in System Docker.\n\n$ sudo system-docker restart network\n\n\n\nNote: For Intel wireless adapters, there are some built-in firmware and modules, which prevents requiring to install any new modules or firmware. For other adapters, you may need to install additional os kernel-extras.\n\n\n4G-LTE\n\nAvailable as of v1.5\n\nIn order to support 4G-LTE, 4G-LTE module will need to be connected to the motherboard and to get a good signal, an external antenna will need to be added. You can assemble such a device, which supports USB interface and SIM cards slot:\n\n\n\nIn order to use RancherOS, you will need to use the ISO built for 4G-LTE support. This ISO has a built-in modem-manager service and is available with each release.\n\nAfter booting the ISO, there will be a 4G NIC, such as wwan0. Use the following cloud-config to set the APN parameter.\nrancher:\n  network:\n    modem_networks:\n      wwan0:\n        apn: xxx\nAfter any configuration changes, restart the modem-manager service to apply these changes.\n\n$ sudo system-docker restart modem-manager\n\n\n\nNote: Currently, RancherOS has some built-in  rules in udev rules to allow RancherOS to recognize specific 4G devices, but there are additional vendors that may be missing. If you need to add these in, please file an issue.\n\n","postref":"9c351bbacc2c86f6b8a2d0f456b78f28","objectID":"fc83b401ea87ed958c6f4fc1dcbed79e","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/networking/interfaces/"},{"anchor":"#","title":"Configuring DNS","content":"If you wanted to configure the DNS through the cloud config file, you’ll need to place DNS configurations within the rancher key.\n#cloud-config\n\n#Remember, any changes for rancher will be within the rancher key\nrancher:\n  network:\n    dns:\n      search:\n        - mydomain.com\n        - example.com\nUsing ros config, you can set the nameservers, and search, which directly map to the fields of the same name in /etc/resolv.conf.\n\n$ sudo ros config set rancher.network.dns.search \"['mydomain.com','example.com']\"\n$ sudo ros config get rancher.network.dns.search\n- mydomain.com\n- example.com\n\n","postref":"e8a4f0f2bafcfc77f53ee5fe05ba126e","objectID":"dc188c695ed957ba7b98e914c62fb1d1","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/networking/dns/"},{"anchor":"#","title":"Configuring Proxy Settings","content":"HTTP proxy settings can be set directly under the network key. This will automatically configure proxy settings for both Docker and System Docker.\n#cloud-config\nrancher:\n  network:\n    http_proxy: https://myproxy.example.com\n    https_proxy: https://myproxy.example.com\n    no_proxy: localhost,127.0.0.1\n\n\n\nNote: System Docker proxy settings will not be applied until after a reboot.\n\n\nTo add the HTTP_PROXY, HTTPS_PROXY, and NO_PROXY environment variables to a system service, specify each under the environment key for the service.\n#cloud-config\nrancher:\n  services:\n    myservice:\n      ...\n      environment:\n      - HTTP_PROXY\n      - HTTPS_PROXY\n      - NO_PROXY","postref":"02511238c8d55702d17185c2886c0902","objectID":"1e0db23afd470acc33f1f6665bc29f86","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/networking/proxy-settings/"},{"anchor":"#","title":"Adding and Removing Nodes","content":"Adding/Removing Nodes\n\nRKE supports adding/removing nodes for worker and controlplane hosts.\n\nIn order to add additional nodes, you update the original cluster.yml file with any additional nodes and specify their role in the Kubernetes cluster.\n\nIn order to remove nodes, remove the node information from the nodes list in the original cluster.yml.\n\nAfter you’ve made changes to add/remove nodes, run rke up with the updated cluster.yml.\n\nAdding/Removing Worker Nodes\n\nYou can add/remove only worker nodes, by running rke up --update-only. This will ignore everything else in the cluster.yml except for any worker nodes.\n\n\nNote: When using --update-only, other actions that do not specifically relate to nodes may be deployed or updated, for example addons.\n\n\nRemoving Kubernetes Components from Nodes\n\nIn order to remove the Kubernetes components from nodes, you use the rke remove command.\n\n\nWarning: This command is irreversible and will destroy the Kubernetes cluster, including etcd snapshots on S3. If there is a disaster and your cluster is inaccessible, refer to the process for restoring your cluster from a snapshot.\n\n\nThe rke remove command does the following to each node in the cluster.yml:\n\n\nRemove the Kubernetes component deployed on it\n\n\netcd\nkube-apiserver\nkube-controller-manager\nkubelet\nkube-proxy\nnginx-proxy\n\n\n\nThe cluster’s etcd snapshots are removed, including both local snapshots and snapshots that are stored on S3.\n\n\nNote: Pods are not removed from the nodes. If the node is re-used, the pods will automatically be removed when the new Kubernetes cluster is created.\n\n\n\nClean each host from the directories left by the services:\n\n\n/etc/kubernetes/ssl\n/var/lib/etcd\n/etc/cni\n/opt/cni\n/var/run/calico\n\n\n","postref":"3bd09b4ae1ca578b69e752a1a8a51837","objectID":"c1748849d1247c7f01b0b087ffa001d6","permalink":"http://jijeesh.github.io/docs/rke/latest/en/managing-clusters/"},{"anchor":"#","title":"Custom Console","content":"When booting from the ISO, RancherOS starts with the default console, which is based on busybox.\n\nYou can select which console you want RancherOS to start with using the cloud-config.\n\nEnabling Consoles using Cloud-Config\n\nWhen launching RancherOS with a cloud-config file, you can select which console you want to use.\n\nCurrently, the list of available consoles are:\n\n\ndefault\nalpine\ncentos\ndebian\nfedora\nubuntu\n\n\nHere is an example cloud-config file that can be used to enable the debian console.\n#cloud-config\nrancher:\n  console: debian\nListing Available Consoles\n\nYou can easily list the available consoles in RancherOS and what their status is with sudo ros console list.\n\n$ sudo ros console list\ndisabled alpine\ndisabled centos\ndisabled debian\ncurrent  default\ndisabled fedora\ndisabled ubuntu\n\n\nChanging Consoles after RancherOS has started\n\nYou can view which console is being used by RancherOS by checking which console container is running in System Docker. If you wanted to switch consoles, you just need to run a simple command and select your new console.\n\nFor our example, we’ll switch to the Ubuntu console.\n\n$ sudo ros console switch ubuntu\nSwitching consoles will\n1. destroy the current console container\n2. log you out\n3. restart Docker\nContinue [y/N]:y\nPulling console (rancher/os-ubuntuconsole:v0.5.0-3)...\nv0.5.0-3: Pulling from rancher/os-ubuntuconsole\n6d3a6d998241: Pull complete\n606b08bdd0f3: Pull complete\n1d99b95ffc1c: Pull complete\na3ed95caeb02: Pull complete\n3fc2f42db623: Pull complete\n2fb84911e8d2: Pull complete\nfff5d987b31c: Pull complete\ne7849ae8f782: Pull complete\nde375d40ae05: Pull complete\n8939c16614d1: Pull complete\nDigest: sha256:37224c3964801d633ea8b9629137bc9d4a8db9d37f47901111b119d3e597d15b\nStatus: Downloaded newer image for rancher/os-ubuntuconsole:v0.5.0-3\nswitch-console_1 | time=\"2016-07-02T01:47:14Z\" level=info msg=\"Project [os]: Starting project \"\nswitch-console_1 | time=\"2016-07-02T01:47:14Z\" level=info msg=\"[0/18] [console]: Starting \"\nswitch-console_1 | time=\"2016-07-02T01:47:14Z\" level=info msg=\"Recreating console\"\nConnection to 127.0.0.1 closed by remote host.\n\n\n\n\nAfter logging back, you’ll be in the Ubuntu console.\n\n$ sudo system-docker ps\nCONTAINER ID        IMAGE                                 COMMAND                  CREATED              STATUS              PORTS               NAMES\n6bf33541b2dc        rancher/os-ubuntuconsole:v0.5.0-rc3   \"/usr/sbin/entry.sh /\"   About a minute ago   Up About a minute\n\n\n\n\n\nNote: When switching between consoles, the currently running console container is destroyed, Docker is restarted and you will be logged out.\n\n\nConsole persistence\n\nAll consoles except the default (busybox) console are persistent. Persistent console means that the console container will remain the same and preserves changes made to its filesystem across reboots. If a container is deleted/rebuilt, state in the console will be lost except what is in the persisted directories.\n\n/home\n/opt\n/var/lib/docker\n/var/lib/rancher\n\n\n\n\n\nNote: When using a persistent console and in the current version’s console, rolling back is not supported. For example, rolling back to v0.4.5 when using a v0.5.0 persistent console is not supported.\n\n\nEnabling Consoles\n\nYou can also enable a console that will be changed at the next reboot.\n\nFor our example, we’ll switch to the Debian console.\n\n# Check the console running in System Docker\n$ sudo system-docker ps\nCONTAINER ID        IMAGE                              COMMAND                  CREATED             STATUS              PORTS               NAMES\n95d548689e82        rancher/os-docker:v0.5.0    \"/usr/sbin/entry.sh /\"   About an hour ago   Up About an hour                        docker\n# Enable the Debian console\n$ sudo ros console enable debian\nPulling console (rancher/os-debianconsole:v0.5.0-3)...\nv0.5.0-3: Pulling from rancher/os-debianconsole\n7268d8f794c4: Pull complete\na3ed95caeb02: Pull complete\n21cb8a645d75: Pull complete\n5ee1d288a088: Pull complete\nc09f41c2bd29: Pull complete\n02b48ce40553: Pull complete\n38a4150e7e9c: Pull complete\nDigest: sha256:5dbca5ba6c3b7ba6cd6ac75a1d054145db4b4ea140db732bfcbd06f17059c5d0\nStatus: Downloaded newer image for rancher/os-debianconsole:v0.5.0-3\n\n\n\n\nAt the next reboot, RancherOS will be using the Debian console.\n","postref":"61d61e6c53eea16c58d0b8f2d09e5f07","objectID":"903ba0c37bf5dac279205fdbb0da71c7","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/custom-builds/custom-console/"},{"anchor":"#","title":"Custom Kernels","content":"Kernel version in RancherOS\n\nRancherOS basically uses the standard Linux kernel, but we maintain a kernel config ourselves. Due to various feature support and security fixes, we are constantly updating the kernel version.\n\n\n\n\nRancherOS\nKernel\n\n\n\n\n\n<=v0.7.1\n4.4.x\n\n\n\n<=v1.3.0\n4.9.x\n\n\n\n>=v1.4.0\n4.14.x\n\n\n\n\nBuilding and Packaging a Kernel to be used in RancherOS\n\nWe build the kernel for RancherOS at the os-kernel repository. You can use this repository to help package your own custom kernel to be used in RancherOS.\n\nCreate a clone of the os-kernel repository to your local machine using git clone.\n\n$ git clone https://github.com/rancher/os-kernel.git\n\n\nIf you want to build kernel v4.14.53, you can refer to the following command. After the build is completed, a ./dist/kernel directory will be created with the freshly built kernel tarball and headers.\n\n$ git tag v4.14.53-rancher\n$ KERNEL_TAG=4.14.53 make release\n...snip...\n./dist/kernel/extra-linux-4.14.53-rancher-x86.tar.gz\n./dist/kernel/build-linux-4.14.53-rancher-x86.tar.gz\n./dist/kernel/linux-4.14.53-rancher-x86.tar.gz\n./dist/kernel/config\n...snip...\nImages ready to push:\nrancher/os-extras:4.14.53-rancher\nrancher/os-headers:4.14.53-rancher\n\n\nFor some users who need a custom kernel, the following information is very useful to you:\n\n\nThe modules defined in modules.list  will be packaged into the built-in modules.\nThe modules defined in modules-extra.list  will be packaged into the extra modules.\nYou can modify config/kernel-config to build the kernel modules you need.\nYou can add your patches in the patches directory, and os-kernel will update these patches after downloading the kernel source.\n\n\nNow you need to either upload the ./dist/kernel/linux-4.14.53-rancher-x86.tar.gz file to somewhere, or copy that file into your clone of the rancher/os repo, as assets/kernel.tar.gz.\n\nThe build-<name>.tar.gz and extra-<name>.tar.gz files are used to build the rancher/os-extras and rancher/os-headers images for your RancherOS release - which you will need to tag them with a different organisation name, push them to a registry, and create custom service.yml files.\n\nYour kernel should be packaged and published as a set of files of the following format:\n\n\n<kernel-name-and-version>.tar.gz is the one KERNEL_URL in rancher/os should point to. It contains the kernel binary, core modules and firmware.\n\nbuild-<kernel-name-and-version>.tar.gz contains build headers to build additional modules: it is a subset of the kernel sources tarball. These files will be installed into /usr/src/<os-kernel-tag> using the kernel-headers-system-docker and kernel-headers services.\n\nextra-<kernel-name-and-version>.tar.gz contains extra modules and firmware for your kernel and should be built into a kernel-extras service.\n\n\nBuilding a RancherOS release using the Packaged kernel files.\n\nBy default, RancherOS ships with the kernel provided by the os-kernel repository. Swapping out the default kernel can by done by building your own custom RancherOS ISO.\n\nCreate a clone of the main RancherOS repository to your local machine with a git clone.\n\n$ git clone https://github.com/rancher/os.git\n\n\nIn the root of the repository, the “General Configuration” section of Dockerfile.dapper will need to be updated. Using your favorite editor, replace the appropriate KERNEL_URL value with a URL of your compiled custom kernel tarball. Ideally, the URL will use HTTPS.\n\n# Update the URL to your own custom kernel tarball\nARG KERNEL_VERSION_amd64=4.14.63-rancher\nARG KERNEL_URL_amd64=https://link/xxxx\n\n\nAfter you’ve replaced the URL with your custom kernel, you can follow the steps in building your own custom RancherOS ISO.\n\n\nNote: KERNEL_URL settings should point to a Linux kernel, compiled and packaged in a specific way. You can fork os-kernel repository to package your own kernel.\n\n","postref":"86ac91fbeefe43408c72ea607498af93","objectID":"d3f4b1cbaec2c146bf735deb43104299","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/custom-builds/custom-kernels/"},{"anchor":"#creating-a-gce-image-archive","title":"Creating a GCE Image Archive","content":"Reduce Memory RequirementsWith changes to the kernel and built Docker, RancherOS booting requires more memory. For details, please refer to the memory requirements.By customizing the ISO, you can reduce the memory usage on boot. The easiest way is to downgrade the built-in Docker version, because Docker takes up a lot of space.\nThis can effectively reduce the memory required to decompress the initrd on boot. Using docker 17.03 is a good choice:# run make\n$ USER_DOCKER_VERSION=17.03.2 make release\nBuilding with a Different ConsoleAvailable as of v1.5.0When building RancherOS, you have the ability to automatically start in a supported console instead of booting into the default console and switching to your desired one.Here is an example of building RancherOS and having the alpine console enabled:$ OS_CONSOLE=alpine make release\nBuilding with Predefined Docker ImagesIf you want to use a custom ISO file to address an offline scenario, you can use predefined images for system-docker and user-docker.RancherOS supports APPEND_SYSTEM_IMAGES. It can save images to the initrd file, and is loaded with system-docker when booting.You can build the ISO like this:APPEND_SYSTEM_IMAGES=\"rancher/os-openvmtools:10.3.10-1\" make release\nRancherOS also supports APPEND_USER_IMAGES. It can save images to the initrd file, and is loaded with user-docker when booting.You can build the ISO like this:APPEND_USER_IMAGES=\"alpine:3.9 ubuntu:bionic\" make release\nPlease note that these will be packaged into the initrd, and the predefined images will affect the resource footprint at startup.Create a clone of the main RancherOS repository to your local machine with a git clone.$ git clone https://github.com/rancher/os-packer.git\nGCE supports KVM virtualization, and we use packer to build KVM images. Before building, you need to verify that the host can support KVM.\nIf you want to build GCE image based on RancherOS v1.4.0, you can run this command:RANCHEROS_VERSION=v1.4.0 make build-gce\n","postref":"24e898012a6c71e2b0c4c328bff977d0","objectID":"0a2252e27a71ceb00e333ad2e7d4e00c","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/custom-builds/custom-rancheros-iso/"},{"anchor":"#","title":"1. Create Nodes and Load Balancer","content":"Use your infrastructure provider of choice to provision three nodes and a load balancer endpoint for your RKE install.\n\n\nNote: These nodes must be in the same region. You may place these servers in separate availability zones (datacenter).\n\n\nRequirements for OS, Docker, Hardware, and Networking\n\nMake sure that your nodes fulfill the general installation requirements.\n\nView the OS requirements for RKE at RKE Requirements.\n\nLoad Balancer\n\nRKE will configure an Ingress controller pod, on each of your nodes. The Ingress controller pods are bound to ports TCP/80 and TCP/443 on the host network and are the entry point for HTTPS traffic to the Rancher server.\n\nConfigure a load balancer as a basic Layer 4 TCP forwarder. The exact configuration will vary depending on your environment.\n\n\nImportant:\nDo not use this load balancer (i.e, the local cluster Ingress) to load balance applications other than Rancher following installation. Sharing this Ingress with other applications may result in websocket errors to Rancher following Ingress configuration reloads for other apps. We recommend dedicating the local cluster to Rancher and no other applications.\n\n\nHow-to Guides\n\n\nFor an example showing how to set up an NGINX load balancer, refer to this page.\nFor an example showing how to setup an Amazon NLB load balancer, refer to this page.\n\n\nNext: Install Kubernetes with RKE\n","postref":"78ad69b918932d031c5dfab7d3653dce","objectID":"f239f39798f661ff5bfbbb9c02726945","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/k8s-install/create-nodes-lb/"},{"anchor":"#","title":"1. Create Nodes and Load Balancer","content":"Use your provider of choice to provision 3 nodes and a Load Balancer endpoint for your RKE install.\n\n\nNote: These nodes must be in the same region/datacenter.  You may place these servers in separate availability zones.\n\n\nNode Requirements\n\nView the supported operating systems and hardware/software/networking requirements for nodes running Rancher at Node Requirements.\n\nView the OS requirements for RKE at RKE Requirements\n\nLoad Balancer\n\nRKE will configure an Ingress controller pod, on each of your nodes. The Ingress controller pods are bound to ports TCP/80 and TCP/443 on the host network and are the entry point for HTTPS traffic to the Rancher server.\n\nConfigure a load balancer as a basic Layer 4 TCP forwarder. The exact configuration will vary depending on your environment.\n\n\nImportant:\nDo not use this load balancer (i.e, the local cluster Ingress) to load balance applications other than Rancher following installation. Sharing this Ingress with other applications may result in websocket errors to Rancher following Ingress configuration reloads for other apps. We recommend dedicating the local cluster to Rancher and no other applications.\n\n\nExamples\n\n\nNginx\nAmazon NLB\n\n\nNext: Install Kubernetes with RKE\n","postref":"9004ef3e1dca5d354f21822e70032718","objectID":"828b99f8dcd2212d8a65182a55c641dc","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/helm2/create-nodes-lb/"},{"anchor":"#","title":"2. Install Kubernetes with RKE","content":"Use RKE to install Kubernetes with a high availability etcd configuration.\n\n\nNote: For systems without direct internet access see Air Gap: Kubernetes install for install details.\n\n\nCreate the rancher-cluster.yml File\n\nUsing the sample below create the rancher-cluster.yml file. Replace the IP Addresses in the nodes list with the IP address or DNS names of the 3 nodes you created.\n\n\nNote:  If your node has public and internal addresses, it is recommended to set the internal_address: so Kubernetes will use it for intra-cluster communication.  Some services like AWS EC2 require setting the internal_address: if you want to use self-referencing security groups or firewalls.\n\nnodes:\n  - address: 165.227.114.63\n    internal_address: 172.16.22.12\n    user: ubuntu\n    role: [controlplane,worker,etcd]\n  - address: 165.227.116.167\n    internal_address: 172.16.32.37\n    user: ubuntu\n    role: [controlplane,worker,etcd]\n  - address: 165.227.127.226\n    internal_address: 172.16.42.73\n    user: ubuntu\n    role: [controlplane,worker,etcd]\n\nservices:\n  etcd:\n    snapshot: true\n    creation: 6h\n    retention: 24h\nCommon RKE Nodes Options\n\n\n\n\nOption\nRequired\nDescription\n\n\n\n\n\naddress\nyes\nThe public DNS or IP address\n\n\n\nuser\nyes\nA user that can run docker commands\n\n\n\nrole\nyes\nList of Kubernetes roles assigned to the node\n\n\n\ninternal_address\nno\nThe private DNS or IP address for internal cluster traffic\n\n\n\nssh_key_path\nno\nPath to SSH private key used to authenticate to the node (defaults to ~/.ssh/id_rsa)\n\n\n\n\nAdvanced Configurations\n\nRKE has many configuration options for customizing the install to suit your specific environment.\n\nPlease see the RKE Documentation for the full list of options and capabilities.\n\nFor tuning your etcd cluster for larger Rancher installations see the etcd settings guide.\n\nRun RKE\n\nrke up --config ./rancher-cluster.yml\n\n\nWhen finished, it should end with the line: Finished building Kubernetes cluster successfully.\n\nTesting Your Cluster\n\nRKE should have created a file kube_config_rancher-cluster.yml. This file has the credentials for kubectl and helm.\n\n\nNote: If you have used a different file name from rancher-cluster.yml, then the kube config file will be named kube_config_<FILE_NAME>.yml.\n\n\nYou can copy this file to $HOME/.kube/config or if you are working with multiple Kubernetes clusters, set the KUBECONFIG environmental variable to the path of kube_config_rancher-cluster.yml.\n\nexport KUBECONFIG=$(pwd)/kube_config_rancher-cluster.yml\n\n\nTest your connectivity with kubectl and see if all your nodes are in Ready state.\n\nkubectl get nodes\n\nNAME                          STATUS    ROLES                      AGE       VERSION\n165.227.114.63                Ready     controlplane,etcd,worker   11m       v1.13.5\n165.227.116.167               Ready     controlplane,etcd,worker   11m       v1.13.5\n165.227.127.226               Ready     controlplane,etcd,worker   11m       v1.13.5\n\n\nCheck the Health of Your Cluster Pods\n\nCheck that all the required pods and containers are healthy are ready to continue.\n\n\nPods are in Running or Completed state.\nREADY column shows all the containers are running (i.e. 3/3) for pods with STATUS Running\nPods with STATUS Completed are run-once Jobs. For these pods READY should be 0/1.\n\n\nkubectl get pods --all-namespaces\n\nNAMESPACE       NAME                                      READY     STATUS      RESTARTS   AGE\ningress-nginx   nginx-ingress-controller-tnsn4            1/1       Running     0          30s\ningress-nginx   nginx-ingress-controller-tw2ht            1/1       Running     0          30s\ningress-nginx   nginx-ingress-controller-v874b            1/1       Running     0          30s\nkube-system     canal-jp4hz                               3/3       Running     0          30s\nkube-system     canal-z2hg8                               3/3       Running     0          30s\nkube-system     canal-z6kpw                               3/3       Running     0          30s\nkube-system     kube-dns-7588d5b5f5-sf4vh                 3/3       Running     0          30s\nkube-system     kube-dns-autoscaler-5db9bbb766-jz2k6      1/1       Running     0          30s\nkube-system     metrics-server-97bc649d5-4rl2q            1/1       Running     0          30s\nkube-system     rke-ingress-controller-deploy-job-bhzgm   0/1       Completed   0          30s\nkube-system     rke-kubedns-addon-deploy-job-gl7t4        0/1       Completed   0          30s\nkube-system     rke-metrics-addon-deploy-job-7ljkc        0/1       Completed   0          30s\nkube-system     rke-network-plugin-deploy-job-6pbgj       0/1       Completed   0          30s\n\n\nSave Your Files\n\n\nImportant\nThe files mentioned below are needed to maintain, troubleshoot and upgrade your cluster.\n\n\nSave a copy of the following files in a secure location:\n\n\nrancher-cluster.yml: The RKE cluster configuration file.\nkube_config_rancher-cluster.yml: The Kubeconfig file for the cluster, this file contains credentials for full access to the cluster.\nrancher-cluster.rkestate: The Kubernetes Cluster State file, this file contains credentials for full access to the cluster.The Kubernetes Cluster State file is only created when using RKE v0.2.0 or higher.\n\n\n\nNote: The “rancher-cluster” parts of the two latter file names are dependent on how you name the RKE cluster configuration file.\n\n\nIssues or errors?\n\nSee the Troubleshooting page.\n\nNext: Initialize Helm (Install tiller)\n","postref":"54ba8f0e911367507966ec560747acc8","objectID":"3a9d74ea6c310439bc5c3b7c9a1734a2","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/helm2/kubernetes-rke/"},{"anchor":"#","title":"2. Set up a Kubernetes Cluster","content":"This section describes how to install a Kubernetes cluster on your three nodes according to our best practices for the Rancher server environment. This cluster should be dedicated to run only the Rancher server. We recommend using RKE to install Kubernetes on this cluster. Hosted Kubernetes providers such as EKS should not be used.\n\nFor systems without direct internet access, refer to Air Gap: Kubernetes install.\n\n\nSingle-node Installation Tip:\nIn a single-node Kubernetes cluster, the Rancher server does not have high availability, which is important for running Rancher in production. However, installing Rancher on a single-node cluster can be useful if you want to save resources by using a single node in the short term, while preserving a high-availability migration path.\n\nTo set up a single-node cluster, configure only one node in the cluster.yml when provisioning the cluster with RKE. The single node should have all three roles: etcd, controlplane and worker. Then Rancher can be installed with Helm on the cluster in the same way that it would be installed on any other cluster.\n\n\nCreate the rancher-cluster.yml File\n\nUsing the sample below, create the rancher-cluster.yml file. Replace the IP Addresses in the nodes list with the IP address or DNS names of the 3 nodes you created.\n\nIf your node has public and internal addresses, it is recommended to set the internal_address: so Kubernetes will use it for intra-cluster communication. Some services like AWS EC2 require setting the internal_address: if you want to use self-referencing security groups or firewalls.\nnodes:\n  - address: 165.227.114.63\n    internal_address: 172.16.22.12\n    user: ubuntu\n    role: [controlplane, worker, etcd]\n  - address: 165.227.116.167\n    internal_address: 172.16.32.37\n    user: ubuntu\n    role: [controlplane, worker, etcd]\n  - address: 165.227.127.226\n    internal_address: 172.16.42.73\n    user: ubuntu\n    role: [controlplane, worker, etcd]\n\nservices:\n  etcd:\n    snapshot: true\n    creation: 6h\n    retention: 24h\n\n# Required for external TLS termination with\n# ingress-nginx v0.22+\ningress:\n  provider: nginx\n  options:\n    use-forwarded-headers: \"true\"\nCommon RKE Nodes Options\n\n\n\n\nOption\nRequired\nDescription\n\n\n\n\n\naddress\nyes\nThe public DNS or IP address\n\n\n\nuser\nyes\nA user that can run docker commands\n\n\n\nrole\nyes\nList of Kubernetes roles assigned to the node\n\n\n\ninternal_address\nno\nThe private DNS or IP address for internal cluster traffic\n\n\n\nssh_key_path\nno\nPath to SSH private key used to authenticate to the node (defaults to ~/.ssh/id_rsa)\n\n\n\n\nAdvanced Configurations\n\nRKE has many configuration options for customizing the install to suit your specific environment.\n\nPlease see the RKE Documentation for the full list of options and capabilities.\n\nFor tuning your etcd cluster for larger Rancher installations see the etcd settings guide.\n\nRun RKE\n\nrke up --config ./rancher-cluster.yml\n\n\nWhen finished, it should end with the line: Finished building Kubernetes cluster successfully.\n\nTesting Your Cluster\n\nRKE should have created a file kube_config_rancher-cluster.yml. This file has the credentials for kubectl and helm.\n\n\nNote: If you have used a different file name from rancher-cluster.yml, then the kube config file will be named kube_config_<FILE_NAME>.yml.\n\n\nYou can copy this file to $HOME/.kube/config or if you are working with multiple Kubernetes clusters, set the KUBECONFIG environmental variable to the path of kube_config_rancher-cluster.yml.\n\nexport KUBECONFIG=$(pwd)/kube_config_rancher-cluster.yml\n\n\nTest your connectivity with kubectl and see if all your nodes are in Ready state.\n\nkubectl get nodes\n\nNAME                          STATUS    ROLES                      AGE       VERSION\n165.227.114.63                Ready     controlplane,etcd,worker   11m       v1.13.5\n165.227.116.167               Ready     controlplane,etcd,worker   11m       v1.13.5\n165.227.127.226               Ready     controlplane,etcd,worker   11m       v1.13.5\n\n\nCheck the Health of Your Cluster Pods\n\nCheck that all the required pods and containers are healthy are ready to continue.\n\n\nPods are in Running or Completed state.\nREADY column shows all the containers are running (i.e. 3/3) for pods with STATUS Running\nPods with STATUS Completed are run-once Jobs. For these pods READY should be 0/1.\n\n\nkubectl get pods --all-namespaces\n\nNAMESPACE       NAME                                      READY     STATUS      RESTARTS   AGE\ningress-nginx   nginx-ingress-controller-tnsn4            1/1       Running     0          30s\ningress-nginx   nginx-ingress-controller-tw2ht            1/1       Running     0          30s\ningress-nginx   nginx-ingress-controller-v874b            1/1       Running     0          30s\nkube-system     canal-jp4hz                               3/3       Running     0          30s\nkube-system     canal-z2hg8                               3/3       Running     0          30s\nkube-system     canal-z6kpw                               3/3       Running     0          30s\nkube-system     kube-dns-7588d5b5f5-sf4vh                 3/3       Running     0          30s\nkube-system     kube-dns-autoscaler-5db9bbb766-jz2k6      1/1       Running     0          30s\nkube-system     metrics-server-97bc649d5-4rl2q            1/1       Running     0          30s\nkube-system     rke-ingress-controller-deploy-job-bhzgm   0/1       Completed   0          30s\nkube-system     rke-kubedns-addon-deploy-job-gl7t4        0/1       Completed   0          30s\nkube-system     rke-metrics-addon-deploy-job-7ljkc        0/1       Completed   0          30s\nkube-system     rke-network-plugin-deploy-job-6pbgj       0/1       Completed   0          30s\n\n\nSave Your Files\n\n\nImportant\nThe files mentioned below are needed to maintain, troubleshoot and upgrade your cluster.\n\n\nSave a copy of the following files in a secure location:\n\n\nrancher-cluster.yml: The RKE cluster configuration file.\nkube_config_rancher-cluster.yml: The Kubeconfig file for the cluster, this file contains credentials for full access to the cluster.\nrancher-cluster.rkestate: The Kubernetes Cluster State file, this file contains credentials for full access to the cluster.The Kubernetes Cluster State file is only created when using RKE v0.2.0 or higher.\n\n\n\nNote: The “rancher-cluster” parts of the two latter file names are dependent on how you name the RKE cluster configuration file.\n\n\nIssues or errors?\n\nSee the Troubleshooting page.\n\nNext: Install Rancher\n","postref":"c8d008bd1b118b62f5e0ac0a1c8e602d","objectID":"7121ae9fd824df5f6dfb52e7f5614ae1","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/k8s-install/kubernetes-rke/"},{"anchor":"#","title":"Amazon ECS (EC2 Container Service)","content":"Amazon ECS is supported, which allows RancherOS EC2 instances to join your cluster.\n\nPre-Requisites\n\nPrior to launching RancherOS EC2 instances, the ECS Container Instance IAM Role will need to have been created. This ecsInstanceRole will need to be used when launching EC2 instances. If you have been using ECS, you created this role if you followed the ECS “Get Started” interactive guide.\n\nLaunching an instance with ECS\n\nRancherOS makes it easy to join your ECS cluster. The ECS agent is a system service that is enabled in the ECS enabled AMI. There may be other RancherOS AMIs that don’t have the ECS agent enabled by default, but it can easily be added in the user data on any RancherOS AMI.\n\nWhen launching the RancherOS AMI, you’ll need to specify the IAM Role and Advanced Details -> User Data in the Configure Instance Details step.\n\nFor the IAM Role, you’ll need to be sure to select the ECS Container Instance IAM role.\n\nFor the User Data, you’ll need to pass in the cloud-config file.\n#cloud-config\nrancher:\n  environment:\n    ECS_CLUSTER: your-ecs-cluster-name\n    # Note: You will need to add this variable, if using awslogs for ECS task.\n    ECS_AVAILABLE_LOGGING_DRIVERS: |-\n      [\"json-file\",\"awslogs\"]\n# If you have selected a RancherOS AMI that does not have ECS enabled by default,\n# you'll need to enable the system service for the ECS agent.\n  services_include:\n    amazon-ecs-agent: true\nVersion\n\nBy default, the ECS agent will be using the latest tag for the amazon-ecs-agent image. In v0.5.0, we introduced the ability to select which version of the amazon-ecs-agent.\n\nTo select the version, you can update your cloud-config file.\n#cloud-config\nrancher:\n  environment:\n    ECS_CLUSTER: your-ecs-cluster-name\n    # Note: You will need to make sure to include the colon in front of the version.\n    ECS_AGENT_VERSION: :v1.9.0\n    # If you have selected a RancherOS AMI that does not have ECS enabled by default,\n    # you'll need to enable the system service for the ECS agent.\n  services_include:\n    amazon-ecs-agent: true\n\n\n\nNote: The : must be in front of the version tag in order for the ECS image to be tagged correctly.\n\n\nAmazon ECS enabled AMIs\n\nLatest Release: v1.5.4\n\n\n\n\nRegion\nType\nAMI\n\n\n\n\n\neu-north-1\nHVM - ECS enabled\nami-0c46c1da6468aa948\n\n\n\nap-south-1\nHVM - ECS enabled\nami-097e5fa868c46e925\n\n\n\neu-west-3\nHVM - ECS enabled\nami-016e7d630d7f608e4\n\n\n\neu-west-2\nHVM - ECS enabled\nami-00aacd261ab72302e\n\n\n\neu-west-1\nHVM - ECS enabled\nami-0812b3f8aec8d2d81\n\n\n\nap-northeast-2\nHVM - ECS enabled\nami-0d9d77df6579e618a\n\n\n\nap-northeast-1\nHVM - ECS enabled\nami-09e957ac11ef430a3\n\n\n\nsa-east-1\nHVM - ECS enabled\nami-09c22f3ce89280ed4\n\n\n\nca-central-1\nHVM - ECS enabled\nami-016ac80225e649cf9\n\n\n\nap-southeast-1\nHVM - ECS enabled\nami-06cdfc80bdbd6f419\n\n\n\nap-southeast-2\nHVM - ECS enabled\nami-0335f7bb1c51c0a74\n\n\n\neu-central-1\nHVM - ECS enabled\nami-0af71ec7ee8b729be\n\n\n\nus-east-1\nHVM - ECS enabled\nami-07209d7ec9e7545b4\n\n\n\nus-east-2\nHVM - ECS enabled\nami-046358fe356dd0e35\n\n\n\nus-west-1\nHVM - ECS enabled\nami-031bcb65b47cb0a77\n\n\n\nus-west-2\nHVM - ECS enabled\nami-0d92d296ecb13ea45\n\n\n\ncn-north-1\nHVM - ECS enabled\nami-04f1668aaf990acf6\n\n\n\ncn-northwest-1\nHVM - ECS enabled\nami-0771f259ffce58280\n\n\n\n","postref":"16e1d89ca8ede9084677908f8bcdd137","objectID":"d2bfcf6ed0fb3614207d59c9695bdcda","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/amazon-ecs/"},{"anchor":"#","title":"Initialize Helm: Install the Tiller Service","content":"Helm is the package management tool of choice for Kubernetes. Helm “charts” provide templating syntax for Kubernetes YAML manifest documents. With Helm we can create configurable deployments instead of just using static files. For more information about creating your own catalog of deployments, check out the docs at https://helm.sh/. To be able to use Helm, the server-side component tiller needs to be installed on your cluster.\n\nFor systems without direct internet access, see Helm - Air Gap for install details.\n\nRefer to the Helm version requirements to choose a version of Helm to install Rancher.\n\n\nNote: The installation instructions assume you are using Helm 2. The instructions will be updated for Helm 3 soon. In the meantime, if you want to use Helm 3, refer to these instructions.\n\n\nInstall Tiller on the Cluster\n\n\nImportant: Due to an issue with Helm v2.12.0 and cert-manager, please use Helm v2.12.1 or higher.\n\n\nHelm installs the tiller service on your cluster to manage charts. Since RKE enables RBAC by default we will need to use kubectl to create a serviceaccount and clusterrolebinding so tiller has permission to deploy to the cluster.\n\n\nCreate the ServiceAccount in the kube-system namespace.\nCreate the ClusterRoleBinding to give the tiller account access to the cluster.\nFinally use helm to install the tiller service\n\nkubectl -n kube-system create serviceaccount tiller\n\nkubectl create clusterrolebinding tiller \\\n  --clusterrole=cluster-admin \\\n  --serviceaccount=kube-system:tiller\n\nhelm init --service-account tiller\n\n# Users in China: You will need to specify a specific tiller-image in order to initialize tiller. \n# The list of tiller image tags are available here: https://dev.aliyun.com/detail.html?spm=5176.1972343.2.18.ErFNgC&repoId=62085. \n# When initializing tiller, you'll need to pass in --tiller-image\n\nhelm init --service-account tiller \\\n--tiller-image registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:<tag>\n\nNote: Thistillerinstall has full cluster access, which should be acceptable if the cluster is dedicated to Rancher server. Check out the helm docs for restricting tiller access to suit your security requirements.\n\n\nTest your Tiller installation\n\nRun the following command to verify the installation of tiller on your cluster:\n\nkubectl -n kube-system  rollout status deploy/tiller-deploy\nWaiting for deployment \"tiller-deploy\" rollout to finish: 0 of 1 updated replicas are available...\ndeployment \"tiller-deploy\" successfully rolled out\n\n\nAnd run the following command to validate Helm can talk to the tiller service:\n\nhelm version\nClient: &version.Version{SemVer:\"v2.12.1\", GitCommit:\"02a47c7249b1fc6d8fd3b94e6b4babf9d818144e\", GitTreeState:\"clean\"}\nServer: &version.Version{SemVer:\"v2.12.1\", GitCommit:\"02a47c7249b1fc6d8fd3b94e6b4babf9d818144e\", GitTreeState:\"clean\"}\n\n\nIssues or errors?\n\nSee the Troubleshooting page.\n\nNext: Install Rancher\n","postref":"6f559ac4b547bbf29f3d692d6114c8be","objectID":"01589898066df3093683adfd911e7944","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/helm2/helm-init/"},{"anchor":"#","title":"2. Collect and Publish Images to your Private Registry","content":"\nPrerequisites: You must have a private registry available to use.\n\nNote: Populating the private registry with images is the same process for HA and Docker installations, the differences in this section is based on whether or not you are planning to provision a Windows cluster or not.\n\n\nBy default, all images used to provision Kubernetes clusters or launch any tools in Rancher, e.g. monitoring, pipelines, alerts, are pulled from Docker Hub. In an air gap installation of Rancher, you will need a private registry that is located somewhere accessible by your Rancher server. Then, you will load the registry with all the images.\n\nThis section describes how to set up your private registry so that when you install Rancher, Rancher will pull all the required images from this registry.\n\nBy default, we provide the steps of how to populate your private registry assuming you are provisioning Linux only clusters, but if you plan on provisioning any Windows clusters, there are separate instructions to support the images needed for a Windows cluster.\n\n\n  \n  \n  For Rancher servers that will only provision Linux clusters, these are the steps to populate your private registry.\n\nA. Find the required assets for your Rancher version \nB. Collect all the required images \nC. Save the images to your workstation \nD. Populate the private registry\n\nPrerequisites\n\nThese steps expect you to use a Linux workstation that has internet access, access to your private registry, and at least 20 GB of disk space.\n\nA. Find the required assets for your Rancher version\n\n\nBrowse to our releases page and find the Rancher v2.x.x release that you want to install. Don’t download releases marked rc or Pre-release, as they are not stable for production environments.\n\nFrom the release’s Assets section (pictured above), download the following files, which are required to install Rancher in an air gap environment:\n\n\n\n\n\nRelease File\nDescription\n\n\n\n\n\nrancher-images.txt\nThis file contains a list of images needed to install Rancher, provision clusters and user Rancher tools.\n\n\n\nrancher-save-images.sh\nThis script pulls all the images in the rancher-images.txt from Docker Hub and saves all of the images as rancher-images.tar.gz.\n\n\n\nrancher-load-images.sh\nThis script loads images from the rancher-images.tar.gz file and pushes them to your private registry.\n\n\n\n\nB. Collect all the required images (For Kubernetes Installs using Rancher Generated Self-Signed Certificate)\n\nIn a Kubernetes Install, if you elect to use the Rancher default self-signed TLS certificates, you must add the cert-manager image to rancher-images.txt as well. You skip this step if you are using you using your own certificates.\n\n\nFetch the latest cert-manager Helm chart and parse the template for image details:\n\n\nNote: Recent changes to cert-manager require an upgrade. If you are upgrading Rancher and using a version of cert-manager older than v0.12.0, please see our upgrade documentation.\n\nhelm repo add jetstack https://charts.jetstack.io\nhelm repo update\nhelm fetch jetstack/cert-manager --version v0.12.0\nhelm template ./cert-manager-<version>.tgz | grep -oP '(?<=image: \").*(?=\")' >> ./rancher-images.txt\n\nSort and unique the images list to remove any overlap between the sources:\nsort -u rancher-images.txt -o rancher-images.txt\n\n\nC. Save the images to your workstation\n\n\nMake rancher-save-images.sh an executable:\n\nchmod +x rancher-save-images.sh\n\n\nRun rancher-save-images.sh with the rancher-images.txt image list to create a tarball of all the required images:\n./rancher-save-images.sh --image-list ./rancher-images.txt\nResult: Docker begins pulling the images used for an air gap install. Be patient. This process takes a few minutes. When the process completes, your current directory will output a tarball named rancher-images.tar.gz. Check that the output is in the directory.\n\n\nD. Populate the private registry\n\nMove the images in the rancher-images.tar.gz to your private registry using the scripts to load the images. The rancher-images.txt is expected to be on the workstation in the same directory that you are running the rancher-load-images.sh script.\n\n\nLog into your private registry if required:\nplain\ndocker login <REGISTRY.YOURDOMAIN.COM:PORT>\n\n\nMake rancher-load-images.sh an executable:\n\nchmod +x rancher-load-images.sh\n\n\nUse rancher-load-images.sh to extract, tag and push rancher-images.txt and rancher-images.tar.gz to your private registry:\n ./rancher-load-images.sh --image-list ./rancher-images.txt --registry <REGISTRY.YOURDOMAIN.COM:PORT>\n\n\n\n\n\n  Available as of v2.3.0\n\nFor Rancher servers that will provision Linux and Windows clusters, there are distinctive steps to populate your private registry for the Windows images and the Linux images. Since a Windows cluster is a mix of Linux and Windows nodes, the Linux images pushed into the private registry are manifests.\n\nWindows Steps\n\nThe Windows images need to be collected and pushed from a Windows server workstation.\n\nA. Find the required assets for your Rancher version \nB. Save the images to your Windows Server workstation \nC. Prepare the Docker daemon \nD. Populate the private registry\n\n\n  \n  Collecting and Populating Windows Images into the Private Registry\n  \n    Prerequisites\n\nThese steps expect you to use a Windows Server 1809 workstation that has internet access, access to your private registry, and at least 50 GB of disk space.\n\nThe workstation must have Docker 18.02+ in order to support manifests, which are required when provisioning Windows clusters.\n\nA. Find the required assets for your Rancher version\n\n\nBrowse to our releases page and find the Rancher v2.x.x release that you want to install. Don’t download releases marked rc or Pre-release, as they are not stable for production environments.\n\nFrom the release’s “Assets” section, download the following files:\n\n\n| Release File                 | Description                                                                                                                                          |\n   | —————————- | —————————————————————————————————————————————————- |\n   | rancher-windows-images.txt | This file contains a list of Windows images needed to provision Windows clusters.                                                                    |\n   | rancher-save-images.ps1    | This script pulls all the images in the rancher-windows-images.txt from Docker Hub and saves all of the images as rancher-windows-images.tar.gz. |\n   | rancher-load-images.ps1    | This script loads the images from the rancher-windows-images.tar.gz file and pushes them to your private registry.                                 |\n\nB. Save the images to your Windows Server workstation\n\n\nUsing powershell, go to the directory that has the files that were downloaded in the previous step.\n\nRun rancher-save-images.ps1 to create a tarball of all the required images:\n\n   ./rancher-save-images.ps1\nStep Result: Docker begins pulling the images used for an air gap install. Be patient. This process takes a few minutes. When the process completes, your current directory will output a tarball named rancher-windows-images.tar.gz. Check that the output is in the directory.\n\nC. Prepare the Docker daemon\n\n\nAppend your private registry address to the allow-nondistributable-artifacts config field in the Docker daemon (C:\\ProgramData\\Docker\\config\\daemon.json). Since the base image of Windows images are maintained by the mcr.microsoft.com registry, this step is required as the layers in the Microsoft registry are missing from Docker Hub and need to be pulled into the private registry.\n\n   {\n     ...\n     \"allow-nondistributable-artifacts\": [\n       ...\n       \"<REGISTRY.YOURDOMAIN.COM:PORT>\"\n     ]\n     ...\n   }\nD. Populate the private registry\n\nMove the images in the rancher-windows-images.tar.gz to your private registry using the scripts to load the images. The rancher-windows-images.txt is expected to be on the workstation in the same directory that you are running the rancher-load-images.ps1 script.\n\n\nUsing powershell, log into your private registry if required:\n\n   docker login <REGISTRY.YOURDOMAIN.COM:PORT>\n\nUsing powershell, use rancher-load-images.ps1 to extract, tag and push the images from rancher-images.tar.gz to your private registry:\n\n   ./rancher-load-images.ps1 --registry <REGISTRY.YOURDOMAIN.COM:PORT>\n  \n\n\nLinux Steps\n\nThe Linux images needs to be collected and pushed from a Linux host, but must be done after populating the Windows images into the private registry. These step are different from the Linux only steps as the Linux images that are pushed will actually manifests that support Windows and Linux images.\n\nA. Find the required assets for your Rancher version \nB. Collect all the required images \nC. Save the images to your Linux workstation \nD. Populate the private registry\n\n\n  \n  Collecting and Populating Linux Images into the Private Registry\n  \n    Prerequisites\n\nYou must populate the private registry with the Windows images before popu","postref":"cb91a1493882f3385298b9866eb2519f","objectID":"879ff954b30269505d01ea65a8921f4e","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/air-gap-helm2/populate-private-registry/"},{"anchor":"#","title":"2. Collect and Publish Images to your Private Registry","content":"\nPrerequisites: You must have a private registry available to use.\n\nNote: Populating the private registry with images is the same process for HA and Docker installations, the differences in this section is based on whether or not you are planning to provision a Windows cluster or not.\n\n\nBy default, all images used to provision Kubernetes clusters or launch any tools in Rancher, e.g. monitoring, pipelines, alerts, are pulled from Docker Hub. In an air gap installation of Rancher, you will need a private registry that is located somewhere accessible by your Rancher server. Then, you will load the registry with all the images.\n\nThis section describes how to set up your private registry so that when you install Rancher, Rancher will pull all the required images from this registry.\n\nBy default, we provide the steps of how to populate your private registry assuming you are provisioning Linux only clusters, but if you plan on provisioning any Windows clusters, there are separate instructions to support the images needed for a Windows cluster.\n\n\n  \n  \n  For Rancher servers that will only provision Linux clusters, these are the steps to populate your private registry.\n\nA. Find the required assets for your Rancher version \nB. Collect all the required images \nC. Save the images to your workstation \nD. Populate the private registry\n\nPrerequisites\n\nThese steps expect you to use a Linux workstation that has internet access, access to your private registry, and at least 20 GB of disk space.\n\nA. Find the required assets for your Rancher version\n\n\nBrowse to our releases page and find the Rancher v2.x.x release that you want to install. Don’t download releases marked rc or Pre-release, as they are not stable for production environments.\n\nFrom the release’s Assets section (pictured above), download the following files, which are required to install Rancher in an air gap environment:\n\n\n\n\n\nRelease File\nDescription\n\n\n\n\n\nrancher-images.txt\nThis file contains a list of images needed to install Rancher, provision clusters and user Rancher tools.\n\n\n\nrancher-save-images.sh\nThis script pulls all the images in the rancher-images.txt from Docker Hub and saves all of the images as rancher-images.tar.gz.\n\n\n\nrancher-load-images.sh\nThis script loads images from the rancher-images.tar.gz file and pushes them to your private registry.\n\n\n\n\nB. Collect all the required images (For Kubernetes Installs using Rancher Generated Self-Signed Certificate)\n\nIn a Kubernetes Install, if you elect to use the Rancher default self-signed TLS certificates, you must add the cert-manager image to rancher-images.txt as well. You skip this step if you are using you using your own certificates.\n\n\nFetch the latest cert-manager Helm chart and parse the template for image details:\n\n\nNote: Recent changes to cert-manager require an upgrade. If you are upgrading Rancher and using a version of cert-manager older than v0.12.0, please see our upgrade documentation.\n\nhelm repo add jetstack https://charts.jetstack.io\nhelm repo update\nhelm fetch jetstack/cert-manager --version v0.12.0\nhelm template ./cert-manager-<version>.tgz | grep -oP '(?<=image: \").*(?=\")' >> ./rancher-images.txt\n\nSort and unique the images list to remove any overlap between the sources:\nsort -u rancher-images.txt -o rancher-images.txt\n\n\nC. Save the images to your workstation\n\n\nMake rancher-save-images.sh an executable:\n\nchmod +x rancher-save-images.sh\n\n\nRun rancher-save-images.sh with the rancher-images.txt image list to create a tarball of all the required images:\n./rancher-save-images.sh --image-list ./rancher-images.txt\nResult: Docker begins pulling the images used for an air gap install. Be patient. This process takes a few minutes. When the process completes, your current directory will output a tarball named rancher-images.tar.gz. Check that the output is in the directory.\n\n\nD. Populate the private registry\n\nMove the images in the rancher-images.tar.gz to your private registry using the scripts to load the images.\n\nThe rancher-images.txt is expected to be on the workstation in the same directory that you are running the rancher-load-images.sh script. The rancher-images.tar.gz should also be in the same directory.\n\n\nLog into your private registry if required:\ndocker login <REGISTRY.YOURDOMAIN.COM:PORT>\n\nMake rancher-load-images.sh an executable:\n\nchmod +x rancher-load-images.sh\n\n\nUse rancher-load-images.sh to extract, tag and push rancher-images.txt and rancher-images.tar.gz to your private registry:\n./rancher-load-images.sh --image-list ./rancher-images.txt --registry <REGISTRY.YOURDOMAIN.COM:PORT>\n\n\n\n\n\n  Available as of v2.3.0\n\nFor Rancher servers that will provision Linux and Windows clusters, there are distinctive steps to populate your private registry for the Windows images and the Linux images. Since a Windows cluster is a mix of Linux and Windows nodes, the Linux images pushed into the private registry are manifests.\n\nWindows Steps\n\nThe Windows images need to be collected and pushed from a Windows server workstation.\n\nA. Find the required assets for your Rancher version \nB. Save the images to your Windows Server workstation \nC. Prepare the Docker daemon \nD. Populate the private registry\n\n\n  \n  Collecting and Populating Windows Images into the Private Registry\n  \n    Prerequisites\n\nThese steps expect you to use a Windows Server 1809 workstation that has internet access, access to your private registry, and at least 50 GB of disk space.\n\nThe workstation must have Docker 18.02+ in order to support manifests, which are required when provisioning Windows clusters.\n\nA. Find the required assets for your Rancher version\n\n\nBrowse to our releases page and find the Rancher v2.x.x release that you want to install. Don’t download releases marked rc or Pre-release, as they are not stable for production environments.\n\nFrom the release’s “Assets” section, download the following files:\n\n\n| Release File                 | Description                                                                                                                                          |\n   | —————————- | —————————————————————————————————————————————————- |\n   | rancher-windows-images.txt | This file contains a list of Windows images needed to provision Windows clusters.                                                                    |\n   | rancher-save-images.ps1    | This script pulls all the images in the rancher-windows-images.txt from Docker Hub and saves all of the images as rancher-windows-images.tar.gz. |\n   | rancher-load-images.ps1    | This script loads the images from the rancher-windows-images.tar.gz file and pushes them to your private registry.                                 |\n\nB. Save the images to your Windows Server workstation\n\n\nUsing powershell, go to the directory that has the files that were downloaded in the previous step.\n\nRun rancher-save-images.ps1 to create a tarball of all the required images:\n\n   ./rancher-save-images.ps1\nStep Result: Docker begins pulling the images used for an air gap install. Be patient. This process takes a few minutes. When the process completes, your current directory will output a tarball named rancher-windows-images.tar.gz. Check that the output is in the directory.\n\nC. Prepare the Docker daemon\n\n\nAppend your private registry address to the allow-nondistributable-artifacts config field in the Docker daemon (C:\\ProgramData\\Docker\\config\\daemon.json). Since the base image of Windows images are maintained by the mcr.microsoft.com registry, this step is required as the layers in the Microsoft registry are missing from Docker Hub and need to be pulled into the private registry.\n\n   {\n     ...\n     \"allow-nondistributable-artifacts\": [\n       ...\n       \"<REGISTRY.YOURDOMAIN.COM:PORT>\"\n     ]\n     ...\n   }\nD. Populate the private registry\n\nMove the images in the rancher-windows-images.tar.gz to your private registry using the scripts to load the images.\n\nThe rancher-windows-images.txt is expected to be on the workstation in the same directory that you are running the rancher-load-images.ps1 script. The rancher-windows-images.tar.gz should also be in the same directory.\n\n\nUsing powershell, log into your private registry if required:\n\n   docker login <REGISTRY.YOURDOMAIN.COM:PORT>\n\nUsing powershell, use rancher-load-images.ps1 to extract, tag and push the images from rancher-images.tar.gz to your private registry:\n\n   ./rancher-load-images.ps1 --registry <REGISTRY.YOURDOMAIN.COM:PORT>\n  \n\n\nLinux Steps\n\nThe Linux images needs to be collected and pushed from a Linux host, but must be done after populating the Windows images into the private registry. These step are different from the Linux only steps as the Linux images that are pushed will actually manifests that support Windows and Linux images.\n\nA. Find the required assets for your Rancher version \nB. Collect all the required images \nC. Save the images to your Linux workstation \nD. Populate the private registry\n\n\n  \n  Collecting and Populating Linux","postref":"57393d19246ec030ab51ba11ee0fea94","objectID":"c38ed8b6be3d6928fcc2396f6f72f08f","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/other-installation-methods/air-gap/populate-private-registry/"},{"anchor":"#","title":"3. Install Rancher on the Kubernetes Cluster","content":"Rancher is installed using the Helm package manager for Kubernetes. Helm charts provide templating syntax for Kubernetes YAML manifest documents.\n\nWith Helm, we can create configurable deployments instead of just using static files. For more information about creating your own catalog of deployments, check out the docs at https://helm.sh/.\n\nFor systems without direct internet access, see Air Gap: Kubernetes install.\n\nTo choose a Rancher version to install, refer to Choosing a Rancher Version.\n\nTo choose a version of Helm to install Rancher with, refer to the Helm version requirements\n\n\nNote: The installation instructions assume you are using Helm 3. For migration of installs started with Helm 2, refer to the official Helm 2 to 3 migration docs. This section provides a copy of the older installation instructions for Rancher installed on Kubernetes with Helm 2, and it is intended to be used if upgrading to Helm 3 is not feasible.\n\n\nInstall Helm\n\nHelm requires a simple CLI tool to be installed. Refer to the instructions provided by the Helm project for your specific platform.\n\nAdd the Helm Chart Repository\n\nUse helm repo add command to add the Helm chart repository that contains charts to install Rancher. For more information about the repository choices and which is best for your use case, see Choosing a Version of Rancher.\n\n\n  \n    \n      \n      Latest: Recommended for trying out the newest features\n    \n  \n  \n    \n      \n      Stable: Recommended for production environments\n    \n  \n  \n    \n      \n      Alpha: Experimental preview of upcoming releases.\n      \n      Note: Upgrades are not supported to, from, or between Alphas.\n    \n  \n\n\n\nhelm repo add rancher-<CHART_REPO> https://releases.rancher.com/server-charts/<CHART_REPO>\n\n\nCreate a Namespace for Rancher\n\nWe’ll need to define a namespace where the resources created by the Chart should be installed. This should always be cattle-system:\n\nkubectl create namespace cattle-system\n\n\nChoose your SSL Configuration\n\nRancher Server is designed to be secure by default and requires SSL/TLS configuration.\n\nThere are three recommended options for the source of the certificate.\n\n\nNote: If you want terminate SSL/TLS externally, see TLS termination on an External Load Balancer.\n\n\n\n\n\nConfiguration\nChart option\nDescription\nRequires cert-manager\n\n\n\n\n\nRancher Generated Certificates\ningress.tls.source=rancher\nUse certificates issued by Rancher’s generated CA (self signed)This is the default\nyes\n\n\n\nLet’s Encrypt\ningress.tls.source=letsEncrypt\nUse Let’s Encrypt to issue a certificate\nyes\n\n\n\nCertificates from Files\ningress.tls.source=secret\nUse your own certificate files by creating Kubernetes Secret(s)\nno\n\n\n\n\nOptional: Install cert-manager\n\nRancher relies on cert-manager to issue certificates from Rancher’s own generated CA or to request Let’s Encrypt certificates.\n\ncert-manager is only required for certificates issued by Rancher’s generated CA (ingress.tls.source=rancher) and Let’s Encrypt issued certificates (ingress.tls.source=letsEncrypt). You should skip this step if you are using your own certificate files (option ingress.tls.source=secret) or if you use TLS termination on an External Load Balancer.\n\n\n  \n  Click to Expand\n  \n    \nImportant:\nDue to an issue with Helm v2.12.0 and cert-manager, please use Helm v2.12.1 or higher.\n\nRecent changes to cert-manager require an upgrade. If you are upgrading Rancher and using a version of cert-manager older than v0.11.0, please see our upgrade documentation.\n\n\nThese instructions are adapted from the official cert-manager documentation.\n\n# Install the CustomResourceDefinition resources separately\nkubectl apply -f https://raw.githubusercontent.com/jetstack/cert-manager/release-0.12/deploy/manifests/00-crds.yaml\n\n> **Important:**\n> If you are running Kubernetes v1.15 or below, you will need to add the `--validate=false flag to your kubectl apply command above else you will receive a validation error relating to the x-kubernetes-preserve-unknown-fields field in cert-manager’s CustomResourceDefinition resources. This is a benign error and occurs due to the way kubectl performs resource validation.\n\n# Create the namespace for cert-manager\nkubectl create namespace cert-manager\n\n# Add the Jetstack Helm repository\nhelm repo add jetstack https://charts.jetstack.io\n\n# Update your local Helm chart repository cache\nhelm repo update\n\n# Install the cert-manager Helm chart\nhelm install \\\n  cert-manager jetstack/cert-manager \\\n  --namespace cert-manager \\\n  --version v0.12.0\n\n\nOnce you’ve installed cert-manager, you can verify it is deployed correctly by checking the cert-manager namespace for running pods:\n\nkubectl get pods --namespace cert-manager\n\nNAME                                       READY   STATUS    RESTARTS   AGE\ncert-manager-5c6866597-zw7kh               1/1     Running   0          2m\ncert-manager-cainjector-577f6d9fd7-tr77l   1/1     Running   0          2m\ncert-manager-webhook-787858fcdb-nlzsq      1/1     Running   0          2m\n\n\n  \n\n\n\nInstall Rancher with Helm and Your Chosen Certificate Option\n\n\n  \n  \n  \nNote: You need to have cert-manager installed before proceeding.\n\n\nThe default is for Rancher to generate a CA and uses cert-manager to issue the certificate for access to the Rancher server interface. Because rancher is the default option for ingress.tls.source, we are not specifying ingress.tls.source when running the helm install command.\n\n\nSet the hostname to the DNS name you pointed at your load balancer.\nIf you are installing an alpha version, Helm requires adding the --devel option to the command.\n\n\nhelm install rancher rancher-<CHART_REPO>/rancher \\\n  --namespace cattle-system \\\n  --set hostname=rancher.my.org\n\n\nWait for Rancher to be rolled out:\n\nkubectl -n cattle-system rollout status deploy/rancher\nWaiting for deployment \"rancher\" rollout to finish: 0 of 3 updated replicas are available...\ndeployment \"rancher\" successfully rolled out\n\n\n\n\n\n  \nNote: You need to have cert-manager installed before proceeding.\n\n\nThis option uses cert-manager to automatically request and renew Let’s Encrypt certificates. This is a free service that provides you with a valid certificate as Let’s Encrypt is a trusted CA. This configuration uses HTTP validation (HTTP-01) so the load balancer must have a public DNS record and be accessible from the internet.\n\n\nSet hostname to the public DNS record, set ingress.tls.source to letsEncrypt and letsEncrypt.email to the email address used for communication about your certificate (for example, expiry notices)\nIf you are installing an alpha version, Helm requires adding the --devel option to the command.\n\n\nhelm install rancher rancher-<CHART_REPO>/rancher \\\n  --namespace cattle-system \\\n  --set hostname=rancher.my.org \\\n  --set ingress.tls.source=letsEncrypt \\\n  --set letsEncrypt.email=me@example.org\n\n\nWait for Rancher to be rolled out:\n\nkubectl -n cattle-system rollout status deploy/rancher\nWaiting for deployment \"rancher\" rollout to finish: 0 of 3 updated replicas are available...\ndeployment \"rancher\" successfully rolled out\n\n\n\n\n\n  Create Kubernetes secrets from your own certificates for Rancher to use.\n\n\nNote: The Common Name or a Subject Alternative Names entry in the server certificate must match the hostname option, or the ingress controller will fail to configure correctly. Although an entry in the Subject Alternative Names is technically required, having a matching Common Name maximizes compatibility with older browsers/applications. If you want to check if your certificates are correct, see How do I check Common Name and Subject Alternative Names in my server certificate?\n\n\n\nSet hostname and set ingress.tls.source to secret.\nIf you are using a Private CA signed certificate , add --set privateCA=true to the command shown below.\nIf you are installing an alpha version, Helm requires adding the --devel option to the command.\n\n\nhelm install rancher rancher-<CHART_REPO>/rancher \\\n  --namespace cattle-system \\\n  --set hostname=rancher.my.org \\\n  --set ingress.tls.source=secret\n\n\nNow that Rancher is deployed, see Adding TLS Secrets to publish the certificate files so Rancher and the ingress controller can use them.\n\nAfter adding the secrets, check if Rancher was rolled out successfully:\n\nkubectl -n cattle-system rollout status deploy/rancher\nWaiting for deployment \"rancher\" rollout to finish: 0 of 3 updated replicas are available...\ndeployment \"rancher\" successfully rolled out\n\n\nIf you see the following error: error: deployment \"rancher\" exceeded its progress deadline, you can check the status of the deployment by running the following command:\n\nkubectl -n cattle-system get deploy rancher\nNAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nrancher   3         3         3            3           3m\n\n\nIt should show the same count for DESIRED and AVAILABLE.\n\n\n\n\n\n\nAdvanced Configurations\n\nThe Rancher chart configuration has many options for customizing the install to suit your specific environment. Here are some common ad","postref":"8b63401440f06751ed1a5fe8d045a1b8","objectID":"9b764ea65102b4eafb7c2dc0919bbdba","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/k8s-install/helm-rancher/"},{"anchor":"#","title":"4.  Install Rancher","content":"Rancher installation is managed using the Helm package manager for Kubernetes.  Use helm to install the prerequisite and charts to install Rancher.\n\nFor systems without direct internet access, see Air Gap: Kubernetes install.\n\nRefer to the Helm version requirements to choose a version of Helm to install Rancher.\n\n\nNote: The installation instructions assume you are using Helm 2. The instructions will be updated for Helm 3 soon. In the meantime, if you want to use Helm 3, refer to these instructions.\n\n\nAdd the Helm Chart Repository\n\nUse helm repo add command to add the Helm chart repository that contains charts to install Rancher. For more information about the repository choices and which is best for your use case, see Choosing a Version of Rancher.\n\n\n  \n    \n      \n      Latest: Recommended for trying out the newest features\n    \n  \n  \n    \n      \n      Stable: Recommended for production environments\n    \n  \n  \n    \n      \n      Alpha: Experimental preview of upcoming releases.\n      \n      Note: Upgrades are not supported to, from, or between Alphas.\n    \n  \n\n\n\nhelm repo add rancher-<CHART_REPO> https://releases.rancher.com/server-charts/<CHART_REPO>\n\n\nChoose your SSL Configuration\n\nRancher Server is designed to be secure by default and requires SSL/TLS configuration.\n\nThere are three recommended options for the source of the certificate.\n\n\nNote: If you want terminate SSL/TLS externally, see TLS termination on an External Load Balancer.\n\n\n\n\n\nConfiguration\nChart option\nDescription\nRequires cert-manager\n\n\n\n\n\nRancher Generated Certificates\ningress.tls.source=rancher\nUse certificates issued by Rancher’s generated CA (self signed)This is the default\nyes\n\n\n\nLet’s Encrypt\ningress.tls.source=letsEncrypt\nUse Let’s Encrypt to issue a certificate\nyes\n\n\n\nCertificates from Files\ningress.tls.source=secret\nUse your own certificate files by creating Kubernetes Secret(s)\nno\n\n\n\n\nOptional: Install cert-manager\n\nNote: cert-manager is only required for certificates issued by Rancher’s generated CA (ingress.tls.source=rancher) and Let’s Encrypt issued certificates (ingress.tls.source=letsEncrypt). You should skip this step if you are using your own certificate files (option ingress.tls.source=secret) or if you use TLS termination on an External Load Balancer.\n\n\nImportant:\nDue to an issue with Helm v2.12.0 and cert-manager, please use Helm v2.12.1 or higher.\n\nRecent changes to cert-manager require an upgrade. If you are upgrading Rancher and using a version of cert-manager older than v0.12.0, please see our upgrade documentation.\n\n\nRancher relies on cert-manager to issue certificates from Rancher’s own generated CA or to request Let’s Encrypt certificates.\n\nThese instructions are adapted from the official cert-manager documentation.\n\n\nInstall the CustomResourceDefinition resources separately\nkubectl apply -f https://raw.githubusercontent.com/jetstack/cert-manager/release-0.9/deploy/manifests/00-crds.yaml\n\nCreate the namespace for cert-manager\nkubectl create namespace cert-manager\n\nLabel the cert-manager namespace to disable resource validation\nkubectl label namespace cert-manager certmanager.k8s.io/disable-validation=true\n\nAdd the Jetstack Helm repository\nhelm repo add jetstack https://charts.jetstack.io\n\nUpdate your local Helm chart repository cache\nhelm repo update\n\nInstall the cert-manager Helm chart\nhelm install \\\n  --name cert-manager \\\n  --namespace cert-manager \\\n  --version v0.12.0 \\\n  jetstack/cert-manager\n\n\nOnce you’ve installed cert-manager, you can verify it is deployed correctly by checking the cert-manager namespace for running pods:\n\nkubectl get pods --namespace cert-manager\n\nNAME                                            READY   STATUS      RESTARTS   AGE\ncert-manager-7cbdc48784-rpgnt                   1/1     Running     0          3m\ncert-manager-webhook-5b5dd6999-kst4x            1/1     Running     0          3m\ncert-manager-cainjector-3ba5cd2bcd-de332x       1/1     Running     0          3m\n\n\nIf the ‘webhook’ pod (2nd line) is in a ContainerCreating state, it may still be waiting for the Secret to be mounted into the pod. Wait a couple of minutes for this to happen but if you experience problems, please check the troubleshooting guide.\n\n\n\nRancher Generated Certificates\n\n\nNote: You need to have cert-manager installed before proceeding.\n\n\nThe default is for Rancher to generate a CA and uses cert-manager to issue the certificate for access to the Rancher server interface. Because rancher is the default option for ingress.tls.source, we are not specifying ingress.tls.source when running the helm install command.\n\n\nSet the hostname to the DNS name you pointed at your load balancer.\nIf you are installing an alpha version, Helm requires adding the --devel option to the command.\n\n\nhelm install rancher-<CHART_REPO>/rancher \\\n  --name rancher \\\n  --namespace cattle-system \\\n  --set hostname=rancher.my.org\n\n\nWait for Rancher to be rolled out:\n\nkubectl -n cattle-system rollout status deploy/rancher\nWaiting for deployment \"rancher\" rollout to finish: 0 of 3 updated replicas are available...\ndeployment \"rancher\" successfully rolled out\n\n\nLet’s Encrypt\n\n\nNote: You need to have cert-manager installed before proceeding.\n\n\nThis option uses cert-manager to automatically request and renew Let’s Encrypt certificates. This is a free service that provides you with a valid certificate as Let’s Encrypt is a trusted CA. This configuration uses HTTP validation (HTTP-01) so the load balancer must have a public DNS record and be accessible from the internet.\n\n\nSet hostname to the public DNS record, set ingress.tls.source to letsEncrypt and letsEncrypt.email to the email address used for communication about your certificate (for example, expiry notices)\nIf you are installing an alpha version, Helm requires adding the --devel option to the command.\n\n\nhelm install rancher-<CHART_REPO>/rancher \\\n  --name rancher \\\n  --namespace cattle-system \\\n  --set hostname=rancher.my.org \\\n  --set ingress.tls.source=letsEncrypt \\\n  --set letsEncrypt.email=me@example.org\n\n\nWait for Rancher to be rolled out:\n\nkubectl -n cattle-system rollout status deploy/rancher\nWaiting for deployment \"rancher\" rollout to finish: 0 of 3 updated replicas are available...\ndeployment \"rancher\" successfully rolled out\n\n\nCertificates from Files\n\nCreate Kubernetes secrets from your own certificates for Rancher to use.\n\n\nNote: The Common Name or a Subject Alternative Names entry in the server certificate must match the hostname option, or the ingress controller will fail to configure correctly. Although an entry in the Subject Alternative Names is technically required, having a matching Common Name maximizes compatibility with older browsers/applications. If you want to check if your certificates are correct, see How do I check Common Name and Subject Alternative Names in my server certificate?\n\n\n\nSet hostname and set ingress.tls.source to secret.\nIf you are using a Private CA signed certificate , add --set privateCA=true to the command shown below.\nIf you are installing an alpha version, Helm requires adding the --devel option to the command.\n\n\nhelm install rancher-<CHART_REPO>/rancher \\\n  --name rancher \\\n  --namespace cattle-system \\\n  --set hostname=rancher.my.org \\\n  --set ingress.tls.source=secret\n\n\nNow that Rancher is deployed, see Adding TLS Secrets to publish the certificate files so Rancher and the ingress controller can use them.\n\nAfter adding the secrets, check if Rancher was rolled out successfully:\n\nkubectl -n cattle-system rollout status deploy/rancher\nWaiting for deployment \"rancher\" rollout to finish: 0 of 3 updated replicas are available...\ndeployment \"rancher\" successfully rolled out\n\n\nIf you see the following error: error: deployment \"rancher\" exceeded its progress deadline, you can check the status of the deployment by running the following command:\n\nkubectl -n cattle-system get deploy rancher\nNAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nrancher   3         3         3            3           3m\n\n\nIt should show the same count for DESIRED and AVAILABLE.\n\nAdvanced Configurations\n\nThe Rancher chart configuration has many options for customizing the install to suit your specific environment. Here are some common advanced scenarios.\n\n\nHTTP Proxy\nPrivate Docker Image Registry\nTLS Termination on an External Load Balancer\n\n\nSee the Chart Options for the full list of options.\n\nSave your options\n\nMake sure you save the --set options you used. You will need to use the same options when you upgrade Rancher to new versions with Helm.\n\nFinishing Up\n\nThat’s it you should have a functional Rancher server. Point a browser at the hostname you picked and you should be greeted by the colorful login page.\n\nDoesn’t work? Take a look at the Troubleshooting Page\n","postref":"bbb0d6cdac74bbc04b2dff2b7f609e9d","objectID":"98e4649997d555be4e6a1c87170eea3a","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/helm2/helm-rancher/"},{"anchor":"#","title":"Deploying Workloads","content":"These guides walk you through the deployment of an application, including how to expose the application for use outside of the cluster.\n\n\nWorkload with Ingress\nWorkload with NodePort\n\n","postref":"ed7030c5f3edf05aa2e6e0f9c961525d","objectID":"4851b99035af25c31218329dab85649f","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/quick-start-guide/workload/"},{"anchor":"#elasticsearch-deployment-configuration","title":"Elasticsearch Deployment Configuration","content":"If your instance of Elasticsearch uses SSL, your Endpoint will need to begin with https://. With the correct endpoint, the SSL Configuration form is enabled and ready to be completed.\nProvide the Client Private Key and Client Certificate. You can either copy and paste them or upload them by using the Read from a file button.\n\n\nYou can use either a self-signed certificate or one provided by a certificate authority.\n\nYou can generate a self-signed certificate using an openssl command. For example:\n\n openssl req -x509 -newkey rsa:2048 -keyout myservice.key -out myservice.cert -days 365 -nodes -subj \"/CN=myservice.example.com\"\n\n\n\nEnter your Client Key Password.\n\nEnter your SSL Version. The default version is TLSv1_2.\n\nSelect whether or not you want to verify your SSL.\n\n\nIf you are using a self-signed certificate, select Enabled - Input trusted server certificate, provide the CA Certificate PEM. You can copy and paste the certificate or upload it using the Read from a file button.\nIf you are using a certificate from a certificate authority, select Enabled - Input trusted server certificate. You do not need to provide a CA Certificate PEM.\n\n\nIn the Endpoint field, enter the IP address and port of your Elasticsearch instance. You can find this information from the dashboard of your Elasticsearch deployment.\n\n\nElasticsearch usually uses port 9200 for HTTP and 9243 for HTTPS.\n\n\nIf you are using X-Pack Security, enter your Elasticsearch Username and Password for authentication.\n\nEnter an Index Pattern.\n","postref":"d1755de81529ba384b7870a7fb305abe","objectID":"337d9105cffdc65563235202b18df930","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/tools/logging/elasticsearch/"},{"anchor":"#cluster-level-options","title":"Cluster Level Options","content":"Cluster NameBy default, the name of your cluster will be local. If you want a different name, you would use the cluster_name directive to change the name of your cluster. The name will be set in your cluster’s generated kubeconfig file.cluster_name: myclusterSupported Docker VersionsBy default, RKE will check the installed Docker version on all hosts and fail with an error if the version is not supported by Kubernetes. The list of supported Docker versions are set specifically for each Kubernetes version. To override this behavior, set this option to true.The default value is false.ignore_docker_version: trueKubernetes VersionFor information on upgrading Kubernetes, refer to the upgrade section.Rolling back to previous Kubernetes versions is not supported.Prefix PathFor some operating systems including ROS, and CoreOS, RKE stores its resources to a different prefix path, this prefix path is by default for these operating systems is:/opt/rke\nSo /etc/kubernetes will be stored in /opt/rke/etc/kubernetes and /var/lib/etcd will be stored in /opt/rke/var/lib/etcd etc.To change the default prefix path for any cluster, you can use the following option in the cluster configuration file cluster.yml:prefix_path: /opt/custom_path\nCluster Level SSH Key PathRKE connects to host(s) using ssh. Typically, each node will have an independent path for each ssh key, i.e. ssh_key_path, in the nodes section, but if you have a SSH key that is able to access all hosts in your cluster configuration file, you can set the path to that ssh key at the top level. Otherwise, you would set the ssh key path in the nodes.If ssh key paths are defined at the cluster level and at the node level, the node-level key will take precedence.ssh_key_path: ~/.ssh/testSSH AgentRKE supports using ssh connection configuration from a local ssh agent. The default value for this option is false. If you want to set using a local ssh agent, you would set this to true.ssh_agent_auth: trueIf you want to use an SSH private key with a passphrase, you will need to add your key to ssh-agent and have the environment variable SSH_AUTH_SOCK configured.$ eval \"$(ssh-agent -s)\"\nAgent pid 3975\n$ ssh-add /home/user/.ssh/id_rsa\nEnter passphrase for /home/user/.ssh/id_rsa:\nIdentity added: /home/user/.ssh/id_rsa (/home/user/.ssh/id_rsa)\n$ echo $SSH_AUTH_SOCK\n/tmp/ssh-118TMqxrXsEx/agent.3974\nAdd-ons Job TimeoutYou can define add-ons to be deployed after the Kubernetes cluster comes up, which uses Kubernetes jobs. RKE will stop attempting to retrieve the job status after the timeout, which is in seconds. The default timeout value is 30 seconds.","postref":"d6ac6995243dbf838eb0b3d69324116d","objectID":"10bfde84f74007c54938d80d588cb139","permalink":"http://jijeesh.github.io/docs/rke/latest/en/config-options/"},{"anchor":"#","title":"Provisioning Errors","content":"Failed to get job complete status\n\nMost common reason for this error is that a node is having issues that block the deploy job from completing successfully. See Get node conditions how to check node conditions.\n\nYou can also retrieve the log from the job to see if it has an indication of the error, make sure you replace rke-network-plugin-deploy-job with the job name from the error:\n\nExample command to get logs for error Failed to get job complete status for job rke-network-plugin-deploy-job:\n\nkubectl -n kube-system get pods -l job-name=rke-network-plugin-deploy-job --no-headers -o custom-columns=NAME:.metadata.name | xargs -L1 kubectl -n kube-system logs\n\n\nFailed to apply the ServiceAccount needed for job execution\n\nBecause this action requires connectivity from the host running rke up to the controlplane nodes, this is usually caused by incorrect proxy configuration on the host running rke up. The message printed after this error usually is the response from the proxy that is blocking the request. Please verify the HTTP_PROXY, HTTPS_PROXY and NO_PROXY environment variables are correctly configured, especially NO_PROXY if the host cannot reach the controlplane nodes via the configured proxy. (this IP range then needs to be added to NO_PROXY to make it work)\n","postref":"33026bc307bf93adff546b8c473f3327","objectID":"2461372a48895b700c31b58625297d72","permalink":"http://jijeesh.github.io/docs/rke/latest/en/troubleshooting/provisioning-errors/"},{"anchor":"#prerequisites","title":"Prerequisites","content":"\nFrom the quickstart/vagrant folder execute vagrant destroy -f.\n\nWait for the confirmation that all resources have been destroyed.\n\nClone Rancher Quickstart to a folder using git clone https://github.com/rancher/quickstart.\n\nGo into the folder containing the Vagrantfile by executing cd quickstart/vagrant.\n\nOptional: Edit config.yaml to:\n\n\nChange the number of nodes and the memory allocations, if required. (node.count, node.cpus, node.memory)\nChange the password of the admin user for logging into Rancher. (default_password)\n\n\nTo initiate the creation of the environment run, vagrant up.\n\nOnce provisioning finishes, go to https://172.22.101.101 in the browser. The default user/password is admin/admin.\nResult: Rancher Server and your Kubernetes cluster is installed on VirtualBox.What’s Next?Use Rancher to create a deployment. For more information, see Creating Deployments.\nVagrant: Vagrant is required as this is used to provision the machine based on the Vagrantfile.\nVirtualbox: The virtual machines that Vagrant provisions need to be provisioned to VirtualBox.\nAt least 4GB of free RAM.\n","postref":"b932edf31a90a0ea664693faaed9809f","objectID":"57719d9486f6fc27bef963024a335b4f","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/quick-start-guide/deployment/quickstart-vagrant/"},{"anchor":"#","title":"Workload with NodePort Quick Start","content":"Prerequisite\n\nYou have a running cluster with at least 1 node.\n\n1. Deploying a Workload\n\nYou’re ready to create your first workload. A workload is an object that includes pods along with other files and info needed to deploy your application.\n\nFor this workload, you’ll be deploying the application Rancher Hello-World.\n\n\nFrom the Clusters page, open the cluster that you just created.\n\nFrom the main menu of the Dashboard, select Projects/Namespaces.\n\nOpen the Project: Default project.\n\nClick Resources > Workloads. In versions prior to v2.3.0, click Workloads > Workloads.\n\nClick Deploy.\n\nStep Result: The Deploy Workload page opens.\n\nEnter a Name for your workload.\n\nFrom the Docker Image field, enter rancher/hello-world. This field is case-sensitive.\n\nFrom Port Mapping, click Add Port.\n\nFrom the As a drop-down, make sure that NodePort (On every node) is selected.\n\n\n\nFrom the On Listening Port field, leave the Random value in place.\n\n\n\nFrom the Publish the container port field, enter port 80.\n\n\n\nLeave the remaining options on their default setting. We’ll tell you about them later.\n\nClick Launch.\n\n\nResult:\n\n\nYour workload is deployed. This process might take a few minutes to complete.\nWhen your workload completes deployment, it’s assigned a state of Active. You can view this status from the project’s Workloads page.\n\n\n\n\n2. Viewing Your Application\n\nFrom the Workloads page, click the link underneath your workload. If your deployment succeeded, your application opens.\n\nAttention: Cloud-Hosted Sandboxes\n\nWhen using a cloud-hosted virtual machine, you may not have access to the port running the container. In this event, you can test Nginx in an ssh session on the local machine using Execute Shell. Use the port number after the : in the link under your workload if available, which is 31568 in this example.\ngettingstarted@rancher:~$ curl http://localhost:31568\n<!DOCTYPE html>\n<html>\n  <head>\n    <title>Rancher</title>\n    <link rel=\"icon\" href=\"img/favicon.png\">\n    <style>\n      body {\n        background-color: white;\n        text-align: center;\n        padding: 50px;\n        font-family: \"Open Sans\",\"Helvetica Neue\",Helvetica,Arial,sans-serif;\n      }\n      button {\n          background-color: #0075a8;\n          border: none;\n          color: white;\n          padding: 15px 32px;\n          text-align: center;\n          text-decoration: none;\n          display: inline-block;\n          font-size: 16px;\n      }\n\n      #logo {\n        margin-bottom: 40px;\n      }\n    </style>\n  </head>\n  <body>\n    <img id=\"logo\" src=\"img/rancher-logo.svg\" alt=\"Rancher logo\" width=400 />\n    <h1>Hello world!</h1>\n    <h3>My hostname is hello-world-66b4b9d88b-78bhx</h3>\n    <div id='Services'>\n      <h3>k8s services found 2</h3>\n\n      <b>INGRESS_D1E1A394F61C108633C4BD37AEDDE757</b> tcp://10.43.203.31:80<br />\n\n      <b>KUBERNETES</b> tcp://10.43.0.1:443<br />\n\n    </div>\n    <br />\n\n    <div id='rancherLinks' class=\"row social\">\n      <a class=\"p-a-xs\" href=\"https://rancher.com/docs\"><img src=\"img/favicon.png\" alt=\"Docs\" height=\"25\" width=\"25\"></a>\n      <a class=\"p-a-xs\" href=\"https://slack.rancher.io/\"><img src=\"img/icon-slack.svg\" alt=\"slack\" height=\"25\" width=\"25\"></a>\n      <a class=\"p-a-xs\" href=\"https://github.com/rancher/rancher\"><img src=\"img/icon-github.svg\" alt=\"github\" height=\"25\" width=\"25\"></a>\n      <a class=\"p-a-xs\" href=\"https://twitter.com/Rancher_Labs\"><img src=\"img/icon-twitter.svg\" alt=\"twitter\" height=\"25\" width=\"25\"></a>\n      <a class=\"p-a-xs\" href=\"https://www.facebook.com/rancherlabs/\"><img src=\"img/icon-facebook.svg\" alt=\"facebook\" height=\"25\" width=\"25\"></a>\n      <a class=\"p-a-xs\" href=\"https://www.linkedin.com/groups/6977008/profile\"><img src=\"img/icon-linkedin.svg\" height=\"25\" alt=\"linkedin\" width=\"25\"></a>\n    </div>\n    <br />\n    <button class='button' onclick='myFunction()'>Show request details</button>\n    <div id=\"reqInfo\" style='display:none'>\n      <h3>Request info</h3>\n      <b>Host:</b> 172.22.101.111:31411 <br />\n      <b>Pod:</b> hello-world-66b4b9d88b-78bhx </b><br />\n\n      <b>Accept:</b> [*/*]<br />\n\n      <b>User-Agent:</b> [curl/7.47.0]<br />\n\n    </div>\n    <br />\n    <script>\n      function myFunction() {\n          var x = document.getElementById(\"reqInfo\");\n          if (x.style.display === \"none\") {\n              x.style.display = \"block\";\n          } else {\n              x.style.display = \"none\";\n          }\n      }\n    </script>\n  </body>\n</html>\ngettingstarted@rancher:~$\nFinished\n\nCongratulations! You have successfully deployed a workload exposed via a NodePort.\n\nWhat’s Next?\n\nWhen you’re done using your sandbox, destroy the Rancher Server and your cluster. See one of the following:\n\n\nAmazon AWS: Destroying the Environment\nDigitalOcean: Destroying the Environment\nVagrant: Destroying the Environment\n\n","postref":"97396ded9bfa5a1babfc84ef6f7637bf","objectID":"3cad695ae7111988b35cc625b510e66e","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/quick-start-guide/workload/quickstart-deploy-workload-nodeport/"},{"anchor":"#","title":"Nodes","content":"The nodes directive is the only required section in the cluster.yml file. It’s used by RKE to specify cluster node(s), ssh credentials used to access the node(s) and which roles these nodes will be in the Kubernetes cluster.\n\nThis section covers the following topics:\n\n\nNode configuration example\nKubernetes roles\n\n\netcd\nControlplane\nWorker\n\nNode options\n\n\nAddress\nInternal address\nOverriding the hostname\nSSH port\nSSH users\nSSH key path\nSSH key\nSSH certificate path\nSSH certificate\nDocker socket\nLabels\nTaints\n\n\n\nNode Configuration Example\n\nThe following example shows node configuration in an example cluster.yml:\nnodes:\n    - address: 1.1.1.1\n      user: ubuntu\n      role:\n      - controlplane\n      - etcd\n      ssh_key_path: /home/user/.ssh/id_rsa\n      port: 2222\n    - address: 2.2.2.2\n      user: ubuntu\n      role:\n      - worker\n      ssh_key: |-\n        -----BEGIN RSA PRIVATE KEY-----\n\n        -----END RSA PRIVATE KEY-----\n    - address: 3.3.3.3\n      user: ubuntu\n      role:\n      - worker\n      ssh_key_path: /home/user/.ssh/id_rsa\n      ssh_cert_path: /home/user/.ssh/id_rsa-cert.pub\n    - address: 4.4.4.4\n      user: ubuntu\n      role:\n      - worker\n      ssh_key_path: /home/user/.ssh/id_rsa\n      ssh_cert: |-\n        ssh-rsa-cert-v01@openssh.com AAAAHHNza...\n      taints: # Available as of v0.3.0\n        - key: test-key\n          value: test-value\n          effect: NoSchedule\n    - address: example.com\n      user: ubuntu\n      role:\n      - worker\n      hostname_override: node3\n      internal_address: 192.168.1.6\n      labels:\n        app: ingress\nKubernetes Roles\n\nYou can specify the list of roles that you want the node to be as part of the Kubernetes cluster. Three roles are supported: controlplane, etcd and worker. Node roles are not mutually exclusive. It’s possible to assign any combination of roles to any node. It’s also possible to change a node’s role using the upgrade process.\n\n\nNote: Prior to v0.1.8, workloads/pods might have run on any nodes with worker or controlplane roles, but as of v0.1.8, they will only be deployed to any worker nodes.\n\n\netcd\n\nWith this role, the etcd container will be run on these nodes.  Etcd keeps the state of your cluster and is the most important component in your cluster, single source of truth of your cluster. Although you can run etcd on just one node, it typically takes 3, 5 or more nodes to create an HA configuration. Etcd is a distributed reliable key-value store which stores all Kubernetes state. Taint set on nodes with the etcd role is shown below:\n\n\n\n\nTaint Key\nTaint Value\nTaint Effect\n\n\n\n\n\nnode-role.kubernetes.io/etcd\ntrue\nNoExecute\n\n\n\n\nControlplane\n\nWith this role, the stateless components that are used to deploy Kubernetes will run on these nodes. These components are used to run the API server, scheduler, and controllers. Taint set on nodes with the controlplane role is shown below:\n\n\n\n\nTaint Key\nTaint Value\nTaint Effect\n\n\n\n\n\nnode-role.kubernetes.io/controlplane\ntrue\nNoSchedule\n\n\n\n\nWorker\n\nWith this role, any workloads or pods that are deployed will land on these nodes.\n\nNode Options\n\nWithin each node, there are multiple directives that can be used.\n\nAddress\n\nThe address directive will be used to set the hostname or IP address of the node. RKE must be able to connect to this address.\n\nInternal Address\n\nThe internal_address provides the ability to have nodes with multiple addresses set a specific address to use for inter-host communication on a private network. If the internal_address is not set, the address is used for inter-host communication.\n\nOverriding the Hostname\n\nThe hostname_override is used to be able to provide a friendly name for RKE to use when registering the node in Kubernetes. This hostname doesn’t need to be a routable address, but it must be a valid Kubernetes resource name. If the hostname_override isn’t set, then the address directive is used when registering the node in Kubernetes.\n\n\nNote: When cloud providers are configured, you may need to override the hostname in order to use the cloud provider correctly. There is an exception for the AWS cloud provider, where the hostname_override field will be explicitly ignored.\n\n\nSSH Port\n\nIn each node, you specify which port to be used when connecting to this node. The default port is 22.\n\nSSH Users\n\nFor each node, you specify the user to be used when connecting to this node. This user must be a member of the Docker group or allowed to write to the node’s Docker socket.\n\nSSH Key Path\n\nFor each node, you specify the path, i.e. ssh_key_path, for the SSH private key to be used when connecting to this node. The default key path for each node is ~/.ssh/id_rsa.\n\n\nNote: If you have a private key that can be used across all nodes, you can set the SSH key path at the cluster level. The SSH key path set in each node will always take precedence.\n\n\nSSH Key\n\nInstead of setting the path to the SSH key, you can alternatively specify the actual key, i.e. ssh_key, to be used to connect to the node.\n\nSSH Certificate Path\n\nFor each node, you can specify the path, i.e. ssh_cert_path, for the signed SSH certificate to be used when connecting to this node.\n\nSSH Certificate\n\nInstead of setting the path to the signed SSH certificate, you can alternatively specify the actual certificate, i.e. ssh_cert, to be used to connect to the node.\n\nDocker Socket\n\nIf the Docker socket is different than the default, you can set the docker_socket. The default is /var/run/docker.sock\n\nLabels\n\nYou have the ability to add an arbitrary map of labels for each node. It can be used when using the ingress controller’s node_selector option.\n\nTaints\n\nAvailable as of v0.3.0\n\nYou have the ability to add taints for each node.\n","postref":"e85bdbdcd4554d01e1c11545be98eb70","objectID":"727861dca19a4e1cb1e32eebac07bae4","permalink":"http://jijeesh.github.io/docs/rke/latest/en/config-options/nodes/"},{"anchor":"#","title":"Private Registries","content":"RKE supports the ability to configure multiple private Docker registries in the cluster.yml. By passing in your registry and credentials, it allows the nodes to pull images from these private registries.\nprivate_registries:\n    - url: registry.com\n      user: Username\n      password: password\n    - url: myregistry.com\n      user: myuser\n      password: mypassword\n\nNote: If you are using a Docker Hub registry, you can omit the url or set it to docker.io.\n\n\nDefault Registry\n\nAs of v0.1.10, RKE supports specifying a default registry from the list of private registries to be used with all system images . In this example .RKE will use registry.com as the default registry for all system images, e.g. rancher/rke-tools:v0.1.14 will become registry.com/rancher/rke-tools:v0.1.14.\nprivate_registries:\n    - url: registry.com\n      user: Username\n      password: password\n      is_default: true # All system images will be pulled using this registry. \nAir-gapped Setups\n\nBy default, all system images are being pulled from DockerHub. If you are on a system that does not have access to DockerHub, you will need to create a private registry that is populated with all the required system images.\n\nAs of v0.1.10, you have to configure your private registry credentials, but you can specify this registry as a default registry so that all system images are pulled from the designated private registry. You can use the command rke config --system-images to get the list of default system images to populate your private registry.\n\nPrior to v0.1.10, you had to configure your private registry credentials and update the names of all the system images in the cluster.yml so that the image names would have the private registry URL appended before each image name.\n","postref":"2c0b100a23ba67f954c0407ffbf51f89","objectID":"7aeed76db40535e87b61991ec6bc615f","permalink":"http://jijeesh.github.io/docs/rke/latest/en/config-options/private-registries/"},{"anchor":"#bastion-host-options","title":"Bastion Host Options","content":"AddressThe address directive will be used to set the hostname or IP address of the bastion host. RKE must be able to connect to this address.SSH PortYou specify which port to be used when connecting to the bastion host. The default port is 22.SSH UsersYou specify the user to be used when connecting to this node.SSH Key PathYou specify the path, i.e. ssh_key_path, for the SSH private key to be used when connecting to the bastion host.SSH KeyInstead of setting the path to the SSH key, you can specify the actual key, i.e. ssh_key, to be used to connect to the bastion host.SSH Certificate PathYou specify the path, i.e. ssh_cert_path, for the signed SSH certificate to be used when connecting to the bastion host.SSH CertificateInstead of setting the path to the signed SSH certificate, you can specify the actual certificate, i.e. ssh_cert, to be used to connect to the bastion host.","postref":"da79072742c38053ca7f383a351ce156","objectID":"add2efa10db08fdd4d80c1ec859261c0","permalink":"http://jijeesh.github.io/docs/rke/latest/en/config-options/bastion-host/"},{"anchor":"#","title":"System Images","content":"When RKE is deploying Kubernetes, there are several images that are pulled. These images are used as Kubernetes system components as well as helping to deploy these system components.\n\nAs of v0.1.6, the functionality of a couple of the system images were consolidated into a single rancher/rke-tools image to simplify and speed the deployment process.\n\nYou can configure the network plug-ins, ingress controller and dns provider as well as the options for these add-ons separately in the cluster.yml.\n\nBelow is an example of the list of system images used to deploy Kubernetes through RKE. The default versions of Kubernetes are tied to specific versions of system images.\n\n\nFor RKE v0.2.x and below, the map of versions and the system image versions is located here: https://github.com/rancher/types/blob/release/v2.2/apis/management.cattle.io/v3/k8s_defaults.go\n\nFor RKE v0.3.0 and above, the map of versions and the system image versions is located here: https://github.com/rancher/kontainer-driver-metadata/blob/master/rke/k8s_rke_system_images.go\n\n\n\nNote: As versions of RKE are released, the tags on these images will no longer be up to date. This list is specific for v1.10.3-rancher2.\n\nsystem_images:\n  etcd: rancher/coreos-etcd:v3.2.24\n  alpine: rancher/rke-tools:v0.1.24\n  nginx_proxy: rancher/rke-tools:v0.1.24\n  cert_downloader: rancher/rke-tools:v0.1.24\n  kubernetes: rancher/hyperkube:v1.13.1-rancher1\n  kubernetes_services_sidecar: rancher/rke-tools:v0.1.24\n  pod_infra_container: rancher/pause-amd64:3.1\n\n  # kube-dns images\n  kubedns: rancher/k8s-dns-kube-dns-amd64:1.15.0\n  dnsmasq: rancher/k8s-dns-dnsmasq-nanny-amd64:1.15.0\n  kubedns_sidecar: rancher/k8s-dns-sidecar-amd64:1.15.0\n  kubedns_autoscaler: rancher/cluster-proportional-autoscaler-amd64:1.0.0\n\n  # CoreDNS images\n  coredns: coredns/coredns:1.2.6\n  coredns_autoscaler: rancher/cluster-proportional-autoscaler-amd64:1.0.0\n\n  # Flannel images\n  flannel: rancher/coreos-flannel:v0.10.0\n  flannel_cni: rancher/coreos-flannel-cni:v0.3.0\n\n  # Calico images\n  calico_node: rancher/calico-node:v3.4.0\n  calico_cni: rancher/calico-cni:v3.4.0\n  calico_controllers: \"\"\n  calico_ctl: rancher/calico-ctl:v2.0.0\n\n  # Canal images\n  canal_node: rancher/calico-node:v3.4.0\n  canal_cni: rancher/calico-cni:v3.4.0\n  canal_flannel: rancher/coreos-flannel:v0.10.0\n\n  # Weave images\n  weave_node: weaveworks/weave-kube:2.5.0\n  weave_cni: weaveworks/weave-npc:2.5.0\n\n  # Ingress controller images\n  ingress: rancher/nginx-ingress-controller:0.21.0-rancher1\n  ingress_backend: rancher/nginx-ingress-controller-defaultbackend:1.4\n\n  # Metrics server image\n  metrics_server: rancher/metrics-server-amd64:v0.3.1\nPrior to v0.1.6, instead of using the rancher/rke-tools image, we used the following images:\nsystem_images:\n    alpine: alpine:latest\n    nginx_proxy: rancher/rke-nginx-proxy:v0.1.1\n    cert_downloader: rancher/rke-cert-deployer:v0.1.1\n    kubernetes_services_sidecar: rancher/rke-service-sidekick:v0.1.0\nAir-gapped Setups\n\nIf you have an air-gapped setup and cannot access docker.io, you will need to set up your private registry in your cluster configuration file. After you set up private registry, you will need to update these images to pull from your private registry.\n","postref":"2ef0af749de822838bf9ae888b9d03f2","objectID":"4b02423df99892b3ec9d9b06c3615533","permalink":"http://jijeesh.github.io/docs/rke/latest/en/config-options/system-images/"},{"anchor":"#","title":"Choosing a Rancher Version","content":"This section describes how to choose a Rancher version.\n\nFor a high-availability installation of Rancher, which is recommended for production, the Rancher server is installed using a Helm chart on a Kubernetes cluster. Refer to the Helm version requirements to choose a version of Helm to install Rancher.\n\nFor Docker installations of Rancher, which is used for development and testing, you will install Rancher as a Docker image.\n\n\n  \n  \n  When installing, upgrading, or rolling back Rancher Server when it is installed on a Kubernetes cluster, Rancher server is installed using a Helm chart on a Kubernetes cluster. Therefore, as you prepare to install or upgrade a high availability Rancher configuration, you must add a Helm chart repository that contains the charts for installing Rancher.\n\nRefer to the Helm version requirements to choose a version of Helm to install Rancher.\n\nHelm Chart Repositories\n\nRancher provides several different Helm chart repositories to choose from. We align our latest and stable Helm chart repositories with the Docker tags that are used for a Docker installation. Therefore, the rancher-latest repository will contain charts for all the Rancher versions that have been tagged as rancher/rancher:latest. When a Rancher version has been promoted to the rancher/rancher:stable, it will get added to the rancher-stable repository.\n\n\n\n\nType\nCommand to Add the Repo\nDescription of the Repo\n\n\n\n\n\nrancher-latest\nhelm repo add rancher-latest https://releases.rancher.com/server-charts/latest\nAdds a repository of Helm charts for the latest versions of Rancher. We recommend using this repo for testing out new Rancher builds.\n\n\n\nrancher-stable\nhelm repo add rancher-stable https://releases.rancher.com/server-charts/stable\nAdds a repository of Helm charts for older, stable versions of Rancher. We recommend using this repo for production environments.\n\n\n\nrancher-alpha\nhelm repo add rancher-alpha https://releases.rancher.com/server-charts/alpha\nAdds a repository of Helm charts for alpha versions of Rancher for previewing upcoming releases. These releases are discouraged in production environments. Upgrades to or from charts in the rancher-alpha repository to any other chart, regardless or repository, aren’t supported.\n\n\n\n\n\nInstructions on when to select these repos are available below in Switching to a Different Helm Chart Repository.\n\n\nNote: The introduction of the rancher-latest and rancher-stable Helm Chart repositories was introduced after Rancher v2.1.0, so the rancher-stable repository contains some Rancher versions that were never marked as rancher/rancher:stable. The versions of Rancher that were tagged as rancher/rancher:stable prior to v2.1.0 are v2.0.4, v2.0.6, v2.0.8. Post v2.1.0, all charts in the rancher-stable repository will correspond with any Rancher version tagged as stable.\n\n\nHelm Chart Versions\n\nRancher Helm chart versions match the Rancher version (i.e appVersion).\n\nFor the Rancher v2.1.x versions, there were some Helm charts, that were using a version that was a build number, i.e. yyyy.mm.<build-number>. These charts have been replaced with the equivalent Rancher version and are no longer available.\n\nSwitching to a Different Helm Chart Repository\n\nAfter installing Rancher, if you want to change which Helm chart repository to install Rancher from, you will need to follow these steps.\n\n\nNote: Because the rancher-alpha repository contains only alpha charts, switching between the rancher-alpha repository and the rancher-stable or rancher-latest repository for upgrades is not supported.\n\n\n\n  \n    \n      \n      Latest: Recommended for trying out the newest features\n    \n  \n  \n    \n      \n      Stable: Recommended for production environments\n    \n  \n  \n    \n      \n      Alpha: Experimental preview of upcoming releases.\n      \n      Note: Upgrades are not supported to, from, or between Alphas.\n    \n  \n\n\n\nList the current Helm chart repositories.\nhelm repo list\n\nNAME                  URL\nstable                https://kubernetes-charts.storage.googleapis.com\nrancher-<CHART_REPO>        https://releases.rancher.com/server-charts/<CHART_REPO>\n\nRemove the existing Helm Chart repository that contains your charts to install Rancher, which will either be rancher-stable or rancher-latest depending on what you had initially added.\nhelm repo remove rancher-<CHART_REPO>\n\nAdd the Helm chart repository that you want to start installing Rancher from.\nhelm repo add rancher-<CHART_REPO> https://releases.rancher.com/server-charts/<CHART_REPO>\n\nContinue to follow the steps to upgrade Rancher from the new Helm chart repository.\n\n\n\n\n\n  When performing Docker installs, upgrades, or rollbacks, you can use tags to install a specific version of Rancher.\n\nServer Tags\n\nRancher Server is distributed as a Docker image, which have tags attached to them. You can specify this tag when entering the command to deploy Rancher. Remember that if you use a tag without an explicit version (like latest or stable), you must explicitly pull a new version of that image tag. Otherwise, any image cached on the host will be used.\n\n\n\n\nTag\nDescription\n\n\n\n\n\nrancher/rancher:latest\nOur latest development release. These builds are validated through our CI automation framework. These releases are not recommended for production environments.\n\n\n\nrancher/rancher:stable\nOur newest stable release. This tag is recommended for production.\n\n\n\nrancher/rancher:<v2.X.X>\nYou can install specific versions of Rancher by using the tag from a previous release. See what’s available at DockerHub.\n\n\n\n\n\nNotes:\n\n\nThe master tag or any tag with -rc or another suffix is meant for the Rancher testing team to validate. You should not use these tags, as these builds are not officially supported.\nWant to install an alpha review for preview? Install using one of the alpha tags listed on our announcements page (e.g., v2.2.0-alpha1). Caveat: Alpha releases cannot be upgraded to or from any other release.\n\n\n\n\n \n\n\n\n","postref":"2ef8db1736828669d1af9d863e717329","objectID":"7e3691ceefbb721039b5246487db7599","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/server-tags/"},{"anchor":"#etcd","title":"etcd","content":"The Kubernetes network proxy service runs on all nodes and manages endpoints created by Kubernetes for TCP/UDP ports.Currently, RKE doesn’t support any specific options for the kubeproxy service.The Kubernetes Scheduler service is responsible for scheduling cluster workloads based on various configurations, metrics, resource requirements and workload-specific requirements.Currently, RKE doesn’t support any specific options for the scheduler service.The kubelet services acts as a “node agent” for Kubernetes. It runs on all nodes deployed by RKE, and gives Kubernetes the ability to manage the container runtime on the node.services:\n    kubelet:\n     # Base domain for the cluster\n     cluster_domain: cluster.local\n     # IP address for the DNS service endpoint\n     cluster_dns_server: 10.43.0.10\n     # Fail if swap is on\n     fail_swap_on: false\n     # Generate per node serving certificate\n     generate_serving_certificate: falseKubelet OptionsRKE supports the following options for the kubelet service:\nCluster Domain (cluster_domain) - The base domain for the cluster. All services and DNS records created on the cluster. By default, the domain is set to cluster.local.\nCluster DNS Server (cluster_dns_server) - The IP address assigned to the DNS service endpoint within the cluster. DNS queries will be sent to this IP address which is used by KubeDNS. The default value for this option is 10.43.0.10\nFail if Swap is On (fail_swap_on) - In Kubernetes, the default behavior for the kubelet is to fail if swap is enabled on the node. RKE does not follow this default and allows deployments on nodes with swap enabled. By default, the value is false. If you’d like to revert to the default kubelet behavior, set this option to true.\nGenerate Serving Certificate (generate_serving_certificate) - Generate a certificate signed by the kube-ca Certificate Authority for the kubelet to use as a server certificate. The default value for this option is false. Before enabling this option, please read the requirements\nKubelet Serving Certificate RequirementsIf hostname_override is configured for one or more nodes in cluster.yml, please make sure the correct IP address is configured in address (and the internal address in internal_address) to make sure the generated certificate contains the correct IP address(es).An example of an error situation is an EC2 instance where the the public IP address is configured in address, and hostname_override is used, the connection between kube-apiserver and kubelet will fail because the kubelet will be contacted on the private IP address and the generated certificate will not be valid (the error x509: certificate is valid for value_in_address, not private_ip will be seen). The resolution is to provide the internal IP address in internal_address.For more information on host overrides, refer to the node configuration page.\nNote for Rancher 2 users If you are configuring Cluster Options using a Config File when creating Rancher Launched Kubernetes, the names of services should contain underscores only: kube_api. This only applies to Rancher v2.0.5 and v2.0.6.\nThe Kubernetes API REST service, which handles requests and data for all Kubernetes objects and provide shared state for all the other Kubernetes components.services:\n  kube-api:\n    # IP range for any services created on Kubernetes\n    # This must match the service_cluster_ip_range in kube-controller\n    service_cluster_ip_range: 10.43.0.0/16\n    # Expose a different port range for NodePort services\n    service_node_port_range: 30000-32767\n    pod_security_policy: false\n    # Enable AlwaysPullImages Admission controller plugin\n    # Available as of v0.2.0\n    always_pull_images: false\n    secrets_encryption_config:\n      enabled: trueKubernetes API Server OptionsRKE supports the following options for the kube-api service :\nService Cluster IP Range (service_cluster_ip_range) - This is the virtual IP address that will be assigned to services created on Kubernetes. By default, the service cluster IP range is 10.43.0.0/16. If you change this value, then it must also be set with the same value on the Kubernetes Controller Manager (kube-controller).\nNode Port Range (service_node_port_range) - The port range to be used for Kubernetes services created with the type NodePort. By default, the port range is 30000-32767.\nPod Security Policy (pod_security_policy) - An option to enable the Kubernetes Pod Security Policy. By default, we do not enable pod security policies as it is set to false.\n> Note: If you set pod_security_policy value to true, RKE will configure an  open policy to allow any pods to work on the cluster. You will need to configure your own policies to fully utilize PSP.\nAlways Pull Images (always_pull_images) - Enable AlwaysPullImages Admission controller plugin.  Enabling AlwaysPullImages is a security best practice. It forces Kubernetes to validate the image and pull credentials with the remote image registry. Local image layer cache will still be used, but it does add a small bit of overhead when launching containers to pull and compare image hashes. Note: Available as of v0.2.0\n\nSecrets Encryption Config (secrets_encryption_config) - Manage Kubernetes at-rest data encryption. Documented here\n\nKubernetes Controller Manager\n\nNote for Rancher 2 users If you are configuring Cluster Options using a Config File when creating Rancher Launched Kubernetes, the names of services should contain underscores only: kube_controller. This only applies to Rancher v2.0.5 and v2.0.6.\nThe Kubernetes Controller Manager service is the component responsible for running Kubernetes main control loops. The controller manager monitors the cluster desired state through the Kubernetes API server and makes the necessary changes to the current state to reach the desired state.services:\n    kube-controller:\n      # CIDR pool used to assign IP addresses to pods in the cluster\n      cluster_cidr: 10.42.0.0/16\n      # IP range for any services created on Kubernetes\n      # This must match the service_cluster_ip_range in kube-api\n      service_cluster_ip_range: 10.43.0.0/16Kubernetes Controller Manager OptionsRKE supports the following options for the kube-controller service:\nCluster CIDR (cluster_cidr) - The CIDR pool used to assign IP addresses to pods in the cluster. By default, each node in the cluster is assigned a /24 network from this pool for pod IP assignments. The default value for this option is 10.42.0.0/16.\nService Cluster IP Range (service_cluster_ip_range) - This is the virtual IP address that will be assigned to services created on Kubernetes. By default, the service cluster IP range is 10.43.0.0/16. If you change this value, then it must also be set with the same value on the Kubernetes API server (kube-api).\nKubernetes uses etcd as a store for cluster state and data. Etcd is a reliable, consistent and distributed key-value store.RKE supports running etcd in a single node mode or in HA cluster mode. It also supports adding and removing etcd nodes to the cluster.You can enable etcd to take recurring snapshots. These snapshots can be used to restore etcd.By default, RKE will deploy a new etcd service, but you can also run Kubernetes with an external etcd service.","postref":"9f7759e5aac20df93f92e62d7adb5d33","objectID":"e8029c1b30a9dda17a9658ec660e31c0","permalink":"http://jijeesh.github.io/docs/rke/latest/en/config-options/services/"},{"anchor":"#","title":"Encrypting Secret Data at Rest","content":"As of version v0.3.1 RKE adds the support for managing secret data encryption at rest, which is supported by Kubernetes since version v1.13.\n\nAt-rest data encryption is required for:\n\n\nCompliance requirements\nAdditional layer of security\nReduce security impact of etcd node compromise\nReduce security impact of etcd backups compromise\nAbility to use external Key Management Systems\n\n\nRKE provides users with two paths of configuration to enable at-rest data encryption:\n\n\nManaged at-rest data encryption\nCustom configuration for at-rest data encryption\n\n\nBoth configuration options can be added during initial cluster provisioning or by updating an existing cluster.\n\nTo utilize this feature, a new field secrets_encryption_config is added to the Kubernetes API service configuration. A full custom configuration looks like this:\nservices:\n  kube-api:\n    secrets_encryption_config:\n      enabled: true\n      custom_config:\n        apiVersion: apiserver.config.k8s.io/v1\n        kind: EncryptionConfiguration\n        resources:\n        - resources:\n          - secrets\n          providers:\n          - aescbc:\n              keys:\n              - name: k-fw5hn\n                secret: RTczRjFDODMwQzAyMDVBREU4NDJBMUZFNDhCNzM5N0I=\n          - identity: {}\nManaged At-Rest Data Encryption\n\nEnabling and disabling at-rest data encryption in Kubernetes is a relatively complex process that requires several steps to be performed by the Kubernetes cluster administrator. The managed configuration aims to reduce this overhead and provides a simple abstraction layer to manage the process.\n\nEnable Encryption\n\nManaged at-rest data encryption is disabled by default and can be enabled by using the following configuration:\nservices:\n  kube-api:\n    secrets_encryption_config:\n      enabled: true\nOnce enabled, RKE will perform the following actions to enable at-rest data encryption:\n\n\nGenerate a new random 32-bit encryption key\nGenerate an encryption provider configuration file using the new key The default provider used is aescbc\nDeploy the provider configuration file to all nodes with controlplane role\nUpdate the kube-apiserver container arguments to point to the provider configuration file.\nRestart the kube-apiserver container.\n\n\nAfter the kube-api server is restarted, data encryption is enabled. However, all existing secrets are still stored in plain text. RKE will rewrite all secrets to ensure encryption is fully in effect.\n\nDisable Encryption\n\nTo disable encryption, you can either set the enabled flag to false, or simply remove the secrets_encryption_config block entirely from cluster.yml.\nservices:\n  kube-api:\n    secrets_encryption_config:\n      enabled: false\nOnce encryption is disabled in cluster.yml, RKE will perform the following actions to disable encryption in your cluster:\n\n\nGenerate a new provider configuration file with the no-encryption identity{} provider as the first provider, and the previous aescbc set in the second place. This will allow Kubernetes to use the first entry to write the secrets, and the second one to decrypt them.\nDeploy the new provider configuration and restart kube-apiserver.\nRewrite all secrets. This is required because, at this point, new data will be written to disk in plain text, but the existing data is still encrypted using the old provider. By rewriting all secrets, RKE ensures that all stored data is decrypted.\nUpdate kube-apiserver arguments to remove the encryption provider configuration and restart the kube-apiserver.\nRemove the provider configuration file.\n\n\nKey Rotation\n\nSometimes there is a need to rotate encryption config in your cluster. For example, the key is compromised. There are two ways to rotate the keys: with an RKE CLI command, or by disabling and re-enabling encryption in cluster.yml.\n\nRotating Keys with the RKE CLI\n\nWith managed configuration, RKE CLI has the ability to perform the key rotation process documented here with one command. To perform this operation, the following subcommand is used:\n$ ./rke encrypt rotate-key --help\nNAME:\n   rke encrypt rotate-key - Rotate cluster encryption provider key\n\nUSAGE:\n   rke encrypt rotate-key [command options] [arguments...]\n\nOPTIONS:\n   --config value           Specify an alternate cluster YAML file (default: \"cluster.yml\") [$RKE_CONFIG]\n   --ssh-agent-auth         Use SSH Agent Auth defined by SSH_AUTH_SOCK\n   --ignore-docker-version  Disable Docker version check\nThis command will perform the following actions:\n\n\nGenerate a new random 32-bit encryption key\nGenerate a new provider configuration with the new key as the first provider and the second key as the second provider. When the secrets are rewritten, the first key will be used to encrypt the data on the write operation, while the second key (the old key) will be used to decrypt the stored data during the the read operation\nDeploy the new provider configuration to all controlplane nodes and restart the kube-apiserver\nRewrite all secrets. This process will re-encrypt all the secrets with the new key.\nUpdate the configuration to remove the old key and restart the kube-apiserver\n\n\nRotating Keys by Disabling and Re-enabling Encryption in cluster.yml\n\nFor a cluster with encryption enabled, you can rotate the encryption keys by updating cluster.yml. If you enable and re-enable the data encryption in the cluster.yml, RKE will not reuse old keys. Instead, it will generate new keys every time, yielding the same result as a key rotation with the RKE CLI.\n\nCustom At-Rest Data Encryption Configuration\n\nWith managed configuration, RKE provides the user with a very simple way to enable and disable encryption with minimal interaction and configuration. However, it doesn’t allow for any customization to the configuration.\n\nWith custom encryption configuration, RKE allows the user to provide their own configuration. Although RKE will help the user to deploy the configuration and rewrite the secrets if needed, it doesn’t provide a configuration validation on user’s behalf. It’s the user responsibility to make sure their configuration is valid.\n\n\nWarning: Using invalid Encryption Provider Configuration could cause several issues with your cluster, ranging from crashing the Kubernetes API service, kube-api,  to completely losing access to encrypted data.\n\n\nExample: Using Custom Encryption Configuration with Amazon KMS\n\nAn example for custom configuration would be enabling an external key management system like Amazon KMS. The following is an example of the configuration for AWS KMS:\nservices:\n  kube-api:\n    extra_binds:\n      - \"/var/run/kmsplugin/:/var/run/kmsplugin/\"\n    secrets_encryption_config:\n      enabled: true\n      custom_config:\n        apiVersion: apiserver.config.k8s.io/v1\n        kind: EncryptionConfiguration\n        resources:\n          - resources:\n            - secrets\n            providers:\n            - kms:\n                name: aws-encryption-provider\n                endpoint: unix:///var/run/kmsplugin/socket.sock\n                cachesize: 1000\n                timeout: 3s\n            - identity: {}\nDocumentation for AWS KMS can be found here. When Custom Configuration is set to to enable the AWS KMS provider, you should consider the following points:\n\n\nSince RKE runs the kube-api  service in a container, it’s required that you use the extra_binds feature to bind-mount the KMS provider socket location inside the kube-api container.\nThe AWS KMS provider runs as a pod in the cluster. Therefor, the proper way to enable it is to:\n\n\nDeploy your cluster with at-rest encryption disabled.\nDeploy the KMS pod and make sure it’s working correctly.\nUpdate your cluster with the custom encryption configuration to utilize the KMS provider.\n\nKube API connects to the KMS provider using a Unix socket. You should configure your KMS deployment to run pods on all controlplane nodes in the cluster.\nYour controlplane node should be configured with an AMI profile that has access to the KMS key you used in your configuration.\n\n\nHow to Prevent Restore Failures after Rotating Keys\n\nIt’s important to understand that enabling encryption for you cluster means that you can no longer access encrypted data in your etcd database and/or etcd database backups without using your encryption keys.\n\nThe encryption configuration is stored in the cluster state file cluster.rkestate, which is decoupled from the etcd backups. For example, in any of the following backup cases, the restore process will fail:\n\n\nThe snapshot is taken while encryption is enabled and restored when it’s disabled. In this case, the encryption keys are no longer stored in the cluster state.\nThe snapshot is taken before the keys are rotated and restore is attempted after. In this case, the old keys used for encryption at the time of the snapshot no longer exist in the cluster state file.\n\n\nTherefore, we recommend that when you enable or disable encryption, or when you rotate keys, you should create a snapshot so that your backup requires the same keys that you have access ","postref":"4314c4b4266ddd1b1a129d004db26c58","objectID":"f84a63c15a1cfcbe79c3883b935e85ee","permalink":"http://jijeesh.github.io/docs/rke/latest/en/config-options/secrets-encryption/"},{"anchor":"#","title":"Extra Args, Extra Binds, and Extra Environment Variables","content":"RKE supports additional service arguments, volume binds and environment variables.\n\nExtra Args\n\nFor any of the Kubernetes services, you can update the extra_args to change the existing defaults.\n\nAs of v0.1.3, using extra_args will add new arguments and override any existing defaults. For example, if you need to modify the default admission plugins list, you need to include the default list and edit it with your changes so all changes are included.\n\nPrior to v0.1.3, using extra_args would only add new arguments to the list and there was no ability to change the default list.\n\nAll service defaults and parameters are defined per kubernetes_version:\n\n\nFor RKE v0.3.0+, the service defaults and parameters are defined per kubernetes_version. The service defaults are located here. The default list of admissions plugins is the same for all Kubernetes versions and is located here.\n\nFor RKE prior to v0.3.0, the service defaults and admission plugins are defined per kubernetes_version and located here.\n\nservices:\n    kube-controller:\n      extra_args:\n        cluster-name: \"mycluster\"\nExtra Binds\n\nAdditional volume binds can be added to services using the extra_binds arguments.\nservices:\n    kubelet:\n      extra_binds:\n        - \"/host/dev:/dev\"\n        - \"/usr/libexec/kubernetes/kubelet-plugins:/usr/libexec/kubernetes/kubelet-plugins:z\"\nExtra Environment Variables\n\nAdditional environment variables can be added to services by using the extra_env arguments.\nservices:\n    kubelet:\n      extra_env:\n        - \"HTTP_PROXY=http://your_proxy\"","postref":"966c23a617ee8a579f49c64996cd3c9e","objectID":"fdc5b0587047d7cba752b99e3f2b19d8","permalink":"http://jijeesh.github.io/docs/rke/latest/en/config-options/services/services-extras/"},{"anchor":"#external-etcd-options","title":"External etcd Options","content":"PathThe path defines the location of where the etcd cluster is on the endpoints.External URLsThe external_urls are the endpoints of where the etcd cluster is hosted. There can be multiple endpoints for the etcd cluster.CA Cert/Cert/KEYThe certificates and private keys used to authenticate and access the etcd service.","postref":"62aae19c56e4db138fed5746bf263b49","objectID":"6b2865ed0471ff5294861d38dc4b62b3","permalink":"http://jijeesh.github.io/docs/rke/latest/en/config-options/services/external-etcd/"},{"anchor":"#","title":"Authentication","content":"RKE supports x509 authentication strategy. You can additionally define a list of SANs (Subject Alternative Names) to add to the Kubernetes API Server PKI certificates. As an example, this allows you to connect to your Kubernetes cluster API Server through a load balancer instead of a single node.\nauthentication:\n    strategy: x509\n    sans:\n      - \"10.18.160.10\"\n      - \"my-loadbalancer-1234567890.us-west-2.elb.amazonaws.com\"\nRKE also supports the webhook authentication strategy. You can enable both x509 and webhook strategies by using a | separator in the configuration. Contents of the webhook config file should be provided, see Kubernetes webhook documentation for information on the file format. Additionally, a cache timeout for webhook authentication responses can be set.\nauthentication:\n    strategy: x509|webhook\n    webhook:\n      config_file: \"....\"\n      cache_timeout: 5s","postref":"6b819353b5577db0bc5f32b1cf25b7ad","objectID":"cd000a6ac70204db6446ef4c143aae6a","permalink":"http://jijeesh.github.io/docs/rke/latest/en/config-options/authentication/"},{"anchor":"#","title":"Authorization","content":"Kubernetes supports multiple Authorization Modules. Currently, RKE only supports the RBAC module.\n\nBy default, RBAC is already enabled. If you wanted to turn off RBAC support, which isn’t recommended, you set the authorization mode to none in your cluster.yml.\nauthorization:\n    # Use `mode: none` to disable authorization\n    mode: rbac","postref":"26b30479d2961072775806ec77895758","objectID":"2229d7f28792ef1b7e9edf472f3c2281","permalink":"http://jijeesh.github.io/docs/rke/latest/en/config-options/authorization/"},{"anchor":"#","title":"Rate Limiting","content":"Using the EventRateLimit admission control enforces a limit on the number of events that the API Server will accept in a given time period. In a large multi-tenant cluster, there might be a small percentage of tenants that flood the server with event requests, which could have a significant impact on the performance of the cluster overall. Therefore, it is recommended to limit the rate of events that the API server will accept.\n\nYou might want to configure event rate limit as part of compliance with the CIS (Center for Internet Security) Kubernetes Benchmark. Event rate limiting corresponds to the CIS Kubernetes Benchmark 1.1.36 - Ensure that the admission control plugin EventRateLimit is set (Scored).\n\nRate limits can be configured for the server, a namespace, a user, or a combination of a source and an object.\n\nFor configuration details, refer to the official Kubernetes documentation.\n\nExample Configurations\n\nThe following configuration in the cluster.yml can be used to enable the event rate limit by default:\nservices:\n  kube-api:\n    event_rate_limit:\n      enabled: true\nWhen the event rate limit is enabled, you should be able to see the default values at /etc/kubernetes/admission.yaml:\n...\nplugins:\n- configuration:\n    apiVersion: eventratelimit.admission.k8s.io/v1alpha1\n    kind: Configuration\n    limits:\n    - burst: 20000\n      qps: 5000\n      type: Server\n...\nTo customize the event rate limit, the entire Kubernetes resource for the configuration must be provided in the configuration directive:\nservices:\n  kube-api:\n    event_rate_limit:\n      enabled: true\n      configuration:\n        apiVersion: eventratelimit.admission.k8s.io/v1alpha1\n        kind: Configuration\n        limits:\n        - type: Server\n          qps: 6000\n          burst: 30000","postref":"8d1161152ec51972a7ded704648ce3f3","objectID":"32b214cead5ac8f883e2ef8d07ade531","permalink":"http://jijeesh.github.io/docs/rke/latest/en/config-options/rate-limiting/"},{"anchor":"#","title":"Cloud Providers","content":"RKE supports the ability to set your specific cloud provider for your Kubernetes cluster. There are specific cloud configurations for these cloud providers.\nTo enable a cloud provider its name as well as any required configuration options must be provided under the cloud_provider directive in the cluster YML.\n\n\nAWS\nAzure\nOpenStack\nvSphere\n\n\nOutside of this list, RKE also supports the ability to handle any custom cloud provider.\n","postref":"c89a4b4084299587321efcfcf72eecd8","objectID":"08d1f78bc4f5404c2414179279b6b7b1","permalink":"http://jijeesh.github.io/docs/rke/latest/en/config-options/cloud-providers/"},{"anchor":"#iam-requirements","title":"IAM Requirements","content":"Any resources used in a Kubernetes cluster with the Amazon cloud provider must be tagged with a cluster ID.Amazon Documentation: Tagging Your Amazon EC2 ResourcesThe following resources need to tagged with a ClusterID:\nNodes: All hosts added in Rancher.\nSubnet: The subnet used for your cluster\n\nSecurity Group: The security group used for your cluster.\n\n\nNote: Do not tag multiple security groups. Tagging multiple groups generates an error when creating Elastic Load Balancer.\n\nThe tag that should be used is:Key=kubernetes.io/cluster/<CLUSTERID>, Value=owned\n<CLUSTERID> can be any string you choose. However, the same string must be used on every resource you tag. Setting the tag value to owned informs the cluster that all resources tagged with the <CLUSTERID> are owned and managed by this cluster.If you share resources between clusters, you can change the tag to:Key=kubernetes.io/cluster/CLUSTERID, Value=shared\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": \"ec2:Describe*\",\n      \"Resource\": \"*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": \"ec2:AttachVolume\",\n      \"Resource\": \"*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": \"ec2:DetachVolume\",\n      \"Resource\": \"*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\"elasticloadbalancing:*\"],\n      \"Resource\": [\"*\"]\n    }\n  ]\n}Deploy files to AWS IAM:$ aws iam create-instance-profile --instance-profile-name rancher-node\n$ aws iam create-role --role-name rancher-node --assume-role-policy-document file://rancher-role.json\n$ aws iam put-role-policy --role-name rancher-node --policy-name rancher-policy --policy-document file://rancher-policy.json\n$ aws iam add-role-to-instance-profile --instance-profile rancher-node --role-name rancher-nodeSet IAM Instance Profile Name in node template to rancher-nodeThe nodes used in RKE that will be running the AWS cloud provider must have at least the following IAM policy (rancher-role.json).{\n  \"Effect\": \"Allow\",\n  \"Action\": \"ec2:Describe*\",\n  \"Resource\": \"*\"\n}In order to use Elastic Load Balancers (ELBs) and EBS with Kubernetes, the node(s) will need to have the an IAM role with appropriate access (rancher-policy.json).","postref":"cccde453cba2a44f494d9d13ea04f06a","objectID":"0c36849ce6621a2c7e6f7f9fa67107cb","permalink":"http://jijeesh.github.io/docs/rke/latest/en/config-options/cloud-providers/aws/"},{"anchor":"#","title":"Audit Log","content":"Kubernetes auditing provides a security-relevant chronological set of records about a cluster. Kube-apiserver performs auditing. Each request on each stage of its execution generates an event, which is then pre-processed according to a certain policy and written to a backend. The policy determines what’s recorded and the backends persist the records.\n\nYou might want to configure the audit log as part of compliance with the CIS (Center for Internet Security) Kubernetes Benchmark controls.\n\nFor configuration details, refer to the official Kubernetes documentation.\n\nExample Configurations\n\nThe audit log can be enabled by default using the following configuration in cluster.yml:\nservices:\n  kube-api:\n    audit_log:\n      enabled: true\nWhen the audit log is enabled, you should be able to see the default values at /etc/kubernetes/audit.yaml:\n# Minimum Configuration: Capture event metadata.\n...\nrules:\n- level: Metadata\n...\nWhen the audit log is enabled, default values are also set for the audit log path, maximum age, maximum number of backups, maximum size in megabytes, and format. To see the default values, run:\n\nps -ef | grep kube-apiserver\n\n\nThe default values for the audit log should be displayed:\n--audit-log-maxage=5 # The maximum number of days to retain old audit log files\n--audit-log-maxbackup=5 # The maximum number of audit log files to retain\n--audit-log-path=/var/log/kube-audit/audit-log.json # The log file path that log backend uses to write audit events\n--audit-log-maxsize=100 # The maximum size in megabytes of the audit log file before it gets rotated\n--audit-policy-file=/etc/kubernetes/audit.yaml # The file containing your audit log rules\n--audit-log-format=json # The log file format\nTo customize the audit log, the configuration directive is used.\n\nA rules policy is passed to kube-apiserver using the --audit-policy-file or the policy directive in the cluster.yml. Below is an example cluster.yml with custom values and an audit log policy nested under the configuration directive. This example audit log policy is taken from the official Kubernetes documentation:\nservices:\n  kube-api:\n    audit_log:\n      enabled: true\n      configuration:\n        max_age: 6\n        max_backup: 6\n        max_size: 110\n        path: /var/log/kube-audit/audit-log.json\n        format: json\n        policy:\n          apiVersion: audit.k8s.io/v1 # This is required.\n          kind: Policy\n          omitStages:\n            - \"RequestReceived\"\n          rules:\n            # Log pod changes at RequestResponse level\n            - level: RequestResponse\n              resources:\n              - group: \"\"\n                # Resource \"pods\" doesn't match requests to any subresource of pods,\n                # which is consistent with the RBAC policy.\n                resources: [\"pods\"]\n            # Log \"pods/log\", \"pods/status\" at Metadata level\n            - level: Metadata\n              resources:\n              - group: \"\"\n                resources: [\"pods/log\", \"pods/status\"]\n\n            # Don't log requests to a configmap called \"controller-leader\"\n            - level: None\n              resources:\n              - group: \"\"\n                resources: [\"configmaps\"]\n                resourceNames: [\"controller-leader\"]\n\n            # Don't log watch requests by the \"system:kube-proxy\" on endpoints or services\n            - level: None\n              users: [\"system:kube-proxy\"]\n              verbs: [\"watch\"]\n              resources:\n              - group: \"\" # core API group\n                resources: [\"endpoints\", \"services\"]\n\n            # Don't log authenticated requests to certain non-resource URL paths.\n            - level: None\n              userGroups: [\"system:authenticated\"]\n              nonResourceURLs:\n              - \"/api*\" # Wildcard matching.\n              - \"/version\"\n\n            # Log the request body of configmap changes in kube-system.\n            - level: Request\n              resources:\n              - group: \"\" # core API group\n                resources: [\"configmaps\"]\n              # This rule only applies to resources in the \"kube-system\" namespace.\n              # The empty string \"\" can be used to select non-namespaced resources.\n              namespaces: [\"kube-system\"]\n\n            # Log configmap and secret changes in all other namespaces at the Metadata level.\n            - level: Metadata\n              resources:\n              - group: \"\" # core API group\n                resources: [\"secrets\", \"configmaps\"]\n\n            # Log all other resources in core and extensions at the Request level.\n            - level: Request\n              resources:\n              - group: \"\" # core API group\n              - group: \"extensions\" # Version of group should NOT be included.\n\n            # A catch-all rule to log all other requests at the Metadata level.\n            - level: Metadata\n              # Long-running requests like watches that fall under this rule will not\n              # generate an audit event in RequestReceived.\n              omitStages:\n                - \"RequestReceived\"","postref":"909d4610634b8121cc5c36882d1af3ad","objectID":"eac159999f976e1828f10e11ba200b82","permalink":"http://jijeesh.github.io/docs/rke/latest/en/config-options/audit-log/"},{"anchor":"#docker-installation","title":"Docker Installation","content":"Passing environment variables to the Rancher container can be done using -e KEY=VALUE or --env KEY=VALUE. Required values for NO_PROXY in a Docker Installation are:\nlocalhost\n127.0.0.1\n0.0.0.0\n10.0.0.0/8\nThe example below is based on a proxy server accessible at http://192.168.0.1:3128, and excluding usage the proxy when accessing network range 192.168.10.0/24 and every hostname under the domain example.com.docker run -d --restart=unless-stopped \\\n  -p 80:80 -p 443:443 \\\n  -e HTTP_PROXY=\"http://192.168.10.1:3128\" \\\n  -e HTTPS_PROXY=\"http://192.168.10.1:3128\" \\\n  -e NO_PROXY=\"localhost,127.0.0.1,0.0.0.0,10.0.0.0/8,192.168.10.0/24,example.com\" \\\n  rancher/rancher:latest\n","postref":"8c83a1afceca0a835947f777307efc26","objectID":"084a0280bcf3014d6c3fd5a8de8c0f7c","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/other-installation-methods/single-node-docker/proxy/"},{"anchor":"#overriding-the-hostname","title":"Overriding the hostname","content":"Besides the minimum set of options, there are many other options that are supported in RKE:\n\n\nAzure Configuration Options\nType\nRequired\n\n\n\n\n\ntenantId\nstring\n*\n\n\n\nsubscriptionId\nstring\n*\n\n\n\naadClientId\nstring\n*\n\n\n\naadClientSecret\nstring\n*\n\n\n\ncloud\nstring\n\n\n\n\nresourceGroup\nstring\n\n\n\n\nlocation\nstring\n\n\n\n\nvnetName\nstring\n\n\n\n\nvnetResourceGroup\nstring\n\n\n\n\nsubnetName\nstring\n\n\n\n\nsecurityGroupName\nstring\n\n\n\n\nrouteTableName\nstring\n\n\n\n\nprimaryAvailabilitySetName\nstring\n\n\n\n\nvmType\nstring\n\n\n\n\nprimaryScaleSetName\nstring\n\n\n\n\naadClientCertPath\nstring\n\n\n\n\naadClientCertPassword\nstring\n\n\n\n\ncloudProviderBackoff\nbool\n\n\n\n\ncloudProviderBackoffRetries\nint\n\n\n\n\ncloudProviderBackoffExponent\nint\n\n\n\n\ncloudProviderBackoffDuration\nint\n\n\n\n\ncloudProviderBackoffJitter\nint\n\n\n\n\ncloudProviderRateLimit\nbool\n\n\n\n\ncloudProviderRateLimitQPS\nint\n\n\n\n\ncloudProviderRateLimitBucket\nint\n\n\n\n\nuseInstanceMetadata\nbool\n\n\n\n\nuseManagedIdentityExtension\nbool\n\n\n\n\nmaximumLoadBalancerRuleCount\nint\n\n\n\nSince the Azure node name must match the Kubernetes node name, you override the Kubernetes name on the node by setting the hostname_override for each node. If you do not set the hostname_override, the Kubernetes node name will be set as the address, which will cause the Azure cloud provider to fail.nodes:\n    - address: x.x.x.x\n      hostname_override: azure-rke1\n      user: ubuntu\n      role:\n        - controlplane\n        - etcd\n        - worker","postref":"b454bf29a15f19f93efbce75513ff962","objectID":"c5ed74c8304aa38982393cf93c7c1c2a","permalink":"http://jijeesh.github.io/docs/rke/latest/en/config-options/cloud-providers/azure/"},{"anchor":"#requirements-for-os-docker-hardware-and-networking","title":"Requirements for OS, Docker, Hardware, and Networking","content":"API AuditingIf you want to record all transactions with the Rancher API, enable the API Auditing feature by adding the flags below into your install command.-e AUDIT_LEVEL=1 \\\n-e AUDIT_LOG_PATH=/var/log/auditlog/rancher-api-audit.log \\\n-e AUDIT_LOG_MAXAGE=20 \\\n-e AUDIT_LOG_MAXBACKUP=20 \\\n-e AUDIT_LOG_MAXSIZE=100 \\\nAir GapIf you are visiting this page to complete an Air Gap Installation, you must pre-pend your private registry URL to the server tag when running the installation command in the option that you choose. Add <REGISTRY.DOMAIN.COM:PORT> with your private registry URL in front of rancher/rancher:latest.Example: <REGISTRY.DOMAIN.COM:PORT>/rancher/rancher:latest\nPersistent Data\n  Rancher uses etcd as datastore. When using the Docker Install, the embedded etcd is\n    being used. The persistent data is at the following path in the container: /var/lib/rancher. You can\n    bind mount a host volume to this location to preserve data on the host it is running on. When using RancherOS,\n    please check what persistent storage\n      directories you can use to store the data.\n\n  Command:\n\n  docker run -d --restart=unless-stopped \\\n  -p 80:80 -p 443:443 \\\n  -v /opt/rancher:/var/lib/rancher \\\n  rancher/rancher:latest\n\nThis layer 7 Nginx configuration is tested on Nginx version 1.13 (mainline) and 1.14 (stable).\nNote: This Nginx configuration is only an example and may not suit your environment. For complete documentation, see NGINX Load Balancing - TCP and UDP Load Balancer.\nupstream rancher {\n    server rancher-server:80;\n}\n\nmap $http_upgrade $connection_upgrade {\n    default Upgrade;\n    ''      close;\n}\n\nserver {\n    listen 443 ssl http2;\n    server_name rancher.yourdomain.com;\n    ssl_certificate /etc/your_certificate_directory/fullchain.pem;\n    ssl_certificate_key /etc/your_certificate_directory/privkey.pem;\n\n    location / {\n        proxy_set_header Host $host;\n        proxy_set_header X-Forwarded-Proto $scheme;\n        proxy_set_header X-Forwarded-Port $server_port;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_pass http://rancher;\n        proxy_http_version 1.1;\n        proxy_set_header Upgrade $http_upgrade;\n        proxy_set_header Connection $connection_upgrade;\n        # This allows the ability for the execute shell window to remain open for up to 15 minutes. Without this parameter, the default is 1 minute and will automatically close.\n        proxy_read_timeout 900s;\n        proxy_buffering off;\n    }\n}\n\nserver {\n    listen 80;\n    server_name rancher.yourdomain.com;\n    return 301 https://$server_name$request_uri;\n}\nHow Do I Know if My Certificates are in PEM Format?You can recognize the PEM format by the following traits:\n  The file begins with the following header: -----BEGIN CERTIFICATE-----\n  The header is followed by a long string of characters. Like, really long.\n  The file ends with a footer: -----END CERTIFICATE-----\nPEM Certificate Example:----BEGIN CERTIFICATE-----\nMIIGVDCCBDygAwIBAgIJAMiIrEm29kRLMA0GCSqGSIb3DQEBCwUAMHkxCzAJBgNV\n... more lines\nVWQqljhfacYPgp8KJUJENQ9h5hZ2nSCrI+W00Jcw4QcEdCI8HL5wmg==\n-----END CERTIFICATE-----\nPEM Certificate Key Example:-----BEGIN RSA PRIVATE KEY-----\nMIIGVDCCBDygAwIBAgIJAMiIrEm29kRLMA0GCSqGSIb3DQEBCwUAMHkxCzAJBgNV\n... more lines\nVWQqljhfacYPgp8KJUJENQ9h5hZ2nSCrI+W00Jcw4QcEdCI8HL5wmg==\n-----END RSA PRIVATE KEY-----\nIf your key looks like the example below, see How Can I Convert My Certificate Key From\n    PKCS8 to PKCS1?\n  \n\n    -----BEGIN PRIVATE KEY-----\nMIIGVDCCBDygAwIBAgIJAMiIrEm29kRLMA0GCSqGSIb3DQEBCwUAMHkxCzAJBgNV\n... more lines\nVWQqljhfacYPgp8KJUJENQ9h5hZ2nSCrI+W00Jcw4QcEdCI8HL5wmg==\n-----END PRIVATE KEY-----\nHow Can I Convert My Certificate Key From PKCS8 to PKCS1?If you are using a PKCS8 certificate key file, Rancher will log the following line:ListenConfigController cli-config [listener] failed with : failed to read private key: asn1: structure error: tags don't match (2 vs {class:0 tag:16 length:13 isCompound:true})\nTo make this work, you will need to convert the key from PKCS8 to PKCS1 using the command below:openssl rsa -in key.pem -out convertedkey.pem\nYou can now use convertedkey.pem as certificate key file for Rancher.What is the Order of Certificates if I Want to Add My Intermediate(s)?The order of adding certificates is as follows:-----BEGIN CERTIFICATE-----\n%YOUR_CERTIFICATE%\n-----END CERTIFICATE-----\n-----BEGIN CERTIFICATE-----\n%YOUR_INTERMEDIATE_CERTIFICATE%\n-----END CERTIFICATE-----\nHow Do I Validate My Certificate Chain?You can validate the certificate chain by using the openssl binary. If the output of the command\n      (see the command example below) ends with Verify return code: 0 (ok), your certificate chain is\n      valid. The ca.pem file must be the same as you added to the rancher/rancher container.\n      When using a certificate signed by a recognized Certificate Authority, you can omit the -CAfile\n      parameter.Command:openssl s_client -CAfile ca.pem -connect rancher.yourdomain.com:443\n...\n    Verify return code: 0 (ok)\n\nRecommended: Review Single Node Backup and Restoration. Although you don’t have any data you need to back up right now, we recommend creating backups after regular Rancher use.\nCreate a Kubernetes cluster: Provisioning Kubernetes Clusters.\nWhen using a load balancer in front of your Rancher container, there’s no need for the container to redirect port communication from port 80 or port 443. By passing the header X-Forwarded-Proto: https header, this redirect is disabled.The load balancer or proxy has to be configured to support the following:\nWebSocket connections\nSPDY / HTTP/2 protocols\n\nPassing / setting the following headers:\n\n\n\n\nHeader\nValue\nDescription\n\n\n\n\n\nHost\nHostname used to reach Rancher.\nTo identify the server requested by the client.\n\n\n\nX-Forwarded-Proto\nhttps\nTo identify the protocol that a client used to connect to the load balancer or proxy.Note: If this header is present, rancher/rancher does not redirect HTTP to HTTPS.\n\n\n\nX-Forwarded-Port\nPort used to reach Rancher.\nTo identify the protocol that client used to connect to the load balancer or proxy.\n\n\n\nX-Forwarded-For\nIP of the client connection.\nTo identify the originating IP address of a client.\n\n\n\n\nExample Nginx configuration\nThis NGINX configuration is tested on NGINX 1.14.\nNote: This Nginx configuration is only an example and may not suit your environment. For complete documentation, see NGINX Load Balancing - HTTP Load Balancing.\n\nReplace rancher-server with the IP address or hostname of the node running the Rancher container.\nReplace both occurrences of FQDN to the DNS name for Rancher.\nReplace /certs/fullchain.pem and /certs/privkey.pem to the location of the server certificate and the server certificate key respectively.\nworker_processes 4;\nworker_rlimit_nofile 40000;\n\nevents {\n    worker_connections 8192;\n}\n\nhttp {\n    upstream rancher {\n        server rancher-server:80;\n    }\n\n    map $http_upgrade $connection_upgrade {\n        default Upgrade;\n        ''      close;\n    }\n\n    server {\n        listen 443 ssl http2;\n        server_name FQDN;\n        ssl_certificate /certs/fullchain.pem;\n        ssl_certificate_key /certs/privkey.pem;\n\n        location / {\n            proxy_set_header Host $host;\n            proxy_set_header X-Forwarded-Proto $scheme;\n            proxy_set_header X-Forwarded-Port $server_port;\n            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n            proxy_pass http://rancher;\n            proxy_http_version 1.1;\n            proxy_set_header Upgrade $http_upgrade;\n            proxy_set_header Connection $connection_upgrade;\n            # This allows the ability for the execute shell window to remain open for up to 15 minutes. Without this parameter, the default is 1 minute and will automatically close.\n            proxy_read_timeout 900s;\n            proxy_buffering off;\n        }\n    }\n\n    server {\n        listen 80;\n        server_name FQDN;\n        return 301 https://$server_name$request_uri;\n    }\n}\nFor security purposes, SSL (Secure Sockets Layer) is required when using Rancher. SSL secures all Rancher network communication, like when you login or interact with a cluster.\nDo you want to…\n\n\nComplete an Air Gap Installation?\nRecord all transactions with the Rancher API?\n\n\nSee Advanced Options below before continuing.\nChoose from the following options:\n  \n  Option A-Bring Your Own Certificate: Self-Signed\n  \n    If you elect to use a self-signed certificate to encrypt communication, you must install the certificate on your load balancer (which you’ll do later) and your Rancher container. Run the Docker command to deploy Rancher, pointing it toward your certificate.\n\n\nPrerequisites:\nCreate a self-signed certificate.\n\n\nThe certificate files must be in PEM format.\n\n\n\nTo Install Rancher Using a Self-Signed Cert:\n\n\nWhile running the Docker command to deploy Rancher, point Docker toward your CA certificate file.\n\n\n   docker run -d --restart=unless-stopped \\\n     -p 80:80 -p 44","postref":"78ff312485955ae74f2340ef270c36b1","objectID":"bb87bcf51620b117f051185e5540b4af","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/other-installation-methods/single-node-docker/single-node-install-external-lb/"},{"anchor":"#overriding-the-hostname","title":"Overriding the hostname","content":"The Openstack configuration options are divided into 5 groups.\nGlobal\nLoad Balancer\nBlock Storage\nRoute\nMetadata\nGlobalThese are the options that are available under the global directive.\n\n\nOpenStack’s Global Configuration Options\nType\nRequired\n\n\n\n\n\nauth_url\nstring\n*\n\n\n\nusername\nstring\n*\n\n\n\nuser-id\nstring\n*\n\n\n\npassword\nstring\n*\n\n\n\ntenant-id\nstring\n*\n\n\n\ntenant-name\nstring\n\n\n\n\ntrust-id\nstring\n\n\n\n\ndomain-id\nstring\n\n\n\n\ndomain-name\nstring\n\n\n\n\nregion\nstring\n\n\n\n\nca-file\nstring\n\n\n\nLoad BalancerThese are the options that are available under the load_balancer directive.\n\n\nOpenStack’s Load Balancer Configuration Options\nType\nRequired\n\n\n\n\n\nlb-version\nstring\n\n\n\n\nuse-octavia\nbool\n\n\n\n\nsubnet-id\nstring\n\n\n\n\nfloating-network-id\nstring\n\n\n\n\nlb-method\nstring\n\n\n\n\nlb-provider\nstring\n\n\n\n\nmanage-security-groups\nbool\n\n\n\n\ncreate-monitor\nbool\n\n\n\n\nmonitor-delay\nint\n* if create-monitor is true\n\n\n\nmonitor-timeout\nint\n* if create-monitor is true\n\n\n\nmonitor-max-retries\nint\n* if create-monitor is true\n\n\nBlock StorageThese are the options that are available under the block_storage directive.\n\n\nOpenStack’s Block Storage Configuration Options\nType\nRequired\n\n\n\n\n\nbs-version\nstring\n\n\n\n\ntrust-device-path\nbool\n\n\n\n\nignore-volume-az\nbool\n\n\n\nRouteThis is the option that is available under the route directive.\n\n\nOpenStack’s Route Configuration Option\nType\nRequired\n\n\n\n\n\nrouter-id\nstring\n\n\n\nMetadataThese are the options that are available under the metadata directive.\n\n\nOpenStack’s Metadata Configuration Options\nType\nRequired\n\n\n\n\n\nsearch-order\nstring\n\n\n\n\nrequest-timeout\nint\n\n\n\nFor more information of Openstack configurations options please refer to the official Kubernetes documentation.The OpenStack cloud provider uses the instance name (as determined from OpenStack metadata) as the name of the Kubernetes Node object, you must override the Kubernetes name on the node by setting the hostname_override for each node. If you do not set the hostname_override, the Kubernetes node name will be set as the address, which will cause the Openstack cloud provider to fail.","postref":"c1675a129d352b6f2a76272f24816aa9","objectID":"57de9c48b9ad889282dbe0806bfa7dba","permalink":"http://jijeesh.github.io/docs/rke/latest/en/config-options/cloud-providers/openstack/"},{"anchor":"#","title":"vSphere Cloud Provider","content":"In order to provision Kubernetes clusters in vSphere with the RKE CLI, you must enable the vSphere cloud provider.\n\nThe vSphere cloud provider must also be enabled in order to provision clusters with Rancher, which uses RKE as a library when provisioning RKE clusters.\n\nThe vSphere Cloud Provider interacts with VMware infrastructure (vCenter or standalone ESXi server) to provision and manage storage for persistent volumes in a Kubernetes cluster.\n\nThis section describes how to enable the vSphere cloud provider. You will need to use the cloud_provider directive in the cluster YAML file.\n\nRelated Links\n\n\nConfiguration: For details on vSphere configuration in RKE, refer to the configuration reference.\nTroubleshooting: For guidance on troubleshooting a cluster with the vSphere cloud provider enabled, refer to the troubleshooting section.\nStorage: If you are setting up storage, see the official vSphere documentation on storage for Kubernetes, or the official Kubernetes documentation on persistent volumes. If you are using Rancher, refer to the Rancher documentation on provisioning storage in vSphere.\nFor Rancher users: Refer to the Rancher documentation on creating vSphere Kubernetes clusters and provisioning storage.\n\n\nPrerequisites\n\n\nCredentials: You’ll need to have credentials of a vCenter/ESXi user account with privileges allowing the cloud provider to interact with the vSphere infrastructure to provision storage. Refer to this document to create and assign a role with the required permissions in vCenter.\nVMware Tools must be running in the Guest OS for all nodes in the cluster.\nDisk UUIDs: All nodes must be configured with disk UUIDs. This is required so that attached VMDKs present a consistent UUID to the VM, allowing the disk to be mounted properly. See the section on enabling disk UUIDs.\n\n\nEnabling the vSphere Provider with the RKE CLI\n\nTo enable the vSphere Cloud Provider in the cluster, you must add the top-level cloud_provider directive to the cluster configuration file, set the name property to vsphere and add the vsphereCloudProvider directive containing the configuration matching your infrastructure. See the configuration reference for the gory details.\n","postref":"62be3d341a5bd60cd5aeea31813e14b3","objectID":"fbe6ba66c3b5e8ce3b24ee3920a5e47c","permalink":"http://jijeesh.github.io/docs/rke/latest/en/config-options/cloud-providers/vsphere/"},{"anchor":"#","title":"Custom Cloud Provider","content":"If you want to enable a different cloud provider, RKE allows for custom cloud provider options. A name must be provided and the custom Cloud Provider options can be passed in as a multiline string in customCloudProvider.\n\nFor example, in order to use the oVirt cloud provider with Kubernetes, here’s the following cloud provider information:\n\n[connection]\nuri = https://localhost:8443/ovirt-engine/api\nusername = admin@internal\npassword = admin\n\n\nTo add this cloud config file to RKE, the cloud_provider would be need to be set.\ncloud_provider:\n    name: ovirt\n    # Note the pipe as this is what indicates a multiline string\n    customCloudProvider: |-\n      [connection]\n      uri = https://localhost:8443/ovirt-engine/api\n      username = admin@internal\n      password = admin","postref":"d6b92be018da221f7dc2e06274d5afc5","objectID":"0abd928816760e95f2c838c6cc58b71b","permalink":"http://jijeesh.github.io/docs/rke/latest/en/config-options/cloud-providers/custom/"},{"anchor":"#critical-and-non-critical-add-ons","title":"Critical and Non-Critical Add-ons","content":"Applies to v0.2.3 and higher\n\n\nComponent\nnodeAffinity nodeSelectorTerms\nnodeSelector\nTolerations\n\n\n\n\n\nCalico\nbeta.kubernetes.io/os:NotIn:windows\nnone\n- NoSchedule:Exists- NoExecute:Exists- CriticalAddonsOnly:Exists\n\n\n\nFlannel\nbeta.kubernetes.io/os:NotIn:windows\nnone\n- operator:Exists\n\n\n\nCanal\nbeta.kubernetes.io/os:NotIn:windows\nnone\n- NoSchedule:Exists- NoExecute:Exists- CriticalAddonsOnly:Exists\n\n\n\nWeave\nbeta.kubernetes.io/os:NotIn:windows\nnone\n- NoSchedule:Exists- NoExecute:Exists\n\n\n\nCoreDNS\nnode-role.kubernetes.io/worker:Exists\nbeta.kubernetes.io/os:linux\n- NoSchedule:Exists- NoExecute:Exists- CriticalAddonsOnly:Exists\n\n\n\nkube-dns\n- beta.kubernetes.io/os:NotIn:windows- node-role.kubernetes.io/worker Exists\nnone\n- NoSchedule:Exists- NoExecute:Exists- CriticalAddonsOnly:Exists\n\n\n\nnginx-ingress\n- beta.kubernetes.io/os:NotIn:windows- node-role.kubernetes.io/worker Exists\nnone\n- NoSchedule:Exists- NoExecute:Exists\n\n\n\nmetrics-server\n- beta.kubernetes.io/os:NotIn:windows- node-role.kubernetes.io/worker Exists\nnone\n- NoSchedule:Exists- NoExecute:Exists\n\n\nRKE uses Kubernetes jobs to deploy add-ons. In some cases, add-ons deployment takes longer than expected. As of with version v0.1.7, RKE provides an option to control the job check timeout in seconds. This timeout is set at the cluster level.addon_job_timeout: 30As of version v0.1.7, add-ons are split into two categories:\nCritical add-ons: If these add-ons fail to deploy for any reason, RKE will error out.\nNon-critical add-ons: If these add-ons fail to deploy, RKE will only log a warning and continue deploying any other add-ons.\nCurrently, only the network plug-in is considered critical. KubeDNS, ingress controllers and user-defined add-ons are considered non-critical.","postref":"d6f7146d9784d17df0468d5bf0f16cc6","objectID":"2050829a24e12df2d8c56697931e7a89","permalink":"http://jijeesh.github.io/docs/rke/latest/en/config-options/add-ons/"},{"anchor":"#canal-network-plug-in-options","title":"Canal Network Plug-in Options","content":"It is possible to add a custom network plug-in by using the user-defined add-on functionality of RKE. In the addons field, you can add the add-on manifest of a cluster that has the network plugin-that you want, as shown in this example.network:\n    plugin: weave\n    weave_network_provider:\n        password: \"Q]SZOQ5wp@n$oijz\"Weave encryptionWeave encryption can be enabled by passing a string password to the network provider config.network:\n    plugin: calico\n    options:\n        calico_cloud_provider: awsCalico Cloud ProviderCalico currently only supports 2 cloud providers, AWS or GCE, which can be set using calico_cloud_provider.Valid Options\naws\ngce\nnetwork:\n    plugin: flannel\n    options:\n        flannel_iface: eth1\n        flannel_backend_type: vxlanFlannel InterfaceBy setting the flannel_iface, you can configure the interface to use for inter-host communication.\nThe flannel_backend_type option allows you to specify the type of flannel backend to use. By default the vxlan backend is used.network:\n    plugin: canal\n    options:\n        canal_iface: eth1\n        canal_flannel_backend_type: vxlanCanal InterfaceBy setting the canal_iface, you can configure the interface to use for inter-host communication.\nThe canal_flannel_backend_type option allows you to specify the type of flannel backend to use. By default the vxlan backend is used.","postref":"c909f9e5b19aa7256398c10b8d4b7236","objectID":"94e8d7d54ab12af9afefb4d064a12d12","permalink":"http://jijeesh.github.io/docs/rke/latest/en/config-options/add-ons/network-plugins/"},{"anchor":"#scheduling-coredns","title":"Scheduling CoreDNS","content":"Upstream nameserversAvailable as of v0.2.0By default, kube-dns will use the host configured nameservers (usually residing at /etc/resolv.conf) to resolve external queries. If you want to configure specific upstream nameservers to be used by kube-dns, you can use the upstreamnameservers directive.When you set upstreamnameservers, the provider also needs to be set.dns:\n    provider: kube-dns\n    upstreamnameservers:\n    - 1.1.1.1  \n    - 8.8.4.4Disabling deployment of a DNS providerAvailable as of v0.2.0You can disable the default DNS provider by specifying none to  the dns provider directive in the cluster configuration. Be aware that this will prevent your pods from doing name resolution in your cluster.dns:\n    provider: noneAvailable as of v0.2.0If you only want the kube-dns pod to be deployed on specific nodes, you can set a node_selector in the dns section. The label in the node_selector would need to match the label on the nodes for the kube-dns pod to be deployed.nodes:\n    - address: 1.1.1.1\n      role: [controlplane,worker,etcd]\n      user: root\n      labels:\n        app: dns\n\ndns:\n    provider: kube-dns\n    node_selector:\n      app: dnsUpstream nameserversBy default, CoreDNS will use the host configured nameservers (usually residing at /etc/resolv.conf) to resolve external queries. If you want to configure specific upstream nameservers to be used by CoreDNS, you can use the upstreamnameservers directive.When you set upstreamnameservers, the provider also needs to be set.dns:\n    provider: coredns\n    upstreamnameservers:\n    - 1.1.1.1\n    - 8.8.4.4kube-dnsRKE will deploy kube-dns as a Deployment with the default replica count of 1. The pod consists of 3 containers: kubedns, dnsmasq and sidecar. RKE will also deploy kube-dns-autoscaler as a Deployment, which will scale the kube-dns Deployment by using the number of cores and nodes. Please see Linear Mode for more information about this logic.The images used for kube-dns are under the system_images directive. For each Kubernetes version, there are default images associated with kube-dns, but these can be overridden by changing the image tag in system_images.If you only want the CoreDNS pod to be deployed on specific nodes, you can set a node_selector in the dns section. The label in the node_selector would need to match the label on the nodes for the CoreDNS pod to be deployed.nodes:\n    - address: 1.1.1.1\n      role: [controlplane,worker,etcd]\n      user: root\n      labels:\n        app: dns\n\ndns:\n    provider: coredns\n    node_selector:\n      app: dns","postref":"de0dd1977d3835369ebcc338ad72412f","objectID":"3226ffc3bd00c324dd1981a88154e9d6","permalink":"http://jijeesh.github.io/docs/rke/latest/en/config-options/add-ons/dns/"},{"anchor":"#scheduling-ingress-controllers","title":"Scheduling Ingress Controllers","content":"When configuring an ingress object with TLS termination, you must provide it with a certificate used for encryption/decryption. Instead of explicitly defining a certificate each time you configure an ingress, you can set up a custom certificate that’s used by default.Setting up a default certificate is especially helpful in environments where a wildcard certificate is used, as the certificate can be applied in multiple subdomains.\nPrerequisites:\n\n\nAccess to the cluster.yml used to create the cluster.\nThe PEM encoded certificate you will use as the default certificate.\n\n\nObtain or generate your certificate key pair in a PEM encoded form.\n\nGenerate a Kubernetes secret from your PEM encoded certificate with the following command, substituting your certificate for mycert.cert and mycert.key.\n\nkubectl -n ingress-nginx create secret tls ingress-default-cert --cert=mycert.cert --key=mycert.key -o yaml --dry-run=true > ingress-default-cert.yaml\n\n\nInclude the contents of ingress-default-cert.yml inline with your RKE cluster.yml file. For example:\naddons: |-\n  ---\n  apiVersion: v1\n  data:\n    tls.crt: [ENCODED CERT]\n    tls.key: [ENCODED KEY]\n  kind: Secret\n  metadata:\n    creationTimestamp: null\n    name: ingress-default-cert\n    namespace: ingress-nginx\n  type: kubernetes.io/tls\n\nDefine your ingress resource with the following default-ssl-certificate argument, which references the secret we created earlier under extra_args in your cluster.yml:\ningress: \n  provider: \"nginx\"\n  extra_args:\n    default-ssl-certificate: \"ingress-nginx/ingress-default-cert\"\n\nOptional: If you want to apply the default certificate to ingresses in a cluster that already exists, you must delete the NGINX ingress controller pods to have Kubernetes schedule new pods with the newly configured extra_args.\n\nkubectl delete pod -l app=ingress-nginx -n ingress-nginx\n\nFor the configuration of NGINX, there are configuration options available in Kubernetes. There are a list of options for the NGINX config map , command line extra_args and annotations.ingress:\n    provider: nginx\n    options:\n      map-hash-bucket-size: \"128\"\n      ssl-protocols: SSLv2\n    extra_args:\n      enable-ssl-passthrough: \"\"You can disable the default controller by specifying none to  the ingress provider directive in the cluster configuration.ingress:\n    provider: noneIf you only wanted ingress controllers to be deployed on specific nodes, you can set a node_selector for the ingress. The label in the node_selector would need to match the label on the nodes for the ingress controller to be deployed.nodes:\n    - address: 1.1.1.1\n      role: [controlplane,worker,etcd]\n      user: root\n      labels:\n        app: ingress\n\ningress:\n    provider: nginx\n    node_selector:\n      app: ingress","postref":"3275143b2758a8d8da078b7dfd53768f","objectID":"166b0faa4505b7a45ccd37b00905eca1","permalink":"http://jijeesh.github.io/docs/rke/latest/en/config-options/add-ons/ingress-controllers/"},{"anchor":"#disabling-the-metrics-server","title":"Disabling the Metrics Server","content":"Available as of v0.2.0You can disable the default controller by specifying none to the monitoring provider directive in the cluster configuration.monitoring:\n    provider: none","postref":"88010a75db658e125e573a3b40cc1280","objectID":"43ff0f9d268955cd98e518f3f0c6da76","permalink":"http://jijeesh.github.io/docs/rke/latest/en/config-options/add-ons/metrics-server/"},{"anchor":"#in-line-add-ons","title":"In-line Add-ons","content":"Use the addons_include directive to reference a local file or a URL for any user-defined add-ons.addons_include:\n    - https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/operator.yaml\n    - https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/cluster.yaml\n    - /opt/manifests/example.yaml\n    - ./nginx.yamlTo define an add-on directly in the YAML file, make sure to use the YAML’s block indicator |- as the addons directive is a multi-line string option. It’s possible to specify multiple YAML resource definitions by separating them using the --- directive.addons: |-\n    ---\n    apiVersion: v1\n    kind: Pod\n    metadata:\n      name: my-nginx\n      namespace: default\n    spec:\n      containers:\n      - name: my-nginx\n        image: nginx\n        ports:\n        - containerPort: 80","postref":"72de33a5c874d10cb2dff7e2b6b04b06","objectID":"520f0b66d3e779eba69903b5d3eda953","permalink":"http://jijeesh.github.io/docs/rke/latest/en/config-options/add-ons/user-defined-add-ons/"},{"anchor":"#install-nginx","title":"Install NGINX","content":"Instead of installing NGINX as a package on the operating system, you can rather run it as a Docker container. Save the edited Example NGINX config as /etc/nginx.conf and run the following command to launch the NGINX container:docker run -d --restart=unless-stopped \\\n  -p 80:80 -p 443:443 \\\n  -v /etc/nginx.conf:/etc/nginx/nginx.conf \\\n  nginx:1.14\nAfter installing NGINX, you need to update the NGINX configuration file, nginx.conf, with the IP addresses for your nodes.\nCopy and paste the code sample below into your favorite text editor. Save it as nginx.conf.\n\nFrom nginx.conf, replace both occurrences (port 80 and port 443) of <IP_NODE_1>, <IP_NODE_2>, and <IP_NODE_3> with the IPs of your nodes.\n\n\nNote: See NGINX Documentation: TCP and UDP Load Balancing for all configuration options.\n\n\nExample NGINX config\n\nworker_processes 4;\nworker_rlimit_nofile 40000;\n\nevents {\n    worker_connections 8192;\n}\n\nstream {\n    upstream rancher_servers_http {\n        least_conn;\n        server <IP_NODE_1>:80 max_fails=3 fail_timeout=5s;\n        server <IP_NODE_2>:80 max_fails=3 fail_timeout=5s;\n        server <IP_NODE_3>:80 max_fails=3 fail_timeout=5s;\n    }\n    server {\n        listen     80;\n        proxy_pass rancher_servers_http;\n    }\n\n    upstream rancher_servers_https {\n        least_conn;\n        server <IP_NODE_1>:443 max_fails=3 fail_timeout=5s;\n        server <IP_NODE_2>:443 max_fails=3 fail_timeout=5s;\n        server <IP_NODE_3>:443 max_fails=3 fail_timeout=5s;\n    }\n    server {\n        listen     443;\n        proxy_pass rancher_servers_https;\n    }\n}\n\n\nSave nginx.conf to your load balancer at the following path: /etc/nginx/nginx.conf.\n\nLoad the updates to your NGINX configuration by running the following command:\n\n# nginx -s reload\n\nStart by installing NGINX on the node you want to use as a load balancer. NGINX has packages available for all known operating systems. The versions tested are 1.14 and 1.15. For help installing NGINX, refer to their install documentation.The stream module is required, which is present when using the official NGINX packages. Please refer to your OS documentation on how to install and enable the NGINX stream module on your operating system.","postref":"b99f5258f1bc911f28412b298d40016e","objectID":"8a557ac330a0de5c889219e039ec9bcd","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/helm2/create-nodes-lb/nginx/"},{"anchor":"#install-nginx","title":"Install NGINX","content":"Instead of installing NGINX as a package on the operating system, you can rather run it as a Docker container. Save the edited Example NGINX config as /etc/nginx.conf and run the following command to launch the NGINX container:docker run -d --restart=unless-stopped \\\n  -p 80:80 -p 443:443 \\\n  -v /etc/nginx.conf:/etc/nginx/nginx.conf \\\n  nginx:1.14\nAfter installing NGINX, you need to update the NGINX configuration file, nginx.conf, with the IP addresses for your nodes.\nCopy and paste the code sample below into your favorite text editor. Save it as nginx.conf.\n\nFrom nginx.conf, replace both occurrences (port 80 and port 443) of <IP_NODE_1>, <IP_NODE_2>, and <IP_NODE_3> with the IPs of your nodes.\n\n\nNote: See NGINX Documentation: TCP and UDP Load Balancing for all configuration options.\n\n\nExample NGINX config\n\nworker_processes 4;\nworker_rlimit_nofile 40000;\n\nevents {\nworker_connections 8192;\n}\n\nstream {\nupstream rancher_servers_http {\nleast_conn;\nserver <IP_NODE_1>:80 max_fails=3 fail_timeout=5s;\nserver <IP_NODE_2>:80 max_fails=3 fail_timeout=5s;\nserver <IP_NODE_3>:80 max_fails=3 fail_timeout=5s;\n}\nserver {\nlisten 80;\nproxy_pass rancher_servers_http;\n}\n\n    upstream rancher_servers_https {\n        least_conn;\n        server <IP_NODE_1>:443 max_fails=3 fail_timeout=5s;\n        server <IP_NODE_2>:443 max_fails=3 fail_timeout=5s;\n        server <IP_NODE_3>:443 max_fails=3 fail_timeout=5s;\n    }\n    server {\n        listen     443;\n        proxy_pass rancher_servers_https;\n    }\n\n}\n\n\n\n```\n\nSave nginx.conf to your load balancer at the following path: /etc/nginx/nginx.conf.\n\nLoad the updates to your NGINX configuration by running the following command:\n\n# nginx -s reload\n\nStart by installing NGINX on the node you want to use as a load balancer. NGINX has packages available for all known operating systems. The versions tested are 1.14 and 1.15. For help installing NGINX, refer to their install documentation.The stream module is required, which is present when using the official NGINX packages. Please refer to your OS documentation on how to install and enable the NGINX stream module on your operating system.","postref":"cce1842a57f75a14d45d95d734961721","objectID":"32b392b49b34847460bcc35edceba6ce","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/k8s-install/create-nodes-lb/nginx/"},{"anchor":"#installation-outline","title":"Installation Outline","content":"How Do I Know if My Certificates are in PEM Format?You can recognize the PEM format by the following traits:\n  The file begins with the following header: -----BEGIN CERTIFICATE-----\n  The header is followed by a long string of characters. Like, really long.\n  The file ends with a footer: -----END CERTIFICATE-----\nPEM Certificate Example:----BEGIN CERTIFICATE-----\nMIIGVDCCBDygAwIBAgIJAMiIrEm29kRLMA0GCSqGSIb3DQEBCwUAMHkxCzAJBgNV\n... more lines\nVWQqljhfacYPgp8KJUJENQ9h5hZ2nSCrI+W00Jcw4QcEdCI8HL5wmg==\n-----END CERTIFICATE-----\nHow Can I Encode My PEM Files in base64?To encode your certificates in base64:\n  Change directory to where the PEM file resides.\n  Run one of the following commands. Replace FILENAME with the name of your certificate.\n    # MacOS\ncat FILENAME | base64\n# Linux\ncat FILENAME | base64 -w0\n# Windows\ncertutil -encode FILENAME FILENAME.base64\n\n  \nHow Can I Verify My Generated base64 String For The Certificates?To decode your certificates in base64:\n  Copy the generated base64 string.\n  Run one of the following commands. Replace YOUR_BASE64_STRING with the previously copied base64\n    string.\n    # MacOS\necho YOUR_BASE64_STRING | base64 -D\n# Linux\necho YOUR_BASE64_STRING | base64 -d\n# Windows\ncertutil -decode FILENAME.base64 FILENAME.verify\n\n  \nWhat is the Order of Certificates if I Want to Add My Intermediate(s)?The order of adding certificates is as follows:-----BEGIN CERTIFICATE-----\n%YOUR_CERTIFICATE%\n-----END CERTIFICATE-----\n-----BEGIN CERTIFICATE-----\n%YOUR_INTERMEDIATE_CERTIFICATE%\n-----END CERTIFICATE-----\nHow Do I Validate My Certificate Chain?You can validate the certificate chain by using the openssl binary. If the output of the command (see\n  the command example below) ends with Verify return code: 0 (ok), your certificate chain is valid. The\n  ca.pem file must be the same as you added to the rancher/rancher container. When using a\n  certificate signed by a recognized Certificate Authority, you can omit the -CAfile parameter.Command:openssl s_client -CAfile ca.pem -connect rancher.yourdomain.com:443 -servername rancher.yourdomain.com\n...\n    Verify return code: 0 (ok)\nYou have a couple of options:\nCreate a backup of your Rancher Server in case of a disaster scenario: High Availability Back Up and Restoration.\nCreate a Kubernetes cluster: Provisioning Kubernetes Clusters.\nDuring installation, RKE automatically generates a config file named kube_config_rancher-cluster.yml in the same directory as the RKE binary. Copy this file and back it up to a safe location. You’ll use this file later when upgrading Rancher Server.With all configuration in place, use RKE to launch Rancher. You can complete this action by running the rke up command and using the --config parameter to point toward your config file.\nFrom your workstation, make sure rancher-cluster.yml and the downloaded rke binary are in the same directory.\n\nOpen a Terminal instance. Change to the directory that contains your config file and rke.\n\nEnter one of the rke up commands listen below.\nrke up --config rancher-cluster.yml\nStep Result: The output should be similar to the snippet below:INFO[0000] Building Kubernetes cluster\nINFO[0000] [dialer] Setup tunnel for host [1.1.1.1]\nINFO[0000] [network] Deploying port listener containers\nINFO[0000] [network] Pulling image [alpine:latest] on host [1.1.1.1]\n...\nINFO[0101] Finished building Kubernetes cluster successfully\nAfter you close your .yml file, back it up to a secure location. You can use this file again when it’s time to upgrade Rancher.The last reference that needs to be replaced is <RANCHER_VERSION>. This needs to be replaced with a Rancher version which is marked as stable. The latest stable release of Rancher can be found in the GitHub README. Make sure the version is an actual version number, and not a named tag like stable or latest. The example below shows the version configured to v2.0.6.      spec:\n        serviceAccountName: cattle-admin\n        containers:\n        - image: rancher/rancher:v2.0.6\n          imagePullPolicy: Always\nThere are two references to <FQDN> in the config file (one in this step and one in the next). Both need to be replaced with the FQDN chosen in Configure DNS.In the kind: Ingress with name: cattle-ingress-http:\nReplace <FQDN> with the FQDN chosen in Configure DNS.\nAfter replacing <FQDN> with the FQDN chosen in Configure DNS, the file should look like the example below (rancher.yourdomain.com is the FQDN used in this example): ---\n  apiVersion: extensions/v1beta1\n  kind: Ingress\n  metadata:\n    namespace: cattle-system\n    name: cattle-ingress-http\n    annotations:\n      nginx.ingress.kubernetes.io/proxy-connect-timeout: \"30\"\n      nginx.ingress.kubernetes.io/proxy-read-timeout: \"1800\"   # Max time in seconds for ws to remain shell window open\n      nginx.ingress.kubernetes.io/proxy-send-timeout: \"1800\"   # Max time in seconds for ws to remain shell window open\n  spec:\n    rules:\n    - host: rancher.yourdomain.com\n      http:\n        paths:\n        - backend:\n            serviceName: cattle-service\n            servicePort: 80\n    tls:\n    - secretName: cattle-keys-ingress\n      hosts:\n      - rancher.yourdomain.comSave the .yml file and close it.For security purposes, SSL (Secure Sockets Layer) is required when using Rancher. SSL secures all Rancher network communication, like when you login or interact with a cluster.Choose from the following options:\n  \n  Option A—Bring Your Own Certificate: Self-Signed\n  \n    \nPrerequisites:\nCreate a self-signed certificate.\n\n\nThe certificate files must be in PEM format.\nThe certificate files must be encoded in base64.\nIn your certificate file, include all intermediate certificates in the chain. Order your certificates with your certificate first, followed by the intermediates. For an example, see Intermediate Certificates.\n\n\n\n\nIn kind: Secret with name: cattle-keys-ingress:\n\n\nReplace <BASE64_CRT> with the base64 encoded string of the Certificate file (usually called cert.pem or domain.crt)\nReplace <BASE64_KEY> with the base64 encoded string of the Certificate Key file (usually called key.pem or domain.key)\n\n\n\nNote:\nThe base64 encoded string should be on the same line as tls.crt or tls.key, without any newline at the beginning, in between or at the end.\n\n\nStep Result: After replacing the values, the file should look like the example below (the base64 encoded strings should be different):\n---\napiVersion: v1\nkind: Secret\nmetadata:\n    name: cattle-keys-ingress\n    namespace: cattle-system\ntype: Opaque\ndata:\n    tls.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM1RENDQWN5Z0F3SUJBZ0lKQUlHc25NeG1LeGxLTUEwR0NTcUdTSWIzRFFFQkN3VUFNQkl4RURBT0JnTlYKQkFNTUIzUmxjM1F0WTJFd0hoY05NVGd3TlRBMk1qRXdOREE1V2hjTk1UZ3dOekExTWpFd05EQTVXakFXTVJRdwpFZ1lEVlFRRERBdG9ZUzV5Ym1Ob2NpNXViRENDQVNJd0RRWUpLb1pJaHZjTkFRRUJCUUFEZ2dFUEFEQ0NBUW9DCmdnRUJBTFJlMXdzekZSb2Rib2pZV05DSHA3UkdJaUVIMENDZ1F2MmdMRXNkUUNKZlcrUFEvVjM0NnQ3bSs3TFEKZXJaV3ZZMWpuY2VuWU5JSGRBU0VnU0ducWExYnhUSU9FaE0zQXpib3B0WDhjSW1OSGZoQlZETGdiTEYzUk0xaQpPM1JLTGdIS2tYSTMxZndjbU9zWGUwaElYQnpUbmxnM20vUzlXL3NTc0l1dDVwNENDUWV3TWlpWFhuUElKb21lCmpkS3VjSHFnMTlzd0YvcGVUalZrcVpuMkJHazZRaWFpMU41bldRV0pjcThTenZxTTViZElDaWlwYU9hWWQ3RFEKYWRTejV5dlF0YkxQNW4wTXpnOU43S3pGcEpvUys5QWdkWDI5cmZqV2JSekp3RzM5R3dRemN6VWtLcnZEb05JaQo0UFJHc01yclFNVXFSYjRSajNQOEJodEMxWXNDQXdFQUFhTTVNRGN3Q1FZRFZSMFRCQUl3QURBTEJnTlZIUThFCkJBTUNCZUF3SFFZRFZSMGxCQll3RkFZSUt3WUJCUVVIQXdJR0NDc0dBUVVGQndNQk1BMEdDU3FHU0liM0RRRUIKQ3dVQUE0SUJBUUNKZm5PWlFLWkowTFliOGNWUW5Vdi9NZkRZVEJIQ0pZcGM4MmgzUGlXWElMQk1jWDhQRC93MgpoOUExNkE4NGNxODJuQXEvaFZYYy9JNG9yaFY5WW9jSEg5UlcvbGthTUQ2VEJVR0Q1U1k4S292MHpHQ1ROaDZ6Ci9wZTNqTC9uU0pYSjRtQm51czJheHFtWnIvM3hhaWpYZG9kMmd3eGVhTklvRjNLbHB2aGU3ZjRBNmpsQTM0MmkKVVlCZ09iN1F5KytRZWd4U1diSmdoSzg1MmUvUUhnU2FVSkN6NW1sNGc1WndnNnBTUXhySUhCNkcvREc4dElSYwprZDMxSk1qY25Fb1Rhc1Jyc1NwVmNGdXZyQXlXN2liakZyYzhienBNcE1obDVwYUZRcEZzMnIwaXpZekhwakFsCk5ZR2I2OHJHcjBwQkp3YU5DS2ErbCtLRTk4M3A3NDYwCi0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K\n    tls.key: LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktLS0tLQpNSUlFb3dJQkFBS0NBUUVBdEY3WEN6TVZHaDF1aU5oWTBJZW50RVlpSVFmUUlLQkMvYUFzU3gxQUlsOWI0OUQ5ClhmanEzdWI3c3RCNnRsYTlqV09keDZkZzBnZDBCSVNCSWFlcHJWdkZNZzRTRXpjRE51aW0xZnh3aVkwZCtFRlUKTXVCc3NYZEV6V0k3ZEVvdUFjcVJjamZWL0J5WTZ4ZDdTRWhjSE5PZVdEZWI5TDFiK3hLd2k2M21uZ0lKQjdBeQpLSmRlYzhnbWlaNk4wcTV3ZXFEWDJ6QVgrbDVPTldTcG1mWUVhVHBDSnFMVTNtZFpCWWx5cnhMTytvemx0MGdLCktLbG81cGgzc05CcDFMUG5LOUMxc3MvbWZRek9EMDNzck1Xa21oTDcwQ0IxZmIydCtOWnRITW5BYmYwYkJETnoKTlNRcXU4T2cwaUxnOUVhd3l1dEF4U3BGdmhHUGMvd0dHMExWaXdJREFRQUJBb0lCQUJKYUErOHp4MVhjNEw0egpwUFd5bDdHVDRTMFRLbTNuWUdtRnZudjJBZXg5WDFBU2wzVFVPckZyTnZpK2xYMnYzYUZoSFZDUEN4N1RlMDVxClhPa2JzZnZkZG5iZFQ2RjgyMnJleVByRXNINk9TUnBWSzBmeDVaMDQwVnRFUDJCWm04eTYyNG1QZk1vbDdya2MKcm9Kd09rOEVpUHZZekpsZUd0bTAwUm1sRysyL2c0aWJsOTVmQXpyc1MvcGUyS3ZoN2NBVEtIcVh6MjlpUmZpbApiTGhBamQwcEVSMjNYU0hHR1ZqRmF3amNJK1c2L2RtbDZURDhrSzFGaUtldmJKTlREeVNXQnpPbXRTYUp1K01JCm9iUnVWWG4yZVNoamVGM1BYcHZRMWRhNXdBa0dJQWxOWjRHTG5QU2ZwVmJyU0plU3RrTGNzdEJheVlJS3BWZVgKSVVTTHM0RUNnWUVBMmNnZUE2WHh0TXdFNU5QW","postref":"fe7757b53e9b733328983e43bb9c17e8","objectID":"aefb75981c61ba9935504d3ccdf4af95","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/helm2/rke-add-on/layer-4-lb/"},{"anchor":"#installation-outline","title":"Installation Outline","content":"How Do I Know if My Certificates are in PEM Format?You can recognize the PEM format by the following traits:\n  The file begins with the following header: -----BEGIN CERTIFICATE-----\n  The header is followed by a long string of characters. Like, really long.\n  The file ends with a footer: -----END CERTIFICATE-----\nPEM Certificate Example:----BEGIN CERTIFICATE-----\nMIIGVDCCBDygAwIBAgIJAMiIrEm29kRLMA0GCSqGSIb3DQEBCwUAMHkxCzAJBgNV\n... more lines\nVWQqljhfacYPgp8KJUJENQ9h5hZ2nSCrI+W00Jcw4QcEdCI8HL5wmg==\n-----END CERTIFICATE-----\nHow Can I Encode My PEM Files in base64?To encode your certificates in base64:\n  Change directory to where the PEM file resides.\n  Run one of the following commands. Replace FILENAME with the name of your certificate.\n    # MacOS\ncat FILENAME | base64\n# Linux\ncat FILENAME | base64 -w0\n# Windows\ncertutil -encode FILENAME FILENAME.base64\n\n  \nHow Can I Verify My Generated base64 String For The Certificates?To decode your certificates in base64:\n  Copy the generated base64 string.\n  Run one of the following commands. Replace YOUR_BASE64_STRING with the previously copied base64\n    string.\n    # MacOS\necho YOUR_BASE64_STRING | base64 -D\n# Linux\necho YOUR_BASE64_STRING | base64 -d\n# Windows\ncertutil -decode FILENAME.base64 FILENAME.verify\n\n  \nWhat is the Order of Certificates if I Want to Add My Intermediate(s)?The order of adding certificates is as follows:-----BEGIN CERTIFICATE-----\n%YOUR_CERTIFICATE%\n-----END CERTIFICATE-----\n-----BEGIN CERTIFICATE-----\n%YOUR_INTERMEDIATE_CERTIFICATE%\n-----END CERTIFICATE-----\nHow Do I Validate My Certificate Chain?You can validate the certificate chain by using the openssl binary. If the output of the command (see\n  the command example below) ends with Verify return code: 0 (ok), your certificate chain is valid. The\n  ca.pem file must be the same as you added to the rancher/rancher container. When using a\n  certificate signed by a recognized Certificate Authority, you can omit the -CAfile parameter.Command:openssl s_client -CAfile ca.pem -connect rancher.yourdomain.com:443 -servername rancher.yourdomain.com\n...\n    Verify return code: 0 (ok)\nYou have a couple of options:\nCreate a backup of your Rancher Server in case of a disaster scenario: High Availability Back Up and Restoration.\nCreate a Kubernetes cluster: Provisioning Kubernetes Clusters.\nDuring installation, RKE automatically generates a config file named kube_config_rancher-cluster.yml in the same directory as the RKE binary. Copy this file and back it up to a safe location. You’ll use this file later when upgrading Rancher Server.With all configuration in place, use RKE to launch Rancher. You can complete this action by running the rke up command and using the --config parameter to point toward your config file.\nFrom your workstation, make sure rancher-cluster.yml and the downloaded rke binary are in the same directory.\n\nOpen a Terminal instance. Change to the directory that contains your config file and rke.\n\nEnter one of the rke up commands listen below.\nrke up --config rancher-cluster.yml\nStep Result: The output should be similar to the snippet below:INFO[0000] Building Kubernetes cluster\nINFO[0000] [dialer] Setup tunnel for host [1.1.1.1]\nINFO[0000] [network] Deploying port listener containers\nINFO[0000] [network] Pulling image [alpine:latest] on host [1.1.1.1]\n...\nINFO[0101] Finished building Kubernetes cluster successfully\nAfter you close your .yml file, back it up to a secure location. You can use this file again when it’s time to upgrade Rancher.The last reference that needs to be replaced is <RANCHER_VERSION>. This needs to be replaced with a Rancher version which is marked as stable. The latest stable release of Rancher can be found in the GitHub README. Make sure the version is an actual version number, and not a named tag like stable or latest. The example below shows the version configured to v2.0.6.      spec:\n        serviceAccountName: cattle-admin\n        containers:\n        - image: rancher/rancher:v2.0.6\n          imagePullPolicy: Always\nThere are two references to <FQDN> in the config file (one in this step and one in the next). Both need to be replaced with the FQDN chosen in Configure DNS.In the kind: Ingress with name: cattle-ingress-http:\nReplace <FQDN> with the FQDN chosen in Configure DNS.\nAfter replacing <FQDN> with the FQDN chosen in Configure DNS, the file should look like the example below (rancher.yourdomain.com is the FQDN used in this example): ---\n  apiVersion: extensions/v1beta1\n  kind: Ingress\n  metadata:\n    namespace: cattle-system\n    name: cattle-ingress-http\n    annotations:\n      nginx.ingress.kubernetes.io/proxy-connect-timeout: \"30\"\n      nginx.ingress.kubernetes.io/proxy-read-timeout: \"1800\"   # Max time in seconds for ws to remain shell window open\n      nginx.ingress.kubernetes.io/proxy-send-timeout: \"1800\"   # Max time in seconds for ws to remain shell window open\n  spec:\n    rules:\n    - host: rancher.yourdomain.com\n      http:\n        paths:\n        - backend:\n            serviceName: cattle-service\n            servicePort: 80\n    tls:\n    - secretName: cattle-keys-ingress\n      hosts:\n      - rancher.yourdomain.comSave the .yml file and close it.For security purposes, SSL (Secure Sockets Layer) is required when using Rancher. SSL secures all Rancher network communication, like when you login or interact with a cluster.Choose from the following options:\n  \n  Option A—Bring Your Own Certificate: Self-Signed\n  \n    \nPrerequisites:\nCreate a self-signed certificate.\n\n\nThe certificate files must be in PEM format.\nThe certificate files must be encoded in base64.\nIn your certificate file, include all intermediate certificates in the chain. Order your certificates with your certificate first, followed by the intermediates. For an example, see Intermediate Certificates.\n\n\n\n\nIn kind: Secret with name: cattle-keys-ingress:\n\n\nReplace <BASE64_CRT> with the base64 encoded string of the Certificate file (usually called cert.pem or domain.crt)\nReplace <BASE64_KEY> with the base64 encoded string of the Certificate Key file (usually called key.pem or domain.key)\n\n\n\nNote:\nThe base64 encoded string should be on the same line as tls.crt or tls.key, without any newline at the beginning, in between or at the end.\n\n\nStep Result: After replacing the values, the file should look like the example below (the base64 encoded strings should be different):\n---\napiVersion: v1\nkind: Secret\nmetadata:\n    name: cattle-keys-ingress\n    namespace: cattle-system\ntype: Opaque\ndata:\n    tls.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM1RENDQWN5Z0F3SUJBZ0lKQUlHc25NeG1LeGxLTUEwR0NTcUdTSWIzRFFFQkN3VUFNQkl4RURBT0JnTlYKQkFNTUIzUmxjM1F0WTJFd0hoY05NVGd3TlRBMk1qRXdOREE1V2hjTk1UZ3dOekExTWpFd05EQTVXakFXTVJRdwpFZ1lEVlFRRERBdG9ZUzV5Ym1Ob2NpNXViRENDQVNJd0RRWUpLb1pJaHZjTkFRRUJCUUFEZ2dFUEFEQ0NBUW9DCmdnRUJBTFJlMXdzekZSb2Rib2pZV05DSHA3UkdJaUVIMENDZ1F2MmdMRXNkUUNKZlcrUFEvVjM0NnQ3bSs3TFEKZXJaV3ZZMWpuY2VuWU5JSGRBU0VnU0ducWExYnhUSU9FaE0zQXpib3B0WDhjSW1OSGZoQlZETGdiTEYzUk0xaQpPM1JLTGdIS2tYSTMxZndjbU9zWGUwaElYQnpUbmxnM20vUzlXL3NTc0l1dDVwNENDUWV3TWlpWFhuUElKb21lCmpkS3VjSHFnMTlzd0YvcGVUalZrcVpuMkJHazZRaWFpMU41bldRV0pjcThTenZxTTViZElDaWlwYU9hWWQ3RFEKYWRTejV5dlF0YkxQNW4wTXpnOU43S3pGcEpvUys5QWdkWDI5cmZqV2JSekp3RzM5R3dRemN6VWtLcnZEb05JaQo0UFJHc01yclFNVXFSYjRSajNQOEJodEMxWXNDQXdFQUFhTTVNRGN3Q1FZRFZSMFRCQUl3QURBTEJnTlZIUThFCkJBTUNCZUF3SFFZRFZSMGxCQll3RkFZSUt3WUJCUVVIQXdJR0NDc0dBUVVGQndNQk1BMEdDU3FHU0liM0RRRUIKQ3dVQUE0SUJBUUNKZm5PWlFLWkowTFliOGNWUW5Vdi9NZkRZVEJIQ0pZcGM4MmgzUGlXWElMQk1jWDhQRC93MgpoOUExNkE4NGNxODJuQXEvaFZYYy9JNG9yaFY5WW9jSEg5UlcvbGthTUQ2VEJVR0Q1U1k4S292MHpHQ1ROaDZ6Ci9wZTNqTC9uU0pYSjRtQm51czJheHFtWnIvM3hhaWpYZG9kMmd3eGVhTklvRjNLbHB2aGU3ZjRBNmpsQTM0MmkKVVlCZ09iN1F5KytRZWd4U1diSmdoSzg1MmUvUUhnU2FVSkN6NW1sNGc1WndnNnBTUXhySUhCNkcvREc4dElSYwprZDMxSk1qY25Fb1Rhc1Jyc1NwVmNGdXZyQXlXN2liakZyYzhienBNcE1obDVwYUZRcEZzMnIwaXpZekhwakFsCk5ZR2I2OHJHcjBwQkp3YU5DS2ErbCtLRTk4M3A3NDYwCi0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K\n    tls.key: LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktLS0tLQpNSUlFb3dJQkFBS0NBUUVBdEY3WEN6TVZHaDF1aU5oWTBJZW50RVlpSVFmUUlLQkMvYUFzU3gxQUlsOWI0OUQ5ClhmanEzdWI3c3RCNnRsYTlqV09keDZkZzBnZDBCSVNCSWFlcHJWdkZNZzRTRXpjRE51aW0xZnh3aVkwZCtFRlUKTXVCc3NYZEV6V0k3ZEVvdUFjcVJjamZWL0J5WTZ4ZDdTRWhjSE5PZVdEZWI5TDFiK3hLd2k2M21uZ0lKQjdBeQpLSmRlYzhnbWlaNk4wcTV3ZXFEWDJ6QVgrbDVPTldTcG1mWUVhVHBDSnFMVTNtZFpCWWx5cnhMTytvemx0MGdLCktLbG81cGgzc05CcDFMUG5LOUMxc3MvbWZRek9EMDNzck1Xa21oTDcwQ0IxZmIydCtOWnRITW5BYmYwYkJETnoKTlNRcXU4T2cwaUxnOUVhd3l1dEF4U3BGdmhHUGMvd0dHMExWaXdJREFRQUJBb0lCQUJKYUErOHp4MVhjNEw0egpwUFd5bDdHVDRTMFRLbTNuWUdtRnZudjJBZXg5WDFBU2wzVFVPckZyTnZpK2xYMnYzYUZoSFZDUEN4N1RlMDVxClhPa2JzZnZkZG5iZFQ2RjgyMnJleVByRXNINk9TUnBWSzBmeDVaMDQwVnRFUDJCWm04eTYyNG1QZk1vbDdya2MKcm9Kd09rOEVpUHZZekpsZUd0bTAwUm1sRysyL2c0aWJsOTVmQXpyc1MvcGUyS3ZoN2NBVEtIcVh6MjlpUmZpbApiTGhBamQwcEVSMjNYU0hHR1ZqRmF3amNJK1c2L2RtbDZURDhrSzFGaUtldmJKTlREeVNXQnpPbXRTYUp1K01JCm9iUnVWWG4yZVNoamVGM1BYcHZRMWRhNXdBa0dJQWxOWjRHTG5QU2ZwVmJyU0plU3RrTGNzdEJheVlJS3BWZVgKSVVTTHM0RUNnWUVBMmNnZUE2WHh0TXdFNU5QW","postref":"4b8a8ba04d4343a65ec89b24c2d3fb8c","objectID":"4c445b4b9725e52d122d3ce905f1fed9","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/rke-add-on/layer-4-lb/"},{"anchor":"#","title":"Adding Kubernetes TLS Secrets","content":"Kubernetes will create all the objects and services for Rancher, but it will not become available until we populate the tls-rancher-ingress secret in the cattle-system namespace with the certificate and key.\n\nCombine the server certificate followed by any intermediate certificate(s) needed into a file named tls.crt. Copy your certificate key into a file named tls.key.\n\nUse kubectl with the tls secret type to create the secrets.\n\nkubectl -n cattle-system create secret tls tls-rancher-ingress \\\n  --cert=tls.crt \\\n  --key=tls.key\n\n\n\nNote: If you want to replace the certificate, you can delete the tls-rancher-ingress secret using kubectl -n cattle-system delete secret tls-rancher-ingress and add a new one using the command shown above. If you are using a private CA signed certificate, replacing the certificate is only possible if the new certificate is signed by the same CA as the certificate currently in use.\n\n\nUsing a Private CA Signed Certificate\n\nIf you are using a private CA, Rancher requires a copy of the CA certificate which is used by the Rancher Agent to validate the connection to the server.\n\nCopy the CA certificate into a file named cacerts.pem and use kubectl to create the tls-ca secret in the cattle-system namespace.\n\n\nImportant: Make sure the file is called cacerts.pem as Rancher uses that filename to configure the CA certificate.\n\n\nkubectl -n cattle-system create secret generic tls-ca \\\n  --from-file=cacerts.pem\n\n","postref":"357daccc66618286758c5c8a14747b0a","objectID":"883ea0282e21c7e35a40ea0025168648","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/helm2/helm-rancher/tls-secrets/"},{"anchor":"#","title":"Adding TLS Secrets","content":"Kubernetes will create all the objects and services for Rancher, but it will not become available until we populate the tls-rancher-ingress secret in the cattle-system namespace with the certificate and key.\n\nCombine the server certificate followed by any intermediate certificate(s) needed into a file named tls.crt. Copy your certificate key into a file named tls.key.\n\nUse kubectl with the tls secret type to create the secrets.\n\nkubectl -n cattle-system create secret tls tls-rancher-ingress \\\n  --cert=tls.crt \\\n  --key=tls.key\n\n\n\nNote: If you want to replace the certificate, you can delete the tls-rancher-ingress secret using kubectl -n cattle-system delete secret tls-rancher-ingress and add a new one using the command shown above. If you are using a private CA signed certificate, replacing the certificate is only possible if the new certificate is signed by the same CA as the certificate currently in use.\n\n\nUsing a Private CA Signed Certificate\n\nIf you are using a private CA, Rancher requires a copy of the CA certificate which is used by the Rancher Agent to validate the connection to the server.\n\nCopy the CA certificate into a file named cacerts.pem and use kubectl to create the tls-ca secret in the cattle-system namespace.\n\n\nImportant: Make sure the file is called cacerts.pem as Rancher uses that filename to configure the CA certificate.\n\n\nkubectl -n cattle-system create secret generic tls-ca \\\n  --from-file=cacerts.pem\n\n","postref":"62105fa32580cc17d4a278cdadd9138e","objectID":"fc7e1cb537d102b11cd2b31930054fef","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/tls-secrets/"},{"anchor":"#","title":"Chart Options","content":"Common Options\n\n\n\n\nOption\nDefault Value\nDescription\n\n\n\n\n\nhostname\n” “\nstring - the Fully Qualified Domain Name for your Rancher Server\n\n\n\ningress.tls.source\n“rancher”\nstring - Where to get the cert for the ingress. - “rancher, letsEncrypt, secret”\n\n\n\nletsEncrypt.email\n” “\nstring - Your email address\n\n\n\nletsEncrypt.environment\n“production”\nstring - Valid options: “staging, production”\n\n\n\nprivateCA\nfalse\nbool - Set to true if your cert is signed by a private CA\n\n\n\n\n\n\nAdvanced Options\n\n\n\n\nOption\nDefault Value\nDescription\n\n\n\n\n\nadditionalTrustedCAs\nfalse\nbool - See Additional Trusted CAs\n\n\n\naddLocal\n“auto”\nstring - Have Rancher detect and import the “local” Rancher server cluster Import “local Cluster\n\n\n\nantiAffinity\n“preferred”\nstring - AntiAffinity rule for Rancher pods - “preferred, required”\n\n\n\nauditLog.destination\n“sidecar”\nstring - Stream to sidecar container console or hostPath volume - “sidecar, hostPath”\n\n\n\nauditLog.hostPath\n”/var/log/rancher/audit”\nstring - log file destination on host (only applies when auditLog.destination is set to hostPath)\n\n\n\nauditLog.level\n0\nint - set the API Audit Log level. 0 is off. [0-3]\n\n\n\nauditLog.maxAge\n1\nint - maximum number of days to retain old audit log files (only applies when auditLog.destination is set to hostPath)\n\n\n\nauditLog.maxBackups\n1\nint - maximum number of audit log files to retain (only applies when auditLog.destination is set to hostPath)\n\n\n\nauditLog.maxSize\n100\nint - maximum size in megabytes of the audit log file before it gets rotated  (only applies when auditLog.destination is set to hostPath)\n\n\n\nbusyboxImage\n“busybox”\nstring - Image location for busybox image used to collect audit logs Note: Available as of v2.2.0\n\n\n\ndebug\nfalse\nbool - set debug flag on rancher server\n\n\n\nextraEnv\n[]\nlist - set additional environment variables for Rancher Note: Available as of v2.2.0\n\n\n\nimagePullSecrets\n[]\nlist - list of names of Secret resource containing private registry credentials\n\n\n\ningress.extraAnnotations\n{}\nmap - additional annotations to customize the ingress\n\n\n\ningress.configurationSnippet\n””\nstring - Add additional Nginx configuration. Can be used for proxy configuration. Note: Available as of v2.0.15, v2.1.10 and v2.2.4\n\n\n\nproxy\n””\nstring -  HTTP[S] proxy server for Rancher\n\n\n\nnoProxy\n“127.0.0.0/8,10.0.0.0/8,172.16.0.0/12,192.168.0.0/16”\nstring - comma separated list of hostnames or ip address not to use the proxy\n\n\n\nresources\n{}\nmap - rancher pod resource requests & limits\n\n\n\nrancherImage\n“rancher/rancher”\nstring - rancher image source\n\n\n\nrancherImageTag\nsame as chart version\nstring - rancher/rancher image tag\n\n\n\ntls\n“ingress”\nstring - See External TLS Termination for details. - “ingress, external”\n\n\n\nsystemDefaultRegistry\n””\nstring - private registry to be used for all system Docker images, e.g., http://registry.example.com/ Available as of v2.3.0\n\n\n\nuseBundledSystemChart\nfalse\nbool - select to use the system-charts packaged with Rancher server. This option is used for air gapped installations. Available as of v2.3.0\n\n\n\n\n\n\nAPI Audit Log\n\nEnabling the API Audit Log.\n\nYou can collect this log as you would any container log. Enable the Logging service under Rancher Tools for the System Project on the Rancher server cluster.\n--set auditLog.level=1\nBy default enabling Audit Logging will create a sidecar container in the Rancher pod. This container (rancher-audit-log) will stream the log to stdout.  You can collect this log as you would any container log. When using the sidecar as the audit log destination, the hostPath, maxAge, maxBackups, and maxSize options do not apply. It’s advised to use your OS or Docker daemon’s log rotation features to control disk space use. Enable the Logging service under Rancher Tools for the Rancher server cluster or System Project.\n\nSet the auditLog.destination to hostPath to forward logs to volume shared with the host system instead of streaming to a sidecar container. When setting the destination to hostPath you may want to adjust the other auditLog parameters for log rotation.\n\nSetting Extra Environment Variables\n\nAvailable as of v2.2.0\n\nYou can set extra environment variables for Rancher server using extraEnv. This list uses the same name and value keys as the container manifest definitions. Remember to quote the values.\n--set 'extraEnv[0].name=CATTLE_TLS_MIN_VERSION'\n--set 'extraEnv[0].value=1.0'\nTLS settings\n\nAvailable as of v2.2.0\n\nTo set a different TLS configuration, you can use the CATTLE_TLS_MIN_VERSION and CATTLE_TLS_CIPHERS environment variables. For example, to configure TLS 1.0 as minimum accepted TLS version:\n--set 'extraEnv[0].name=CATTLE_TLS_MIN_VERSION'\n--set 'extraEnv[0].value=1.0'\nSee TLS settings for more information and options.\n\nImport local Cluster\n\nBy default Rancher server will detect and import the local cluster it’s running on.  User with access to the local cluster will essentially have “root” access to all the clusters managed by Rancher server.\n\nIf this is a concern in your environment you can set this option to “false” on your initial install.\n\n\nNote: This option is only effective on the initial Rancher install. See Issue 16522 for more information.\n\n--set addLocal=\"false\"\nCustomizing your Ingress\n\nTo customize or use a different ingress with Rancher server you can set your own Ingress annotations.\n\nExample on setting a custom certificate issuer:\n--set ingress.extraAnnotations.'certmanager\\.k8s\\.io/cluster-issuer'=ca-key-pair\nAvailable as of v2.0.15, v2.1.10 and v2.2.4\n\nExample on setting a static proxy header with ingress.configurationSnippet. This value is parsed like a template so variables can be used.\n--set ingress.configurationSnippet='more_set_input_headers X-Forwarded-Host {{ .Values.hostname }};'\nHTTP Proxy\n\nRancher requires internet access for some functionality (helm charts). Use proxy to set your proxy server.\n\nAdd your IP exceptions to the noProxy list. Make sure you add the Service cluster IP range (default: 10.43.0.1⁄16) and any worker cluster controlplane nodes. Rancher supports CIDR notation ranges in this list.\n--set proxy=\"http://<username>:<password>@<proxy_url>:<proxy_port>/\"\n--set noProxy=\"127.0.0.0/8\\,10.0.0.0/8\\,172.16.0.0/12\\,192.168.0.0/16\"\nAdditional Trusted CAs\n\nIf you have private registries, catalogs or a proxy that intercepts certificates, you may need to add additional trusted CAs to Rancher.\n--set additionalTrustedCAs=true\nOnce the Rancher deployment is created, copy your CA certs in pem format into a file named ca-additional.pem and use kubectl to create the tls-ca-additional secret in the cattle-system namespace.\nkubectl -n cattle-system create secret generic tls-ca-additional --from-file=ca-additional.pem\nPrivate Registry and Air Gap Installs\n\nFor details on installing Rancher with a private registry, see:\n\n\nAir Gap: Docker Install\nAir Gap: Kubernetes Install\n\n\nExternal TLS Termination\n\nWe recommend configuring your load balancer as a Layer 4 balancer, forwarding plain 80/tcp and 443/tcp to the Rancher Management cluster nodes. The Ingress Controller on the cluster will redirect http traffic on port 80 to https on port 443.\n\nYou may terminate the SSL/TLS on a L7 load balancer external to the Rancher cluster (ingress). Use the --set tls=external option and point your load balancer at port http 80 on all of the Rancher cluster nodes. This will expose the Rancher interface on http port 80. Be aware that clients that are allowed to connect directly to the Rancher cluster will not be encrypted. If you choose to do this we recommend that you restrict direct access at the network level to just your load balancer.\n\n\nNote: If you are using a Private CA signed certificate, add --set privateCA=true and see Adding TLS Secrets - Using a Private CA Signed Certificate to add the CA cert for Rancher.\n\n\nYour load balancer must support long lived websocket connections and will need to insert proxy headers so Rancher can route links correctly.\n\nConfiguring Ingress for External TLS when Using NGINX v0.25\n\nIn NGINX v0.25, the behavior of NGINX has changed regarding forwarding headers and external TLS termination. Therefore, in the scenario that you are using external TLS termination configuration with NGINX v0.25, you must edit the cluster.yml to enable the use-forwarded-headers option for ingress:\ningress:\n  provider: nginx\n  options:\n    use-forwarded-headers: \"true\"\nRequired Headers\n\n\nHost\nX-Forwarded-Proto\nX-Forwarded-Port\nX-Forwarded-For\n\n\nRecommended Timeouts\n\n\nRead Timeout: 1800 seconds\nWrite Timeout: 1800 seconds\nConnect Timeout: 30 seconds\n\n\nHealth Checks\n\nRancher will respond 200 to health checks on the /healthz endpoint.\n\nExample NGINX config\n\nThis NGINX configuration is tested on NGINX 1.14.\n\n\nNote: This NGINX configuration is only an example and may not suit your environment. For complete documentation, see NGINX Load Balancing - HTTP Load Balancing.\n\n\n\nReplace IP_NODE1, IP_NODE2 and IP_NODE3 with the IP addresses of the nodes in your cluster.\nReplace both occurrences of FQDN to the DNS name ","postref":"37a6b5aaa2d6954f31155dee704b0f94","objectID":"6abaaef2d9beea01a01f43fa2daf97ac","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/helm2/helm-rancher/chart-options/"},{"anchor":"#","title":"Helm Chart Options for Kubernetes Installations","content":"Common Options\n\n\n\n\nOption\nDefault Value\nDescription\n\n\n\n\n\nhostname\n” “\nstring - the Fully Qualified Domain Name for your Rancher Server\n\n\n\ningress.tls.source\n“rancher”\nstring - Where to get the cert for the ingress. - “rancher, letsEncrypt, secret”\n\n\n\nletsEncrypt.email\n” “\nstring - Your email address\n\n\n\nletsEncrypt.environment\n“production”\nstring - Valid options: “staging, production”\n\n\n\nprivateCA\nfalse\nbool - Set to true if your cert is signed by a private CA\n\n\n\n\n\n\nAdvanced Options\n\n\n\n\nOption\nDefault Value\nDescription\n\n\n\n\n\nadditionalTrustedCAs\nfalse\nbool - See Additional Trusted CAs\n\n\n\naddLocal\n“auto”\nstring - Have Rancher detect and import the “local” Rancher server cluster Import “local Cluster\n\n\n\nantiAffinity\n“preferred”\nstring - AntiAffinity rule for Rancher pods - “preferred, required”\n\n\n\nauditLog.destination\n“sidecar”\nstring - Stream to sidecar container console or hostPath volume - “sidecar, hostPath”\n\n\n\nauditLog.hostPath\n”/var/log/rancher/audit”\nstring - log file destination on host (only applies when auditLog.destination is set to hostPath)\n\n\n\nauditLog.level\n0\nint - set the API Audit Log level. 0 is off. [0-3]\n\n\n\nauditLog.maxAge\n1\nint - maximum number of days to retain old audit log files (only applies when auditLog.destination is set to hostPath)\n\n\n\nauditLog.maxBackups\n1\nint - maximum number of audit log files to retain (only applies when auditLog.destination is set to hostPath)\n\n\n\nauditLog.maxSize\n100\nint - maximum size in megabytes of the audit log file before it gets rotated (only applies when auditLog.destination is set to hostPath)\n\n\n\nbusyboxImage\n“busybox”\nstring - Image location for busybox image used to collect audit logs Note: Available as of v2.2.0\n\n\n\ndebug\nfalse\nbool - set debug flag on rancher server\n\n\n\ncertmanager.version\n””\nstring - set cert-manager compatibility\n\n\n\n\n\n\n\n\n\nextraEnv\n[]\nlist - set additional environment variables for Rancher Note: Available as of v2.2.0\n\n\n\nimagePullSecrets\n[]\nlist - list of names of Secret resource containing private registry credentials\n\n\n\ningress.extraAnnotations\n{}\nmap - additional annotations to customize the ingress\n\n\n\ningress.configurationSnippet\n””\nstring - Add additional Nginx configuration. Can be used for proxy configuration. Note: Available as of v2.0.15, v2.1.10 and v2.2.4\n\n\n\nproxy\n””\nstring - HTTP[S] proxy server for Rancher\n\n\n\nnoProxy\n“127.0.0.0/8,10.0.0.0/8,172.16.0.0/12,192.168.0.0/16”\nstring - comma separated list of hostnames or ip address not to use the proxy\n\n\n\nresources\n{}\nmap - rancher pod resource requests & limits\n\n\n\nrancherImage\n“rancher/rancher”\nstring - rancher image source\n\n\n\nrancherImageTag\nsame as chart version\nstring - rancher/rancher image tag\n\n\n\ntls\n“ingress”\nstring - See External TLS Termination for details. - “ingress, external”\n\n\n\nsystemDefaultRegistry\n””\nstring - private registry to be used for all system Docker images, e.g., http://registry.example.com/ Available as of v2.3.0\n\n\n\nuseBundledSystemChart\nfalse\nbool - select to use the system-charts packaged with Rancher server. This option is used for air gapped installations. Available as of v2.3.0\n\n\n\n\n\n\nAPI Audit Log\n\nEnabling the API Audit Log.\n\nYou can collect this log as you would any container log. Enable the Logging service under Rancher Tools for the System Project on the Rancher server cluster.\n--set auditLog.level=1\nBy default enabling Audit Logging will create a sidecar container in the Rancher pod. This container (rancher-audit-log) will stream the log to stdout. You can collect this log as you would any container log. When using the sidecar as the audit log destination, the hostPath, maxAge, maxBackups, and maxSize options do not apply. It’s advised to use your OS or Docker daemon’s log rotation features to control disk space use. Enable the Logging service under Rancher Tools for the Rancher server cluster or System Project.\n\nSet the auditLog.destination to hostPath to forward logs to volume shared with the host system instead of streaming to a sidecar container. When setting the destination to hostPath you may want to adjust the other auditLog parameters for log rotation.\n\nSetting Extra Environment Variables\n\nAvailable as of v2.2.0\n\nYou can set extra environment variables for Rancher server using extraEnv. This list uses the same name and value keys as the container manifest definitions. Remember to quote the values.\n--set 'extraEnv[0].name=CATTLE_TLS_MIN_VERSION'\n--set 'extraEnv[0].value=1.0'\nTLS settings\n\nAvailable as of v2.2.0\n\nTo set a different TLS configuration, you can use the CATTLE_TLS_MIN_VERSION and CATTLE_TLS_CIPHERS environment variables. For example, to configure TLS 1.0 as minimum accepted TLS version:\n--set 'extraEnv[0].name=CATTLE_TLS_MIN_VERSION'\n--set 'extraEnv[0].value=1.0'\nSee TLS settings for more information and options.\n\nImport local Cluster\n\nBy default Rancher server will detect and import the local cluster it’s running on. User with access to the local cluster will essentially have “root” access to all the clusters managed by Rancher server.\n\nIf this is a concern in your environment you can set this option to “false” on your initial install.\n\n\nNote: This option is only effective on the initial Rancher install. See Issue 16522 for more information.\n\n--set addLocal=\"false\"\nCustomizing your Ingress\n\nTo customize or use a different ingress with Rancher server you can set your own Ingress annotations.\n\nExample on setting a custom certificate issuer:\n--set ingress.extraAnnotations.'certmanager\\.k8s\\.io/cluster-issuer'=ca-key-pair\nAvailable as of v2.0.15, v2.1.10 and v2.2.4\n\nExample on setting a static proxy header with ingress.configurationSnippet. This value is parsed like a template so variables can be used.\n--set ingress.configurationSnippet='more_set_input_headers X-Forwarded-Host {{ .Values.hostname }};'\nHTTP Proxy\n\nRancher requires internet access for some functionality (helm charts). Use proxy to set your proxy server.\n\nAdd your IP exceptions to the noProxy list. Make sure you add the Service cluster IP range (default: 10.43.0.1⁄16) and any worker cluster controlplane nodes. Rancher supports CIDR notation ranges in this list.\n--set proxy=\"http://<username>:<password>@<proxy_url>:<proxy_port>/\"\n--set noProxy=\"127.0.0.0/8\\,10.0.0.0/8\\,172.16.0.0/12\\,192.168.0.0/16\"\nAdditional Trusted CAs\n\nIf you have private registries, catalogs or a proxy that intercepts certificates, you may need to add additional trusted CAs to Rancher.\n--set additionalTrustedCAs=true\nOnce the Rancher deployment is created, copy your CA certs in pem format into a file named ca-additional.pem and use kubectl to create the tls-ca-additional secret in the cattle-system namespace.\nkubectl -n cattle-system create secret generic tls-ca-additional --from-file=ca-additional.pem\nPrivate Registry and Air Gap Installs\n\nFor details on installing Rancher with a private registry, see:\n\n\nAir Gap: Docker Install\nAir Gap: Kubernetes Install\n\n\nExternal TLS Termination\n\nWe recommend configuring your load balancer as a Layer 4 balancer, forwarding plain 80/tcp and 443/tcp to the Rancher Management cluster nodes. The Ingress Controller on the cluster will redirect http traffic on port 80 to https on port 443.\n\nYou may terminate the SSL/TLS on a L7 load balancer external to the Rancher cluster (ingress). Use the --set tls=external option and point your load balancer at port http 80 on all of the Rancher cluster nodes. This will expose the Rancher interface on http port 80. Be aware that clients that are allowed to connect directly to the Rancher cluster will not be encrypted. If you choose to do this we recommend that you restrict direct access at the network level to just your load balancer.\n\n\nNote: If you are using a Private CA signed certificate, add --set privateCA=true and see Adding TLS Secrets - Using a Private CA Signed Certificate to add the CA cert for Rancher.\n\n\nYour load balancer must support long lived websocket connections and will need to insert proxy headers so Rancher can route links correctly.\n\nConfiguring Ingress for External TLS when Using NGINX v0.25\n\nIn NGINX v0.25, the behavior of NGINX has changed regarding forwarding headers and external TLS termination. Therefore, in the scenario that you are using external TLS termination configuration with NGINX v0.25, you must edit the cluster.yml to enable the use-forwarded-headers option for ingress:\ningress:\n  provider: nginx\n  options:\n    use-forwarded-headers: 'true'\nRequired Headers\n\n\nHost\nX-Forwarded-Proto\nX-Forwarded-Port\nX-Forwarded-For\n\n\nRecommended Timeouts\n\n\nRead Timeout: 1800 seconds\nWrite Timeout: 1800 seconds\nConnect Timeout: 30 seconds\n\n\nHealth Checks\n\nRancher will respond 200 to health checks on the /healthz endpoint.\n\nExample NGINX config\n\nThis NGINX configuration is tested on NGINX 1.14.\n\n\nNote: This NGINX configuration is only an example and may not suit your environment. For complete documentation, see NGINX Load Balancing - HTTP Load Balancing.\n\n\n\nReplace IP_NODE1, IP_NODE2 and IP_NODE3 with the IP addresses of the node","postref":"65b775c31fe356a8007a0651ad04ca0f","objectID":"bb1e8d71a13769d924a30f810d57c6da","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/chart-options/"},{"anchor":"#installation-outline","title":"Installation Outline","content":"How Do I Know if My Certificates are in PEM Format?You can recognize the PEM format by the following traits:\n  The file begins with the following header: -----BEGIN CERTIFICATE-----\n  The header is followed by a long string of characters. Like, really long.\n  The file ends with a footer: -----END CERTIFICATE-----\nPEM Certificate Example:----BEGIN CERTIFICATE-----\nMIIGVDCCBDygAwIBAgIJAMiIrEm29kRLMA0GCSqGSIb3DQEBCwUAMHkxCzAJBgNV\n... more lines\nVWQqljhfacYPgp8KJUJENQ9h5hZ2nSCrI+W00Jcw4QcEdCI8HL5wmg==\n-----END CERTIFICATE-----\nHow Can I Encode My PEM Files in base64?To encode your certificates in base64:\n  Change directory to where the PEM file resides.\n  Run one of the following commands. Replace FILENAME with the name of your certificate.\n    # MacOS\ncat FILENAME | base64\n# Linux\ncat FILENAME | base64 -w0\n# Windows\ncertutil -encode FILENAME FILENAME.base64\n\n  \nHow Can I Verify My Generated base64 String For The Certificates?To decode your certificates in base64:\n  Copy the generated base64 string.\n  Run one of the following commands. Replace YOUR_BASE64_STRING with the previously copied base64\n    string.\n    # MacOS\necho YOUR_BASE64_STRING | base64 -D\n# Linux\necho YOUR_BASE64_STRING | base64 -d\n# Windows\ncertutil -decode FILENAME.base64 FILENAME.verify\n\n  \nWhat is the Order of Certificates if I Want to Add My Intermediate(s)?The order of adding certificates is as follows:-----BEGIN CERTIFICATE-----\n%YOUR_CERTIFICATE%\n-----END CERTIFICATE-----\n-----BEGIN CERTIFICATE-----\n%YOUR_INTERMEDIATE_CERTIFICATE%\n-----END CERTIFICATE-----\nHow Do I Validate My Certificate Chain?You can validate the certificate chain by using the openssl binary. If the output of the command (see\n  the command example below) ends with Verify return code: 0 (ok), your certificate chain is valid. The\n  ca.pem file must be the same as you added to the rancher/rancher container. When using a\n  certificate signed by a recognized Certificate Authority, you can omit the -CAfile parameter.Command:openssl s_client -CAfile ca.pem -connect rancher.yourdomain.com:443 -servername rancher.yourdomain.com\n...\n    Verify return code: 0 (ok)\n\nRecommended: Review Creating Backups—High Availability Back Up and Restoration to learn how to backup your Rancher Server in case of a disaster scenario.\nCreate a Kubernetes cluster: Creating a Cluster.\nDuring installation, RKE automatically generates a config file named kube_config_rancher-cluster.yml in the same directory as the rancher-cluster.yml file. Copy this file and back it up to a safe location. You’ll use this file later when upgrading Rancher Server.With all configuration in place, use RKE to launch Rancher. You can complete this action by running the rke up command and using the --config parameter to point toward your config file.\nFrom your workstation, make sure rancher-cluster.yml and the downloaded rke binary are in the same directory.\n\nOpen a Terminal instance. Change to the directory that contains your config file and rke.\n\nEnter one of the rke up commands listen below.\n\nrke up --config rancher-cluster.yml\n\n\nStep Result: The output should be similar to the snippet below:\n\nINFO[0000] Building Kubernetes cluster\nINFO[0000] [dialer] Setup tunnel for host [1.1.1.1]\nINFO[0000] [network] Deploying port listener containers\nINFO[0000] [network] Pulling image [alpine:latest] on host [1.1.1.1]\n...\nINFO[0101] Finished building Kubernetes cluster successfully\n\nAfter you close your RKE config file, rancher-cluster.yml, back it up to a secure location. You can use this file again when it’s time to upgrade Rancher.The last reference that needs to be replaced is <RANCHER_VERSION>. This needs to be replaced with a Rancher version which is marked as stable. The latest stable release of Rancher can be found in the GitHub README. Make sure the version is an actual version number, and not a named tag like stable or latest. The example below shows the version configured to v2.0.6.      spec:\n        serviceAccountName: cattle-admin\n        containers:\n        - image: rancher/rancher:v2.0.6\n          imagePullPolicy: Always\nThere is one reference to <FQDN> in the RKE config file. Replace this reference with the FQDN you chose in 3. Configure DNS.\nOpen rancher-cluster.yml.\n\nIn the kind: Ingress with name: cattle-ingress-http:\n\nReplace <FQDN> with the FQDN chosen in 3. Configure DNS.\n\nStep Result: After replacing the values, the file should look like the example below (the base64 encoded strings should be different):\n\napiVersion: extensions/v1beta1\n  kind: Ingress\n  metadata:\n    namespace: cattle-system\n    name: cattle-ingress-http\n    annotations:\n      nginx.ingress.kubernetes.io/proxy-connect-timeout: \"30\"\n      nginx.ingress.kubernetes.io/proxy-read-timeout: \"1800\"   # Max time in seconds for ws to remain shell window open\n      nginx.ingress.kubernetes.io/proxy-send-timeout: \"1800\"   # Max time in seconds for ws to remain shell window open\n  spec:\n    rules:\n    - host: rancher.yourdomain.com\n      http:\n        paths:\n        - backend:\n            serviceName: cattle-service\n            servicePort: 80\n\n\nSave the file and close it.\nFor security purposes, SSL (Secure Sockets Layer) is required when using Rancher. SSL secures all Rancher network communication, like when you login or interact with a cluster.Choose from the following options:\n  \n  Option A—Bring Your Own Certificate: Self-Signed\n  \n    \nPrerequisites:\nCreate a self-signed certificate.\n\n\nThe certificate files must be in PEM format.\nThe certificate files must be encoded in base64.\nIn your certificate file, include all intermediate certificates in the chain. Order your certificates with your certificate first, followed by the intermediates. For an example, see SSL FAQ / Troubleshooting.\n\n\n\nIn kind: Secret with name: cattle-keys-ingress, replace <BASE64_CA> with the base64 encoded string of the CA Certificate file (usually called ca.pem or ca.crt)\n\n\nNote: The base64 encoded string should be on the same line as cacerts.pem, without any newline at the beginning, in between or at the end.\n\n\nAfter replacing the values, the file should look like the example below (the base64 encoded strings should be different):\n\n    ---\n    apiVersion: v1\n    kind: Secret\n    metadata:\n        name: cattle-keys-server\n        namespace: cattle-system\n    type: Opaque\n    data:\n        cacerts.pem: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUNvRENDQVlnQ0NRRHVVWjZuMEZWeU16QU5CZ2txaGtpRzl3MEJBUXNGQURBU01SQXdEZ1lEVlFRRERBZDAKWlhOMExXTmhNQjRYRFRFNE1EVXdOakl4TURRd09Wb1hEVEU0TURjd05USXhNRFF3T1Zvd0VqRVFNQTRHQTFVRQpBd3dIZEdWemRDMWpZVENDQVNJd0RRWUpLb1pJaHZjTkFRRUJCUUFEZ2dFUEFEQ0NBUW9DZ2dFQkFNQmpBS3dQCndhRUhwQTdaRW1iWWczaTNYNlppVmtGZFJGckJlTmFYTHFPL2R0RUdmWktqYUF0Wm45R1VsckQxZUlUS3UzVHgKOWlGVlV4Mmo1Z0tyWmpwWitCUnFiZ1BNbk5hS1hocmRTdDRtUUN0VFFZdGRYMVFZS0pUbWF5NU45N3FoNTZtWQprMllKRkpOWVhHWlJabkdMUXJQNk04VHZramF0ZnZOdmJ0WmtkY2orYlY3aWhXanp2d2theHRUVjZlUGxuM2p5CnJUeXBBTDliYnlVcHlad3E2MWQvb0Q4VUtwZ2lZM1dOWmN1YnNvSjhxWlRsTnN6UjVadEFJV0tjSE5ZbE93d2oKaG41RE1tSFpwZ0ZGNW14TU52akxPRUc0S0ZRU3laYlV2QzlZRUhLZTUxbGVxa1lmQmtBZWpPY002TnlWQUh1dApuay9DMHpXcGdENkIwbkVDQXdFQUFUQU5CZ2txaGtpRzl3MEJBUXNGQUFPQ0FRRUFHTCtaNkRzK2R4WTZsU2VBClZHSkMvdzE1bHJ2ZXdia1YxN3hvcmlyNEMxVURJSXB6YXdCdFJRSGdSWXVtblVqOGo4T0hFWUFDUEthR3BTVUsKRDVuVWdzV0pMUUV0TDA2eTh6M3A0MDBrSlZFZW9xZlVnYjQrK1JLRVJrWmowWXR3NEN0WHhwOVMzVkd4NmNOQQozZVlqRnRQd2hoYWVEQmdma1hXQWtISXFDcEsrN3RYem9pRGpXbi8walI2VDcrSGlaNEZjZ1AzYnd3K3NjUDIyCjlDQVZ1ZFg4TWpEQ1hTcll0Y0ZINllBanlCSTJjbDhoSkJqa2E3aERpVC9DaFlEZlFFVFZDM3crQjBDYjF1NWcKdE03Z2NGcUw4OVdhMnp5UzdNdXk5bEthUDBvTXl1Ty82Tm1wNjNsVnRHeEZKSFh4WTN6M0lycGxlbTNZQThpTwpmbmlYZXc9PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==\n\n\n  \n\n  \n  Option B—Bring Your Own Certificate: Signed by Recognized CA\n  \n    If you are using a Certificate Signed By A Recognized Certificate Authority, you don’t need to perform any step in this part.\n\n  \nOnce you have the rancher-cluster.yml config file template, edit the nodes section to point toward your Linux hosts.\nOpen rancher-cluster.yml in your favorite text editor.\n\nUpdate the nodes section with the information of your Linux hosts.\n\nFor each node in your cluster, update the following placeholders: IP_ADDRESS_X and USER. The specified user should be able to access the Docket socket, you can test this by logging in with the specified user and run docker ps.\n\n\nNote:\n\nWhen using RHEL/CentOS, the SSH user can’t be root due to https://bugzilla.redhat.com/show_bug.cgi?id=1527565. See Operating System Requirements for RHEL/CentOS specific requirements.\n\n\nnodes:\n    # The IP address or hostname of the node\n- address: IP_ADDRESS_1\n    # User that can login to the node and has access to the Docker socket (i.e. can execute `docker ps` on the node)\n    # When using RHEL/CentOS, this can't be root due to https://bugzilla.redhat.com/show_bug.cgi?id=1527565\n    user: USER\n    role: [controlplane,etcd,worker]\n    # Path the SSH key that can be used to access to node with the specified user\n    ssh_key_path: ~/.ssh/id_rsa\n- address: IP","postref":"109e8af97c2534fd56f58d4739b98f22","objectID":"9970f658a66ab8f4e0e6308f826478f2","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/helm2/rke-add-on/layer-7-lb/"},{"anchor":"#installation-outline","title":"Installation Outline","content":"How Do I Know if My Certificates are in PEM Format?You can recognize the PEM format by the following traits:\n  The file begins with the following header: -----BEGIN CERTIFICATE-----\n  The header is followed by a long string of characters. Like, really long.\n  The file ends with a footer: -----END CERTIFICATE-----\nPEM Certificate Example:----BEGIN CERTIFICATE-----\nMIIGVDCCBDygAwIBAgIJAMiIrEm29kRLMA0GCSqGSIb3DQEBCwUAMHkxCzAJBgNV\n... more lines\nVWQqljhfacYPgp8KJUJENQ9h5hZ2nSCrI+W00Jcw4QcEdCI8HL5wmg==\n-----END CERTIFICATE-----\nHow Can I Encode My PEM Files in base64?To encode your certificates in base64:\n  Change directory to where the PEM file resides.\n  Run one of the following commands. Replace FILENAME with the name of your certificate.\n    # MacOS\ncat FILENAME | base64\n# Linux\ncat FILENAME | base64 -w0\n# Windows\ncertutil -encode FILENAME FILENAME.base64\n\n  \nHow Can I Verify My Generated base64 String For The Certificates?To decode your certificates in base64:\n  Copy the generated base64 string.\n  Run one of the following commands. Replace YOUR_BASE64_STRING with the previously copied base64\n    string.\n    # MacOS\necho YOUR_BASE64_STRING | base64 -D\n# Linux\necho YOUR_BASE64_STRING | base64 -d\n# Windows\ncertutil -decode FILENAME.base64 FILENAME.verify\n\n  \nWhat is the Order of Certificates if I Want to Add My Intermediate(s)?The order of adding certificates is as follows:-----BEGIN CERTIFICATE-----\n%YOUR_CERTIFICATE%\n-----END CERTIFICATE-----\n-----BEGIN CERTIFICATE-----\n%YOUR_INTERMEDIATE_CERTIFICATE%\n-----END CERTIFICATE-----\nHow Do I Validate My Certificate Chain?You can validate the certificate chain by using the openssl binary. If the output of the command (see\n  the command example below) ends with Verify return code: 0 (ok), your certificate chain is valid. The\n  ca.pem file must be the same as you added to the rancher/rancher container. When using a\n  certificate signed by a recognized Certificate Authority, you can omit the -CAfile parameter.Command:openssl s_client -CAfile ca.pem -connect rancher.yourdomain.com:443 -servername rancher.yourdomain.com\n...\n    Verify return code: 0 (ok)\n\nRecommended: Review Creating Backups—High Availability Back Up and Restoration to learn how to backup your Rancher Server in case of a disaster scenario.\nCreate a Kubernetes cluster: Creating a Cluster.\nDuring installation, RKE automatically generates a config file named kube_config_rancher-cluster.yml in the same directory as the rancher-cluster.yml file. Copy this file and back it up to a safe location. You’ll use this file later when upgrading Rancher Server.With all configuration in place, use RKE to launch Rancher. You can complete this action by running the rke up command and using the --config parameter to point toward your config file.\nFrom your workstation, make sure rancher-cluster.yml and the downloaded rke binary are in the same directory.\n\nOpen a Terminal instance. Change to the directory that contains your config file and rke.\n\nEnter one of the rke up commands listen below.\n\nrke up --config rancher-cluster.yml\n\n\nStep Result: The output should be similar to the snippet below:\n\nINFO[0000] Building Kubernetes cluster\nINFO[0000] [dialer] Setup tunnel for host [1.1.1.1]\nINFO[0000] [network] Deploying port listener containers\nINFO[0000] [network] Pulling image [alpine:latest] on host [1.1.1.1]\n...\nINFO[0101] Finished building Kubernetes cluster successfully\n\nAfter you close your RKE config file, rancher-cluster.yml, back it up to a secure location. You can use this file again when it’s time to upgrade Rancher.The last reference that needs to be replaced is <RANCHER_VERSION>. This needs to be replaced with a Rancher version which is marked as stable. The latest stable release of Rancher can be found in the GitHub README. Make sure the version is an actual version number, and not a named tag like stable or latest. The example below shows the version configured to v2.0.6.      spec:\n        serviceAccountName: cattle-admin\n        containers:\n        - image: rancher/rancher:v2.0.6\n          imagePullPolicy: Always\nThere is one reference to <FQDN> in the RKE config file. Replace this reference with the FQDN you chose in 3. Configure DNS.\nOpen rancher-cluster.yml.\n\nIn the kind: Ingress with name: cattle-ingress-http:\n\nReplace <FQDN> with the FQDN chosen in 3. Configure DNS.\n\nStep Result: After replacing the values, the file should look like the example below (the base64 encoded strings should be different):\n\napiVersion: extensions/v1beta1\n  kind: Ingress\n  metadata:\n    namespace: cattle-system\n    name: cattle-ingress-http\n    annotations:\n      nginx.ingress.kubernetes.io/proxy-connect-timeout: \"30\"\n      nginx.ingress.kubernetes.io/proxy-read-timeout: \"1800\"   # Max time in seconds for ws to remain shell window open\n      nginx.ingress.kubernetes.io/proxy-send-timeout: \"1800\"   # Max time in seconds for ws to remain shell window open\n  spec:\n    rules:\n    - host: rancher.yourdomain.com\n      http:\n        paths:\n        - backend:\n            serviceName: cattle-service\n            servicePort: 80\n\n\nSave the file and close it.\nFor security purposes, SSL (Secure Sockets Layer) is required when using Rancher. SSL secures all Rancher network communication, like when you login or interact with a cluster.Choose from the following options:\n  \n  Option A—Bring Your Own Certificate: Self-Signed\n  \n    \nPrerequisites:\nCreate a self-signed certificate.\n\n\nThe certificate files must be in PEM format.\nThe certificate files must be encoded in base64.\nIn your certificate file, include all intermediate certificates in the chain. Order your certificates with your certificate first, followed by the intermediates. For an example, see SSL FAQ / Troubleshooting.\n\n\n\nIn kind: Secret with name: cattle-keys-ingress, replace <BASE64_CA> with the base64 encoded string of the CA Certificate file (usually called ca.pem or ca.crt)\n\n\nNote: The base64 encoded string should be on the same line as cacerts.pem, without any newline at the beginning, in between or at the end.\n\n\nAfter replacing the values, the file should look like the example below (the base64 encoded strings should be different):\n\n    ---\n    apiVersion: v1\n    kind: Secret\n    metadata:\n        name: cattle-keys-server\n        namespace: cattle-system\n    type: Opaque\n    data:\n        cacerts.pem: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUNvRENDQVlnQ0NRRHVVWjZuMEZWeU16QU5CZ2txaGtpRzl3MEJBUXNGQURBU01SQXdEZ1lEVlFRRERBZDAKWlhOMExXTmhNQjRYRFRFNE1EVXdOakl4TURRd09Wb1hEVEU0TURjd05USXhNRFF3T1Zvd0VqRVFNQTRHQTFVRQpBd3dIZEdWemRDMWpZVENDQVNJd0RRWUpLb1pJaHZjTkFRRUJCUUFEZ2dFUEFEQ0NBUW9DZ2dFQkFNQmpBS3dQCndhRUhwQTdaRW1iWWczaTNYNlppVmtGZFJGckJlTmFYTHFPL2R0RUdmWktqYUF0Wm45R1VsckQxZUlUS3UzVHgKOWlGVlV4Mmo1Z0tyWmpwWitCUnFiZ1BNbk5hS1hocmRTdDRtUUN0VFFZdGRYMVFZS0pUbWF5NU45N3FoNTZtWQprMllKRkpOWVhHWlJabkdMUXJQNk04VHZramF0ZnZOdmJ0WmtkY2orYlY3aWhXanp2d2theHRUVjZlUGxuM2p5CnJUeXBBTDliYnlVcHlad3E2MWQvb0Q4VUtwZ2lZM1dOWmN1YnNvSjhxWlRsTnN6UjVadEFJV0tjSE5ZbE93d2oKaG41RE1tSFpwZ0ZGNW14TU52akxPRUc0S0ZRU3laYlV2QzlZRUhLZTUxbGVxa1lmQmtBZWpPY002TnlWQUh1dApuay9DMHpXcGdENkIwbkVDQXdFQUFUQU5CZ2txaGtpRzl3MEJBUXNGQUFPQ0FRRUFHTCtaNkRzK2R4WTZsU2VBClZHSkMvdzE1bHJ2ZXdia1YxN3hvcmlyNEMxVURJSXB6YXdCdFJRSGdSWXVtblVqOGo4T0hFWUFDUEthR3BTVUsKRDVuVWdzV0pMUUV0TDA2eTh6M3A0MDBrSlZFZW9xZlVnYjQrK1JLRVJrWmowWXR3NEN0WHhwOVMzVkd4NmNOQQozZVlqRnRQd2hoYWVEQmdma1hXQWtISXFDcEsrN3RYem9pRGpXbi8walI2VDcrSGlaNEZjZ1AzYnd3K3NjUDIyCjlDQVZ1ZFg4TWpEQ1hTcll0Y0ZINllBanlCSTJjbDhoSkJqa2E3aERpVC9DaFlEZlFFVFZDM3crQjBDYjF1NWcKdE03Z2NGcUw4OVdhMnp5UzdNdXk5bEthUDBvTXl1Ty82Tm1wNjNsVnRHeEZKSFh4WTN6M0lycGxlbTNZQThpTwpmbmlYZXc9PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==\n\n\n  \n\n  \n  Option B—Bring Your Own Certificate: Signed by Recognized CA\n  \n    If you are using a Certificate Signed By A Recognized Certificate Authority, you don’t need to perform any step in this part.\n\n  \nOnce you have the rancher-cluster.yml config file template, edit the nodes section to point toward your Linux hosts.\nOpen rancher-cluster.yml in your favorite text editor.\n\nUpdate the nodes section with the information of your Linux hosts.\n\nFor each node in your cluster, update the following placeholders: IP_ADDRESS_X and USER. The specified user should be able to access the Docket socket, you can test this by logging in with the specified user and run docker ps.\n\n\nNote:\n\nWhen using RHEL/CentOS, the SSH user can’t be root due to https://bugzilla.redhat.com/show_bug.cgi?id=1527565. See Operating System Requirements for RHEL/CentOS specific requirements.\n\n\nnodes:\n    # The IP address or hostname of the node\n- address: IP_ADDRESS_1\n    # User that can login to the node and has access to the Docker socket (i.e. can execute `docker ps` on the node)\n    # When using RHEL/CentOS, this can't be root due to https://bugzilla.redhat.com/show_bug.cgi?id=1527565\n    user: USER\n    role: [controlplane,etcd,worker]\n    # Path the SSH key that can be used to access to node with the specified user\n    ssh_key_path: ~/.ssh/id_rsa\n- address: IP","postref":"45b7a997e1bd158ff908b17f9f6b6fa1","objectID":"580a61a52e898070fb3b8fa18e6f04d7","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/rke-add-on/layer-7-lb/"},{"anchor":"#","title":"RKE Add-On Install","content":"\nImportant: RKE add-on install is only supported up to Rancher v2.0.8\n\nPlease use the Rancher helm chart to install Rancher on a Kubernetes cluster. For details, see the Kubernetes Install - Installation Outline.\n\nIf you are currently using the RKE add-on install method, see Migrating from a Kubernetes Install with an RKE Add-on for details on how to move to using the helm chart.\n\n\n\nKubernetes installation with External Load Balancer (TCP/Layer 4)\nKubernetes installation with External Load Balancer (HTTPS/Layer 7)\nHTTP Proxy Configuration for a Kubernetes installation\nTroubleshooting RKE Add-on Installs\n\n","postref":"1c3969c10bce938db9438a15e5b6ee1f","objectID":"6316a6ea0dc44109e656576114c0cc23","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/helm2/rke-add-on/"},{"anchor":"#","title":"RKE Add-On Install","content":"\nImportant: RKE add-on install is only supported up to Rancher v2.0.8\n\nPlease use the Rancher helm chart to install Rancher on a Kubernetes cluster. For details, see the Kubernetes Install - Installation Outline.\n\nIf you are currently using the RKE add-on install method, see Migrating from a Kubernetes Install with an RKE Add-on for details on how to move to using the helm chart.\n\n\n\nKubernetes Installation with External Load Balancer (TCP/Layer 4)\nKubernetes Installation with External Load Balancer (HTTPS/Layer 7)\nHTTP Proxy Configuration for a Kubernetes installation\nTroubleshooting RKE Add-on Installs\n\n","postref":"05c5d919297b84d2bc333d694beb7ff0","objectID":"d8b864aea5c73b3115e10fe9ce55dd8b","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/rke-add-on/"},{"anchor":"#","title":"Troubleshooting","content":"Helm commands show forbidden\n\nWhen Helm is initiated in the cluster without specifying the correct ServiceAccount, the command helm init will succeed but you won’t be able to execute most of the other helm commands. The following error will be shown:\n\nError: configmaps is forbidden: User \"system:serviceaccount:kube-system:default\" cannot list configmaps in the namespace \"kube-system\"\n\n\nTo resolve this, the server component (tiller) needs to be removed and added with the correct ServiceAccount. You can use helm reset --force to remove the tiller from the cluster. Please check if it is removed using helm version --server.\n\nhelm reset --force\nTiller (the Helm server-side component) has been uninstalled from your Kubernetes Cluster.\nhelm version --server\nError: could not find tiller\n\n\nWhen you have confirmed that tiller has been removed, please follow the steps provided in Initialize Helm (Install tiller) to install tiller with the correct ServiceAccount.\n","postref":"a5e0787c503b1c5270c48cf3d303613f","objectID":"f7063b19e20ff1e49447a81563cd7fa9","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/helm2/helm-init/troubleshooting/"},{"anchor":"#","title":"Troubleshooting","content":"Where is everything\n\nMost of the troubleshooting will be done on objects in these 3 namespaces.\n\n\ncattle-system - rancher deployment and pods.\ningress-nginx - Ingress controller pods and services.\nkube-system - tiller and cert-manager pods.\n\n\n“default backend - 404”\n\nA number of things can cause the ingress-controller not to forward traffic to your rancher instance. Most of the time its due to a bad ssl configuration.\n\nThings to check\n\n\nIs Rancher Running\nCert CN is “Kubernetes Ingress Controller Fake Certificate”\n\n\nIs Rancher Running\n\nUse kubectl to check the cattle-system system namespace and see if the Rancher pods are in a Running state.\n\nkubectl -n cattle-system get pods\n\nNAME                           READY     STATUS    RESTARTS   AGE\npod/rancher-784d94f59b-vgqzh   1/1       Running   0          10m\n\n\nIf the state is not Running, run a describe on the pod and check the Events.\n\nkubectl -n cattle-system describe pod\n\n...\nEvents:\n  Type     Reason                 Age   From                Message\n  ----     ------                 ----  ----                -------\n  Normal   Scheduled              11m   default-scheduler   Successfully assigned rancher-784d94f59b-vgqzh to localhost\n  Normal   SuccessfulMountVolume  11m   kubelet, localhost  MountVolume.SetUp succeeded for volume \"rancher-token-dj4mt\"\n  Normal   Pulling                11m   kubelet, localhost  pulling image \"rancher/rancher:v2.0.4\"\n  Normal   Pulled                 11m   kubelet, localhost  Successfully pulled image \"rancher/rancher:v2.0.4\"\n  Normal   Created                11m   kubelet, localhost  Created container\n  Normal   Started                11m   kubelet, localhost  Started container\n\n\nChecking the rancher logs\n\nUse kubectl to list the pods.\n\nkubectl -n cattle-system get pods\n\nNAME                           READY     STATUS    RESTARTS   AGE\npod/rancher-784d94f59b-vgqzh   1/1       Running   0          10m\n\n\nUse kubectl and the pod name to list the logs from the pod.\n\nkubectl -n cattle-system logs -f rancher-784d94f59b-vgqzh\n\n\nCert CN is “Kubernetes Ingress Controller Fake Certificate”\n\nUse your browser to check the certificate details. If it says the Common Name is “Kubernetes Ingress Controller Fake Certificate”, something may have gone wrong with reading or issuing your SSL cert.\n\n\nNote: if you are using LetsEncrypt to issue certs it can sometimes take a few minuets to issue the cert.\n\n\ncert-manager issued certs (Rancher Generated or LetsEncrypt)\n\ncert-manager has 3 parts.\n\n\ncert-manager pod in the kube-system namespace.\nIssuer object in the cattle-system namespace.\nCertificate object in the cattle-system namespace.\n\n\nWork backwards and do a kubectl describe on each object and check the events. You can track down what might be missing.\n\nFor example there is a problem with the Issuer:\n\nkubectl -n cattle-system describe certificate\n...\nEvents:\n  Type     Reason          Age                 From          Message\n  ----     ------          ----                ----          -------\n  Warning  IssuerNotReady  18s (x23 over 19m)  cert-manager  Issuer rancher not ready\n\n\nkubectl -n cattle-system describe issuer\n...\nEvents:\n  Type     Reason         Age                 From          Message\n  ----     ------         ----                ----          -------\n  Warning  ErrInitIssuer  19m (x12 over 19m)  cert-manager  Error initializing issuer: secret \"tls-rancher\" not found\n  Warning  ErrGetKeyPair  9m (x16 over 19m)   cert-manager  Error getting keypair for CA issuer: secret \"tls-rancher\" not found\n\n\nBring Your Own SSL Certs\n\nYour certs get applied directly to the Ingress object in the cattle-system namespace.\n\nCheck the status of the Ingress object and see if its ready.\n\nkubectl -n cattle-system describe ingress\n\n\nIf its ready and the SSL is still not working you may have a malformed cert or secret.\n\nCheck the nginx-ingress-controller logs. Because the nginx-ingress-controller has multiple containers in its pod you will need to specify the name of the container.\n\nkubectl -n ingress-nginx logs -f nginx-ingress-controller-rfjrq nginx-ingress-controller\n...\nW0705 23:04:58.240571       7 backend_ssl.go:49] error obtaining PEM from secret cattle-system/tls-rancher-ingress: error retrieving secret cattle-system/tls-rancher-ingress: secret cattle-system/tls-rancher-ingress was not found\n\n\nno matches for kind “Issuer”\n\nThe SSL configuration option you have chosen requires cert-manager to be installed before installing Rancher or else the following error is shown:\n\nError: validation failed: unable to recognize \"\": no matches for kind \"Issuer\" in version \"certmanager.k8s.io/v1alpha1\"\n\n\nInstall cert-manager and try installing Rancher again.\n","postref":"56ec789de407bba763ac0ec3380644e9","objectID":"2bd032543f555be9c11a193292ecc01c","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/helm2/helm-rancher/troubleshooting/"},{"anchor":"#","title":"Troubleshooting","content":"canal Pods show READY 2⁄3\n\nThe most common cause of this issue is port 8472/UDP is not open between the nodes. Check your local firewall, network routing or security groups.\n\nOnce the network issue is resolved, the canal pods should timeout and restart to establish their connections.\n\nnginx-ingress-controller Pods show RESTARTS\n\nThe most common cause of this issue is the canal pods have failed to establish the overlay network. See canal Pods show READY 2/3 for troubleshooting.\n\nFailed to set up SSH tunneling for host [xxx.xxx.xxx.xxx]: Can’t retrieve Docker Info\n\nFailed to dial to /var/run/docker.sock: ssh: rejected: administratively prohibited (open failed)\n\n\nUser specified to connect with does not have permission to access the Docker socket. This can be checked by logging into the host and running the command docker ps:\n\n\n$ ssh user@server\nuser@server$ docker ps\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                    NAMES\n\n\nSee Manage Docker as a non-root user how to set this up properly.\n\n\nWhen using RedHat/CentOS as operating system, you cannot use the user root to connect to the nodes because of Bugzilla #1527565. You will need to add a separate user and configure it to access the Docker socket. See Manage Docker as a non-root user how to set this up properly.\n\nSSH server version is not version 6.7 or higher. This is needed for socket forwarding to work, which is used to connect to the Docker socket over SSH. This can be checked using sshd -V on the host you are connecting to, or using netcat:\n\n$ nc xxx.xxx.xxx.xxx 22\nSSH-2.0-OpenSSH_6.6.1p1 Ubuntu-2ubuntu2.10\n\n\n\nFailed to dial ssh using address [xxx.xxx.xxx.xxx:xx]: Error configuring SSH: ssh: no key found\n\n\nThe key file specified as ssh_key_path cannot be accessed. Make sure that you specified the private key file (not the public key, .pub), and that the user that is running the rke command can access the private key file.\n\n\nFailed to dial ssh using address [xxx.xxx.xxx.xxx:xx]: ssh: handshake failed: ssh: unable to authenticate, attempted methods [none publickey], no supported methods remain\n\n\nThe key file specified as ssh_key_path is not correct for accessing the node. Double-check if you specified the correct ssh_key_path for the node and if you specified the correct user to connect with.\n\n\nFailed to dial ssh using address [xxx.xxx.xxx.xxx:xx]: Error configuring SSH: ssh: cannot decode encrypted private keys\n\n\nIf you want to use encrypted private keys, you should use ssh-agent to load your keys with your passphrase. If the SSH_AUTH_SOCK environment variable is found in the environment where the rke command is run, it will be used automatically to connect to the node.\n\n\nCannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\n\n\nThe node is not reachable on the configured address and port.\n\n","postref":"3a98b26b0c2c2e2ccf7d07e2bb821c8b","objectID":"b47c0f2c7c49802414f7082870daf1f0","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/helm2/kubernetes-rke/troubleshooting/"},{"anchor":"#","title":"Troubleshooting the Rancher Server Kubernetes Cluster","content":"This section describes how to troubleshoot an installation of Rancher on a Kubernetes cluster.\n\nRelevant Namespaces\n\nMost of the troubleshooting will be done on objects in these 3 namespaces.\n\n\ncattle-system - rancher deployment and pods.\ningress-nginx - Ingress controller pods and services.\nkube-system - tiller and cert-manager pods.\n\n\n“default backend - 404”\n\nA number of things can cause the ingress-controller not to forward traffic to your rancher instance. Most of the time its due to a bad ssl configuration.\n\nThings to check\n\n\nIs Rancher Running\nCert CN is “Kubernetes Ingress Controller Fake Certificate”\n\n\nCheck if Rancher is Running\n\nUse kubectl to check the cattle-system system namespace and see if the Rancher pods are in a Running state.\n\nkubectl -n cattle-system get pods\n\nNAME                           READY     STATUS    RESTARTS   AGE\npod/rancher-784d94f59b-vgqzh   1/1       Running   0          10m\n\n\nIf the state is not Running, run a describe on the pod and check the Events.\n\nkubectl -n cattle-system describe pod\n\n...\nEvents:\n  Type     Reason                 Age   From                Message\n  ----     ------                 ----  ----                -------\n  Normal   Scheduled              11m   default-scheduler   Successfully assigned rancher-784d94f59b-vgqzh to localhost\n  Normal   SuccessfulMountVolume  11m   kubelet, localhost  MountVolume.SetUp succeeded for volume \"rancher-token-dj4mt\"\n  Normal   Pulling                11m   kubelet, localhost  pulling image \"rancher/rancher:v2.0.4\"\n  Normal   Pulled                 11m   kubelet, localhost  Successfully pulled image \"rancher/rancher:v2.0.4\"\n  Normal   Created                11m   kubelet, localhost  Created container\n  Normal   Started                11m   kubelet, localhost  Started container\n\n\nCheck the Rancher Logs\n\nUse kubectl to list the pods.\n\nkubectl -n cattle-system get pods\n\nNAME                           READY     STATUS    RESTARTS   AGE\npod/rancher-784d94f59b-vgqzh   1/1       Running   0          10m\n\n\nUse kubectl and the pod name to list the logs from the pod.\n\nkubectl -n cattle-system logs -f rancher-784d94f59b-vgqzh\n\n\nCert CN is “Kubernetes Ingress Controller Fake Certificate”\n\nUse your browser to check the certificate details. If it says the Common Name is “Kubernetes Ingress Controller Fake Certificate”, something may have gone wrong with reading or issuing your SSL cert.\n\n\nNote: if you are using LetsEncrypt to issue certs it can sometimes take a few minuets to issue the cert.\n\n\nChecking for issues with cert-manager issued certs (Rancher Generated or LetsEncrypt)\n\ncert-manager has 3 parts.\n\n\ncert-manager pod in the kube-system namespace.\nIssuer object in the cattle-system namespace.\nCertificate object in the cattle-system namespace.\n\n\nWork backwards and do a kubectl describe on each object and check the events. You can track down what might be missing.\n\nFor example there is a problem with the Issuer:\n\nkubectl -n cattle-system describe certificate\n...\nEvents:\n  Type     Reason          Age                 From          Message\n  ----     ------          ----                ----          -------\n  Warning  IssuerNotReady  18s (x23 over 19m)  cert-manager  Issuer rancher not ready\n\n\nkubectl -n cattle-system describe issuer\n...\nEvents:\n  Type     Reason         Age                 From          Message\n  ----     ------         ----                ----          -------\n  Warning  ErrInitIssuer  19m (x12 over 19m)  cert-manager  Error initializing issuer: secret \"tls-rancher\" not found\n  Warning  ErrGetKeyPair  9m (x16 over 19m)   cert-manager  Error getting keypair for CA issuer: secret \"tls-rancher\" not found\n\n\nChecking for Issues with Your Own SSL Certs\n\nYour certs get applied directly to the Ingress object in the cattle-system namespace.\n\nCheck the status of the Ingress object and see if its ready.\n\nkubectl -n cattle-system describe ingress\n\n\nIf its ready and the SSL is still not working you may have a malformed cert or secret.\n\nCheck the nginx-ingress-controller logs. Because the nginx-ingress-controller has multiple containers in its pod you will need to specify the name of the container.\n\nkubectl -n ingress-nginx logs -f nginx-ingress-controller-rfjrq nginx-ingress-controller\n...\nW0705 23:04:58.240571       7 backend_ssl.go:49] error obtaining PEM from secret cattle-system/tls-rancher-ingress: error retrieving secret cattle-system/tls-rancher-ingress: secret cattle-system/tls-rancher-ingress was not found\n\n\nNo matches for kind “Issuer”\n\nThe SSL configuration option you have chosen requires cert-manager to be installed before installing Rancher or else the following error is shown:\n\nError: validation failed: unable to recognize \"\": no matches for kind \"Issuer\" in version \"certmanager.k8s.io/v1alpha1\"\n\n\nInstall cert-manager and try installing Rancher again.\n\nCanal Pods show READY 2⁄3\n\nThe most common cause of this issue is port 8472/UDP is not open between the nodes. Check your local firewall, network routing or security groups.\n\nOnce the network issue is resolved, the canal pods should timeout and restart to establish their connections.\n\nnginx-ingress-controller Pods show RESTARTS\n\nThe most common cause of this issue is the canal pods have failed to establish the overlay network. See canal Pods show READY 2/3 for troubleshooting.\n\nFailed to dial to /var/run/docker.sock: ssh: rejected: administratively prohibited (open failed)\n\nSome causes of this error include:\n\n\nUser specified to connect with does not have permission to access the Docker socket. This can be checked by logging into the host and running the command docker ps:\n\n\n$ ssh user@server\nuser@server$ docker ps\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                    NAMES\n\n\nSee Manage Docker as a non-root user how to set this up properly.\n\n\nWhen using RedHat/CentOS as operating system, you cannot use the user root to connect to the nodes because of Bugzilla #1527565. You will need to add a separate user and configure it to access the Docker socket. See Manage Docker as a non-root user how to set this up properly.\n\nSSH server version is not version 6.7 or higher. This is needed for socket forwarding to work, which is used to connect to the Docker socket over SSH. This can be checked using sshd -V on the host you are connecting to, or using netcat:\n\n$ nc xxx.xxx.xxx.xxx 22\nSSH-2.0-OpenSSH_6.6.1p1 Ubuntu-2ubuntu2.10\n\n\n\nFailed to dial ssh using address [xxx.xxx.xxx.xxx:xx]: Error configuring SSH: ssh: no key found\n\nThe key file specified as ssh_key_path cannot be accessed. Make sure that you specified the private key file (not the public key, .pub), and that the user that is running the rke command can access the private key file.\n\nFailed to dial ssh using address [xxx.xxx.xxx.xxx:xx]: ssh: handshake failed: ssh: unable to authenticate, attempted methods [none publickey], no supported methods remain\n\nThe key file specified as ssh_key_path is not correct for accessing the node. Double-check if you specified the correct ssh_key_path for the node and if you specified the correct user to connect with.\n\nFailed to dial ssh using address [xxx.xxx.xxx.xxx:xx]: Error configuring SSH: ssh: cannot decode encrypted private keys\n\nIf you want to use encrypted private keys, you should use ssh-agent to load your keys with your passphrase. If the SSH_AUTH_SOCK environment variable is found in the environment where the rke command is run, it will be used automatically to connect to the node.\n\nCannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\n\nThe node is not reachable on the configured address and port.\n","postref":"bfad45969d1a96d22a8ff65a920c56d6","objectID":"32a6bda1866e7d205b6478579c4436ec","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/troubleshooting/"},{"anchor":"#objectives","title":"Objectives","content":"Next, add your Linux nodes to your target group.Amazon Documentation: Register Targets with Your Target GroupCreate Your ALBUse Amazon’s Wizard to create an Application Load Balancer. As part of this process, you’ll add the target group you created in Create Target Group.\nFrom your web browser, navigate to the Amazon EC2 Console.\n\nFrom the navigation pane, choose LOAD BALANCING > Load Balancers.\n\nClick Create Load Balancer.\n\nChoose Application Load Balancer.\n\nComplete the Step 1: Configure Load Balancer form.\n\n\nBasic Configuration\n\n\nName: rancher-http\nScheme: internet-facing\nIP address type: ipv4\n\n\nListeners\n\nAdd the Load Balancer Protocols and Load Balancer Ports below.\n\n\nHTTP: 80\nHTTPS: 443\n\n\nAvailability Zones\n\n\nSelect Your VPC and Availability Zones.\n\n\n\nComplete the Step 2: Configure Security Settings form.\n\nConfigure the certificate you want to use for SSL termination.\n\nComplete the Step 3: Configure Security Groups form.\n\nComplete the Step 4: Configure Routing form.\n\n\nFrom the Target Group drop-down, choose Existing target group.\n\nAdd target group rancher-http-80.\n\n\nComplete Step 5: Register Targets. Since you registered your targets earlier, all you have to do it click Next: Review.\n\nComplete Step 6: Review. Look over the load balancer details and click Create when you’re satisfied.\n\nAfter AWS creates the ALB, click Close.\nYour first ALB configuration step is to create one target group for HTTP.Log into the Amazon AWS Console to get started.The document below will guide you through this process. Use the data in the tables below to complete the procedure.Amazon Documentation: Create a Target GroupTarget Group (HTTP)\n\n\nOption\nSetting\n\n\n\n\n\nTarget Group Name\nrancher-http-80\n\n\n\nProtocol\nHTTP\n\n\n\nPort\n80\n\n\n\nTarget type\ninstance\n\n\n\nVPC\nChoose your VPC\n\n\n\nProtocol(Health Check)\nHTTP\n\n\n\nPath(Health Check)\n/healthz\n\n\nConfiguring an Amazon ALB is a multistage process. We’ve broken it down into multiple tasks so that it’s easy to follow.\nCreate Target Group\n\nBegin by creating one target group for the http protocol. You’ll add your Linux nodes to this group.\n\nRegister Targets\n\nAdd your Linux nodes to the target group.\n\nCreate Your ALB\n\nUse Amazon’s Wizard to create an Application Load Balancer. As part of this process, you’ll add the target groups you created in 1. Create Target Groups.\n","postref":"c5896ce49173b544e860518f60d1291f","objectID":"8f39959b2ecc6cff23b0641e746898f9","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/helm2/rke-add-on/layer-7-lb/alb/"},{"anchor":"#objectives","title":"Objectives","content":"Next, add your Linux nodes to your target group.Amazon Documentation: Register Targets with Your Target GroupCreate Your ALBUse Amazon’s Wizard to create an Application Load Balancer. As part of this process, you’ll add the target group you created in Create Target Group.\nFrom your web browser, navigate to the Amazon EC2 Console.\n\nFrom the navigation pane, choose LOAD BALANCING > Load Balancers.\n\nClick Create Load Balancer.\n\nChoose Application Load Balancer.\n\nComplete the Step 1: Configure Load Balancer form.\n\n\nBasic Configuration\n\n\nName: rancher-http\nScheme: internet-facing\nIP address type: ipv4\n\n\nListeners\n\nAdd the Load Balancer Protocols and Load Balancer Ports below.\n\n\nHTTP: 80\nHTTPS: 443\n\n\nAvailability Zones\n\n\nSelect Your VPC and Availability Zones.\n\n\n\nComplete the Step 2: Configure Security Settings form.\n\nConfigure the certificate you want to use for SSL termination.\n\nComplete the Step 3: Configure Security Groups form.\n\nComplete the Step 4: Configure Routing form.\n\n\nFrom the Target Group drop-down, choose Existing target group.\n\nAdd target group rancher-http-80.\n\n\nComplete Step 5: Register Targets. Since you registered your targets earlier, all you have to do it click Next: Review.\n\nComplete Step 6: Review. Look over the load balancer details and click Create when you’re satisfied.\n\nAfter AWS creates the ALB, click Close.\nYour first ALB configuration step is to create one target group for HTTP.Log into the Amazon AWS Console to get started.The document below will guide you through this process. Use the data in the tables below to complete the procedure.Amazon Documentation: Create a Target GroupTarget Group (HTTP)\n\n\nOption\nSetting\n\n\n\n\n\nTarget Group Name\nrancher-http-80\n\n\n\nProtocol\nHTTP\n\n\n\nPort\n80\n\n\n\nTarget type\ninstance\n\n\n\nVPC\nChoose your VPC\n\n\n\nProtocol(Health Check)\nHTTP\n\n\n\nPath(Health Check)\n/healthz\n\n\nConfiguring an Amazon ALB is a multistage process. We’ve broken it down into multiple tasks so that it’s easy to follow.\nCreate Target Group\n\nBegin by creating one target group for the http protocol. You’ll add your Linux nodes to this group.\n\nRegister Targets\n\nAdd your Linux nodes to the target group.\n\nCreate Your ALB\n\nUse Amazon’s Wizard to create an Application Load Balancer. As part of this process, you’ll add the target groups you created in 1. Create Target Groups.\n","postref":"46c191836b27aeaafc043ba153958362","objectID":"ece86b65eddd86af5cb72f4432d6ee2e","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/rke-add-on/layer-7-lb/alb/"},{"anchor":"#objectives","title":"Objectives","content":"\nSelect your newly created NLB and select the Listeners tab.\n\nClick Add listener.\n\nUse TCP:80 as Protocol : Port\n\nClick Add action and choose Forward to…\n\nFrom the Forward to drop-down, choose rancher-tcp-80.\n\nClick Save in the top right of the screen.\nUse Amazon’s Wizard to create an Network Load Balancer. As part of this process, you’ll add the target groups you created in Create Target Groups.\nFrom your web browser, navigate to the Amazon EC2 Console.\n\nFrom the navigation pane, choose LOAD BALANCING > Load Balancers.\n\nClick Create Load Balancer.\n\nChoose Network Load Balancer and click Create.\n\nComplete the Step 1: Configure Load Balancer form.\n\n\nBasic Configuration\n\n\nName: rancher\nScheme: internal or internet-facing\n\n\n\nThe Scheme that you choose for your NLB is dependent on the configuration of your instances/VPC. If your instances do not have public IPs associated with them, or you will only be accessing Rancher internally, you should set your NLB Scheme to internal rather than internet-facing.\n\nListeners\n\nAdd the Load Balancer Protocols and Load Balancer Ports below.\n\n\nTCP: 443\n\n\nAvailability Zones\n\n\nSelect Your VPC and Availability Zones.\n\n\n\nComplete the Step 2: Configure Routing form.\n\n\nFrom the Target Group drop-down, choose Existing target group.\n\nFrom the Name drop-down, choose rancher-tcp-443.\n\nOpen Advanced health check settings, and configure Interval to 10 seconds.\n\n\nComplete Step 3: Register Targets. Since you registered your targets earlier, all you have to do is click Next: Review.\n\nComplete Step 4: Review. Look over the load balancer details and click Create when you’re satisfied.\n\nAfter AWS creates the NLB, click Close.\nNext, add your Linux nodes to both target groups.Select the target group named rancher-tcp-443, click the tab Targets and choose Edit.Select the instances (Linux nodes) you want to add, and click Add to registered.Screenshot Add targets to target group TCP port 443Screenshot Added targets to target group TCP port 443When the instances are added, click Save on the bottom right of the screen.Repeat those steps, replacing rancher-tcp-443 with rancher-tcp-80. The same instances need to be added as targets to this target group.Your first NLB configuration step is to create two target groups. Technically, only port 443 is needed to access Rancher, but its convenient to add a listener for port 80 which will be redirected to port 443 automatically. The NGINX ingress controller on the nodes will make sure that port 80 gets redirected to port 443.Log into the Amazon AWS Console to get started, make sure to select the Region where your EC2 instances (Linux nodes) are created.The Target Groups configuration resides in the Load Balancing section of the EC2 service. Select Services and choose EC2, find the section Load Balancing and open Target Groups.Click Create target group to create the first target group, regarding TCP port 443.Target Group (TCP port 443)Configure the first target group according to the table below. Screenshots of the configuration are shown just below the table.\n\n\nOption\nSetting\n\n\n\n\n\nTarget Group Name\nrancher-tcp-443\n\n\n\nProtocol\nTCP\n\n\n\nPort\n443\n\n\n\nTarget type\ninstance\n\n\n\nVPC\nChoose your VPC\n\n\n\nProtocol(Health Check)\nHTTP\n\n\n\nPath(Health Check)\n/healthz\n\n\n\nPort (Advanced health check)\noverride,80\n\n\n\nHealthy threshold (Advanced health)\n3\n\n\n\nUnhealthy threshold (Advanced)\n3\n\n\n\nTimeout (Advanced)\n6 seconds\n\n\n\nInterval (Advanced)\n10 second\n\n\n\nSuccess codes\n200-399\n\n\nScreenshot Target group TCP port 443 settings\n\n\n\n    \n    \n    \n    \n    \n    \n\nScreenshot Target group TCP port 443 Advanced settings\n\n\n\n    \n    \n    \n    \n    \n    \n\nClick Create target group to create the second target group, regarding TCP port 80.Target Group (TCP port 80)Configure the second target group according to the table below. Screenshots of the configuration are shown just below the table.\n\n\nOption\nSetting\n\n\n\n\n\nTarget Group Name\nrancher-tcp-80\n\n\n\nProtocol\nTCP\n\n\n\nPort\n80\n\n\n\nTarget type\ninstance\n\n\n\nVPC\nChoose your VPC\n\n\n\nProtocol(Health Check)\nHTTP\n\n\n\nPath(Health Check)\n/healthz\n\n\n\nPort (Advanced health check)\ntraffic port\n\n\n\nHealthy threshold (Advanced health)\n3\n\n\n\nUnhealthy threshold (Advanced)\n3\n\n\n\nTimeout (Advanced)\n6 seconds\n\n\n\nInterval (Advanced)\n10 second\n\n\n\nSuccess codes\n200-399\n\n\nScreenshot Target group TCP port 80 settings\n\n\n\n    \n    \n    \n    \n    \n    \n\nScreenshot Target group TCP port 80 Advanced settings\n\n\n\n    \n    \n    \n    \n    \n    \n\nConfiguring an Amazon NLB is a multistage process. We’ve broken it down into multiple tasks so that it’s easy to follow.\nCreate Target Groups\n\nBegin by creating two target groups for the TCP protocol, one regarding TCP port 443 and one regarding TCP port 80 (providing redirect to TCP port 443). You’ll add your Linux nodes to these groups.\n\nRegister Targets\n\nAdd your Linux nodes to the target groups.\n\nCreate Your NLB\n\nUse Amazon’s Wizard to create an Network Load Balancer. As part of this process, you’ll add the target groups you created in 1. Create Target Groups.\n\nNote: Rancher only supports using the Amazon NLB when terminating traffic in tcp mode for port 443 rather than tls mode. This is due to the fact that the NLB does not inject the correct headers into requests when terminated at the NLB. This means that if you want to use certificates managed by the Amazon Certificate Manager (ACM), you should use an ELB or ALB.\n","postref":"02a9ac33d744d2c1c269e92304bc12fa","objectID":"635f7a92758b9168088bb3ac0d70501e","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/helm2/create-nodes-lb/nlb/"},{"anchor":"#objectives","title":"Objectives","content":"\nSelect your newly created NLB and select the Listeners tab.\n\nClick Add listener.\n\nUse TCP:80 as Protocol : Port\n\nClick Add action and choose Forward to…\n\nFrom the Forward to drop-down, choose rancher-tcp-80.\n\nClick Save in the top right of the screen.\nUse Amazon’s Wizard to create an Network Load Balancer. As part of this process, you’ll add the target groups you created in Create Target Groups.\nFrom your web browser, navigate to the Amazon EC2 Console.\n\nFrom the navigation pane, choose LOAD BALANCING > Load Balancers.\n\nClick Create Load Balancer.\n\nChoose Network Load Balancer and click Create.\n\nComplete the Step 1: Configure Load Balancer form.\n\n\nBasic Configuration\n\n\nName: rancher\nScheme: internet-facing\n\n\nListeners\n\nAdd the Load Balancer Protocols and Load Balancer Ports below.\n\n\nTCP: 443\n\n\nAvailability Zones\n\n\nSelect Your VPC and Availability Zones.\n\n\n\nComplete the Step 2: Configure Routing form.\n\n\nFrom the Target Group drop-down, choose Existing target group.\n\nFrom the Name drop-down, choose rancher-tcp-443.\n\nOpen Advanced health check settings, and configure Interval to 10 seconds.\n\n\nComplete Step 3: Register Targets. Since you registered your targets earlier, all you have to do is click Next: Review.\n\nComplete Step 4: Review. Look over the load balancer details and click Create when you’re satisfied.\n\nAfter AWS creates the NLB, click Close.\nNext, add your Linux nodes to both target groups.Select the target group named rancher-tcp-443, click the tab Targets and choose Edit.Select the instances (Linux nodes) you want to add, and click Add to registered.Screenshot Add targets to target group TCP port 443Screenshot Added targets to target group TCP port 443When the instances are added, click Save on the bottom right of the screen.Repeat those steps, replacing rancher-tcp-443 with rancher-tcp-80. The same instances need to be added as targets to this target group.Your first NLB configuration step is to create two target groups. Technically, only port 443 is needed to access Rancher, but its convenient to add a listener for port 80 which will be redirected to port 443 automatically. The NGINX controller on the nodes will make sure that port 80 gets redirected to port 443.Log into the Amazon AWS Console to get started, make sure to select the Region where your EC2 instances (Linux nodes) are created.The Target Groups configuration resides in the Load Balancing section of the EC2 service. Select Services and choose EC2, find the section Load Balancing and open Target Groups.Click Create target group to create the first target group, regarding TCP port 443.Target Group (TCP port 443)Configure the first target group according to the table below. Screenshots of the configuration are shown just below the table.\n\n\nOption\nSetting\n\n\n\n\n\nTarget Group Name\nrancher-tcp-443\n\n\n\nProtocol\nTCP\n\n\n\nPort\n443\n\n\n\nTarget type\ninstance\n\n\n\nVPC\nChoose your VPC\n\n\n\nProtocol(Health Check)\nHTTP\n\n\n\nPath(Health Check)\n/healthz\n\n\n\nPort (Advanced health check)\noverride,80\n\n\n\nHealthy threshold (Advanced health)\n3\n\n\n\nUnhealthy threshold (Advanced)\n3\n\n\n\nTimeout (Advanced)\n6 seconds\n\n\n\nInterval (Advanced)\n10 second\n\n\n\nSuccess codes\n200-399\n\n\nScreenshot Target group TCP port 443 settings\n\n\n\n    \n    \n    \n    \n    \n    \n\nScreenshot Target group TCP port 443 Advanced settings\n\n\n\n    \n    \n    \n    \n    \n    \n\nClick Create target group to create the second target group, regarding TCP port 80.Target Group (TCP port 80)Configure the second target group according to the table below. Screenshots of the configuration are shown just below the table.\n\n\nOption\nSetting\n\n\n\n\n\nTarget Group Name\nrancher-tcp-80\n\n\n\nProtocol\nTCP\n\n\n\nPort\n80\n\n\n\nTarget type\ninstance\n\n\n\nVPC\nChoose your VPC\n\n\n\nProtocol(Health Check)\nHTTP\n\n\n\nPath(Health Check)\n/healthz\n\n\n\nPort (Advanced health check)\ntraffic port\n\n\n\nHealthy threshold (Advanced health)\n3\n\n\n\nUnhealthy threshold (Advanced)\n3\n\n\n\nTimeout (Advanced)\n6 seconds\n\n\n\nInterval (Advanced)\n10 second\n\n\n\nSuccess codes\n200-399\n\n\nScreenshot Target group TCP port 80 settings\n\n\n\n    \n    \n    \n    \n    \n    \n\nScreenshot Target group TCP port 80 Advanced settings\n\n\n\n    \n    \n    \n    \n    \n    \n\nConfiguring an Amazon NLB is a multistage process. We’ve broken it down into multiple tasks so that it’s easy to follow.\nCreate Target Groups\n\nBegin by creating two target groups for the TCP protocol, one regarding TCP port 443 and one regarding TCP port 80 (providing redirect to TCP port 443). You’ll add your Linux nodes to these groups.\n\nRegister Targets\n\nAdd your Linux nodes to the target groups.\n\nCreate Your NLB\n\nUse Amazon’s Wizard to create an Network Load Balancer. As part of this process, you’ll add the target groups you created in 1. Create Target Groups.\n","postref":"34a7f7e6e428cecbb97d4e8eae429575","objectID":"b5c1a92562c90fb2f08b09744493ec84","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/helm2/rke-add-on/layer-4-lb/nlb/"},{"anchor":"#objectives","title":"Objectives","content":"\nSelect your newly created NLB and select the Listeners tab.\n\nClick Add listener.\n\nUse TCP:80 as Protocol : Port\n\nClick Add action and choose Forward to…\n\nFrom the Forward to drop-down, choose rancher-tcp-80.\n\nClick Save in the top right of the screen.\nUse Amazon’s Wizard to create an Network Load Balancer. As part of this process, you’ll add the target groups you created in Create Target Groups.\nFrom your web browser, navigate to the Amazon EC2 Console.\n\nFrom the navigation pane, choose LOAD BALANCING > Load Balancers.\n\nClick Create Load Balancer.\n\nChoose Network Load Balancer and click Create.\n\nComplete the Step 1: Configure Load Balancer form.\n\n\nBasic Configuration\n\n\nName: rancher\nScheme: internet-facing\n\n\nListeners\n\nAdd the Load Balancer Protocols and Load Balancer Ports below.\n\n\nTCP: 443\n\n\nAvailability Zones\n\n\nSelect Your VPC and Availability Zones.\n\n\n\nComplete the Step 2: Configure Routing form.\n\n\nFrom the Target Group drop-down, choose Existing target group.\n\nFrom the Name drop-down, choose rancher-tcp-443.\n\nOpen Advanced health check settings, and configure Interval to 10 seconds.\n\n\nComplete Step 3: Register Targets. Since you registered your targets earlier, all you have to do is click Next: Review.\n\nComplete Step 4: Review. Look over the load balancer details and click Create when you’re satisfied.\n\nAfter AWS creates the NLB, click Close.\nNext, add your Linux nodes to both target groups.Select the target group named rancher-tcp-443, click the tab Targets and choose Edit.Select the instances (Linux nodes) you want to add, and click Add to registered.Screenshot Add targets to target group TCP port 443Screenshot Added targets to target group TCP port 443When the instances are added, click Save on the bottom right of the screen.Repeat those steps, replacing rancher-tcp-443 with rancher-tcp-80. The same instances need to be added as targets to this target group.Your first NLB configuration step is to create two target groups. Technically, only port 443 is needed to access Rancher, but its convenient to add a listener for port 80 which will be redirected to port 443 automatically. The NGINX controller on the nodes will make sure that port 80 gets redirected to port 443.Log into the Amazon AWS Console to get started, make sure to select the Region where your EC2 instances (Linux nodes) are created.The Target Groups configuration resides in the Load Balancing section of the EC2 service. Select Services and choose EC2, find the section Load Balancing and open Target Groups.Click Create target group to create the first target group, regarding TCP port 443.Target Group (TCP port 443)Configure the first target group according to the table below. Screenshots of the configuration are shown just below the table.\n\n\nOption\nSetting\n\n\n\n\n\nTarget Group Name\nrancher-tcp-443\n\n\n\nProtocol\nTCP\n\n\n\nPort\n443\n\n\n\nTarget type\ninstance\n\n\n\nVPC\nChoose your VPC\n\n\n\nProtocol(Health Check)\nHTTP\n\n\n\nPath(Health Check)\n/healthz\n\n\n\nPort (Advanced health check)\noverride,80\n\n\n\nHealthy threshold (Advanced health)\n3\n\n\n\nUnhealthy threshold (Advanced)\n3\n\n\n\nTimeout (Advanced)\n6 seconds\n\n\n\nInterval (Advanced)\n10 second\n\n\n\nSuccess codes\n200-399\n\n\nScreenshot Target group TCP port 443 settings\n\n\n\n    \n    \n    \n    \n    \n    \n\nScreenshot Target group TCP port 443 Advanced settings\n\n\n\n    \n    \n    \n    \n    \n    \n\nClick Create target group to create the second target group, regarding TCP port 80.Target Group (TCP port 80)Configure the second target group according to the table below. Screenshots of the configuration are shown just below the table.\n\n\nOption\nSetting\n\n\n\n\n\nTarget Group Name\nrancher-tcp-80\n\n\n\nProtocol\nTCP\n\n\n\nPort\n80\n\n\n\nTarget type\ninstance\n\n\n\nVPC\nChoose your VPC\n\n\n\nProtocol(Health Check)\nHTTP\n\n\n\nPath(Health Check)\n/healthz\n\n\n\nPort (Advanced health check)\ntraffic port\n\n\n\nHealthy threshold (Advanced health)\n3\n\n\n\nUnhealthy threshold (Advanced)\n3\n\n\n\nTimeout (Advanced)\n6 seconds\n\n\n\nInterval (Advanced)\n10 second\n\n\n\nSuccess codes\n200-399\n\n\nScreenshot Target group TCP port 80 settings\n\n\n\n    \n    \n    \n    \n    \n    \n\nScreenshot Target group TCP port 80 Advanced settings\n\n\n\n    \n    \n    \n    \n    \n    \n\nConfiguring an Amazon NLB is a multistage process. We’ve broken it down into multiple tasks so that it’s easy to follow.\nCreate Target Groups\n\nBegin by creating two target groups for the TCP protocol, one regarding TCP port 443 and one regarding TCP port 80 (providing redirect to TCP port 443). You’ll add your Linux nodes to these groups.\n\nRegister Targets\n\nAdd your Linux nodes to the target groups.\n\nCreate Your NLB\n\nUse Amazon’s Wizard to create an Network Load Balancer. As part of this process, you’ll add the target groups you created in 1. Create Target Groups.\n","postref":"c66d472989a3a3eb6ed168b36bba906c","objectID":"1ee274dcf9612b503f9f8b89b265ffbe","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/rke-add-on/layer-4-lb/nlb/"},{"anchor":"#kubernetes-installation","title":"Kubernetes installation","content":"When using Kubernetes installation, the environment variables need to be added to the RKE Config File template.\nKubernetes Installation with External Load Balancer (TCP/Layer 4) RKE Config File Template\nKubernetes Installation with External Load Balancer (HTTPS/Layer 7) RKE Config File Template\nThe environment variables should be defined in the Deployment inside the RKE Config File Template. You only have to add the part starting with env: to (but not including) ports:. Make sure the indentation is identical to the preceding name:. Required values for NO_PROXY are:\nlocalhost\n127.0.0.1\n0.0.0.0\nConfigured service_cluster_ip_range (default: 10.43.0.0/16)\nThe example below is based on a proxy server accessible at http://192.168.0.1:3128, and excluding usage of the proxy when accessing network range 192.168.10.0/24, the configured service_cluster_ip_range (10.43.0.0/16) and every hostname under the domain example.com. If you have changed the service_cluster_ip_range, you have to update the value below accordingly....\n---\n  kind: Deployment\n  apiVersion: extensions/v1beta1\n  metadata:\n    namespace: cattle-system\n    name: cattle\n  spec:\n    replicas: 1\n    template:\n      metadata:\n        labels:\n          app: cattle\n      spec:\n        serviceAccountName: cattle-admin\n        containers:\n        - image: rancher/rancher:latest\n          imagePullPolicy: Always\n          name: cattle-server\n          env:\n          - name: HTTP_PROXY\n            value: \"http://192.168.10.1:3128\"\n          - name: HTTPS_PROXY\n            value: \"http://192.168.10.1:3128\"\n          - name: NO_PROXY\n            value: \"localhost,127.0.0.1,0.0.0.0,10.43.0.0/16,192.168.10.0/24,example.com\"\n          ports:\n...","postref":"854ddb290d87a00197ff06b6feaa0e1a","objectID":"406c181a4c7da9ed7b5a5881b9aa450a","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/helm2/rke-add-on/proxy/"},{"anchor":"#installing-rancher-on-a-kubernetes-cluster","title":"Installing Rancher on a Kubernetes Cluster","content":"When using Kubernetes installation, the environment variables need to be added to the RKE Config File template.\nKubernetes Installation with External Load Balancer (TCP/Layer 4) RKE Config File Template\nKubernetes Installation with External Load Balancer (HTTPS/Layer 7) RKE Config File Template\nThe environment variables should be defined in the Deployment inside the RKE Config File Template. You only have to add the part starting with env: to (but not including) ports:. Make sure the indentation is identical to the preceding name:. Required values for NO_PROXY are:\nlocalhost\n127.0.0.1\n0.0.0.0\nConfigured service_cluster_ip_range (default: 10.43.0.0/16)\nThe example below is based on a proxy server accessible at http://192.168.0.1:3128, and excluding usage of the proxy when accessing network range 192.168.10.0/24, the configured service_cluster_ip_range (10.43.0.0/16) and every hostname under the domain example.com. If you have changed the service_cluster_ip_range, you have to update the value below accordingly....\n---\n  kind: Deployment\n  apiVersion: extensions/v1beta1\n  metadata:\n    namespace: cattle-system\n    name: cattle\n  spec:\n    replicas: 1\n    template:\n      metadata:\n        labels:\n          app: cattle\n      spec:\n        serviceAccountName: cattle-admin\n        containers:\n        - image: rancher/rancher:latest\n          imagePullPolicy: Always\n          name: cattle-server\n          env:\n          - name: HTTP_PROXY\n            value: \"http://192.168.10.1:3128\"\n          - name: HTTPS_PROXY\n            value: \"http://192.168.10.1:3128\"\n          - name: NO_PROXY\n            value: \"localhost,127.0.0.1,0.0.0.0,10.43.0.0/16,192.168.10.0/24,example.com\"\n          ports:\n...","postref":"2050ed4bb27e0fe84a9e28e02ac16321","objectID":"87f7cdb888db9f9c14b4ef02f6227ce0","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/rke-add-on/proxy/"},{"anchor":"#install-nginx","title":"Install NGINX","content":"You should now be to able to browse to https://FQDN.\nReload or restart NGINX\n\n# Reload NGINX\nnginx -s reload\n\n# Restart NGINX\n# Depending on your Linux distribution\nservice nginx restart\nsystemctl restart nginx\n\nSee Example NGINX config.Start by installing NGINX on your load balancer host. NGINX has packages available for all known operating systems.For help installing NGINX, refer to their install documentation.","postref":"b067d89111fb29e5b513e1bb0fbf7833","objectID":"f7d49617bf1389ca314efb206a4c583b","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/helm2/rke-add-on/layer-7-lb/nginx/"},{"anchor":"#install-nginx","title":"Install NGINX","content":"You should now be to able to browse to https://FQDN.\nReload or restart NGINX\n\n# Reload NGINX\nnginx -s reload\n\n# Restart NGINX\n# Depending on your Linux distribution\nservice nginx restart\nsystemctl restart nginx\n\nSee Example NGINX config.Start by installing NGINX on your load balancer host. NGINX has packages available for all known operating systems.For help installing NGINX, refer to their install documentation.","postref":"dd68e8efb439721e6d7aa12b65c8be11","objectID":"b1ae0897234c91e8bb23e3c0e75f8dbf","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/rke-add-on/layer-7-lb/nginx/"},{"anchor":"#","title":"Setting up an Amazon NLB Load Balancer","content":"This how-to guide describes how to set up a load balancer in Amazon’s EC2 service that will direct traffic to multiple instances on EC2.\n\n\nNote: Rancher only supports using the Amazon NLB when terminating traffic in tcp mode for port 443 rather than tls mode. This is due to the fact that the NLB does not inject the correct headers into requests when terminated at the NLB. This means that if you want to use certificates managed by the Amazon Certificate Manager (ACM), you should use an ELB or ALB.\n\n\nConfiguring an Amazon NLB is a multistage process:\n\n\nCreate Target Groups\nRegister Targets\nCreate Your NLB\nAdd listener to NLB for TCP port 80\n\n\n\nPrerequisite: These instructions assume you have already created Linux instances in EC2. The load balancer will direct traffic to these two nodes.\n\n\n1. Create Target Groups\n\nBegin by creating two target groups for the TCP protocol, one with TCP port 443 and one regarding TCP port 80 (providing redirect to TCP port 443). You’ll add your Linux nodes to these groups.\n\nYour first NLB configuration step is to create two target groups. Technically, only port 443 is needed to access Rancher, but its convenient to add a listener for port 80 which will be redirected to port 443 automatically. The NGINX ingress controller on the nodes will make sure that port 80 gets redirected to port 443.\n\n\nLog into the Amazon AWS Console to get started. Make sure to select the Region where your EC2 instances (Linux nodes) are created.\nSelect Services and choose EC2, find the section Load Balancing and open Target Groups.\nClick Create target group to create the first target group, regarding TCP port 443.\n\n\nTarget Group (TCP port 443)\n\nConfigure the first target group according to the table below. Screenshots of the configuration are shown just below the table.\n\n\n\n\nOption\nSetting\n\n\n\n\n\nTarget Group Name\nrancher-tcp-443\n\n\n\nProtocol\nTCP\n\n\n\nPort\n443\n\n\n\nTarget type\ninstance\n\n\n\nVPC\nChoose your VPC\n\n\n\nProtocol(Health Check)\nHTTP\n\n\n\nPath(Health Check)\n/healthz\n\n\n\nPort (Advanced health check)\noverride,80\n\n\n\nHealthy threshold (Advanced health)\n3\n\n\n\nUnhealthy threshold (Advanced)\n3\n\n\n\nTimeout (Advanced)\n6 seconds\n\n\n\nInterval (Advanced)\n10 second\n\n\n\nSuccess codes\n200-399\n\n\n\n\nClick Create target group to create the second target group, regarding TCP port 80.\n\nTarget Group (TCP port 80)\n\nConfigure the second target group according to the table below. Screenshots of the configuration are shown just below the table.\n\n\n\n\nOption\nSetting\n\n\n\n\n\nTarget Group Name\nrancher-tcp-80\n\n\n\nProtocol\nTCP\n\n\n\nPort\n80\n\n\n\nTarget type\ninstance\n\n\n\nVPC\nChoose your VPC\n\n\n\nProtocol(Health Check)\nHTTP\n\n\n\nPath(Health Check)\n/healthz\n\n\n\nPort (Advanced health check)\ntraffic port\n\n\n\nHealthy threshold (Advanced health)\n3\n\n\n\nUnhealthy threshold (Advanced)\n3\n\n\n\nTimeout (Advanced)\n6 seconds\n\n\n\nInterval (Advanced)\n10 second\n\n\n\nSuccess codes\n200-399\n\n\n\n\n2. Register Targets\n\nNext, add your Linux nodes to both target groups.\n\nSelect the target group named rancher-tcp-443, click the tab Targets and choose Edit.\n\n\n\n\n    \n    \n    \n    \n    \n    \n\n\n\nSelect the instances (Linux nodes) you want to add, and click Add to registered.\n\n\n\nScreenshot Add targets to target group TCP port 443\n\n\n\n\n    \n    \n    \n    \n    \n    \n\n\n\n\n\nScreenshot Added targets to target group TCP port 443\n\n\n\n\n    \n    \n    \n    \n    \n    \n\n\n\nWhen the instances are added, click Save on the bottom right of the screen.\n\nRepeat those steps, replacing rancher-tcp-443 with rancher-tcp-80. The same instances need to be added as targets to this target group.\n\n3. Create Your NLB\n\nUse Amazon’s Wizard to create an Network Load Balancer. As part of this process, you’ll add the target groups you created in 1. Create Target Groups.\n\nUse Amazon’s Wizard to create an Network Load Balancer. As part of this process, you’ll add the target groups you created in Create Target Groups.\n\n\nFrom your web browser, navigate to the Amazon EC2 Console.\n\nFrom the navigation pane, choose LOAD BALANCING > Load Balancers.\n\nClick Create Load Balancer.\n\nChoose Network Load Balancer and click Create. Then complete each form.\n\n\n\nStep 1: Configure Load Balancer\nStep 2: Configure Routing\nStep 3: Register Targets\nStep 4: Review\n\n\nStep 1: Configure Load Balancer\n\nSet the following fields in the form:\n\n\nName: rancher\nScheme: internal or internet-facing. The scheme that you choose for your NLB is dependent on the configuration of your instances and VPC. If your instances do not have public IPs associated with them, or you will only be accessing Rancher internally, you should set your NLB Scheme to internal rather than internet-facing.\nListeners: The Load Balancer Protocol should be TCP and the corresponding Load Balancer Port should be set to 443.\nAvailability Zones: Select Your VPC and Availability Zones.\n\n\nStep 2: Configure Routing\n\n\nFrom the Target Group drop-down, choose Existing target group.\nFrom the Name drop-down, choose rancher-tcp-443.\nOpen Advanced health check settings, and configure Interval to 10 seconds.\n\n\nStep 3: Register Targets\n\nSince you registered your targets earlier, all you have to do is click Next: Review.\n\nStep 4: Review\n\nLook over the load balancer details and click Create when you’re satisfied.\n\nAfter AWS creates the NLB, click Close.\n\n4. Add listener to NLB for TCP port 80\n\n\nSelect your newly created NLB and select the Listeners tab.\n\nClick Add listener.\n\nUse TCP:80 as Protocol : Port\n\nClick Add action and choose Forward to…\n\nFrom the Forward to drop-down, choose rancher-tcp-80.\n\nClick Save in the top right of the screen.\n\n","postref":"ee18e38cf7221a6811e21390ea084a6c","objectID":"b53f0acb31b44cab54904253af376fac","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/k8s-install/create-nodes-lb/nlb/"},{"anchor":"#","title":"3. Install Kubernetes with RKE (Kubernetes Installs Only)","content":"This section is about how to prepare to launch a Kubernetes cluster which is used to deploy Rancher server for your air gapped environment.\n\nSince a Kubernetes Installation requires a Kubernetes cluster, we will create a Kubernetes cluster using Rancher Kubernetes Engine (RKE). Before being able to start your Kubernetes cluster, you’ll need to install RKE and create a RKE config file.\n\n\nA. Create an RKE Config File\nB. Run RKE\nC. Save Your Files\n\n\nA. Create an RKE Config File\n\nFrom a system that can access ports 22/tcp and 6443/tcp on your host nodes, use the sample below to create a new file named rancher-cluster.yml. This file is a Rancher Kubernetes Engine configuration file (RKE config file), which is a configuration for the cluster you’re deploying Rancher to.\n\nReplace values in the code sample below with help of the RKE Options table. Use the IP address or DNS names of the 3 nodes you created.\n\n\nTip: For more details on the options available, see the RKE Config Options.\n\n\nRKE Options\n\n\n\n\nOption\nRequired\nDescription\n\n\n\n\n\naddress\n✓\nThe DNS or IP address for the node within the air gap network.\n\n\n\nuser\n✓\nA user that can run docker commands.\n\n\n\nrole\n✓\nList of Kubernetes roles assigned to the node.\n\n\n\ninternal_address\noptional1\nThe DNS or IP address used for internal cluster traffic.\n\n\n\nssh_key_path\n\nPath to SSH private key used to authenticate to the node (defaults to ~/.ssh/id_rsa).\n\n\n\n\n\n1 Some services like AWS EC2 require setting the internal_address if you want to use self-referencing security groups or firewalls.\n\nnodes:\n  - address: 10.10.3.187 # node air gap network IP\n    internal_address: 172.31.7.22 # node intra-cluster IP\n    user: rancher\n    role: ['controlplane', 'etcd', 'worker']\n    ssh_key_path: /home/user/.ssh/id_rsa\n  - address: 10.10.3.254 # node air gap network IP\n    internal_address: 172.31.13.132 # node intra-cluster IP\n    user: rancher\n    role: ['controlplane', 'etcd', 'worker']\n    ssh_key_path: /home/user/.ssh/id_rsa\n  - address: 10.10.3.89 # node air gap network IP\n    internal_address: 172.31.3.216 # node intra-cluster IP\n    user: rancher\n    role: ['controlplane', 'etcd', 'worker']\n    ssh_key_path: /home/user/.ssh/id_rsa\n\nprivate_registries:\n  - url: <REGISTRY.YOURDOMAIN.COM:PORT> # private registry url\n    user: rancher\n    password: '*********'\n    is_default: true\nB. Run RKE\n\nAfter configuring rancher-cluster.yml, bring up your Kubernetes cluster:\n\nrke up --config ./rancher-cluster.yml\n\n\nC. Save Your Files\n\n\nImportant\nThe files mentioned below are needed to maintain, troubleshoot and upgrade your cluster.\n\n\nSave a copy of the following files in a secure location:\n\n\nrancher-cluster.yml: The RKE cluster configuration file.\nkube_config_rancher-cluster.yml: The Kubeconfig file for the cluster, this file contains credentials for full access to the cluster.\nrancher-cluster.rkestate: The Kubernetes Cluster State file, this file contains credentials for full access to the cluster.The Kubernetes Cluster State file is only created when using RKE v0.2.0 or higher.\n\n\n\nNote: The “rancher-cluster” parts of the two latter file names are dependent on how you name the RKE cluster configuration file.\n\n\nNext: Install Rancher\n","postref":"d23894624ad58c0e19b347b9bd3ae855","objectID":"ed04b8e6d375ab80151f4ddd6a7b0056","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/air-gap-helm2/launch-kubernetes/"},{"anchor":"#","title":"3. Install Kubernetes with RKE (Kubernetes Installs Only)","content":"This section is about how to prepare to launch a Kubernetes cluster which is used to deploy Rancher server for your air gapped environment.\n\nSince a Kubernetes Installation requires a Kubernetes cluster, we will create a Kubernetes cluster using Rancher Kubernetes Engine (RKE). Before being able to start your Kubernetes cluster, you’ll need to install RKE and create a RKE config file.\n\n\nA. Create an RKE Config File\nB. Run RKE\nC. Save Your Files\n\n\nA. Create an RKE Config File\n\nFrom a system that can access ports 22/tcp and 6443/tcp on your host nodes, use the sample below to create a new file named rancher-cluster.yml. This file is a Rancher Kubernetes Engine configuration file (RKE config file), which is a configuration for the cluster you’re deploying Rancher to.\n\nReplace values in the code sample below with help of the RKE Options table. Use the IP address or DNS names of the 3 nodes you created.\n\n\nTip: For more details on the options available, see the RKE Config Options.\n\n\nRKE Options\n\n\n\n\nOption\nRequired\nDescription\n\n\n\n\n\naddress\n✓\nThe DNS or IP address for the node within the air gap network.\n\n\n\nuser\n✓\nA user that can run docker commands.\n\n\n\nrole\n✓\nList of Kubernetes roles assigned to the node.\n\n\n\ninternal_address\noptional1\nThe DNS or IP address used for internal cluster traffic.\n\n\n\nssh_key_path\n\nPath to SSH private key used to authenticate to the node (defaults to ~/.ssh/id_rsa).\n\n\n\n\n\n1 Some services like AWS EC2 require setting the internal_address if you want to use self-referencing security groups or firewalls.\n\nnodes:\n  - address: 10.10.3.187 # node air gap network IP\n    internal_address: 172.31.7.22 # node intra-cluster IP\n    user: rancher\n    role: ['controlplane', 'etcd', 'worker']\n    ssh_key_path: /home/user/.ssh/id_rsa\n  - address: 10.10.3.254 # node air gap network IP\n    internal_address: 172.31.13.132 # node intra-cluster IP\n    user: rancher\n    role: ['controlplane', 'etcd', 'worker']\n    ssh_key_path: /home/user/.ssh/id_rsa\n  - address: 10.10.3.89 # node air gap network IP\n    internal_address: 172.31.3.216 # node intra-cluster IP\n    user: rancher\n    role: ['controlplane', 'etcd', 'worker']\n    ssh_key_path: /home/user/.ssh/id_rsa\n\nprivate_registries:\n  - url: <REGISTRY.YOURDOMAIN.COM:PORT> # private registry url\n    user: rancher\n    password: '*********'\n    is_default: true\nB. Run RKE\n\nAfter configuring rancher-cluster.yml, bring up your Kubernetes cluster:\n\nrke up --config ./rancher-cluster.yml\n\n\nC. Save Your Files\n\n\nImportant\nThe files mentioned below are needed to maintain, troubleshoot and upgrade your cluster.\n\n\nSave a copy of the following files in a secure location:\n\n\nrancher-cluster.yml: The RKE cluster configuration file.\nkube_config_rancher-cluster.yml: The Kubeconfig file for the cluster, this file contains credentials for full access to the cluster.\nrancher-cluster.rkestate: The Kubernetes Cluster State file, this file contains the current state of the cluster including the RKE configuration and the certificates.The Kubernetes Cluster State file is only created when using RKE v0.2.0 or higher.\n\n\n\nNote: The “rancher-cluster” parts of the two latter file names are dependent on how you name the RKE cluster configuration file.\n\n\nIssues or errors?\n\nSee the Troubleshooting page.\n\nNext: Install Rancher\n","postref":"276cd56d80ce159e4cb099f817bf64f4","objectID":"45307fbe6f4ba054bec56c4930e9e296","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/other-installation-methods/air-gap/launch-kubernetes/"},{"anchor":"#in-line-arguments","title":"In-line Arguments","content":"Enable API Auditing using RKE by adding arguments to your Rancher container.To enable API auditing:\nAdd API Auditing arguments (args) to your Rancher container.\nDeclare a mountPath in the volumeMounts directive of the container.\nDeclare a path in the volumes directive.\nFor more information about each argument, its syntax, and how to view API Audit logs, see Rancher v2.0 Documentation: API Auditing....\ncontainers:\n        - image: rancher/rancher:latest\n          imagePullPolicy: Always\n          name: cattle-server\n          args: [\"--audit-log-path\", \"/var/log/auditlog/rancher-api-audit.log\", \"--audit-log-maxbackup\", \"5\", \"--audit-log-maxsize\", \"50\", \"--audit-level\", \"2\"]\n          ports:\n          - containerPort: 80\n            protocol: TCP\n          - containerPort: 443\n            protocol: TCP\n          volumeMounts:\n          - mountPath: /etc/rancher/ssl\n            name: cattle-keys-volume\n            readOnly: true\n          - mountPath: /var/log/auditlog\n            name: audit-log-dir\n        volumes:\n        - name: cattle-keys-volume\n          secret:\n            defaultMode: 420\n            secretName: cattle-keys-server\n        - name: audit-log-dir\n          hostPath:\n            path: /var/log/rancher/auditlog\n            type: Directory","postref":"289c53658fb5afbdaa137b6065d940ae","objectID":"1e3f63fd5bed523abf710d0d664414e8","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/helm2/rke-add-on/api-auditing/"},{"anchor":"#in-line-arguments","title":"In-line Arguments","content":"Enable API Auditing using RKE by adding arguments to your Rancher container.To enable API auditing:\nAdd API Auditing arguments (args) to your Rancher container.\nDeclare a mountPath in the volumeMounts directive of the container.\nDeclare a path in the volumes directive.\nFor more information about each argument, its syntax, and how to view API Audit logs, see Rancher v2.0 Documentation: API Auditing....\ncontainers:\n        - image: rancher/rancher:latest\n          imagePullPolicy: Always\n          name: cattle-server\n          args: [\"--audit-log-path\", \"/var/log/auditlog/rancher-api-audit.log\", \"--audit-log-maxbackup\", \"5\", \"--audit-log-maxsize\", \"50\", \"--audit-level\", \"2\"]\n          ports:\n          - containerPort: 80\n            protocol: TCP\n          - containerPort: 443\n            protocol: TCP\n          volumeMounts:\n          - mountPath: /etc/rancher/ssl\n            name: cattle-keys-volume\n            readOnly: true\n          - mountPath: /var/log/auditlog\n            name: audit-log-dir\n        volumes:\n        - name: cattle-keys-volume\n          secret:\n            defaultMode: 420\n            secretName: cattle-keys-server\n        - name: audit-log-dir\n          hostPath:\n            path: /var/log/rancher/auditlog\n            type: Directory","postref":"ab2941001ee84ad020b1409df6d467c2","objectID":"f4e860b85e0889bfeee6ad16c9b76753","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/rke-add-on/api-auditing/"},{"anchor":"#minimal-cluster-yml-example","title":"Minimal cluster.yml example","content":"nodes:\n    - address: 1.1.1.1\n      user: ubuntu\n      role:\n        - controlplane\n        - etcd\n      ssh_key_path: /home/user/.ssh/id_rsa\n      port: 2222\n    - address: 2.2.2.2\n      user: ubuntu\n      role:\n        - worker\n      ssh_key: |-\n        -----BEGIN RSA PRIVATE KEY-----\n\n        -----END RSA PRIVATE KEY-----\n    - address: example.com\n      user: ubuntu\n      role:\n        - worker\n      hostname_override: node3\n      internal_address: 192.168.1.6\n      labels:\n        app: ingress\n\n# If set to true, RKE will not fail when unsupported Docker version\n# are found\nignore_docker_version: false\n\n# Cluster level SSH private key\n# Used if no ssh information is set for the node\nssh_key_path: ~/.ssh/test\n\n# Enable use of SSH agent to use SSH private keys with passphrase\n# This requires the environment `SSH_AUTH_SOCK` configured pointing\n#to your SSH agent which has the private key added\nssh_agent_auth: true\n\n# List of registry credentials\n# If you are using a Docker Hub registry, you can omit the `url`\n# or set it to `docker.io`\n# is_default set to `true` will override the system default\n# registry set in the global settings\nprivate_registries:\n     - url: registry.com\n       user: Username\n       password: password\n       is_default: true\n\n# Bastion/Jump host configuration\nbastion_host:\n    address: x.x.x.x\n    user: ubuntu\n    port: 22\n    ssh_key_path: /home/user/.ssh/bastion_rsa\n# or\n#   ssh_key: |-\n#     -----BEGIN RSA PRIVATE KEY-----\n#\n#     -----END RSA PRIVATE KEY-----\n\n# Set the name of the Kubernetes cluster  \ncluster_name: mycluster\n\n\n# The Kubernetes version used. The default versions of Kubernetes\n# are tied to specific versions of the system images.\n#\n# For RKE v0.2.x and below, the map of Kubernetes versions and their system images is\n# located here:\n# https://github.com/rancher/types/blob/release/v2.2/apis/management.cattle.io/v3/k8s_defaults.go\n#\n# For RKE v0.3.0 and above, the map of Kubernetes versions and their system images is\n# located here:\n# https://github.com/rancher/kontainer-driver-metadata/blob/master/rke/k8s_rke_system_images.go\n#\n# In case the kubernetes_version and kubernetes image in\n# system_images are defined, the system_images configuration\n# will take precedence over kubernetes_version.\nkubernetes_version: v1.10.3-rancher2\n\n# System Images are defaulted to a tag that is mapped to a specific\n# Kubernetes Version and not required in a cluster.yml. \n# Each individual system image can be specified if you want to use a different tag.\n#\n# For RKE v0.2.x and below, the map of Kubernetes versions and their system images is\n# located here:\n# https://github.com/rancher/types/blob/release/v2.2/apis/management.cattle.io/v3/k8s_defaults.go\n#\n# For RKE v0.3.0 and above, the map of Kubernetes versions and their system images is\n# located here:\n# https://github.com/rancher/kontainer-driver-metadata/blob/master/rke/k8s_rke_system_images.go\n#\nsystem_images:\n    kubernetes: rancher/hyperkube:v1.10.3-rancher2\n    etcd: rancher/coreos-etcd:v3.1.12\n    alpine: rancher/rke-tools:v0.1.9\n    nginx_proxy: rancher/rke-tools:v0.1.9\n    cert_downloader: rancher/rke-tools:v0.1.9\n    kubernetes_services_sidecar: rancher/rke-tools:v0.1.9\n    kubedns: rancher/k8s-dns-kube-dns-amd64:1.14.8\n    dnsmasq: rancher/k8s-dns-dnsmasq-nanny-amd64:1.14.8\n    kubedns_sidecar: rancher/k8s-dns-sidecar-amd64:1.14.8\n    kubedns_autoscaler: rancher/cluster-proportional-autoscaler-amd64:1.0.0\n    pod_infra_container: rancher/pause-amd64:3.1\n\nservices:\n    etcd:\n      # if external etcd is used\n      # path: /etcdcluster\n      # external_urls:\n      #   - https://etcd-example.com:2379\n      # ca_cert: |-\n      #   -----BEGIN CERTIFICATE-----\n      #   xxxxxxxxxx\n      #   -----END CERTIFICATE-----\n      # cert: |-\n      #   -----BEGIN CERTIFICATE-----\n      #   xxxxxxxxxx\n      #   -----END CERTIFICATE-----\n      # key: |-\n      #   -----BEGIN PRIVATE KEY-----\n      #   xxxxxxxxxx\n      #   -----END PRIVATE KEY-----\n    # Note for Rancher v2.0.5 and v2.0.6 users: If you are configuring\n    # Cluster Options using a Config File when creating Rancher Launched\n    # Kubernetes, the names of services should contain underscores\n    # only: `kube_api`.\n    kube-api:\n      # IP range for any services created on Kubernetes\n      # This must match the service_cluster_ip_range in kube-controller\n      service_cluster_ip_range: 10.43.0.0/16\n      # Expose a different port range for NodePort services\n      service_node_port_range: 30000-32767    \n      pod_security_policy: false\n      # Add additional arguments to the kubernetes API server\n      # This WILL OVERRIDE any existing defaults\n      extra_args:\n        # Enable audit log to stdout\n        audit-log-path: \"-\"\n        # Increase number of delete workers\n        delete-collection-workers: 3\n        # Set the level of log output to debug-level\n        v: 4\n    # Note for Rancher 2 users: If you are configuring Cluster Options\n    # using a Config File when creating Rancher Launched Kubernetes,\n    # the names of services should contain underscores only:\n    # `kube_controller`. This only applies to Rancher v2.0.5 and v2.0.6.\n    kube-controller:\n      # CIDR pool used to assign IP addresses to pods in the cluster\n      cluster_cidr: 10.42.0.0/16\n      # IP range for any services created on Kubernetes\n      # This must match the service_cluster_ip_range in kube-api\n      service_cluster_ip_range: 10.43.0.0/16\n    kubelet:\n      # Base domain for the cluster\n      cluster_domain: cluster.local\n      # IP address for the DNS service endpoint\n      cluster_dns_server: 10.43.0.10\n      # Fail if swap is on\n      fail_swap_on: false\n      # Set max pods to 250 instead of default 110\n      extra_args:\n        max-pods: 250\n      # Optionally define additional volume binds to a service\n      extra_binds:\n        - \"/usr/libexec/kubernetes/kubelet-plugins:/usr/libexec/kubernetes/kubelet-plugins\"\n\n# Currently, only authentication strategy supported is x509.\n# You can optionally create additional SANs (hostnames or IPs) to\n# add to the API server PKI certificate.\n# This is useful if you want to use a load balancer for the\n# control plane servers.\nauthentication:\n    strategy: x509\n    sans:\n      - \"10.18.160.10\"\n      - \"my-loadbalancer-1234567890.us-west-2.elb.amazonaws.com\"\n\n# Kubernetes Authorization mode\n# Use `mode: rbac` to enable RBAC\n# Use `mode: none` to disable authorization\nauthorization:\n    mode: rbac\n\n# If you want to set a Kubernetes cloud provider, you specify\n# the name and configuration\ncloud_provider:\n    name: aws\n\n# Add-ons are deployed using kubernetes jobs. RKE will give\n# up on trying to get the job status after this timeout in seconds..\naddon_job_timeout: 30\n\n# Specify network plugin-in (canal, calico, flannel, weave, or none)\nnetwork:\n    plugin: canal\n\n# Specify DNS provider (coredns or kube-dns)\ndns:\n    provider: coredns\n\n# Currently only nginx ingress provider is supported.\n# To disable ingress controller, set `provider: none`\n# `node_selector` controls ingress placement and is optional\ningress:\n    provider: nginx\n    node_selector:\n      app: ingress\n      \n# All add-on manifests MUST specify a namespace\naddons: |-\n    ---\n    apiVersion: v1\n    kind: Pod\n    metadata:\n      name: my-nginx\n      namespace: default\n    spec:\n      containers:\n      - name: my-nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n\naddons_include:\n    - https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/rook-operator.yaml\n    - https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/rook-cluster.yaml\n    - /path/to/manifestnodes:\n    - address: 1.2.3.4\n      user: ubuntu\n      role:\n        - controlplane\n        - etcd\n        - worker","postref":"ceb9a17bbb645fd3eee4969d249ce6d0","objectID":"ab95f44ee3c1d205c852f0732172a699","permalink":"http://jijeesh.github.io/docs/rke/latest/en/example-yamls/"},{"anchor":"#quick-start-outline","title":"Quick Start Outline","content":"This Quick Start Guide is divided into different tasks for easier consumption.\nProvision a Linux Host\n\nInstall Rancher\n\nLog In\n\nCreate the Cluster\n1. Provision a Linux HostBegin creation of a custom cluster by provisioning a Linux host. Your host can be:\nA cloud-host virtual machine (VM)\nAn on-premise VM\nA bare-metal server\n\nNote:\nWhen using a cloud-hosted virtual machine you need to allow inbound TCP communication to ports 80 and 443.  Please see your cloud-host’s documentation for information regarding port configuration.\n\nFor a full list of port requirements, refer to Docker Installation.\nProvision the host according to our Requirements.2. Install RancherTo install Rancher on your host, connect to it and then use a shell to install.\nLog in to your Linux host using your preferred shell, such as PuTTy or a remote Terminal connection.\n\nFrom your shell, enter the following command:\n\n$ sudo docker run -d --restart=unless-stopped -p 80:80 -p 443:443 rancher/rancher\n\nResult: Rancher is installed.3. Log InLog in to Rancher to begin using the application. After you log in, you’ll make some one-time configurations.\nOpen a web browser and enter the IP address of your host: https://<SERVER_IP>.\n\nReplace <SERVER_IP> with your host IP address.\n\nWhen prompted, create a password for the default admin account there cowpoke!\n\nSet the Rancher Server URL. The URL can either be an IP address or a host name. However, each node added to your cluster must be able to connect to this URL.If you use a hostname in the URL, this hostname must be resolvable by DNS on the nodes you want to add to you cluster.\n4. Create the ClusterWelcome to Rancher! You are now able to create your first Kubernetes cluster.In this task, you can use the versatile Custom option. This option lets you add any Linux host (cloud-hosted VM, on-premise VM, or bare-metal) to be used in a cluster.\nFrom the Clusters page, click Add Cluster.\n\nChoose Custom.\n\nEnter a Cluster Name.\n\nSkip Member Roles and Cluster Options. We’ll tell you about them later.\n\nClick Next.\n\nFrom Node Role, select all the roles: etcd, Control, and Worker.\n\nOptional: Rancher auto-detects the IP addresses used for Rancher communication and cluster communication. You can override these using Public Address and Internal Address in the Node Address section.\n\nSkip the Labels stuff. It’s not important for now.\n\nCopy the command displayed on screen to your clipboard.\n\nLog in to your Linux host using your preferred shell, such as PuTTy or a remote Terminal connection. Run the command copied to your clipboard.\n\nWhen you finish running the command on your Linux host, click Done.\n\n\tResult:\n\t\n\t\tYour cluster is created and assigned a state of Provisioning. Rancher is standing up your cluster.\n\t\tYou can access your cluster after its state is updated to Active.\n\t\tActive clusters are assigned two Projects, Default (containing the namespace default) and System (containing the namespaces cattle-system,ingress-nginx,kube-public and kube-system, if present).\n\t\nFinishedCongratulations! You have created your first cluster.What’s Next?Use Rancher to create a deployment. For more information, see Creating Deployments.","postref":"7f019cf3c74ffec8f68c6b5525163125","objectID":"18afc8a978ced7dedfeaa79984db36e9","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/quick-start-guide/deployment/quickstart-manual-setup/"},{"anchor":"#rancher-nodes","title":"Rancher Nodes","content":"Commonly Used PortsThese ports are typically opened on your Kubernetes nodes, regardless of what type of cluster it is.\n\n\nProtocol\nPort\nDescription\n\n\n\n\n\nTCP\n22\nNode driver SSH provisioning\n\n\n\nTCP\n2376\nNode driver Docker daemon TLS port\n\n\n\nTCP\n2379\netcd client requests\n\n\n\nTCP\n2380\netcd peer communication\n\n\n\nUDP\n8472\nCanal/Flannel VXLAN overlay networking\n\n\n\nUDP\n4789\nFlannel VXLAN overlay networking on Windows cluster\n\n\n\nTCP\n9099\nCanal/Flannel livenessProbe/readinessProbe\n\n\n\nTCP\n6783\nWeave Port\n\n\n\nUDP\n6783-6784\nWeave UDP Ports\n\n\n\nTCP\n10250\nkubelet API\n\n\n\nTCP\n10254\nIngress controller livenessProbe/readinessProbe\n\n\n\nTCP/UDP\n30000-32767\nNodePort port range\n\n\nLocal Node TrafficPorts marked as local traffic (i.e., 9099 TCP) in the above requirements are used for Kubernetes healthchecks (livenessProbe andreadinessProbe).\nThese healthchecks are executed on the node itself. In most cloud environments, this local traffic is allowed by default.However, this traffic may be blocked when:\nYou have applied strict host firewall policies on the node.\nYou are using nodes that have multiple interfaces (multihomed).\nIn these cases, you have to explicitly allow this traffic in your host firewall, or in case of public/private cloud hosted machines (i.e. AWS or OpenStack), in your security group configuration. Keep in mind that when using a security group as source or destination in your security group, explicitly opening ports only applies to the private interface of the nodes / instances.Rancher AWS EC2 security groupWhen using the AWS EC2 node driver to provision cluster nodes in Rancher, you can choose to let Rancher create a security group called rancher-nodes. The following rules are automatically added to this security group.\n\n\nType\nProtocol\nPort Range\nSource/Destination\nRule Type\n\n\n\n\n\nSSH\nTCP\n22\n0.0.0.0/0\nInbound\n\n\n\nHTTP\nTCP\n80\n0.0.0.0/0\nInbound\n\n\n\nCustom TCP Rule\nTCP\n443\n0.0.0.0/0\nInbound\n\n\n\nCustom TCP Rule\nTCP\n2376\n0.0.0.0/0\nInbound\n\n\n\nCustom TCP Rule\nTCP\n2379-2380\nsg-xxx (rancher-nodes)\nInbound\n\n\n\nCustom UDP Rule\nUDP\n4789\nsg-xxx (rancher-nodes)\nInbound\n\n\n\nCustom TCP Rule\nTCP\n6443\n0.0.0.0/0\nInbound\n\n\n\nCustom UDP Rule\nUDP\n8472\nsg-xxx (rancher-nodes)\nInbound\n\n\n\nCustom TCP Rule\nTCP\n10250-10252\nsg-xxx (rancher-nodes)\nInbound\n\n\n\nCustom TCP Rule\nTCP\n10256\nsg-xxx (rancher-nodes)\nInbound\n\n\n\nCustom TCP Rule\nTCP\n30000-32767\n0.0.0.0/0\nInbound\n\n\n\nCustom UDP Rule\nUDP\n30000-32767\n0.0.0.0/0\nInbound\n\n\n\nAll traffic\nAll\nAll\n0.0.0.0/0\nOutbound\n\n\nThe ports required to be open for cluster nodes changes depending on how the cluster was launched. Each of the tabs below list the ports that need to be opened for different cluster creation options.\nTip:\n\nIf security isn’t a large concern and you’re okay with opening a few additional ports, you can use the table in Commonly Used Ports as your port reference instead of the comprehensive tables below.\n\n  \n  \n  The following table depicts the port requirements for Rancher Launched Kubernetes with nodes created in an Infrastructure Provider.\n\n\nNote:\nThe required ports are automatically opened by Rancher during creation of clusters in cloud providers like Amazon EC2 or DigitalOcean.\n\n\n\n    \n        \n          From / To\n          Rancher Nodes\n          etcd Plane Nodes\n          Control Plane Nodes\n          Worker Plane Nodes\n          External Load Balancer\n          Internet\n        \n    \n    \n        \n          Rancher Nodes (1)\n          \n          22 TCP\n          \n          git.rancher.io (2):35.160.43.145:3235.167.242.46:3252.33.59.17:32\n        \n        \n          \n          2376 TCP\n          \n        \n        \n          etcd Plane Nodes\n          443 TCP (3)\n          2379 TCP\n          \n          \n          443 TCP\n          \n        \n        \n          2380 TCP\n          \n          \n          \n        \n        \n          \n          6443 TCP\n          \n          \n        \n        \n          8472 UDP\n          \n        \n        \n          9099 TCP (4)\n          \n          \n          \n        \n        \n          Control Plane Nodes\n          443 TCP (3)\n          2379 TCP\n          \n          \n          443 TCP\n          \n        \n        \n          2380 TCP\n          \n          \n          \n        \n        \n          \n          6443 TCP\n          \n          \n        \n        \n          8472 UDP\n          \n        \n        \n          10250 TCP\n          \n        \n        \n          \n          9099 TCP (4)\n          \n          \n        \n        \n          \n          10254 TCP (4)\n          \n          \n        \n        \n          Worker Plane Nodes\n          443 TCP (3)\n          \n          6443 TCP\n          \n          443 TCP\n          \n        \n        \n          8472 UDP\n          \n        \n        \n          \n          \n          9099 TCP (4)\n          \n        \n        \n          \n          \n          10254 TCP (4)\n          \n        \n        \n          External Load Balancer (5)\n          80 TCP\n          \n          \n          \n          \n          \n        \n        \n          443 TCP (6)\n          \n          \n          \n          \n          \n        \n        \n          API / UI Clients\n          80 TCP (3)\n          \n          \n          \n          80 TCP\n          \n        \n        \n          443 TCP (3)\n          \n          \n          \n          443 TCP\n          \n        \n        \n          Workload Clients\n          \n          \n          \n          30000-32767 TCP / UDP(nodeport)\n          \n          \n        \n        \n          \n          \n          80 TCP (Ingress)\n          \n          \n        \n        \n          \n          \n          443 TCP (Ingress)\n          \n          \n        \n        \n          Notes:1. Nodes running standalone server or Rancher HA deployment.2. Required to fetch Rancher chart library.3. Only without external load balancer.4. Local traffic to the node itself (not across nodes).5. Load balancer / proxy that handles tragging to the Rancher UI / API.6. Only if SSL is not terminated at external load balancer.\n        \n    \n\n\n\n\n\n  The following table depicts the port requirements for Rancher Launched Kubernetes with Custom Nodes.\n\n\n    \n        \n          From / To\n          Rancher Nodes\n          etcd Plane Nodes\n          Control Plane Nodes\n          Worker Plane Nodes\n          External Load Balancer\n          Internet\n        \n    \n    \n        \n          Rancher Nodes (1)\n          \n          \n          \n          \n          \n          git.rancher.io (2):35.160.43.145:3235.167.242.46:3252.33.59.17:32\n        \n        \n          etcd Plane Nodes\n          443 TCP (3)\n          2379 TCP\n          \n          \n          443 TCP\n          \n        \n        \n          2380 TCP\n          \n          \n          \n        \n        \n          \n          6443 TCP\n          \n          \n        \n        \n          8472 UDP\n          \n        \n        \n          4789 UDP (7)\n          \n        \n        \n          9099 TCP (4)\n          \n          \n          \n        \n        \n          Control Plane Nodes\n          443 TCP (3)\n          2379 TCP\n          \n          \n          443 TCP\n          \n        \n        \n          2380 TCP\n          \n          \n          \n        \n        \n          \n          6443 TCP\n          \n          \n        \n        \n          8472 UDP\n          \n        \n        \n          4789 UDP (7)\n          \n        \n        \n          10250 TCP\n          \n        \n        \n          \n          9099 TCP (4)\n          \n          \n        \n        \n          \n          10254 TCP (4)\n          \n          \n        \n        \n          Worker Plane Nodes\n          443 TCP (3)\n          \n          6443 TCP\n          \n          443 TCP\n          \n        \n        \n          8472 UDP\n          \n        \n        \n          4789 UDP (7)\n          \n        \n        \n          \n          \n          9099 TCP (4)\n          \n        \n        \n          \n          \n          10254 TCP (4)\n          \n        \n        \n          External Load Balancer (5)\n          80 TCP\n          \n          \n          \n          \n          \n        \n        \n          443 TCP (6)\n          \n          \n          \n          \n          \n        \n        \n          API / UI Clients\n          80 TCP (3)\n          \n          \n          \n          80 TCP\n          \n        \n        \n          443 TCP (3)\n          \n          \n          \n          443 TCP\n          \n        \n        \n          Workload Clients\n          \n          \n          \n          30000-32767 TCP / UDP(nodeport)\n          \n          \n        \n        \n          \n          \n          80 TCP (Ingress)\n          \n          \n        \n        \n          \n          \n          443 TCP (Ingress)\n          \n          \n        \n        \n          Notes:1. Nodes running standalone server or Rancher HA deployment.2. Required to fetch Rancher chart library.3. Only without external load balancer.4. Local traffic to the node itself (not across nodes).5. Load balancer / proxy that handles tragging to the Rancher UI / API.6. Only if SSL is not terminated ","postref":"42a96c6651f7437b0c8d6064008ae277","objectID":"52a5a40d859f976949244511aee74bd9","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/requirements/ports/"},{"anchor":"#splunk-configuration","title":"Splunk Configuration","content":"You can use curl to see if HEC is listening for HTTP event data.$ curl http://splunk-server:8088/services/collector/event \\\n    -H 'Authorization: Splunk 8da70994-b1b0-4a79-b154-bfaae8f93432' \\\n    -d '{\"event\": \"hello world\"}'\nIf Splunk is configured correctly, you should receive json data returning success code 0. You should be able\nto send logging data to HEC.If you received an error, check your configuration in Splunk and Rancher.\nLog into your Splunk server.\n\nClick on Search & Reporting. The number of Indexed Events listed should be increasing.\n\nClick on Data Summary and select the Sources tab.\n\n\nTo view the actual logs, click on the source that you declared earlier.\n\nIf your instance of Splunk uses SSL, your Endpoint will need to begin with https://. With the correct endpoint, the SSL Configuration form is enabled and ready to be completed.\nProvide the Client Private Key and Client Certificate. You can either copy and paste them or upload them by using the Read from a file button.\n\n\nYou can use either a self-signed certificate or one provided by a certificate authority.\n\nYou can generate a self-signed certificate using an openssl command. For example:\n\n openssl req -x509 -newkey rsa:2048 -keyout myservice.key -out myservice.cert -days 365 -nodes -subj \"/CN=myservice.example.com\"\n\n\n\nEnter your Client Key Password.\n\nSelect whether or not you want to verify your SSL.\n\n\nIf you are using a self-signed certificate, select Enabled - Input trusted server certificate, provide the CA Certificate PEM. You can copy and paste the certificate or upload it using the Read from a file button.\nIf you are using a certificate from a certificate authority, select Enabled - Input trusted server certificate. You do not need to provide a CA Certificate PEM.\n\n\nIn the Endpoint field, enter the IP address and port for you Splunk instance (i.e. http://splunk-server:8088)\n\n\nSplunk usually uses port 8088. If you’re using Splunk Cloud, you’ll need to work with Splunk support to get an endpoint URL.\n\n\nEnter the Token you obtained while completing the prerequisites (i.e., when you created a token in Splunk).\n\nIn the Source field, enter the name of the token as entered in Splunk.\n\nOptional: Provide one or more index that’s allowed for your token.\n","postref":"6145e5fc10edda4a06af568720fe59c2","objectID":"cf41bb3d55b76620f596d7ce6ea315b2","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/tools/logging/splunk/"},{"anchor":"#","title":"FAQs","content":"What is required to run RancherOS?\n\nRancherOS runs on any laptop, physical, or virtual servers.\n\nWhat are some commands?\n\n\n\n\nCommand\nDescription\n\n\n\n\n\ndocker\nGood old Docker, use that to run stuff.\n\n\n\nsystem-docker\nThe Docker instance running the system containers.  Must run as root or using sudo\n\n\n\nros\nControl and configure RancherOS\n\n\n\n\nHow can I extend my disk size in Amazon?\n\nAssuming your EC2 instance with RancherOS with more disk space than what’s being read, run the following command to extend the disk size. This allows RancherOS to see the disk size.\n\n$ docker run --privileged --rm --it debian:jessie resize2fs /dev/xvda1\n\n\nxvda1 should be the right disk for your own setup. In the future, we will be trying to create a system service that would automatically do this on boot in AWS.\n","postref":"e6ac348bb1b3c8956e046be9060fd989","objectID":"307edea3086a5cbbb381deb1fb7cc2e1","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/about/faqs/"},{"anchor":"#","title":"Tips on using Rancher v1.x with RancherOS","content":"RancherOS can be used to launch Rancher and be used as the OS to add nodes to Rancher.\n\nLaunching Agents using Cloud-Config\n\nYou can easily add hosts into Rancher by using cloud-config to launch the rancher/agent container.\n\nAfter Rancher is launched and host registration has been saved, you will be able to find use the custom option to add Rancher OS nodes.\n$ sudo docker run --d --privileged -v /var/run/docker.sock:/var/run/docker.sock \\\n    rancher/agent:v0.8.2  http://<rancher-server-ip>:8080/v1/projects/1a5/scripts/<registrationToken>\n\n\n\nNote: The rancher/agent version is correlated to the Rancher server version. You will need to check the custom command to get the appropriate tag for the version to use.\n\n\nCloud-Config Example\n\nHere’s using the command above and converting it into a cloud-config file to launch the rancher/agent in docker when RancherOS boots up.\n#cloud-config\nrancher:\n  services:\n    rancher-agent1:\n      image: rancher/agent:v0.8.2\n      command: http://<rancher-server-ip>:8080/v1/projects/1a5/scripts/<registrationToken>\n      privileged: true\n      volumes:\n        - /var/run/docker.sock:/var/run/docker.sock\n\n\n\nNote: You can not name the service rancher-agent as this will not allow the rancher/agent container to be launched correctly. Please read more about why you can’t name your container as rancher-agent.\n\n\nAdding in Host Labels\n\nWith each host, you have the ability to add labels to help you organize your hosts. The labels are added as an environment variable when launching the rancher/agent container. The host label in the UI will be a key/value pair and the keys must be unique identifiers. If you added two keys with different values, we’ll take the last inputted value to use as the key/value pair.\n\nBy adding labels to hosts, you can use these labels when to schedule services/load balancers/services and create a whitelist or blacklist of hosts for your services to run on.\n\nWhen adding a custom host, you can add the labels using the UI and it will automatically add the environment variable (CATTLE_HOST_LABELS) with the key/value pair into the command on the UI screen.\n\nNative Docker Commands Example\n# Adding one host label to the rancher/agent command\n$  sudo docker run -e CATTLE_HOST_LABELS='foo=bar' -d --privileged \\\n  -v /var/run/docker.sock:/var/run/docker.sock rancher/agent:v0.8.2 \\\n  http://<rancher-server-ip>:8080/v1/projects/1a5/scripts/<registrationToken>\n\n# Adding more than one host label requires joining the additional host labels with an `&`\n$  sudo docker run -e CATTLE_HOST_LABELS='foo=bar&hello=world' -d --privileged \\\n  -v /var/run/docker.sock:/var/run/docker.sock rancher/agent:v0.8.2 \\\n  http://<rancher-server-ip>:8080/v1/projects/1a5/scripts/<registrationToken>\nCloud-Config Example\n\nAdding one host label\n#cloud-config\nrancher:\n  services:\n    rancher-agent1:\n      image: rancher/agent:v0.8.2\n      command: http://<rancher-server-ip>:8080/v1/projects/1a5/scripts/<registrationToken>\n      privileged: true\n      volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n      environment:\n        CATTLE_HOST_LABELS: foo=bar\n\n\nAdding more than one host label requires joining the additional host labels with an &\n#cloud-config\nrancher:\n  services:\n    rancher-agent1:\n      image: rancher/agent:v0.8.2\n      command: http://<rancher-server-ip>:8080/v1/projects/1a5/scripts/<registrationToken>\n      privileged: true\n      volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n      environment:\n        CATTLE_HOST_LABELS: foo=bar&hello=world","postref":"8b5fbdbebf8c3a8db01695948a3e80b2","objectID":"b4a96586b6790f9cc3aebae140c9530f","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/about/running-rancher-on-rancheros/"},{"anchor":"#","title":"RancherOS Security","content":"\n\n\nSecurity policy\nRancher Labs supports responsible disclosure, and endeavours to resolve all issues in a reasonable time frame. RancherOS is a minimal Linux distribution, built with entirely using open source components.\n\n\nReporting process\nPlease submit possible security issues by emailing security@rancher.com\n\n\nAnnouncements\nSubscribe to the Rancher announcements forum for release updates.\n\n\n\n\nRancherOS Vulnerabilities\n\n\n\n\nID\nDescription\nDate\nResolution\n\n\n\n\n\nCVE-2017-6074\nLocal privilege-escalation using a user after free issue in Datagram Congestion Control Protocol (DCCP). DCCP is built into the RancherOS kernel as a dynamically loaded module, and isn’t loaded by default.\n17 Feb 2017\nRancherOS v0.8.1 using a patched 4.9.12 Linux kernel\n\n\n\nCVE-2017-7184\nAllows local users to obtain root privileges or cause a denial of service (heap-based out-of-bounds access) by leveraging the CAP_NET_ADMIN capability.\n3 April 2017\nRancherOS v0.9.2-rc1 using Linux 4.9.20\n\n\n\nCVE-2017-1000364\nLinux Kernel is prone to a local memory-corruption vulnerability. Attackers may be able to exploit this issue to execute arbitrary code with elevated privileges\n19 June 2017\nRancherOS v1.0.3\n\n\n\nCVE-2017-1000366\nglibc contains a vulnerability that allows manipulation of the heap/stack. Attackers may be able to exploit this issue to execute arbitrary code with elevated privileges\n19 June 2017\nRancherOS v1.0.3\n\n\n\nCVE-2017-1000405\nThe Linux Kernel versions 2.6.38 through 4.14 have a problematic use of pmd_mkdirty() in the touch_pmd() function inside the THP implementation. touch_pmd() can be reached by get_user_pages(). In such case, the pmd will become dirty.\n10 Dec 2017\nRancherOS v1.1.1\n\n\n\nCVE-2017-5754\nSystems with microprocessors utilizing speculative execution and indirect branch prediction may allow unauthorized disclosure of information to an attacker with local user access via a side-channel analysis of the data cache.\n5 Jan 2018\nRancherOS v1.1.3 using Linux v4.9.75\n\n\n\nCVE-2017-5715\nSystems with microprocessors utilizing speculative execution and indirect branch prediction may allow unauthorized disclosure of information to an attacker with local user access via a side-channel analysis\n6 Feb 2018\nRancherOS v1.1.4 using Linux v4.9.78 with the Retpoline support\n\n\n\nCVE-2017-5753\nSystems with microprocessors utilizing speculative execution and branch prediction may allow unauthorized disclosure of information to an attacker with local user access via a side-channel analysis.\n31 May 2018\nRancherOS v1.4.0 using Linux v4.14.32\n\n\n\nCVE-2018-8897\nA statement in the System Programming Guide of the Intel 64 and IA-32 Architectures Software Developer’s Manual (SDM) was mishandled in the development of some or all operating-system kernels, resulting in unexpected behavior for #DB exceptions that are deferred by MOV SS or POP SS, as demonstrated by (for example) privilege escalation in Windows, macOS, some Xen configurations, or FreeBSD, or a Linux kernel crash.\n31 May 2018\nRancherOS v1.4.0 using Linux v4.14.32\n\n\n\nCVE-2018-3620\nL1 Terminal Fault is a hardware vulnerability which allows unprivileged speculative access to data which is available in the Level 1 Data Cache when the page table entry controlling the virtual address, which is used for the access, has the Present bit cleared or other reserved bits set.\n19 Sep 2018\nRancherOS v1.4.1 using Linux v4.14.67\n\n\n\nCVE-2018-3639\nSystems with microprocessors utilizing speculative execution and speculative execution of memory reads before the addresses of all prior memory writes are known may allow unauthorized disclosure of information to an attacker with local user access via a side-channel analysis, aka Speculative Store Bypass (SSB), Variant 4.\n19 Sep 2018\nRancherOS v1.4.1 using Linux v4.14.67\n\n\n\nCVE-2018-17182\nThe vmacache_flush_all function in mm/vmacache.c mishandles sequence number overflows. An attacker can trigger a use-after-free (and possibly gain privileges) via certain thread creation, map, unmap, invalidation, and dereference operations.\n18 Oct 2018\nRancherOS v1.4.2 using Linux v4.14.73\n\n\n\nCVE-2019-5736\nrunc through 1.0-rc6, as used in Docker before 18.09.2 and other products, allows attackers to overwrite the host runc binary (and consequently obtain host root access) by leveraging the ability to execute a command as root within one of these types of containers: (1) a new container with an attacker-controlled image, or (2) an existing container, to which the attacker previously had write access, that can be attached with docker exec. This occurs because of file-descriptor mishandling, related to /proc/self/exe.\n12 Feb 2019\nRancherOS v1.5.1\n\n\n\nMicroarchitectural Data Sampling (MDS)\nMicroarchitectural Data Sampling (MDS) is a family of side channel attacks on internal buffers in Intel CPUs. The variants are: CVE-2018-12126, CVE-2018-12130, CVE-2018-12127, CVE-2019-11091\n31 May 2019\nRancherOS v1.5.2 using Linux v4.14.122\n\n\n\nThe TCP SACK panic\nSelective acknowledgment (SACK) is a technique used by TCP to help alleviate congestion that can arise due to the retransmission of dropped packets. It allows the endpoints to describe which pieces of the data they have received, so that only the missing pieces need to be retransmitted. However, a bug was recently found in the Linux implementation of SACK that allows remote attackers to panic the system by sending crafted SACK information.\n11 July 2019\nRancherOS v1.5.3\n\n\n\n","postref":"be3e4a807889015a7347f145ce7320cd","objectID":"ac147cfe18a5023628b8017db6826025","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/about/security/"},{"anchor":"#","title":"How to use recovery console","content":"Test Environment\n\nIn order to demonstrate how to use the recovery console, we choose a scene that the disk space is full and the OS cannot boot.\n\n\n\n\nTerm\nDefinition\n\n\n\n\n\nRancherOS\nv1.4.0\n\n\n\nPlatform\nVirtualbox\n\n\n\nRoot Disk\n2GB\n\n\n\nCPU\n1C\n\n\n\nMEM\n2GB\n\n\n\n\nFill up the disk\n\nStart this VM to check disk usage:\n\n/dev/sda1            ext4            1.8G    567.2M      1.2G  32% /opt\n/dev/sda1            ext4            1.8G    567.2M      1.2G  32% /mnt\n...\n...\n\n\nFill the remaining space with dd:\n\n$ cd /opt/\n$ dd if=/dev/zero of=2GB.img bs=1M count=2000\ndd: writing '2GB.img': No space left on device\n1304+0 records in\n1302+1 records out\n\n$ ls -ahl\ntotal 1334036\ndrwxr-xr-x    2 root     root        4.0K Jul 19 07:32 .\ndrwxr-xr-x    1 root     root        4.0K Jul 19 06:58 ..\n-rw-r--r--    1 root     root        1.3G Jul 19 07:32 2GB.img\n\n\nAt this point you cannot reboot in the OS,  but you can reboot via Virtualbox:\n\n$ shutdown -h now\nFailed to write to log, write /var/log/boot/shutdown.log: no space left on device\n[            ] shutdown:info: Setting shutdown timeout to 60 (rancher.shutdown_timeout set to 60)\nFailed to write to log, write /var/log/boot/shutdown.log: no space left on device\nFailed to write to log, write /var/log/boot/shutdown.log: no space left on device\n.[            ] shutdown:fatal: Error response from daemon: {\"message\":\"mkdir /var/lib/system-docker/overlay2/7c7dffbed40e7b0ed4c68d5630b17a179751643ca7b7a4ac183e48a767071684-init: no space left on device\"}\nFailed to write to log, write /var/log/boot/shutdown.log: no space left on device\n\n\nAfter rebooting, you will not be able to enter the OS and there will be a kernel panic.\n\n\n\nBoot with recovery console\n\nWhen you can access the bootloader, you should select the Recovery console and  press <Tab> to edit:\n\n\n\nYou need add rancher.autologin=tty1 to the end, then press <Enter>. If all goes well, you will automatically login to the recovery console.\n\nHow to recover\n\nWe need to mount the root disk in the recovery console and delete some data:\n\n# If you couldn't see any disk devices created under `/dev/`, please try this command:\n$ ros udev-settle\n\n$ mkdir /mnt/root-disk\n$ mount /dev/sda1 /mnt/root-disk\n\n# delete data previously generated using dd\n$ ls -ahl /mnt/root-disk/opt\n-rw-r--r--    1 root     root        1.3G Jul 19 07:32 2GB.img\n$ rm -f /mnt/root-disk/opt/2GB.img\n\n\nAfter rebooting, you can enter the OS normally.\n","postref":"de51c046d8e955573db89872245c7792","objectID":"ab45368b86a1ac7767a793d19ea84cbf","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/about/recovery-console/"},{"anchor":"#","title":"How to custom partition layout","content":"When users use the default ros install, ROS will automatically create one partition on the root disk.\nIt will be the only partition with the label RANCHER_STATE.\nBut sometimes users want to be able to customize the root disk partition to isolate the data.\n\n\nThe following defaults to MBR mode, GPT mode has not been tested.\n\n\nUse RANCHER_STATE partition\n\nAs mentioned above, the default mode is that ROS will automatically create one partition with the label RANCHER_STATE.\n\nIn addition, we can have other partitions, e.g.: two partitions, one is RANCHER_STATE and the other is a normal partition.\n\nFirst boot a ROS instance from ISO, then manually format and partition /dev/sda , the reference configuration is as follows:\n\n[root@rancher oem]# fdisk -l\nDisk /dev/sda: 5 GiB, 5377622016 bytes, 10503168 sectors\nUnits: sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisklabel type: dos\nDisk identifier: 0x9fff87e9\n\nDevice     Boot   Start      End Sectors  Size Id Type\n/dev/sda1  *       2048  7503167 7501120  3.6G 83 Linux\n/dev/sda2       7503872 10503167 2999296  1.4G 83 Linux\n\n[root@rancher oem]# blkid\n/dev/sda1: LABEL=\"RANCHER_STATE\" UUID=\"512f212b-3130-458e-a2d1-1d601c34d4e4\" TYPE=\"ext4\" PARTUUID=\"9fff87e9-01\"\n/dev/sda2: UUID=\"3828e3ac-b825-4898-9072-45da9d37c2a6\" TYPE=\"ext4\" PARTUUID=\"9fff87e9-02\"\n\n\nThen install ROS to the disk with ros install -t noformat -d /dev/sda ....\n\nAfter rebooting, you can use /dev/sda2. For example, changing the data root of user-docker:\n\n$ ros config set mounts '[[\"/dev/sda2\",\"/mnt/s\",\"ext4\",\"\"]]’\n$ ros config set rancher.docker.graph /mnt/s\n$ reboot\n\n\n\nIn this mode, the RANCHER_STATE partition capacity cannot exceed 3.8GiB, otherwise the bootloader may not recognize the boot disk. This is the test result on VirtualBox.\n\n\nUse RANCHER_BOOT partition\n\nWhen you only use the RANCHER_STATE partition, the bootloader will be installed in the /boot directory.\n\n$ system-docker run -it --rm -v /:/host alpine\nls /host/boot\n...\n\n\nIf you want to use a separate boot partition, you also need to boot a ROS instance from ISO, then manually format and partition /dev/sda:\n\n[root@rancher rancher]# fdisk -l\nDisk /dev/sda: 5 GiB, 5377622016 bytes, 10503168 sectors\nUnits: sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisklabel type: dos\nDisk identifier: 0xe32b3025\n\nDevice     Boot   Start      End Sectors  Size Id Type\n/dev/sda1          2048  2503167 2501120  1.2G 83 Linux\n/dev/sda2       2504704  7503167 4998464  2.4G 83 Linux\n/dev/sda3       7503872 10503167 2999296  1.4G 83 Linux\n\n[root@rancher rancher]# mkfs.ext4 -L RANCHER_BOOT /dev/sda1\n[root@rancher rancher]# mkfs.ext4 -L RANCHER_STATE /dev/sda2\n[root@rancher rancher]# mkfs.ext4 /dev/sda3\n\n[root@rancher rancher]# blkid\n/dev/sda1: LABEL=\"RANCHER_BOOT\" UUID=\"43baeac3-11f3-4eed-acfa-64daf66b26c8\" TYPE=\"ext4\" PARTUUID=\"e32b3025-01\"\n/dev/sda2: LABEL=\"RANCHER_STATE\" UUID=\"16f1ecef-dbe4-42a2-87a1-611939684e0b\" TYPE=\"ext4\" PARTUUID=\"e32b3025-02\"\n/dev/sda3: UUID=\"9f34e161-0eee-48f9-93de-3b7c54dea437\" TYPE=\"ext4\" PARTUUID=\"c9b8f181-03\"\n\n\nThen install ROS to the disk with ros install -t noformat -d /dev/sda ....\n\nAfter rebooting, you can check the boot partition:\n\n[root@rancher rancher]# mkdir /boot\n[root@rancher rancher]# mount /dev/sda1 /boot\n[root@rancher rancher]# ls -ahl /boot/\ntotal 175388\ndrwxr-xr-x    4 root     root        4.0K Sep 27 03:35 .\ndrwxr-xr-x    1 root     root        4.0K Sep 27 03:38 ..\n-rw-r--r--    1 root     root          24 Sep 27 03:05 append\n-rw-r--r--    1 root     root         128 Sep 27 03:35 global.cfg\n-rw-r--r--    1 root     root       96.8M Sep 27 03:05 initrd\n\n\nIf you are not using the first partition as a BOOT partition, you need to set BOOT flag via the fdisk tool.\n\n\nIn this mode, the RANCHER_BOOT partition capacity cannot exceed 3.8GiB, otherwise the bootloader may not recognize the boot disk. This is the test result on VirtualBox.\n\n\nUse RANCHER_OEM partition\n\nIf you format any partition with the label RANCHER_OEM, ROS will mount this partition to /usr/share/ros/oem:\n\n[root@rancher rancher]# blkid\n/dev/sda2: LABEL=\"RANCHER_OEM\" UUID=\"4f438455-63a3-4d29-ac90-50adbeced412\" TYPE=\"ext4\" PARTUUID=\"9fff87e9-02\"\n\n[root@rancher rancher]# df -hT | grep sda2\n/dev/sda2            ext4            1.4G      4.3M      1.3G   0% /usr/share/ros/oem\n\n\nCurrently, this OEM directory is hardcoded and not configurable.\n\nUse RANCHER_SWAP partition\n\nSuppose you have a partition(/dev/sda2) and you want to use it as a SWAP partition:\n\n$ mkswap -L RANCHER_SWAP /dev/sda2\n\n$ blkid\n/dev/sda1: LABEL=\"RANCHER_STATE\" UUID=\"512f212b-3130-458e-a2d1-1d601c34d4e4\" TYPE=\"ext4\" PARTUUID=\"9fff87e9-01\"\n/dev/sda2: LABEL=\"RANCHER_SWAP\" UUID=\"772b6e76-f89c-458e-931e-10902d78d3e4\" TYPE=\"swap\" PARTUUID=\"9fff87e9-02\"\n\n\nAfter you install ROS to the disk, you can add the runcmd to enable SWAP:\n\nruncmd:\n- swapon -L RANCHER_SWAP\n\n\nThen check the memory information:\n\n[root@rancher rancher]# free -m\n            total       used       free     shared    buffers     cached\nMem:          1996        774       1221        237         20        614\n-/+ buffers/cache:        139       1856\nSwap:          487          0        487\n\n","postref":"15192c1c39666c179152d124ebd7e6f2","objectID":"37238f23fe3e7283e9e86cf557004edf","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/about/custom-partition-layout/"},{"anchor":"#","title":"How to update microcode","content":"Processor manufacturers release stability and security updates to the processor microcode. While microcode can be updated through the BIOS, the Linux kernel is also able to apply these updates.\nThese updates provide bug fixes that can be critical to the stability of your system. Without these updates, you may experience spurious crashes or unexpected system halts that can be difficult to track down.\n\nThe microcode loader supports three loading methods:\n\n\nEarly load microcode\nLate loading\nBuiltin microcode\n\n\nYou can get more details from here.\n\nRancherOS supports Late loading. To update the Intel microcode, get the latest Intel microcode. An example is here. Then copy the data files to the firmware directory:\n\nmkdir -p /lib/firmware/intel-ucode/\ncp -v intel-ucode/* /lib/firmware/intel-ucode/\n\n\nReload the microcode. This file does not exist if you are running RancherOS on the hypervisor. Usually, the VM does not need to update the microcode.\n\necho 1 > /sys/devices/system/cpu/microcode/reload\n\n\nCheck the result:\n\ndmesg | grep microcode\n[   13.659429] microcode: sig=0x306f2, pf=0x1, revision=0x36\n[   13.665981] microcode: Microcode Update Driver: v2.01 <tigran@aivazian.fsnet.co.uk>, Peter Oruba\n[  510.899733] microcode: updated to revision 0x3b, date = 2017-11-17\n\n\nYou can use runcmd to reload the microcode every boot:\n\nruncmd:\n- echo 1 > /sys/devices/system/cpu/microcode/reload\n\n","postref":"e20ed9018990893810139113595365e3","objectID":"c76ef0aa5733887c3f6703cdf48a043a","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/about/microcode-loader/"},{"anchor":"#before-you-start","title":"Before You Start","content":"Using a backup that you created earlier, restore Rancher to its last known healthy state.\nUsing a remote Terminal connection, log into the node running your Rancher Server.\n\nStop the container currently running Rancher Server. Replace <RANCHER_CONTAINER_NAME> with the name of your Rancher container.\n\ndocker stop <RANCHER_CONTAINER_NAME>\n\n\nMove the backup tarball that you created during completion of Creating Backups—Docker Installs onto your Rancher Server. Change to the directory that you moved it to. Enter dir to confirm that it’s there.\n\nIf you followed the naming convention we suggested in Creating Backups—Docker Installs, it will have a name similar to  rancher-data-backup-<RANCHER_VERSION>-<DATE>.tar.gz.\n\nEnter the following command to delete your current state data and replace it with your backup data, replacing the placeholders. Don’t forget to close the quotes.\n\n\nWarning! This command deletes all current state data from your Rancher Server container. Any changes saved after your backup tarball was created will be lost.\n\n\ndocker run  --volumes-from <RANCHER_CONTAINER_NAME> -v $PWD:/backup \\\nbusybox sh -c \"rm /var/lib/rancher/* -rf  && \\\ntar pzxvf /backup/rancher-data-backup-<RANCHER_VERSION>-<DATE>.tar.gz\"\n\n\nStep Result: A series of commands should run.\n\nRestart your Rancher Server container, replacing the placeholder. It will restart using your backup data.\n\ndocker start <RANCHER_CONTAINER_NAME>\n\n\nWait a few moments and then open Rancher in a web browser. Confirm that the restoration succeeded and that your data is restored.\nDuring restoration of your backup, you’ll enter a series of commands, filling placeholders with data from your environment. These placeholders are denoted with angled brackets and all capital letters (<EXAMPLE>). Here’s an example of a command with a placeholder:docker run  --volumes-from <RANCHER_CONTAINER_NAME> -v $PWD:/backup \\\nbusybox sh -c \"rm /var/lib/rancher/* -rf  && \\\ntar pzxvf /backup/rancher-data-backup-<RANCHER_VERSION>-<DATE>\"\nIn this command, <RANCHER_CONTAINER_NAME> and <RANCHER_VERSION>-<DATE> are environment variables for your Rancher deployment.Cross reference the image and reference table below to learn how to obtain this placeholder data. Write down or copy this information before starting the procedure below.Terminal docker ps Command, Displaying Where to Find <RANCHER_CONTAINER_TAG> and <RANCHER_CONTAINER_NAME>\n\n\n\nPlaceholder\nExample\nDescription\n\n\n\n\n\n<RANCHER_CONTAINER_TAG>\nv2.0.5\nThe rancher/rancher image you pulled for initial install.\n\n\n\n<RANCHER_CONTAINER_NAME>\nfestive_mestorf\nThe name of your Rancher container.\n\n\n\n<RANCHER_VERSION>\nv2.0.5\nThe version number for your Rancher backup.\n\n\n\n<DATE>\n9-27-18\nThe date that the data container or backup was created.\n\n\nYou can obtain <RANCHER_CONTAINER_TAG> and <RANCHER_CONTAINER_NAME> by logging into your Rancher Server by remote connection and entering the command to view the containers that are running: docker ps. You can also view containers that are stopped using a different command: docker ps -a. Use these commands for help anytime during while creating backups.","postref":"9f128b71259f20cf8600a79d9e13c879","objectID":"251651f68ad6e7ea62699d716deac64d","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/backups/restorations/single-node-restoration/"},{"anchor":"#restore-outline","title":"Restore Outline","content":"\n1. Preparation\n2. Place Snapshot\n3. Configure RKE\n4. Restore Database\n5. Bring Up the Cluster\n1. PreparationYou will need RKE and kubectl CLI utilities installed.Prepare by creating 3 new nodes to be the target for the restored Rancher instance.  See Kubernetes Install for node requirements.We recommend that you start with fresh nodes and a clean state. Alternatively you can clear Kubernetes and Rancher configurations from the existing nodes. This will destroy the data on these nodes. See Node Cleanup for the procedure.\nIMPORTANT: Before starting the restore make sure all the Kubernetes services on the old cluster nodes are stopped. We recommend powering off the nodes to be sure.\n2. Place SnapshotThe snapshot used to restore your etcd cluster is handled differently based on your version of RKE.RKE v0.2.0+As of RKE v0.2.0, snapshots could be saved in an S3 compatible backend. To restore your cluster from the snapshot stored in S3 compatible backend, you can skip this step and retrieve the snapshot in Step 4: Restore Database. Otherwise, you will need to place the snapshot directly on the nodes.Pick one of the clean nodes. That node will be the “target node” for the initial restore. Place your snapshot in /opt/rke/etcd-snapshots on the target node.RKE v0.1.xWhen you take a snapshot, RKE saves a backup of the certificates, i.e. a file named pki.bundle.tar.gz, in the same location. The snapshot and PKI bundle file are required for the restore process, and they are expected to be in the same location.Pick one of the clean nodes. That node will be the “target node” for the initial restore. Place the snapshot and PKI certificate bundle files in the /opt/rke/etcd-snapshots directory on the target node.\nSnapshot - <snapshot>.db\nPKI Bundle - pki.bundle.tar.gz\n3. Configure RKEMake a copy of your original rancher-cluster.yml file.cp rancher-cluster.yml rancher-cluster-restore.yml\nModify the copy and make the following changes.\nRemove or comment out entire the addons: section. The Rancher deployment and supporting configuration is already in the etcd database.\nChange your nodes: section to point to the restore nodes.\nComment out the nodes that are not your “target node”. We want the cluster to only start on that one node.\nExample rancher-cluster-restore.ymlnodes:\n- address: 52.15.238.179     # New Target Node\n  user: ubuntu\n  role: [ etcd, controlplane, worker ]\n# - address: 52.15.23.24\n#   user: ubuntu\n#   role: [ etcd, controlplane, worker ]\n# - address: 52.15.238.133\n#   user: ubuntu\n#   role: [ etcd, controlplane, worker ]\n\n# addons: |-\n#   ---\n#   kind: Namespace\n#   apiVersion: v1\n#   metadata:\n#     name: cattle-system\n#   ---\n...4. Restore DatabaseUse RKE with the new rancher-cluster-restore.yml configuration and restore the database to the single “target node”.RKE will create an etcd container with the restored database on the target node. This container will not complete the etcd initialization and stay in a running state until the cluster brought up in the next step.Restoring from a Local SnapshotWhen restoring etcd from a local snapshot, the snapshot is assumed to be located on the target node in the directory /opt/rke/etcd-snapshots.\nNote: For RKE v0.1.x, the pki.bundle.tar.gz file is also expected to be in the same location.\nrke etcd snapshot-restore --name <snapshot>.db --config ./rancher-cluster-restore.yml\nRestoring from a Snapshot in S3Available as of RKE v0.2.0When restoring etcd from a snapshot located in an S3 compatible backend, the command needs the S3 information in order to connect to the S3 backend and retrieve the snapshot.\nNote: Ensure your cluster.rkestate is present before starting the restore, as this contains your certificate data for the cluster.\n$ rke etcd snapshot-restore --config cluster.yml --name snapshot-name \\\n--s3 --access-key S3_ACCESS_KEY --secret-key S3_SECRET_KEY \\\n--bucket-name s3-bucket-name --s3-endpoint s3.amazonaws.com \\\n--folder folder-name # Available as of v2.3.0\nOptions for rke etcd snapshot-restoreS3 specific options are only available for RKE v0.2.0+.\n\n\nOption\nDescription\nS3 Specific\n\n\n\n\n\n--name value\nSpecify snapshot name\n\n\n\n\n--config value\nSpecify an alternate cluster YAML file (default: “cluster.yml”) [$RKE_CONFIG]\n\n\n\n\n--s3\nEnabled backup to s3\n*\n\n\n\n--s3-endpoint value\nSpecify s3 endpoint url (default: “s3.amazonaws.com”)\n*\n\n\n\n--access-key value\nSpecify s3 accessKey\n*\n\n\n\n--secret-key value\nSpecify s3 secretKey\n*\n\n\n\n--bucket-name value\nSpecify s3 bucket name\n*\n\n\n\n--folder value\nSpecify s3 folder in the bucket name Available as of v2.3.0\n*\n\n\n\n--region value\nSpecify the s3 bucket location (optional)\n*\n\n\n\n--ssh-agent-auth\nUse SSH Agent Auth defined by SSH_AUTH_SOCK\n\n\n\n\n--ignore-docker-version\nDisable Docker version check\n\n\n\n5. Bring Up the ClusterUse RKE and bring up the cluster on the single “target node.”\nNote: For users running RKE v0.2.0+, ensure your cluster.rkestate is present before starting the restore, as this contains your certificate data for the cluster.\nrke up --config ./rancher-cluster-restore.yml\nTesting the ClusterOnce RKE completes it will have created a credentials file in the local directory.  Configure kubectl to use the kube_config_rancher-cluster-restore.yml credentials file and check on the state of the cluster. See Installing and Configuring kubectl for details.Your new cluster will take a few minutes to stabilize. Once you see the new “target node” transition to Ready and three old nodes in NotReady you are ready to continue.kubectl get nodes\n\nNAME            STATUS    ROLES                      AGE       VERSION\n52.15.238.179   Ready     controlplane,etcd,worker    1m       v1.10.5\n18.217.82.189   NotReady  controlplane,etcd,worker   16d       v1.10.5\n18.222.22.56    NotReady  controlplane,etcd,worker   16d       v1.10.5\n18.191.222.99   NotReady  controlplane,etcd,worker   16d       v1.10.5\nCleaning up Old NodesUse kubectl to delete the old nodes from the cluster.kubectl delete node 18.217.82.189 18.222.22.56 18.191.222.99\nReboot the Target NodeReboot the target node to ensure the cluster networking and services are in a clean state before continuing.Check Kubernetes PodsWait for the pods running in kube-system, ingress-nginx and the rancher pod in cattle-system to return to the Running state.\nNote: cattle-cluster-agent and cattle-node-agent pods will be in an Error or CrashLoopBackOff state until Rancher server is up and the DNS/Load Balancer have been pointed at the new cluster.\nkubectl get pods --all-namespaces\n\nNAMESPACE       NAME                                    READY     STATUS    RESTARTS   AGE\ncattle-system   cattle-cluster-agent-766585f6b-kj88m    0/1       Error     6          4m\ncattle-system   cattle-node-agent-wvhqm                 0/1       Error     8          8m\ncattle-system   rancher-78947c8548-jzlsr                0/1       Running   1          4m\ningress-nginx   default-http-backend-797c5bc547-f5ztd   1/1       Running   1          4m\ningress-nginx   nginx-ingress-controller-ljvkf          1/1       Running   1          8m\nkube-system     canal-4pf9v                             3/3       Running   3          8m\nkube-system     cert-manager-6b47fc5fc-jnrl5            1/1       Running   1          4m\nkube-system     kube-dns-7588d5b5f5-kgskt               3/3       Running   3          4m\nkube-system     kube-dns-autoscaler-5db9bbb766-s698d    1/1       Running   1          4m\nkube-system     metrics-server-97bc649d5-6w7zc          1/1       Running   1          4m\nkube-system     tiller-deploy-56c4cf647b-j4whh          1/1       Running   1          4m\nAdding in Additional NodesEdit the rancher-cluster-restore.yml RKE config file and uncomment the additional nodes.Example rancher-cluster-restore.ymlnodes:\n- address: 52.15.238.179     # New Target Node\n  user: ubuntu\n  role: [ etcd, controlplane, worker ]\n- address: 52.15.23.24\n  user: ubuntu\n  role: [ etcd, controlplane, worker ]\n- address: 52.15.238.133\n  user: ubuntu\n  role: [ etcd, controlplane, worker ]\n\n# addons: |-\n#   ---\n#   kind: Namespace\n...Run RKE and add the nodes to the new cluster.rke up --config ./rancher-cluster-restore.yml\nFinishing UpRancher should now be running and available to manage your Kubernetes clusters. Review the recommended architecture for Kubernetes installations and update the endpoints for Rancher DNS or the Load Balancer that you built during Step 1 of the Kubernetes install (1. Create Nodes and Load Balancer) to target the new cluster. Once the endpoints are updated, the agents on your managed clusters should automatically reconnect. This may take 10-15 minutes due to reconnect back off timeouts.\nIMPORTANT: Remember to save your new RKE config (rancher-cluster-restore.yml) and kubectl credentials (kube_config_rancher-cluster-restore.yml) files in a safe place for future maintenance.\n","postref":"98460da2fc36dd8e0f557e32dfb329dd","objectID":"e977cf771b7aaa288c2a93b8c69a3384","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/backups/restorations/ha-restoration/"},{"anchor":"#","title":"Troubleshooting HA RKE Add-On Install","content":"\nImportant: RKE add-on install is only supported up to Rancher v2.0.8\n\nPlease use the Rancher Helm chart to install Rancher on a Kubernetes cluster. For details, see the Kubernetes Install - Installation Outline.\n\nIf you are currently using the RKE add-on install method, see Migrating from a Kubernetes Install with an RKE Add-on for details on how to move to using the helm chart.\n\n\nThis section contains common errors seen when setting up a Kubernetes installation.\n\nChoose from the following options:\n\n\nGeneric troubleshooting\n\nIn this section, you can find generic ways to debug your Kubernetes cluster.\n\nFailed to set up SSH tunneling for host\n\nIn this section, you can find errors related to SSH tunneling when you run the rke command to setup your nodes.\n\nFailed to get job complete status\n\nIn this section, you can find errors related to deploying addons.\n\n404 - default backend\n\nIn this section, you can find errors related to the 404 - default backend page that is shown when trying to access Rancher.\n\n","postref":"df5f458943e135b0bb127f66c1af36b4","objectID":"58603e0888b66dccbbad9f5e77f953e4","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/helm2/rke-add-on/troubleshooting/"},{"anchor":"#","title":"Troubleshooting HA RKE Add-On Install","content":"\nImportant: RKE add-on install is only supported up to Rancher v2.0.8\n\nPlease use the Rancher Helm chart to install Rancher on a Kubernetes cluster. For details, see the Kubernetes Install - Installation Outline.\n\nIf you are currently using the RKE add-on install method, see Migrating from a Kubernetes Install with an RKE Add-on for details on how to move to using the helm chart.\n\n\nThis section contains common errors seen when setting up a Kubernetes installation.\n\nChoose from the following options:\n\n\nGeneric troubleshooting\n\nIn this section, you can find generic ways to debug your Kubernetes cluster.\n\nFailed to set up SSH tunneling for host\n\nIn this section, you can find errors related to SSH tunneling when you run the rke command to setup your nodes.\n\nFailed to get job complete status\n\nIn this section, you can find errors related to deploying addons.\n\n404 - default backend\n\nIn this section, you can find errors related to the 404 - default backend page that is shown when trying to access Rancher.\n\n","postref":"e6d4fbe86168543b4dcf200f60e50c9c","objectID":"56bcfe9bbd40eb6f4e850c975de245fd","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/rke-add-on/troubleshooting/"},{"anchor":"#in-this-document","title":"In This Document","content":"You can set a NodePort for migrated workloads (i.e., services) using the Rancher v2.x UI. To add a NodePort, browse to the project containing your workloads, and edit each workload that you want to expose, as shown below. Map the port that your service container exposes to a NodePort, which you’ll be able to access from each cluster node.For example, for the web-deployment.yml file parsed from v1.6 that we’ve been using as a sample, we would edit its Kubernetes manifest, set the publish the port that the container uses, and then declare a NodePort. You can then access your workload by clicking the link created in the Rancher UI.\nNote:\n\n\nIf you set a NodePort without giving it a value, Rancher chooses a port at random from the following range: 30000-32767.\nIf you manually set a NodePort, you must assign it a value within the 30000-32767 range.\n\nPort Mapping: Setting NodePortNext: Configure Health ChecksA NodePort is a port that’s open to the public on each of your cluster nodes. When the NodePort receives a request for any of the cluster hosts’ IP address for the set NodePort value, NodePort (which is a Kubernetes service) routes traffic to a specific pod, regardless of what node it’s running on. NodePort provides a static endpoint where external requests can reliably reach your pods.NodePorts help you circumvent an IP address shortcoming. Although pods can be reached by their IP addresses, they are disposable by nature. Pods are routinely destroyed and recreated, getting a new IP address with each replication. Therefore, IP addresses are not a reliable way to access your pods. NodePorts help you around this issue by providing a static service where they can always be reached.  Even if your pods change their IP addresses, external clients dependent on them can continue accessing them without disruption, all without any knowledge of the pod re-creation occurring on the back end.In the following diagram, a user is trying to connect to an instance of Nginx running in a Kubernetes cluster managed by Rancher. Although he knows what NodePort Nginx is operating on (30216 in this case), he does not know the IP address of the specific node that the pod is running on. However, with NodePort enabled, he can connect to the pod using the IP address for any node in the cluster. Kubeproxy will forward the request to the correct node and pod.NodePorts are available within your Kubernetes cluster on an internal IP. If you want to expose pods external to the cluster, use NodePorts in conjunction with an external load balancer. Traffic requests from outside your cluster for <NodeIP>:<NodePort> are directed to the workload. The <NodeIP> can be the IP address of any node in your Kubernetes cluster.NodePort Pros\nCreating a NodePort service provides a static public endpoint to your workload pods. There, even if the pods are destroyed, Kubernetes can deploy the workload anywhere in the cluster without altering the public endpoint.\nThe scale of the pods is not limited by the number of nodes in the cluster. NodePort allows decoupling of public access from the number and location of pods.\nNodePort Cons\nWhen a NodePort is used, that <NodeIP>:<NodePort> is reserved in your Kubernetes cluster on all nodes, even if the workload is never deployed to the other nodes.\nYou can only specify a port from a configurable range (by default, it is 30000-32767).\nAn extra Kubernetes object (a Kubernetes service of type NodePort) is needed to expose your workload. Thus, finding out how your application is exposed is not straightforward.\nYou can set a HostPort for migrated workloads (i.e., services) using the Rancher v2.x UI. To add a HostPort, browse to the project containing your workloads, and edit each workload that you want to expose, as shown below. Map the port that your service container exposes to the HostPort exposed on your target node.For example, for the web-deployment.yml file parsed from v1.6 that we’ve been using as a sample, we would edit its Kubernetes manifest, set the publish the port that the container uses, and then declare a HostPort listening on the port of your choice (9890) as shown below. You can then access your workload by clicking the link created in the Rancher UI.Port Mapping: Setting HostPortA HostPort is a port exposed to the public on a specific node running one or more pod. Traffic to the node and the exposed port (<HOST_IP>:<HOSTPORT>) are routed to the requested container’s private port. Using a HostPort for a Kubernetes pod in Rancher v2.x is synonymous with creating a public port mapping for a container in Rancher v1.6.In the following diagram, a user is trying to access an instance of Nginx, which is running within a pod on port 80. However, the Nginx deployment is assigned a HostPort of 9890. The user can connect to this pod by browsing to its host IP address, followed by the HostPort in use (9890 in case).HostPort Pros\nAny port available on the host can be exposed.\nConfiguration is simple, and the HostPort is set directly in the Kubernetes pod specifications. Unlike NodePort, no other objects need to be created to expose your app.\nHostPort Cons\nLimits the scheduling options for your pod, as only hosts with vacancies for your chosen port can be used.\nIf the scale of your workload is larger than the number of nodes in your Kubernetes cluster, the deployment fails.\nAny two workloads that specify the same HostPort cannot be deployed to the same node.\nIf the host where your pods are running becomes unavailable, Kubernetes reschedules the pods to different nodes. Thus, if the IP address for your workload changes, external clients of your application will lose access to the pod. The same thing happens when you restart your pods—Kubernetes reschedules them to a different node.\nIn Rancher v1.6, we used the term Port Mapping for exposing an IP address and port where your you and your users can access a service.In Rancher v2.x, the mechanisms and terms for service exposure have changed and expanded. You now have two port mapping options: HostPorts (which is most synonymous with v1.6 port mapping, allows you to expose your app at a single IP and port) and NodePorts (which allows you to map ports on all of your cluster nodes, not just one).Unfortunately, port mapping cannot be parsed by the migration-tools CLI. If the services you’re migrating from v1.6 to v2.x have port mappings set, you’ll have to either set a HostPort or NodePort as a replacement.\nWhat’s Different About Exposing Services in Rancher v2.x?\nHostPorts\nSetting HostPort\nNodePorts\nSetting NodePort\n","postref":"22b216173de8407705833831ca3b76af","objectID":"e8e72023cd73deb2a8607191fb44b191","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/v1.6-migration/expose-services/"},{"anchor":"#in-this-document","title":"In This Document","content":"The migration-tool CLI cannot parse health checks from Compose files to Kubernetes manifest. Therefore, if want you to add health checks to your Rancher v2.x workloads, you’ll have to add them manually.Using the Rancher v2.x UI, you can add TCP or HTTP health checks to Kubernetes workloads. By default, Rancher asks you to configure a readiness check for your workloads and applies a liveness check using the same configuration. Optionally, you can define a separate liveness check.If the probe fails, the container is restarted per the restartPolicy defined in the workload specs. This setting is equivalent to the strategy parameter for health checks in Rancher v1.6.Configure probes by using the Health Check section while editing deployments called out in output.txt.Edit Deployment: Health Check SectionConfiguring ChecksWhile you create a workload using Rancher v2.x, we recommend configuring a check that monitors the health of the deployment’s pods.\n  \n  \n  TCP checks monitor your deployment’s health by attempting to open a connection to the pod over a specified port. If the probe can open the port, it’s considered healthy. Failure to open it is considered unhealthy, which notifies Kubernetes that it should kill the pod and then replace it according to its restart policy. (this applies to Liveness probes, for Readiness probes, it will mark the pod as Unready).\n\nYou can configure the probe along with values for specifying its behavior by selecting the TCP connection opens successfully option in the Health Check section. For more information, see Deploying Workloads. For help setting probe timeout and threshold values, see Health Check Parameter Mappings.\n\n\n\nWhen you configure a readiness check using Rancher v2.x, the readinessProbe directive and the values you’ve set are added to the deployment’s Kubernetes manifest. Configuring a readiness check also automatically adds a liveness check (livenessProbe) to the deployment.\n\n\n\n\n\n\n  HTTP checks monitor your deployment’s health by sending an HTTP GET request to a specific URL path that you define. If the pod responds with a message range of 200-400, the health check is considered successful. If the pod replies with any other value, the check is considered unsuccessful, so Kubernetes kills and replaces the pod according to its restart policy. (this applies to Liveness probes, for Readiness probes, it will mark the pod as Unready).\n\nYou can configure the probe along with values for specifying its behavior by selecting the HTTP returns successful status or HTTPS returns successful status. For more information, see Deploying Workloads.  For help setting probe timeout and threshold values, see Health Check Parameter Mappings.\n\n\n\nWhen you configure a readiness check using Rancher v2.x, the readinessProbe directive and the values you’ve set are added to the deployment’s Kubernetes manifest. Configuring a readiness check also automatically adds a liveness check (livenessProbe) to the deployment.\n\n\n\nConfiguring Separate Liveness ChecksWhile configuring a readiness check for either the TCP or HTTP protocol, you can configure a separate liveness check by clicking the Define a separate liveness check. For help setting probe timeout and threshold values, see Health Check Parameter Mappings.Additional Probing OptionsRancher v2.x, like v1.6, lets you perform health checks using the TCP and HTTP protocols. However, Rancher v2.x also lets you check the health of a pod by running a command inside of it. If the container exits with a code of 0 after running the command, the pod is considered healthy.You can configure a liveness or readiness check that executes a command that you specify by selecting the Command run inside the container exits with status 0 option from Health Checks while deploying a workload.Health Check Parameter MappingsWhile configuring readiness checks and liveness checks, Rancher prompts you to fill in various timeout and threshold values that determine whether the probe is a success or failure. The reference table below shows you the equivalent health check values from Rancher v1.6.\n\n\nRancher v1.6 Compose Parameter\nRancher v2.x Kubernetes Parameter\n\n\n\n\n\nport\ntcpSocket.port\n\n\n\nresponse_timeout\ntimeoutSeconds\n\n\n\nhealthy_threshold\nfailureThreshold\n\n\n\nunhealthy_threshold\nsuccessThreshold\n\n\n\ninterval\nperiodSeconds\n\n\n\ninitializing_timeout\ninitialDelaySeconds\n\n\n\nstrategy\nrestartPolicy\n\n\nNext: Schedule Your ServicesIn Rancher v2.x, the health check microservice is replaced with Kubernetes’s native health check mechanisms, called probes. These probes, similar to the Rancher v1.6 health check microservice, monitor the health of pods over TCP and HTTP.However, probes in Rancher v2.x have some important differences, which are described below. For full details about probes, see the Kubernetes documentation.Local Health ChecksUnlike the Rancher v1.6 health checks performed across hosts, probes in Rancher v2.x occur on same host, performed by the kubelet.Multiple Probe TypesKubernetes includes two different types of probes: liveness checks and readiness checks.\nLiveness Check:\n\nChecks if the monitored container is running. If the probe reports failure, Kubernetes kills the pod, and then restarts it according to the deployment restart policy.\n\nReadiness Check:\n\nChecks if the container is ready to accept and serve requests. If the probe reports failure, the pod is sequestered from the public until it self heals.\nThe following diagram displays kubelets running probes on containers they are monitoring (kubelets are the primary “agent” running on each node). The node on the left is running a liveness probe, while the one of the right is running a readiness check. Notice that the kubelet is scanning containers on its host node rather than across nodes, as in Rancher v1.6.In Rancher v1.6, you could add health checks to monitor a particular service’s operations. These checks were performed by the Rancher health check microservice, which is launched in a container on a node separate from the node hosting the monitored service (however, Rancher v1.6.20 and later also runs a local health check container as a redundancy for the primary health check container on another node). Health check settings were stored in the rancher-compose.yml file for your stack.The health check microservice features two types of health checks, which have a variety of options for timeout, check interval, etc.:\nTCP health checks:\n\nThese health checks check if a TCP connection opens at the specified port for the monitored service. For full details, see the Rancher v1.6 documentation.\n\nHTTP health checks:\n\nThese health checks monitor HTTP requests to a specified path and check whether the response is expected response (which is configured along with the health check).\nThe following diagram displays the health check microservice evaluating a container running Nginx. Notice that the microservice is making its check across nodes.\nRancher v1.6 Health Checks\nRancher v2.x Health Checks\nConfiguring Probes in Rancher v2.x\n","postref":"0ee82650003e83527903d101a489ea9d","objectID":"2fe33d8514c886f944af3e0a1ab48d4a","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/v1.6-migration/monitor-apps/"},{"anchor":"#","title":"4. Install Rancher","content":"This section is about how to deploy Rancher for your air gapped environment. An air gapped environment could be where Rancher server will be installed offline, behind a firewall, or behind a proxy. There are tabs for either a high availability (recommended) or a Docker installation.\n\n\n  \n  \n  Rancher recommends installing Rancher on a Kubernetes cluster. A highly available Kubernetes Installation is comprised of three nodes running the Rancher server components on a Kubernetes cluster. The persistence layer (etcd) is also replicated on these three nodes, providing redundancy and data duplication in case one of the nodes fails.\n\nThis section describes installing Rancher in five parts:\n\n\nA. Add the Helm Chart Repository\nB. Choose your SSL Configuration\nC. Render the Rancher Helm Template\nD. Install Rancher\nE. For Rancher versions prior to v2.3.0, Configure System Charts\n\n\nA. Add the Helm Chart Repository\n\nFrom a system that has access to the internet, fetch the latest Helm chart and copy the resulting manifests to a system that has access to the Rancher server cluster.\n\n\nIf you haven’t already, initialize helm locally on a workstation that has internet access. Note: Refer to the Helm version requirements to choose a version of Helm to install Rancher.\nhelm init -c\n\nUse helm repo add command to add the Helm chart repository that contains charts to install Rancher. For more information about the repository choices and which is best for your use case, see Choosing a Version of Rancher.\n\n\n\n  \n  Latest: Recommended for trying out the newest features\n\n\n\n\n  \n  Stable: Recommended for production environments\n\n\n\n\n  \n  Alpha: Experimental preview of upcoming releases.\n  \n  Note: Upgrades are not supported to, from, or between Alphas.\n\n\n\n\nhelm repo add rancher-<CHART_REPO> https://releases.rancher.com/server-charts/<CHART_REPO>\n\n\nFetch the latest Rancher chart. This will pull down the chart and save it in the current directory as a .tgz file.\nhelm fetch rancher-<CHART_REPO>/rancher\n\n\n\nWant additional options? Need help troubleshooting? See Kubernetes Install: Advanced Options.\n\n\nB. Choose your SSL Configuration\n\nRancher Server is designed to be secure by default and requires SSL/TLS configuration.\n\nWhen Rancher is installed on an air gapped Kubernetes cluster, there are two recommended options for the source of the certificate.\n\n\nNote: If you want terminate SSL/TLS externally, see TLS termination on an External Load Balancer.\n\n\n\n\n\nConfiguration\nChart option\nDescription\nRequires cert-manager\n\n\n\n\n\nRancher Generated Self-Signed Certificates\ningress.tls.source=rancher\nUse certificates issued by Rancher’s generated CA (self signed) This is the default and does not need to be added when rendering the Helm template.\nyes\n\n\n\nCertificates from Files\ningress.tls.source=secret\nUse your own certificate files by creating Kubernetes Secret(s).  This option must be passed when rendering the Rancher Helm template.\nno\n\n\n\n\nC. Render the Rancher Helm Template\n\nWhen setting up the Rancher Helm template, there are several options in the Helm chart that are designed specifically for air gap installations.\n\n\n\n\nChart Option\nChart Value\nDescription\n\n\n\n\n\ncertmanager.version\n““\nConfigure proper Rancher TLS issuer depending of running cert-manager version.\n\n\n\nsystemDefaultRegistry\n<REGISTRY.YOURDOMAIN.COM:PORT>\nConfigure Rancher server to always pull from your private registry when provisioning clusters.\n\n\n\nuseBundledSystemChart\ntrue\nConfigure Rancher server to use the packaged copy of Helm system charts. The system charts repository contains all the catalog items required for features such as monitoring, logging, alerting and global DNS. These Helm charts are located in GitHub, but since you are in an air gapped environment, using the charts that are bundled within Rancher is much easier than setting up a Git mirror. Available as of v2.3.0\n\n\n\n\nBased on the choice your made in B. Choose your SSL Configuration, complete one of the procedures below.\n\n\n  \n  Option A-Default Self-Signed Certificate\n  \n    By default, Rancher generates a CA and uses cert-manager to issue the certificate for access to the Rancher server interface.\n\n\nNote:\nRecent changes to cert-manager require an upgrade. If you are upgrading Rancher and using a version of cert-manager older than v0.11.0, please see our upgrade cert-manager documentation.\n\n\n\nFrom a system connected to the internet, add the cert-manager repo to Helm.\nhelm repo add jetstack https://charts.jetstack.io\nhelm repo update\n\nFetch the latest cert-manager chart available from the Helm chart repository.\n\n   helm fetch jetstack/cert-manager --version v0.12.0\n\nRender the cert manager template with the options you would like to use to install the chart. Remember to set the image.repository option to pull the image from your private registry. This will create a cert-manager directory with the Kubernetes manifest files.\nhelm template ./cert-manager-v0.12.0.tgz --output-dir . \\\n   --name cert-manager --namespace cert-manager \\\n   --set image.repository=<REGISTRY.YOURDOMAIN.COM:PORT>/quay.io/jetstack/cert-manager-controller\n   --set webhook.image.repository=<REGISTRY.YOURDOMAIN.COM:PORT>/quay.io/jetstack/cert-manager-webhook\n   --set cainjector.image.repository=<REGISTRY.YOURDOMAIN.COM:PORT>/quay.io/jetstack/cert-manager-cainjector\n\nDownload the required CRD file for cert-manager\ncurl -L -o cert-manager/cert-manager-crd.yaml https://raw.githubusercontent.com/jetstack/cert-manager/release-0.12/deploy/manifests/00-crds.yaml\n\nRender the Rancher template, declaring your chosen options. Use the reference table below to replace each placeholder. Rancher needs to be configured to use the private registry in order to provision any Rancher launched Kubernetes clusters or Rancher tools.\n\n\n\n\nPlaceholder\nDescription\n\n\n\n\n\n<VERSION>\nThe version number of the output tarball.\n\n\n\n<RANCHER.YOURDOMAIN.COM>\nThe DNS name you pointed at your load balancer.\n\n\n\n<REGISTRY.YOURDOMAIN.COM:PORT>\nThe DNS name for your private registry.\n\n\n\n<CERTMANAGER_VERSION>\nCert-manager version running on k8s cluster.\n\n\n\nhelm template ./rancher-<VERSION>.tgz --output-dir . \\\n --name rancher \\\n --namespace cattle-system \\\n --set hostname=<RANCHER.YOURDOMAIN.COM> \\\n --set certmanager.version=<CERTMANAGER_VERSION> \\\n --set rancherImage=<REGISTRY.YOURDOMAIN.COM:PORT>/rancher/rancher \\\n --set systemDefaultRegistry=<REGISTRY.YOURDOMAIN.COM:PORT> \\ # Available as of v2.2.0, set a default private registry to be used in Rancher\n --set useBundledSystemChart=true # Available as of v2.3.0, use the packaged Rancher system charts\n\n\n  \n\n\n\n  \n  Option B: Certificates From Files using Kubernetes Secrets\n  \n    Create Kubernetes secrets from your own certificates for Rancher to use. The common name for the cert will need to match the hostname option in the command below, or the ingress controller will fail to provision the site for Rancher.\n\nRender the Rancher template, declaring your chosen options. Use the reference table below to replace each placeholder. Rancher needs to be configured to use the private registry in order to provision any Rancher launched Kubernetes clusters or Rancher tools.\n\nIf you are using a Private CA signed cert, add --set privateCA=true following --set ingress.tls.source=secret.\n\n\n\n\nPlaceholder\nDescription\n\n\n\n\n\n<VERSION>\nThe version number of the output tarball.\n\n\n\n<RANCHER.YOURDOMAIN.COM>\nThe DNS name you pointed at your load balancer.\n\n\n\n<REGISTRY.YOURDOMAIN.COM:PORT>\nThe DNS name for your private registry.\n\n\n\n   helm template ./rancher-<VERSION>.tgz --output-dir . \\\n    --name rancher \\\n    --namespace cattle-system \\\n    --set hostname=<RANCHER.YOURDOMAIN.COM> \\\n    --set rancherImage=<REGISTRY.YOURDOMAIN.COM:PORT>/rancher/rancher \\\n    --set ingress.tls.source=secret \\\n    --set systemDefaultRegistry=<REGISTRY.YOURDOMAIN.COM:PORT> \\ # Available as of v2.2.0, set a default private registry to be used in Rancher\n    --set useBundledSystemChart=true # Available as of v2.3.0, use the packaged Rancher system charts\nThen refer to Adding TLS Secrets to publish the certificate files so Rancher and the ingress controller can use them.\n\n  \n\n\nD. Install Rancher\n\nCopy the rendered manifest directories to a system that has access to the Rancher server cluster to complete installation.\n\nUse kubectl to create namespaces and apply the rendered manifests.\n\nIf you chose to use self-signed certificates in B. Choose your SSL Configuration, install cert-manager.\n\n\n  \n  Self-Signed Certificate Installs - Install Cert-manager\n  \n    If you are using self-signed certificates, install cert-manager:\n\n\nCreate the namespace for cert-manager.\nkubectl create namespace cert-manager\n\nCreate the cert-manager CustomResourceDefinitions (CRDs).\nkubectl apply -f cert-manager/cert-manager-crd.yaml\n\n\n\nImportant:\nIf you are running Kubernetes v1.15 or below, you will need to add the `–validate=false flag to your kubectl apply command above else you will receive a validation error relating to the x-kubernetes-pr","postref":"55260b33d4d486c72b0cad8ed156f54a","objectID":"b887b5fcf5c9d307af28ce6c203606bd","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/air-gap-helm2/install-rancher/"},{"anchor":"#","title":"4. Install Rancher","content":"This section is about how to deploy Rancher for your air gapped environment. An air gapped environment could be where Rancher server will be installed offline, behind a firewall, or behind a proxy. There are tabs for either a high availability (recommended) or a Docker installation.\n\n\nNote: These installation instructions assume you are using Helm 3. For migration of installs started with Helm 2, refer to the official Helm 2 to 3 migration docs. This section provides a copy of the older air gap installation instructions for Rancher installed on Kubernetes with Helm 2, and it is intended to be used if upgrading to Helm 3 is not feasible.\n\n\n\n  \n  \n  Rancher recommends installing Rancher on a Kubernetes cluster. A highly available Kubernetes install is comprised of three nodes running the Rancher server components on a Kubernetes cluster. The persistence layer (etcd) is also replicated on these three nodes, providing redundancy and data duplication in case one of the nodes fails.\n\nThis section describes installing Rancher in five parts:\n\n\nA. Add the Helm Chart Repository\nB. Choose your SSL Configuration\nC. Render the Rancher Helm Template\nD. Install Rancher\nE. For Rancher versions prior to v2.3.0, Configure System Charts\n\n\nA. Add the Helm Chart Repository\n\nFrom a system that has access to the internet, fetch the latest Helm chart and copy the resulting manifests to a system that has access to the Rancher server cluster.\n\n\nIf you haven’t already, initialize helm locally on a workstation that has internet access. Note: Refer to the Helm version requirements to choose a version of Helm to install Rancher.\nhelm init -c\n\nUse helm repo add command to add the Helm chart repository that contains charts to install Rancher. For more information about the repository choices and which is best for your use case, see Choosing a Version of Rancher.\n\n\n\n  \n  Latest: Recommended for trying out the newest features\n\n\n\n\n  \n  Stable: Recommended for production environments\n\n\n\n\n  \n  Alpha: Experimental preview of upcoming releases.\n  \n  Note: Upgrades are not supported to, from, or between Alphas.\n\n\n\n\nhelm repo add rancher-<CHART_REPO> https://releases.rancher.com/server-charts/<CHART_REPO>\n\n\nFetch the latest Rancher chart. This will pull down the chart and save it in the current directory as a .tgz file.\nhelm fetch rancher-<CHART_REPO>/rancher\n\n\n\nWant additional options? Need help troubleshooting? See Kubernetes Install: Advanced Options.\n\n\nB. Choose your SSL Configuration\n\nRancher Server is designed to be secure by default and requires SSL/TLS configuration.\n\nWhen Rancher is installed on an air gapped Kubernetes cluster, there are two recommended options for the source of the certificate.\n\n\nNote: If you want terminate SSL/TLS externally, see TLS termination on an External Load Balancer.\n\n\n\n\n\nConfiguration\nChart option\nDescription\nRequires cert-manager\n\n\n\n\n\nRancher Generated Self-Signed Certificates\ningress.tls.source=rancher\nUse certificates issued by Rancher’s generated CA (self signed) This is the default and does not need to be added when rendering the Helm template.\nyes\n\n\n\nCertificates from Files\ningress.tls.source=secret\nUse your own certificate files by creating Kubernetes Secret(s).  This option must be passed when rendering the Rancher Helm template.\nno\n\n\n\n\nC. Render the Rancher Helm Template\n\nWhen setting up the Rancher Helm template, there are several options in the Helm chart that are designed specifically for air gap installations.\n\n\n\n\nChart Option\nChart Value\nDescription\n\n\n\n\n\ncertmanager.version\n““\nConfigure proper Rancher TLS issuer depending of running cert-manager version.\n\n\n\nsystemDefaultRegistry\n<REGISTRY.YOURDOMAIN.COM:PORT>\nConfigure Rancher server to always pull from your private registry when provisioning clusters.\n\n\n\nuseBundledSystemChart\ntrue\nConfigure Rancher server to use the packaged copy of Helm system charts. The system charts repository contains all the catalog items required for features such as monitoring, logging, alerting and global DNS. These Helm charts are located in GitHub, but since you are in an air gapped environment, using the charts that are bundled within Rancher is much easier than setting up a Git mirror. Available as of v2.3.0\n\n\n\n\nBased on the choice your made in B. Choose your SSL Configuration, complete one of the procedures below.\n\n\n  \n  Option A-Default Self-Signed Certificate\n  \n    By default, Rancher generates a CA and uses cert-manager to issue the certificate for access to the Rancher server interface.\n\n\nNote:\nRecent changes to cert-manager require an upgrade. If you are upgrading Rancher and using a version of cert-manager older than v0.11.0, please see our upgrade cert-manager documentation.\n\n\n\nFrom a system connected to the internet, add the cert-manager repo to Helm.\nhelm repo add jetstack https://charts.jetstack.io\nhelm repo update\n\nFetch the latest cert-manager chart available from the Helm chart repository.\n\n   helm fetch jetstack/cert-manager --version v0.12.0\n\nRender the cert manager template with the options you would like to use to install the chart. Remember to set the image.repository option to pull the image from your private registry. This will create a cert-manager directory with the Kubernetes manifest files.\nhelm template cert-manager ./cert-manager-v0.12.0.tgz --output-dir . \\\n   --namespace cert-manager \\\n   --set image.repository=<REGISTRY.YOURDOMAIN.COM:PORT>/quay.io/jetstack/cert-manager-controller \\\n   --set webhook.image.repository=<REGISTRY.YOURDOMAIN.COM:PORT>/quay.io/jetstack/cert-manager-webhook \\\n   --set cainjector.image.repository=<REGISTRY.YOURDOMAIN.COM:PORT>/quay.io/jetstack/cert-manager-cainjector\n\nDownload the required CRD file for cert-manager\ncurl -L -o cert-manager/cert-manager-crd.yaml https://raw.githubusercontent.com/jetstack/cert-manager/release-0.12/deploy/manifests/00-crds.yaml\n\nRender the Rancher template, declaring your chosen options. Use the reference table below to replace each placeholder. Rancher needs to be configured to use the private registry in order to provision any Rancher launched Kubernetes clusters or Rancher tools.\n\n\n\n\nPlaceholder\nDescription\n\n\n\n\n\n<VERSION>\nThe version number of the output tarball.\n\n\n\n<RANCHER.YOURDOMAIN.COM>\nThe DNS name you pointed at your load balancer.\n\n\n\n<REGISTRY.YOURDOMAIN.COM:PORT>\nThe DNS name for your private registry.\n\n\n\n<CERTMANAGER_VERSION>\nCert-manager version running on k8s cluster.\n\n\n\nhelm template rancher ./rancher-<VERSION>.tgz --output-dir . \\\n --namespace cattle-system \\\n --set hostname=<RANCHER.YOURDOMAIN.COM> \\\n --set certmanager.version=<CERTMANAGER_VERSION> \\\n --set rancherImage=<REGISTRY.YOURDOMAIN.COM:PORT>/rancher/rancher \\\n --set systemDefaultRegistry=<REGISTRY.YOURDOMAIN.COM:PORT> \\ # Available as of v2.2.0, set a default private registry to be used in Rancher\n --set useBundledSystemChart=true # Available as of v2.3.0, use the packaged Rancher system charts\n\n\n  \n\n\n\n  \n  Option B: Certificates From Files using Kubernetes Secrets\n  \n    Create Kubernetes secrets from your own certificates for Rancher to use. The common name for the cert will need to match the hostname option in the command below, or the ingress controller will fail to provision the site for Rancher.\n\nRender the Rancher template, declaring your chosen options. Use the reference table below to replace each placeholder. Rancher needs to be configured to use the private registry in order to provision any Rancher launched Kubernetes clusters or Rancher tools.\n\nIf you are using a Private CA signed cert, add --set privateCA=true following --set ingress.tls.source=secret.\n\n\n\n\nPlaceholder\nDescription\n\n\n\n\n\n<VERSION>\nThe version number of the output tarball.\n\n\n\n<RANCHER.YOURDOMAIN.COM>\nThe DNS name you pointed at your load balancer.\n\n\n\n<REGISTRY.YOURDOMAIN.COM:PORT>\nThe DNS name for your private registry.\n\n\n\n   helm template rancher ./rancher-<VERSION>.tgz --output-dir . \\\n    --namespace cattle-system \\\n    --set hostname=<RANCHER.YOURDOMAIN.COM> \\\n    --set rancherImage=<REGISTRY.YOURDOMAIN.COM:PORT>/rancher/rancher \\\n    --set ingress.tls.source=secret \\\n    --set systemDefaultRegistry=<REGISTRY.YOURDOMAIN.COM:PORT> \\ # Available as of v2.2.0, set a default private registry to be used in Rancher\n    --set useBundledSystemChart=true # Available as of v2.3.0, use the packaged Rancher system charts\nThen refer to Adding TLS Secrets to publish the certificate files so Rancher and the ingress controller can use them.\n\n  \n\n\nD. Install Rancher\n\nCopy the rendered manifest directories to a system that has access to the Rancher server cluster to complete installation.\n\nUse kubectl to create namespaces and apply the rendered manifests.\n\nIf you chose to use self-signed certificates in B. Choose your SSL Configuration, install cert-manager.\n\n\n  \n  Self-Signed Certificate Installs - Install Cert-manager\n  \n    If you are using self-signed certificates, install cert-manager:\n\n\nCreate the namespace for cert-manager.\nkubectl create namespace cert-man","postref":"70099a6420b2d7c36db7fce3101bba24","objectID":"82acfb044626b12fd2911909a7f33a9a","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/other-installation-methods/air-gap/install-rancher/"},{"anchor":"#","title":"Configuring a Global Default Private Registry","content":"You might want to use a private Docker registry to share your custom base images within your organization. With a private registry, you can keep a private, consistent, and centralized source of truth for the Docker images that are used in your clusters.\n\nThere are two main ways to set up private registries in Rancher: by setting up the global default registry through the Settings tab in the global view, and by setting up a private registry in the advanced options in the cluster-level settings. The global default registry is intended to be used for air-gapped setups, for registries that do not require credentials. The cluster-level private registry is intended to be used in all setups in which the private registry requires credentials.\n\nThis section is about configuring the global default private registry, and focuses on how to configure the registry from the Rancher UI after Rancher is installed.\n\nFor instructions on setting up a private registry with command line options during the installation of Rancher, refer to the air gapped Docker installation or air gapped Kubernetes installation instructions.\n\nIf your private registry requires credentials, it cannot be used as the default registry. There is no global way to set up a private registry with authorization for every Rancher-provisioned cluster. Therefore, if you want a Rancher-provisioned cluster to pull images from a private registry with credentials, you will have to pass in the registry credentials through the advanced cluster options every time you create a new cluster.\n\nSetting a Private Registry with No Credentials as the Default Registry\n\n\nLog into Rancher and configure the default administrator password.\n\nGo into the Settings view.\n\n\n\n\n    \n    \n    \n    \n    \n    \n\n\n\nLook for the setting called system-default-registry and choose Edit.\n\n\n\n\n    \n    \n    \n    \n    \n    \n\n\n\nChange the value to your registry (e.g. registry.yourdomain.com:port). Do not prefix the registry with http:// or https://.\n\n\n\n\n    \n    \n    \n    \n    \n    \n\n\n\n\nResult: Rancher will use your private registry to pull system images.\n\nSetting a Private Registry with Credentials when Deploying a Cluster\n\nYou can follow these steps to configure a private registry when you provision a cluster with Rancher:\n\n\nWhen you create a cluster through the Rancher UI, go to the Cluster Options section and click Show Advanced Options.\nIn the Enable Private Registries section, click Enabled.\nEnter the registry URL and credentials.\nClick Save.\n\n\nResult: The new cluster will be able to pull images from the private registry.\n","postref":"fdd2c65f8dcfbcff5f79a9c72a5add96","objectID":"dc4b2c59322c490eaa830be236166332","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/config-private-registry/"},{"anchor":"#","title":"Helm Version Requirements","content":"This section contains the requirements for Helm, which is the tool used to install Rancher on a high-availability Kubernetes cluster.\n\n\nThe installation instructions have been updated for Helm 3. For migration of installs started with Helm 2, refer to the official Helm 2 to 3 Migration Docs. This section provides a copy of the older high-availability Rancher installation instructions that used Helm 2, and it is intended to be used if upgrading to Helm 3 is not feasible.\n\n\n\nHelm v2.16.0 or higher is required for Kubernetes v1.16. For the default Kubernetes version, refer to the release notes for the version of RKE that you are using.\nHelm v2.15.0 should not be used, because of an issue with converting/comparing numbers.\nHelm v2.12.0 should not be used, because of an issue with cert-manager.\n\n","postref":"b37b4c128ccdbdf977e9948410d588ab","objectID":"22a9fed7fb8b2483b861b25a87f077c1","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/helm-version/"},{"anchor":"#kafka-server-configuration","title":"Kafka Server Configuration","content":"SSL ConfigurationIf your Kafka cluster is using SSL for the Broker, you need to complete the SSL Configuration form.\nProvide the Client Private Key and Client Certificate. You can either copy and paste them or upload them by using the Read from a file button.\n\nProvide the CA Certificate PEM. You can either copy and paste the certificate or upload it using the Read from a file button.\n\nNote: Kafka does not support self-signed certificates when client authentication is enabled.\nSASL configurationIf your Kafka cluster is using SASL authentication for the Broker, you need to complete the SASL Configuration form.\nEnter the SASL Username and Password.\n\nSelect the SASL Type that your Kafka cluster is using.\n\n\nIf your Kafka is using Plain, please ensure your Kafka cluster is using SSL.\n\nIf your Kafka is using Scram, you need to select which Scram Mechanism Kafka is using.\n\n\nSelect the type of Endpoint your Kafka server is using:\n\n\nZookeeper: Enter the IP address and port. By default, Zookeeper uses port 2181. Please note that a Zookeeper endpoint cannot enable TLS.\nBroker: Click on Add Endpoint. For each Kafka broker, enter the IP address and port. By default, Kafka brokers use port 9092.\n\n\nIn the Topic field, enter the name of a Kafka topic that your Kubernetes cluster submits logs to.\n","postref":"6e9f63e315a7c716ecac1f22e29d02dd","objectID":"c97cedb20f6333662f4864c6d4df3562","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/tools/logging/kafka/"},{"anchor":"#","title":"Troubleshooting","content":"\nSSH Connectivity Errors\nProvisioning Errors\n\n","postref":"7b6281a9471118849160e71d1e1d98b9","objectID":"cdcaa98db8baacac2b565b45169d3ea9","permalink":"http://jijeesh.github.io/docs/rke/latest/en/troubleshooting/"},{"anchor":"#in-this-document","title":"In This Document","content":"Rancher offers a variety of options when scheduling nodes to host workload pods (i.e., scheduling hosts for containers in Rancher v1.6).You can choose a scheduling option as you deploy a workload. The term workload is synonymous with adding a service to a Stack in Rancher v1.6). You can deploy a workload by using the context menu to browse to a cluster project (<CLUSTER> > <PROJECT> > Workloads).The sections that follow provide information on using each scheduling options, as well as any notable changes from Rancher v1.6. For full instructions on deploying a workload in Rancher v2.x beyond just scheduling options, see Deploying Workloads.\n\n\nOption\nv1.6 Feature\nv2.x Feature\n\n\n\n\n\nSchedule a certain number of pods?\n✓\n✓\n\n\n\nSchedule pods to specific node?\n✓\n✓\n\n\n\nSchedule to nodes using labels?\n✓\n✓\n\n\n\nSchedule to nodes using label affinity/anti-affinity rules?\n✓\n✓\n\n\n\nSchedule based on resource constraints?\n✓\n✓\n\n\n\nPreventing scheduling specific services to specific hosts?\n✓\n✓\n\n\n\nSchedule services globally?\n✓\n✓\n\n\nSchedule a certain number of podsIn v1.6, you could control the number of container replicas deployed for a service. You can schedule pods the same way in v2.x, but you’ll have to set the scale manually while editing a workload.During migration, you can resolve scale entries in output.txt by setting a value for the Workload Type option Scalable deployment depicted below.Scalable Deployment OptionScheduling Pods to a Specific NodeJust as you could schedule containers to a single host in Rancher v1.6, you can schedule pods to single node in Rancher v2.xAs you deploy a workload, use the Node Scheduling section to choose a node to run your pods on. The workload below is being scheduled to deploy an Nginx image with a scale of two pods on a specific node.\nRancher v2.x: Workload DeploymentRancher schedules pods to the node you select if 1) there are compute resource available for the node and 2) you’ve configured port mapping to use the HostPort option, that there are no port conflicts.If you expose the workload using a NodePort that conflicts with another workload, the deployment gets created successfully, but no NodePort service is created. Therefore, the workload isn’t exposed outside of the cluster.After the workload is created, you can confirm that the pods are scheduled to your chosen node. From the project view, click Resources > Workloads. (In versions prior to v2.3.0, click the Workloads tab.) Click the Group by Node icon to sort your workloads by node. Note that both Nginx pods are scheduled to the same node.Scheduling Using LabelsIn Rancher v2.x, you can constrain pods for scheduling to specific nodes (referred to as hosts in v1.6). Using labels, which are key/value pairs that you can attach to different Kubernetes objects, you can configure your workload so that pods you’ve labeled are assigned to specific nodes (or nodes with specific labels are automatically assigned workload pods).Label Scheduling Options\n\n\nLabel Object\nRancher v1.6\nRancher v2.x\n\n\n\n\n\nSchedule by Node?\n✓\n✓\n\n\n\nSchedule by Pod?\n✓\n✓\n\n\nApplying Labels to Nodes and PodsBefore you can schedule pods based on labels, you must first apply labels to your pods or nodes.\nHooray!\nAll the labels that you manually applied in Rancher v1.6 (but not the ones automatically created by Rancher) are parsed by migration-tools CLI, meaning you don’t have to manually reapply labels.\nTo apply labels to pods, make additions to the Labels and Annotations section as you configure your workload. After you complete workload configuration, you can view the label by viewing each pod that you’ve scheduled. To apply labels to nodes, edit your node and make additions to the Labels section.Label Affinity/AntiAffinitySome of the most-used scheduling features in v1.6 were affinity and anti-affinity rules.output.txt Affinity Label\nAffinity\n\nAny pods that share the same label are scheduled to the same node. Affinity can be configured in one of two ways:\n\n\n\n\nAffinity\nDescription\n\n\n\n\n\nHard\nA hard affinity rule means that the host chosen must satisfy all the scheduling rules. If no such host can be found, the workload will fail to deploy. In the Kubernetes manifest, this rule translates to the nodeAffinity directive.To use hard affinity, configure a rule using the Require ALL of section (see figure below).\n\n\n\nSoft\nRancher v1.6 user are likely familiar with soft affinity rules, which try to schedule the deployment per the rule, but can deploy even if the rule is not satisfied by any host.To use soft affinity, configure a rule using the Prefer Any of section (see figure below).\n\n\n\n\n\nAffinity Rules: Hard and Soft![Affinity Rules](http://jijeesh.github.io/docs/img/rancher/node-scheduling-affinity.png)\n\nAntiAffinity\n\nAny pods that share the same label are scheduled to different nodes. In other words, while affinity attracts a specific label to each other, anti-affinity repels a label from itself, so that pods are scheduled to different nodes.\n\nYou can create an anti-affinity rules using either hard or soft affinity. However, when creating your rule, you must use either the is not set or not in list operator.\n\nFor anti-affinity rules, we recommend using labels with phrases like NotIn and DoesNotExist, as these terms are more intuitive when users are applying anti-affinity rules.\n\nAntiAffinity Operators\n\n\nDetailed documentation for affinity/anti-affinity is available in the Kubernetes Documentation.Affinity rules that you create in the UI update your workload, adding pod affinity/anti-affinity directives to the workload Kubernetes manifest specs.Preventing Scheduling Specific Services to Specific NodesIn Rancher v1.6 setups, you could prevent services from being scheduled to specific nodes with the use of labels. In Rancher v2.x, you can reproduce this behavior using native Kubernetes scheduling options.In Rancher v2.x, you can prevent pods from being scheduled to specific nodes by applying taints to a node. Pods will not be scheduled to a tainted node unless it has special permission, called a toleration. A toleration is a special label that allows a pod to be deployed to a tainted node. While editing a workload, you can apply tolerations using the Node Scheduling section. Click Show advanced options.Applying TolerationsFor more information, see the Kubernetes documentation on taints and tolerations.Scheduling Global ServicesRancher v1.6 included the ability to deploy global services, which are services that deploy duplicate containers to each host in the environment (i.e.,  nodes in your cluster using Rancher v2.x terms). If a service has the io.rancher.scheduler.global: 'true' label declared, then Rancher v1.6 schedules a service container on each host in the environment.output.txt Global Service LabelIn Rancher v2.x, you can schedule a pod to each node using a Kubernetes DaemonSet, which is a specific type of workload ). A DaemonSet functions exactly like a Rancher v1.6 global service. The Kubernetes scheduler deploys a pod on each node of the cluster, and as new nodes are added, the scheduler will start new pods on them provided they match the scheduling requirements of the workload. Additionally, in v2.x, you can also limit a DaemonSet to be deployed to nodes that have a specific label.To create a daemonset while configuring a workload, choose Run one pod on each node from the Workload Type options.Workload Configuration: Choose run one pod on each node to configure daemonsetScheduling Pods Using Resource ConstraintsWhile creating a service in the Rancher v1.6 UI, you could schedule its containers to hosts based on hardware requirements that you choose. The containers are then scheduled to hosts based on which ones have bandwidth, memory, and CPU capacity.In Rancher v2.x, you can still specify the resources required by your pods. However, these options are unavailable in the UI. Instead, you must edit your workload’s manifest file to declare these resource constraints.To declare resource constraints, edit your migrated workloads, editing the Security & Host sections.\nTo reserve a minimum hardware reservation available for your pod(s), edit the following sections:\n\n\nMemory Reservation\nCPU Reservation\nNVIDIA GPU Reservation\n\n\nTo set a maximum hardware limit for your pods, edit:\n\n\nMemory Limit\nCPU Limit\n\nScheduling: Resource Constraint SettingsYou can find more detail about these specs and how to use them in the Kubernetes Documentation.Next: Service DiscoveryRancher v2.x retains all methods available in v1.6 for scheduling your services. However, because the default container orchestration system has changed from Cattle to Kubernetes, the terminology and implementation for each scheduling option has changed.In v1.6, you would schedule a service to a host while adding a service to a Stack. In Rancher v2.x., the equivalent action is to schedule a workload for deployment. The following composite image shows a comparison of the UI used for scheduling in Rancher v2.x versus v1.6.\nWhat’s Different for Sch","postref":"8bcaaca2c539fd4b40a12b81e3bfe8b0","objectID":"46cfa653858ab6e36a10562de044ef6a","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/v1.6-migration/schedule-workloads/"},{"anchor":"#configure-repositories","title":"Configure Repositories","content":"For detailed information about setting up your own pipeline for your repository, configure a version control provider, enable a repository and finally configure your pipeline.After enabling an example repository, run the pipeline to see how it works.\nFrom the Global view, navigate to the project that you want to test out pipelines.\n\nClick Resources > Pipelines. In versions prior to v2.3.0, click Workloads > Pipelines.\n\nFind the example repository, select the vertical Ellipsis (…) > Run.\n\n\nNote: When you run a pipeline the first time, it takes a few minutes to pull relevant images and provision necessary pipeline components.\n\nResult: The pipeline runs. You can see the results in the logs.After enabling an example repository, review the pipeline to see how it is set up.\nFrom the Global view, navigate to the project that you want to test out pipelines.\n\nClick Resources > Pipelines. In versions prior to v2.3.0, click Workloads > Pipelines.\n\nFind the example repository, select the vertical Ellipsis (…). There are two ways to view the pipeline:\n\n\nRancher UI: Click on Edit Config to view the stages and steps of the pipeline.\nYAML: Click on View/Edit YAML to view the ./rancher-pipeline.yml file.\n\nBy default, the example pipeline repositories are disabled. Enable one (or more) to test out the pipeline feature and see how it works.\nFrom the Global view, navigate to the project that you want to test out pipelines.\n\nClick Resources > Pipelines. In versions prior to v2.3.0, click Workloads > Pipelines.\n\nClick Configure Repositories.\n\nStep Result: A list of example repositories displays.\n\n\nNote: Example repositories only display if you haven’t fetched your own repos.\n\n\nClick Enable for one of the example repos (e.g., https://github.com/rancher/pipeline-example-go.git). Then click Done.\nResults:\nThe example repository is enabled to work with a pipeline is available in the Pipeline tab.\n\nThe following workloads are deployed to a new namespace:\n\n\ndocker-registry\njenkins\nminio\n\n","postref":"de89ada5135eee597873ad47b7b9b10b","objectID":"2bded021b8d984ebcf7a95af6a1a1ceb","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/k8s-in-rancher/pipelines/example-repos/"},{"anchor":"#syslog-server-configuration","title":"Syslog Server Configuration","content":"If your Syslog server is using TCP protocol and uses TLS, you need to select Use TLS and complete the Encryption Configuration form.\nProvide the Client Private Key and Client Certificate. You can either copy and paste them or upload them by using the Read from a file button.\n\n\nYou can use either a self-signed certificate or one provided by a certificate authority.\n\nYou can generate a self-signed certificate using an openssl command. For example:\n\n openssl req -x509 -newkey rsa:2048 -keyout myservice.key -out myservice.cert -days 365 -nodes -subj \"/CN=myservice.example.com\"\n\n\n\nSelect whether or not you want to verify your SSL.\n\n\nIf you are using a self-signed certificate, select Enabled - Input trusted server certificate, provide the CA Certificate PEM. You can copy and paste the certificate or upload it using the Read from a file button.\nIf you are using a certificate from a certificate authority, select Enabled - Input trusted server certificate. You do not need to provide a CA Certificate PEM.\n\n\nIn the Endpoint field, enter the IP address and port for your Syslog server. Additionally, in the dropdown, select the protocol that your Syslog server uses.\n\nIn the Program field, enter the name of the application sending logs to your Syslog server, e.g. Rancher.\n\nIf you are using a cloud logging service, e.g. Sumologic, enter a Token that authenticates with your Syslog server. You will need to create this token in the cloud logging service.\n\nSelect a Log Severity for events that are logged to the Syslog server. For more information on each severity level, see the Syslog protocol documentation.\n\n\nBy specifying a Log Severity does not mean that will act as a filtering mechanism for logs. To do that you should use a parser on the Syslog server.\n\n","postref":"273160a7df2efcc2f2e062a5b4c96515","objectID":"02b5018b7954e09a16dc2528a0605dd3","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/tools/logging/syslog/"},{"anchor":"#","title":"Example YAML File","content":"Pipelines can be configured either through the UI or using a yaml file in the repository, i.e. .rancher-pipeline.yml or .rancher-pipeline.yaml.\n\nIn the [pipeline configuration docs](), we provide examples of each available feature within pipelines. Here is a full example for those who want to jump right in.\n# example\nstages:\n  - name: Build something\n    # Conditions for stages\n    when:\n      branch: master\n      event: [ push, pull_request ]\n    # Multiple steps run concurrently\n    steps:\n    - runScriptConfig:\n        image: busybox\n        shellScript: echo ${FIRST_KEY} && echo ${ALIAS_ENV}\n      # Set environment variables in container for the step\n      env:\n        FIRST_KEY: VALUE\n        SECOND_KEY: VALUE2\n      # Set environment variables from project secrets\n      envFrom:\n      - sourceName: my-secret\n        sourceKey: secret-key\n        targetKey: ALIAS_ENV\n    - runScriptConfig:\n        image: busybox\n        shellScript: date -R\n      # Conditions for steps\n      when:\n        branch: [ master, dev ]\n        event: push\n  - name: Publish my image\n    steps:\n    - publishImageConfig:\n        dockerfilePath: ./Dockerfile\n        buildContext: .\n        tag: rancher/rancher:v2.0.0\n        # Optionally push to remote registry\n        pushRemote: true\n        registry: reg.example.com\n  - name: Deploy some workloads\n    steps:\n    - applyYamlConfig:\n        path: ./deployment.yaml\n# branch conditions for the pipeline\nbranch:\n  include: [ master, feature/*]\n  exclude: [ dev ]\n# timeout in minutes\ntimeout: 30\nnotification:\n  recipients:\n  - # Recipient\n    recipient: \"#mychannel\"\n    # ID of Notifier\n    notifier: \"c-wdcsr:n-c9pg7\"\n  - recipient: \"test@example.com\"\n    notifier: \"c-wdcsr:n-lkrhd\"\n  # Select which statuses you want the notification to be sent  \n  condition: [\"Failed\", \"Success\", \"Changed\"]\n  # Ability to override the default message (Optional)\n  message: \"my-message\"","postref":"a2d0c0e0ced3f1e3131b15c4bffe55cb","objectID":"c8b2fce8470a421e72d122ecf45668a4","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/k8s-in-rancher/pipelines/example/"},{"anchor":"#in-this-document","title":"In This Document","content":"For Rancher v2.x, we’ve replaced the Rancher DNS microservice used in v1.6 with native Kubernetes DNS support, which provides equivalent service discovery for Kubernetes workloads and pods. Former Cattle users can replicate all the service discovery features from Rancher v1.6 in v2.x. There’s no loss of functionality.Kubernetes schedules a DNS pod and service in the cluster, which is similar to the Rancher v1.6 DNS microservice. Kubernetes then configures its kubelets to route all DNS lookups to this DNS service, which is skyDNS, a flavor of the default Kube-DNS implementation.The following table displays each service discovery feature available in the two Rancher releases.\n\n\nService Discovery Feature\nRancher v1.6\nRancher v2.x\nDescription\n\n\n\n\n\nservice discovery within and across stack (i.e., clusters)\n✓\n✓\nAll services in the stack are resolvable by <service_name> and by <service_name>.<stack_name> across stacks.\n\n\n\ncontainer discovery\n✓\n✓\nAll containers are resolvable globally by their name.\n\n\n\nservice alias name creation\n✓\n✓\nAdding an alias name to services and linking to other services using aliases.\n\n\n\ndiscovery of external services\n✓\n✓\nPointing to services deployed outside of Rancher using the external IP(s) or a domain name.\n\n\nService Discovery Within and Across NamespacesWhen you create a new workload in v2.x (not migrated, more on that below), Rancher automatically creates a service with an identical name, and then links the service and workload together. If you don’t explicitly expose a port, the default port of 42 is used. This practice makes the workload discoverable within and across namespaces by its name.Container DiscoveryIndividual pods running in the Kubernetes cluster also get a DNS record assigned, which uses dot notation as well: <POD_IP_ADDRESS>.<NAMESPACE_NAME>.pod.cluster.local. For example, a pod with an IP of 10.42.2.7 in the namespace default with a DNS name of cluster.local would have an entry of 10-42-2-7.default.pod.cluster.local.Pods can also be resolved using the hostname and subdomain fields if set in the pod spec. Details about this resolution is covered in the Kubernetes docs.Linking Migrated Workloads and ServicesWhen you migrate v1.6 services to v2.x, Rancher does not automatically create a Kubernetes service record for each migrated deployment. Instead, you’ll have to link the deployment and service together manually, using any of the methods listed below.In the image below, the web-deployment.yml and web-service.yml files created after parsing our migration example services are linked together.Linked Workload and Kubernetes ServiceService Name Alias CreationJust as you can create an alias for Rancher v1.6 services, you can do the same for Rancher v2.x workloads. Similarly, you can also create DNS records pointing to services running externally, using either their hostname or IP address. These DNS records are Kubernetes service objects.Using the v2.x UI, use the context menu to navigate to the Project view. Then click Resources > Workloads > Service Discovery. (In versions prior to v2.3.0, click the Workloads > Service Discovery tab.) All existing DNS records created for your workloads are listed under each namespace.Click Add Record to create new DNS records. Then view the various options supported to link to external services or to create aliases for another workload, DNS record, or set of pods.Add Service Discovery RecordThe following table indicates which alias options are implemented natively by Kubernetes and which options are implemented by Rancher leveraging Kubernetes.\n\n\nOption\nKubernetes-implemented?\nRancher-implemented?\n\n\n\n\n\nPointing to an external hostname\n✓\n\n\n\n\nPointing to a set of pods that match a selector\n✓\n\n\n\n\nPointing to an external IP address\n\n✓\n\n\n\nPointing to another workload\n\n✓\n\n\n\nCreate alias for another DNS record\n\n✓\n\n\nNext: Load Balancing\nService Discovery: Rancher v1.6 vs. v2.x\nService Discovery Within and Across Namespaces\nContainer Discovery\nService Name Alias Creation\n","postref":"91fb2df0238f1952e330addb7cf34703","objectID":"1cf47d18e3def89ecba9257d147c7c9b","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/v1.6-migration/discover-services/"},{"anchor":"#fluentd-configuration","title":"Fluentd Configuration","content":"If your Fluentd servers are using TLS, you need to select Use TLS. If you are using a self-signed certificate, provide the CA Certificate PEM. You can copy and paste the certificate or upload it using the Read from a file button.\nNote: Fluentd does not support self-signed certificates when client authentication is enabled.\nYou can add multiple Fluentd Servers. If you want to add additional Fluentd servers, click Add Fluentd Server. For each Fluentd server, complete the configuration information:\nIn the Endpoint field, enter the address and port of your Fluentd instance, e.g. http://Fluentd-server:24224.\n\nEnter the Shared Key if your Fluentd Server is using a shared key for authentication.\n\nEnter the Username and Password if your Fluentd Server is using username and password for authentication.\n\nOptional: Enter the Hostname of the Fluentd server.\n\nEnter the load balancing Weight of the Fluentd server. If the weight of one server is 20 and the other server is 30, events will be sent in a 2:3 ratio. If you do not enter a weight, the default weight is 60.\n\nIf this server is a standby server, check Use as Standby Only. Standby servers are used when all other servers are not available.\nAfter adding all the Fluentd servers, you have the option to select Enable Gzip Compression. By default, this is enabled because the transferred payload size will be reduced.","postref":"255e941097968c90d9b11aa8550caeb2","objectID":"33bf094ba14146477bd6907d18b70275","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/tools/logging/fluentd/"},{"anchor":"#in-this-document","title":"In This Document","content":"Cattle provided feature-rich load balancer support that is well documented. Some of these features do not have equivalents in Rancher v2.x. This is the list of such features:\nNo support for SNI in current NGINX Ingress Controller.\nTCP load balancing requires a load balancer appliance enabled by cloud provider within the cluster. There is no Ingress support for TCP on Kubernetes.\nOnly ports 80 and 443 can be configured for HTTP/HTTPS routing via Ingress. Also Ingress Controller is deployed globally as a DaemonSet and not launched as a scalable service. Also, users cannot assign random external ports to be used for balancing. Therefore, users need to ensure that they configure unique hostname/path combinations to avoid routing conflicts using the same two ports.\nThere is no way to specify port rule priority and ordering.\nRancher v1.6 added support for draining backend connections and specifying a drain timeout. This is not supported in Rancher v2.x.\nThere is no support for specifying a custom stickiness policy and a custom load balancer config to be appended to the default config as of now in Rancher v2.x. There is some support, however, available in native Kubernetes for customizing the NGINX configuration as noted in the NGINX Ingress Controller Custom Configuration Documentation.\nFinished!You can launch a new load balancer to replace your load balancer from v1.6. Using the Rancher v2.x UI, browse to the applicable project and choose Resources > Workloads > Load Balancing. (In versions prior to v2.3.0, click Workloads > Load Balancing.) Then click Deploy. During deployment, you can choose a target project or namespace.\nPrerequisite: Before deploying Ingress, you must have a workload deployed that’s running a scale of two or more pods.\nFor balancing between these two pods, you must create a Kubernetes Ingress rule. To create this rule, navigate to your cluster and project, and click Resources > Workloads > Load Balancing. (In versions prior to v2.3.0, click Workloads > Load Balancing.) Then click Add Ingress. This GIF below depicts how to add Ingress to one of your projects.Browsing to Load Balancer Tab and Adding IngressSimilar to a service/port rules in Rancher v1.6, here you can specify rules targeting your workload’s container port. The sections below demonstrate how to create Ingress rules.Configuring Host- and Path-Based RoutingUsing Rancher v2.x, you can add Ingress rules that are based on host names or a URL path. Based on the rules you create, your NGINX Ingress Controller routes traffic to multiple target workloads or Kubernetes services.For example, let’s say you have multiple workloads deployed to a single namespace. You can add an Ingress to route traffic to these two workloads using the same hostname but different paths, as depicted in the image below. URL requests to foo.com/name.html will direct users to the web workload, and URL requests to foo.com/login will direct users to the chat workload.Ingress: Path-Based Routing ConfigurationRancher v2.x also places a convenient link to the workloads on the Ingress record. If you configure an external DNS to program the DNS records, this hostname can be mapped to the Kubernetes Ingress address.Workload LinksThe Ingress address is the IP address in your cluster that the Ingress Controller allocates for your workload. You can reach your workload by browsing to this IP address. Use kubectl command below to see the Ingress address assigned by the controller:kubectl get ingress\nHTTPS/Certificates OptionRancher v2.x Ingress functionality supports the HTTPS protocol, but if you want to use it, you need to use a valid SSL/TLS certificate. While configuring Ingress rules, use the SSL/TLS Certificates section to configure a certificate.\nWe recommend uploading a certificate from a known certificate authority (you’ll have to do this before configuring Ingress). Then, while configuring your load balancer, use the Choose a certificate option and select the uploaded certificate that you want to use.\nIf you have configured NGINX default certificate, you can select Use default ingress controller certificate.\nLoad Balancer Configuration: SSL/TLS Certificate SectionTCP Load Balancing OptionsLayer-4 Load BalancerFor the TCP protocol, Rancher v2.x supports configuring a Layer 4 load balancer using the cloud provider in which your Kubernetes cluster is deployed. Once this load balancer appliance is configured for your cluster, when you choose the option of a Layer-4 Load Balancer for port-mapping during workload deployment, Rancher automatically creates a corresponding load balancer service. This service will call the corresponding cloud provider and configure the load balancer appliance to route requests to the appropriate pods. See Cloud Providers for information on how to configure LoadBalancer services for your cloud provider.For example, if we create a deployment named myapp and specify a Layer 4 load balancer in the Port Mapping section, Rancher will automatically add an entry to the Load Balancer tab named myapp-loadbalancer.Workload Deployment: Layer 4 Load Balancer CreationOnce configuration of the load balancer succeeds, the Rancher UI provides a link to your workload’s public endpoint.NGINX Ingress Controller TCP Support by ConfigMapsAlthough NGINX supports TCP, Kubernetes Ingress itself does not support the TCP protocol. Therefore, out-of-the-box configuration of NGINX Ingress Controller for TCP balancing isn’t possible.However, there is a workaround to use NGINX’s TCP balancing by creating a Kubernetes ConfigMap, as described in the Ingress GitHub readme. You can create a ConfigMap object that stores pod configuration parameters as key-value pairs, separate from the pod image, as described in the Kubernetes documentation.To configure NGINX to expose your services via TCP, you can add the ConfigMap tcp-services that should exist in the ingress-nginx namespace. This namespace also contains the NGINX Ingress Controller pods.The key in the ConfigMap entry should be the TCP port that you want to expose for public access: <namespace/service name>:<service port>. As shown above, two workloads are listed in the Default namespace. For example, the first entry in the ConfigMap above instructs NGINX to expose the myapp workload (the one in the default namespace that’s listening on private port 80) over external port 6790. Adding these entries to the ConfigMap automatically updates the NGINX pods to configure these workloads for TCP balancing. The workloads exposed should be available at <NodeIP>:<TCP Port>. If they are not accessible, you might have to expose the TCP port explicitly using a NodePort service.Although Rancher v2.x supports HTTP and HTTPS hostname and path-based load balancing, you must use unique host names and paths when configuring your workloads. This limitation derives from:\nIngress confinement to ports 80 and 443 (i.e, the ports HTTP[S] uses for routing).\nThe load balancer and the Ingress Controller is launched globally for the cluster as a DaemonSet.\n\nTCP Required? Rancher v2.x still supports TCP. See TCP Load Balancing Options for workarounds.\nDeployment of Ingress Controller in v2.x as a DaemonSet brings some architectural changes that v1.6 users should know about.In Rancher v1.6 you could deploy a scalable load balancer service within your stack. If you had four hosts in your Cattle environment, you could deploy one load balancer service with a scale of two and point to your application by appending port 80 to your two host IP Addresses. You could also launch another load balancer on the remaining two hosts to balance a different service again using port 80 because your load balancer is using different host IP Addresses).Rancher v1.6 Load Balancing ArchitectureThe Rancher v2.x Ingress Controller is a DaemonSet, it is globally deployed on all schedulable nodes to serve your entire Kubernetes Cluster. Therefore, when you program the Ingress rules, you must use a unique hostname and path to point to your workloads, as the load balancer node IP addresses and ports 80 and 443 are common access points for all workloads.Rancher v2.x Load Balancing ArchitectureIn Rancher v1.6, you could add port/service rules for configuring your HAProxy to load balance for target services. You could also configure the hostname/path-based routing rules.Rancher v2.x offers similar functionality, but load balancing is instead handled by Ingress. An Ingress is a specification of rules that a controller component applies to your load balancer. The actual load balancer can run outside of your cluster or within it.By default, Rancher v2.x deploys NGINX Ingress Controller on clusters provisioned using RKE (Rancher’s own Kubernetes installer) to process the Kubernetes Ingress rules. The NGINX Ingress Controller is installed by default only in clusters provisioned by RKE. Clusters provisioned by cloud providers like GKE have their own Ingress Controllers that configure the load balancer. For this docu","postref":"e4264cec5feef8d807f82ddb6691a6cf","objectID":"2736f9da387dc412e0101fba6f1306cb","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/v1.6-migration/load-balancing/"},{"anchor":"#","title":"Backups and Disaster Recovery","content":"This section is devoted to protecting your data in a disaster scenario.\n\nTo protect yourself from a disaster scenario, you should create backups on a regular basis.\n\n\nRancher Server Backups\nBacking up Rancher Launched Kubernetes Clusters\n\n\nIn a disaster scenario, you can restore your etcd database by restoring a backup.\n\n\nRancher Server Restorations\nRestoring Rancher Launched Kubernetes Clusters\n\n","postref":"5f4d4640868252e2bba272f1c3bd93c0","objectID":"e9c8bb59eb25292969f93b6bea7a6c07","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/backups/"},{"anchor":"#","title":"Best Practices Guide","content":"The purpose of this section is to consolidate best practices for Rancher implementations. This also includes recommendations for related technologies, such as Kubernetes, Docker, containers, and more. The objective is to improve the outcome of a Rancher implementation using the operational experience of Rancher and its customers.\n\nIf you have any questions about how these might apply to your use case, please contact your Customer Success Manager or Support.\n\nUse the navigation bar on the left to find the current best practices for managing and deploying the Rancher Server.\n\nFor more guidance on best practices, you can consult these resources:\n\n\nRancher Docs\n\n\nMonitoring\nBackups and Disaster Recovery\nSecurity\n\nRancher Blog\n\n\nArticles about best practices on the Rancher blog\n101 More Security Best Practices for Kubernetes\n\nRancher Forum\nRancher Users Slack\nRancher Labs YouTube Channel - Online Meetups, Demos, Training, and Webinars\n\n","postref":"8049ecd06408c66f06905e6c48f9687d","objectID":"361982b679795c6cbf9e0e6ba684b182","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/best-practices/"},{"anchor":"#","title":"Networking Requirements for Host Gateway (L2bridge)","content":"This section describes how to configure custom Windows clusters that are using Host Gateway (L2bridge) mode.\n\nDisabling Private IP Address Checks\n\nIf you are using Host Gateway (L2bridge) mode and hosting your nodes on any of the cloud services listed below, you must disable the private IP address checks for both your Linux or Windows hosts on startup. To disable this check for each node, follow the directions provided by each service below.\n\n\n\n\nService\nDirections to disable private IP address checks\n\n\n\n\n\nAmazon EC2\nDisabling Source/Destination Checks\n\n\n\nGoogle GCE\nEnabling IP Forwarding for Instances (By default, a VM cannot forward a packet originated by another VM)\n\n\n\nAzure VM\nEnable or Disable IP Forwarding\n\n\n\n\nCloud-hosted VM Routes Configuration\n\nIf you are using the Host Gateway (L2bridge) backend of Flannel, all containers on the same node belong to a private subnet, and traffic routes from a subnet on one node to a subnet on another node through the host network.\n\n\nWhen worker nodes are provisioned on AWS, virtualization clusters, or bare metal servers, make sure they belong to the same layer 2 subnet. If the nodes don’t belong to the same layer 2 subnet, host-gw networking will not work.\n\nWhen worker nodes are provisioned on GCE or Azure, they are not on the same layer 2 subnet. Nodes on GCE and Azure belong to a routable layer 3 network. Follow the instructions below to configure GCE and Azure so that the cloud network knows how to route the host subnets on each node.\n\n\nTo configure host subnet routing on GCE or Azure, first run the following command to find out the host subnets on each worker node:\nkubectl get nodes -o custom-columns=nodeName:.metadata.name,nodeIP:status.addresses[0].address,routeDestination:.spec.podCIDR\nThen follow the instructions for each cloud provider to configure routing rules for each node:\n\n\n\n\nService\nInstructions\n\n\n\n\n\nGoogle GCE\nFor GCE, add a static route for each node: Adding a Static Route.\n\n\n\nAzure VM\nFor Azure, create a routing table: Custom Routes: User-defined.\n\n\n\n","postref":"196b8e0a909e9cfac3e52c2587c479d7","objectID":"40fffc43735a1a685cc3cd1d65cad431","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/windows-clusters/host-gateway-requirements/"},{"anchor":"#","title":"Upgrades","content":"This section contains information about how to upgrade your Rancher server to a newer version. Regardless if you installed in an air gap environment or not, the upgrade steps mainly depend on whether you have a single node or high-availability installation of Rancher. Select from the following options:\n\n\nUpgrading Rancher installed with Docker\nUpgrading Rancher installed on a Kubernetes cluster\n\n\nKnown Upgrade Issues\n\nThe following table lists some of the most noteworthy issues to be considered when upgrading Rancher. A more complete list of known issues for each Rancher version can be found in the release notes on GitHub and on the Rancher forums.\n\n\n\n\nUpgrade Scenario\nIssue\n\n\n\n\n\nUpgrading to v2.3.0+\nAny user provisioned cluster will be automatically updated upon any edit as tolerations were added to the images used for Kubernetes provisioning.\n\n\n\nUpgrading to v2.2.0-v2.2.x\nRancher introduced the system charts repository which contains all the catalog items required for features such as monitoring, logging, alerting and global DNS. To be able to use these features in an air gap install, you will need to mirror the system-charts repository locally and configure Rancher to use that repository. Please follow the instructions to configure Rancher system charts.\n\n\n\nUpgrading from v2.0.13 or earlier\nIf your cluster’s certificates have expired, you will need to perform additional steps to rotate the certificates.\n\n\n\nUpgrading from v2.0.7 or earlier\nRancher introduced the system project, which is a project that’s automatically created to store important namespaces that Kubernetes needs to operate. During upgrade to v2.0.7+, Rancher expects these namespaces to be unassigned from all projects. Before beginning upgrade, check your system namespaces to make sure that they’re unassigned to prevent cluster networking issues.\n\n\n\n\nCaveats\n\nUpgrades to or from any chart in the rancher-alpha repository aren’t supported.\n\nRKE Add-on Installs\n\nImportant: RKE add-on install is only supported up to Rancher v2.0.8\n\nPlease use the Rancher helm chart to install Rancher on a Kubernetes cluster. For details, see the Kubernetes Install - Installation Outline.\n\nIf you are currently using the RKE add-on install method, see Migrating from a RKE add-on install for details on how to move to using the helm chart.\n","postref":"9d31801c2be2ef4687fa46600bb56496","objectID":"f92bd17f4e6f49a3404e91c6ec656fe7","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/upgrades/upgrades/"},{"anchor":"#","title":"Restorations","content":"If you lose the data on your Rancher Server, you can restore it if you have backups stored in a safe location.\n\n\nRestoring Backups—Docker Installs\nRestoring Backups—Kubernetes installs\n\n\nIf you are looking to restore your Rancher launched Kubernetes cluster, please refer here.\n","postref":"b0e1149866492404f66ef2849f67caf1","objectID":"c61eb534ef196f2d02b6c4b3a973755f","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/backups/restorations/"},{"anchor":"#","title":"Rollbacks","content":"This section contains information about how to rollback your Rancher server to a previous version.\n\n\nRolling back Rancher installed with Docker\nRolling back Rancher installed on a Kubernetes cluster\n\n\nSpecial Scenarios regarding Rollbacks\n\nIf you are rolling back to versions in either of these scenarios, you must follow some extra instructions in order to get your clusters working.\n\n\nRolling back from v2.1.6+ to any version between v2.1.0 - v2.1.5 or v2.0.0 - v2.0.10.\nRolling back from v2.0.11+ to any version between v2.0.0 - v2.0.10.\n\n\nBecause of the changes necessary to address CVE-2018-20321, special steps are necessary if the user wants to roll back to a previous version of Rancher where this vulnerability exists. The steps are as follows:\n\n\nRecord the serviceAccountToken for each cluster.  To do this, save the following script on a machine with kubectl access to the Rancher management plane and execute it.  You will need to run these commands on the machine where the rancher container is running. Ensure JQ is installed before running the command. The commands will vary depending on how you installed Rancher.\n\nRancher Installed with Docker\n\ndocker exec <NAME OF RANCHER CONTAINER> kubectl get clusters -o json | jq '[.items[] | select(any(.status.conditions[]; .type == \"ServiceAccountMigrated\")) | {name: .metadata.name, token: .status.serviceAccountToken}]' > tokens.json\n\n\nRancher Installed on a Kubernetes Cluster\n\nkubectl get clusters -o json | jq '[.items[] | select(any(.status.conditions[]; .type == \"ServiceAccountMigrated\")) | {name: .metadata.name, token: .status.serviceAccountToken}]' > tokens.json\n\n\nAfter executing the command a tokens.json file will be created.  Important! Back up this file in a safe place.** You will need it to restore functionality to your clusters after rolling back Rancher.  If you lose this file, you may lose access to your clusters.\n\nRollback Rancher following the normal instructions.\n\nOnce Rancher comes back up, every cluster managed by Rancher (except for Imported clusters) will be in an Unavailable state.\n\nApply the backed up tokens based on how you installed Rancher.\n\nRancher Installed with Docker\n\nSave the following script as apply_tokens.sh to the machine where the Rancher docker container is running. Also copy the tokens.json file created previously to the same directory as the script.\n\nset -e\n\ntokens=$(jq .[] -c tokens.json)\nfor token in $tokens; do\n    name=$(echo $token | jq -r .name)\n    value=$(echo $token | jq -r .token)\n\n    docker exec $1 kubectl patch --type=merge clusters $name -p \"{\\\"status\\\": {\\\"serviceAccountToken\\\": \\\"$value\\\"}}\"\ndone\n\n\nthe script to allow execution (chmod +x apply_tokens.sh) and execute the script as follows:\n\n./apply_tokens.sh <DOCKER CONTAINER NAME>\n\n\nAfter a few moments the clusters will go from Unavailable back to Available.\n\nRancher Installed on a Kubernetes Cluster\n\nSave the following script as apply_tokens.sh to a machine with kubectl access to the Rancher management plane. Also copy the tokens.json file created previously to the same directory as the script.\n\nset -e\n\ntokens=$(jq .[] -c tokens.json)\nfor token in $tokens; do\n    name=$(echo $token | jq -r .name)\n    value=$(echo $token | jq -r .token)\n\n   kubectl patch --type=merge clusters $name -p \"{\\\"status\\\": {\\\"serviceAccountToken\\\": \\\"$value\\\"}}\"\ndone\n\n\nSet the script to allow execution (chmod +x apply_tokens.sh) and execute the script as follows:\n\n./apply_tokens.sh\n\n\nAfter a few moments the clusters will go from Unavailable back to Available.\n\nContinue using Rancher as normal.\n\n","postref":"8dfe83379f313fdd258f038448a0c777","objectID":"15b2dcb38125c388853abb88f754de5d","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/upgrades/rollbacks/"},{"anchor":"#rolling-back","title":"Rolling Back","content":"If your upgrade does not complete successfully, you can roll back Rancher server and its data back to its last healthy state. For more information, see Docker Rollback.","postref":"68f02be98e0bb3a552577f1f3e2b35e7","objectID":"8c428631523a5725ebfc229c40914fa3","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/upgrades/upgrades/single-node/"},{"anchor":"#before-you-start","title":"Before You Start","content":"If you have issues upgrading Rancher, roll it back to its latest known healthy state by pulling the last version you used and then restoring the backup you made before upgrade.\nWarning! Rolling back to a previous version of Rancher destroys any changes made to Rancher following the upgrade. Unrecoverable data loss may occur.\n\nUsing a remote Terminal connection, log into the node running your Rancher Server.\n\nPull the version of Rancher that you were running prior to upgrade. Replace the <PRIOR_RANCHER_VERSION> with that version.\n\nFor example, if you were running Rancher v2.0.5 before upgrade, pull v2.0.5.\n\ndocker pull rancher/rancher:<PRIOR_RANCHER_VERSION>\n\n\nStop the container currently running Rancher Server. Replace <RANCHER_CONTAINER_NAME> with the name of your Rancher container.\n\ndocker stop <RANCHER_CONTAINER_NAME>\n\n\nYou can obtain the name for your Rancher container by entering docker ps.\n\nMove the backup tarball that you created during completion of Docker Upgrade onto your Rancher Server. Change to the directory that you moved it to. Enter dir to confirm that it’s there.\n\nIf you followed the naming convention we suggested in Docker Upgrade, it will have a name similar to  (rancher-data-backup-<RANCHER_VERSION>-<DATE>.tar.gz).\n\nRun the following command to replace the data in the rancher-data container with the data in the backup tarball, replacing the placeholder. Don’t forget to close the quotes.\n\ndocker run  --volumes-from rancher-data \\\n-v $PWD:/backup busybox sh -c \"rm /var/lib/rancher/* -rf \\\n&& tar zxvf /backup/rancher-data-backup-<RANCHER_VERSION>-<DATE>.tar.gz\"\n\n\nStart a new Rancher Server container with the <PRIOR_RANCHER_VERSION> tag placeholder pointing to the data container.\n\ndocker run -d --volumes-from rancher-data \\\n--restart=unless-stopped -p 80:80 -p 443:443 rancher/rancher:<PRIOR_RANCHER_VERSION>\n\n\n\nNote: Do not stop the rollback after initiating it, even if the rollback process seems longer than expected. Stopping the rollback may result in database issues during future upgrades.\n\n\nWait a few moments and then open Rancher in a web browser. Confirm that the rollback succeeded and that your data is restored.\nResult: Rancher is rolled back to its version and data state prior to upgrade.During rollback to a prior version of Rancher, you’ll enter a series of commands, filling placeholders with data from your environment. These placeholders are denoted with angled brackets and all capital letters (<EXAMPLE>). Here’s an example of a command with a placeholder:docker pull rancher/rancher:<PRIOR_RANCHER_VERSION>\nIn this command, <PRIOR_RANCHER_VERSION> is the version of Rancher you were running before your unsuccessful upgrade. v2.0.5 for example.Cross reference the image and reference table below to learn how to obtain this placeholder data. Write down or copy this information before starting the procedure below.Terminal docker ps Command, Displaying Where to Find <PRIOR_RANCHER_VERSION> and <RANCHER_CONTAINER_NAME>\n\n\n\nPlaceholder\nExample\nDescription\n\n\n\n\n\n<PRIOR_RANCHER_VERSION>\nv2.0.5\nThe rancher/rancher image you used before upgrade.\n\n\n\n<RANCHER_CONTAINER_NAME>\nfestive_mestorf\nThe name of your Rancher container.\n\n\n\n<RANCHER_VERSION>\nv2.0.5\nThe version of Rancher that the backup is for.\n\n\n\n<DATE>\n9-27-18\nThe date that the data container or backup was created.\n\n\nYou can obtain <PRIOR_RANCHER_VERSION> and <RANCHER_CONTAINER_NAME> by logging into your Rancher Server by remote connection and entering the command to view the containers that are running: docker ps. You can also view containers that are stopped using a different command: docker ps -a. Use these commands for help anytime during while creating backups.","postref":"f6e581543324a640d67a3679576b65b6","objectID":"e14ce69d09abcbf9061c4deb0ef00f88","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/upgrades/rollbacks/single-node-rollbacks/"},{"anchor":"#rolling-back","title":"Rolling Back","content":"Should something go wrong, follow the roll back instructions to restore the snapshot you took before you preformed the upgrade.","postref":"f995ea6a52e74be6300dbf4f655f7bb7","objectID":"4370625da3f0acf05a5f902cd86ac37a","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/upgrades/upgrades/ha/"},{"anchor":"#","title":"Kubernetes Rollback","content":"If you upgrade Rancher and the upgrade does not complete successfully, you may need to rollback your Rancher Server to its last healthy state.\n\nTo restore Rancher follow the procedure detailed here: Restoring Backups — Kubernetes installs\n\nRestoring a snapshot of the Rancher Server cluster will revert Rancher to the version and state at the time of the snapshot.\n\n\nNote: Managed cluster are authoritative for their state. This means restoring the rancher server will not revert workload deployments or changes made on managed clusters after the snapshot was taken.\n\n","postref":"e4e0952338761340ba4b912148fbadb0","objectID":"f590a53935482881472d763d4cda7e61","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/upgrades/rollbacks/ha-server-rollbacks/"},{"anchor":"#","title":"Migrating from a Kubernetes Install with an RKE Add-on","content":"\nImportant: RKE add-on install is only supported up to Rancher v2.0.8\n\nIf you are currently using the RKE add-on install method, please follow these directions to migrate to the Helm install.\n\n\nThe following instructions will help guide you through migrating from the RKE Add-on install to managing Rancher with the Helm package manager.\n\nYou will need the to have kubectl installed and the kubeconfig YAML file (kube_config_rancher-cluster.yml) generated by RKE.\n\n\nNote: This guide assumes a standard Rancher install. If you have modified any of the object names or namespaces, please adjust accordingly.\n\nNote: If you are upgrading from from Rancher v2.0.13 or earlier, or v2.1.8 or earlier, and your cluster’s certificates have expired, you will need to perform additional steps to rotate the certificates.\n\n\nPoint kubectl at your Rancher Cluster\n\nMake sure kubectl is using the correct kubeconfig YAML file. Set the KUBECONFIG environmental variable to point to kube_config_rancher-cluster.yml:\n\nexport KUBECONFIG=$(pwd)/kube_config_rancher-cluster.yml\n\n\nAfter setting the KUBECONFIG environment variable, verify that it contains the correct server parameter. It should point directly to one of your cluster nodes on port 6443.\n\nkubectl config view -o=jsonpath='{.clusters[*].cluster.server}'\nhttps://NODE:6443\n\n\nIf the output from the command shows your Rancher hostname with the suffix /k8s/clusters, the wrong kubeconfig YAML file is configured. It should be the file that was created when you used RKE to create the cluster to run Rancher.\n\nSave your certificates\n\nIf you have terminated ssl on the Rancher cluster ingress, recover your certificate and key for use in the Helm install.\n\nUse kubectl to get the secret, decode the value and direct the output to a file.\n\nkubectl -n cattle-system get secret cattle-keys-ingress -o jsonpath --template='{ .data.tls\\.crt }' | base64 -d > tls.crt\nkubectl -n cattle-system get secret cattle-keys-ingress -o jsonpath --template='{ .data.tls\\.key }' | base64 -d > tls.key\n\n\nIf you specified a private CA root cert\n\nkubectl -n cattle-system get secret cattle-keys-server -o jsonpath --template='{ .data.cacerts\\.pem }' | base64 -d > cacerts.pem\n\n\nRemove previous Kubernetes objects\n\nRemove the Kubernetes objects created by the RKE install.\n\n\nNote: Removing these Kubernetes components will not affect the Rancher configuration or database, but with any maintenance it is a good idea to create a backup of the data before hand. See Creating Backups-Kubernetes Install for details.\n\n\nkubectl -n cattle-system delete ingress cattle-ingress-http\nkubectl -n cattle-system delete service cattle-service\nkubectl -n cattle-system delete deployment cattle\nkubectl -n cattle-system delete clusterrolebinding cattle-crb\nkubectl -n cattle-system delete serviceaccount cattle-admin\n\n\nRemove addons section from rancher-cluster.yml\n\nThe addons section from rancher-cluster.yml contains all the resources needed to deploy Rancher using RKE. By switching to Helm, this part of the cluster configuration file is no longer needed. Open rancher-cluster.yml in your favorite text editor and remove the addons section:\n\n\nImportant: Make sure you only remove the addons section from the cluster configuration file.\n\n\nnodes:\n  - address: <IP> # hostname or IP to access nodes\n    user: <USER> # root user (usually 'root')\n    role: [controlplane,etcd,worker] # K8s roles for node\n    ssh_key_path: <PEM_FILE> # path to PEM file\n  - address: <IP>\n    user: <USER>\n    role: [controlplane,etcd,worker]\n    ssh_key_path: <PEM_FILE>\n  - address: <IP>\n    user: <USER>\n    role: [controlplane,etcd,worker]\n    ssh_key_path: <PEM_FILE>\n\nservices:\n  etcd:\n    snapshot: true\n    creation: 6h\n    retention: 24h\n\n# Remove addons section from here til end of file\naddons: |-\n  ---\n  ...\n# End of file\n\n\nFollow Helm and Rancher install steps\n\nFrom here follow the standard install steps.\n\n\n3 - Initialize Helm\n4 - Install Rancher\n\n","postref":"336afb6aab3ebe4749390ebe8a53b98e","objectID":"ce0f6664b58f1d61d8487f7a03b614fd","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/upgrades/upgrades/migrating-from-rke-add-on/"},{"anchor":"#preventing-cluster-networking-issues","title":"Preventing Cluster Networking Issues","content":"Reset the cluster nodes’ network policies to restore connectivity.\nPrerequisites:\n\nDownload and setup kubectl.\n\n  \n  \n  \nFrom Terminal, change directories to your kubectl file that’s generated during Rancher install, kube_config_rancher-cluster.yml. This file is usually in the directory where you ran RKE during Rancher installation.\n\nBefore repairing networking, run the following two commands to make sure that your nodes have a status of Ready and that your cluster components are Healthy.\n\nkubectl --kubeconfig kube_config_rancher-cluster.yml get nodes\n\nNAME                          STATUS    ROLES                      AGE       VERSION\n165.227.114.63                Ready     controlplane,etcd,worker   11m       v1.10.1\n165.227.116.167               Ready     controlplane,etcd,worker   11m       v1.10.1\n165.227.127.226               Ready     controlplane,etcd,worker   11m       v1.10.1\n\nkubectl --kubeconfig kube_config_rancher-cluster.yml get cs\n\nNAME                 STATUS    MESSAGE              ERROR\nscheduler            Healthy   ok\ncontroller-manager   Healthy   ok\netcd-0               Healthy   {\"health\": \"true\"}\netcd-2               Healthy   {\"health\": \"true\"}\netcd-1               Healthy   {\"health\": \"true\"}\n\n\nCheck the networkPolicy for all clusters by running the following command.\n\nkubectl --kubeconfig kube_config_rancher-cluster.yml get cluster -o=custom-columns=ID:.metadata.name,NAME:.spec.displayName,NETWORKPOLICY:.spec.enableNetworkPolicy,APPLIEDNP:.status.appliedSpec.enableNetworkPolicy,ANNOTATION:.metadata.annotations.\"networking\\.management\\.cattle\\.io/enable-network-policy\"\n\nID      NAME    NETWORKPOLICY   APPLIEDNP   ANNOTATION\nc-59ptz custom  <nil>           <nil>       <none>\nlocal   local   <nil>           <nil>       <none>\n\n\nDisable the networkPolicy for all clusters, still pointing toward your kube_config_rancher-cluster.yml.\n\nkubectl --kubeconfig kube_config_rancher-cluster.yml get cluster -o jsonpath='{range .items[*]}{@.metadata.name}{\"\\n\"}{end}' | xargs -I {} kubectl --kubeconfig kube_config_rancher-cluster.yml patch cluster {} --type merge -p '{\"spec\": {\"enableNetworkPolicy\": false},\"status\": {\"appliedSpec\": {\"enableNetworkPolicy\": false }}}'\n\n\n\nTip: If you want to keep networkPolicy enabled for all created clusters, you can run the following command to disable networkPolicy for local cluster (i.e., your Rancher Server nodes):\n\n kubectl --kubeconfig kube_config_rancher-cluster.yml patch cluster local --type merge -p '{\"spec\": {\"enableNetworkPolicy\": false},\"status\": {\"appliedSpec\": {\"enableNetworkPolicy\": false }}}'\n\n\n\nRemove annotations for network policy for all clusters\n\nkubectl --kubeconfig kube_config_rancher-cluster.yml get cluster -o jsonpath='{range .items[*]}{@.metadata.name}{\"\\n\"}{end}' | xargs -I {} kubectl --kubeconfig kube_config_rancher-cluster.yml annotate cluster {} \"networking.management.cattle.io/enable-network-policy\"=\"false\" --overwrite\n\n\n\nTip: If you want to keep networkPolicy enabled for all created clusters, you can run the following command to disable networkPolicy for local cluster (i.e., your Rancher Server nodes):\n\n kubectl --kubeconfig kube_config_rancher-cluster.yml annotate cluster local \"networking.management.cattle.io/enable-network-policy\"=\"false\" --overwrite\n\n\n\nCheck the networkPolicy for all clusters again to make sure the policies have a status of false.\n\nkubectl --kubeconfig kube_config_rancher-cluster.yml get cluster -o=custom-columns=ID:.metadata.name,NAME:.spec.displayName,NETWORKPOLICY:.spec.enableNetworkPolicy,APPLIEDNP:.status.appliedSpec.enableNetworkPolicy,ANNOTATION:.metadata.annotations.\"networking\\.management\\.cattle\\.io/enable-network-policy\"\n\nID      NAME    NETWORKPOLICY   APPLIEDNP   ANNOTATION\nc-59ptz custom  false           false       false\nlocal   local   false           false       false\n\n\nRemove all network policies from all namespaces.  Run this command for each cluster, using the kubeconfig generated by RKE.\n\nfor namespace in $(kubectl --kubeconfig kube_config_rancher-cluster.yml get ns -o custom-columns=NAME:.metadata.name --no-headers); do\n    kubectl --kubeconfig kube_config_rancher-cluster.yml -n $namespace delete networkpolicy --all;\ndone\n\n\nRemove all the projectnetworkpolicies created for the clusters, to make sure networkpolicies are not recreated.\n\nfor cluster in $(kubectl --kubeconfig kube_config_rancher-cluster.yml get clusters -o custom-columns=NAME:.metadata.name --no-headers); do\n    for project in $(kubectl --kubeconfig kube_config_rancher-cluster.yml get project -n $cluster -o custom-columns=NAME:.metadata.name --no-headers); do\n        kubectl --kubeconfig kube_config_rancher-cluster.yml delete projectnetworkpolicy -n $project --all\n    done\ndone\n\n\n\nTip: If you want to keep networkPolicy enabled for all created clusters, you can run the following command to disable networkPolicy for local cluster (i.e., your Rancher Server nodes):\n\n for project in $(kubectl --kubeconfig kube_config_rancher-cluster.yml get project -n local -o custom-columns=NAME:.metadata.name --no-headers); do\n     kubectl --kubeconfig kube_config_rancher-cluster.yml -n $project delete projectnetworkpolicy --all;\n done\n\n\n\nWait a few minutes and then log into the Rancher UI.\n\n\nIf you can access Rancher, you’re done, so you can skip the rest of the steps.\nIf you still can’t access Rancher, complete the steps below.\n\n\nForce your pods to recreate themselves by entering the following command.\n\nkubectl --kubeconfig kube_config_rancher-cluster.yml delete pods -n cattle-system --all\n\n\nLog into the Rancher UI and view your clusters. Created clusters will show errors from attempting to contact Rancher while it was unavailable. However, these errors should resolve automatically.\n\n\n\n\n\n  \nIf you can access Rancher, but one or more of the clusters that you launched using Rancher has no networking, you can repair them by moving the\n\n\nFrom the cluster’s embedded kubectl shell.\n\nBy downloading the cluster kubeconfig file and running it from your workstation.\n\nfor namespace in $(kubectl --kubeconfig kube_config_rancher-cluster.yml get ns -o custom-columns=NAME:.metadata.name --no-headers); do\n  kubectl --kubeconfig kube_config_rancher-cluster.yml -n $namespace delete networkpolicy --all;\ndone\n\n\n\n\n\nYou can prevent cluster networking issues from occurring during your upgrade to v2.0.7+ by unassigning system namespaces from all of your Rancher projects. Complete this task if you’ve assigned any of a cluster’s system namespaces into a Rancher project.\nLog into the Rancher UI prior to upgrade.\n\nFrom the context menu, open the local cluster (or any of your other clusters).\n\nFrom the main menu, select Project/Namespaces.\n\nFind and select the following namespaces. Click Move and then choose None to move them out of your projects. Click Move again.\n\n\nNote: Some or all of these namespaces may already be unassigned from all projects.\n\n\n\nkube-system\nkube-public\ncattle-system\ncattle-alerting1\ncattle-logging1\ncattle-pipeline1\ningress-nginx\n\n\n\n1 Only displays if this feature is enabled for the cluster.\n\n\nMoving namespaces out of projects\n\n\nRepeat these steps for each cluster where you’ve assigned system namespaces to projects.\nResult: All system namespaces are moved out of Rancher projects. You can now safely begin the upgrade.","postref":"a7f94487424da2841a4ccc9f2beb2d78","objectID":"725e3ded1d3d36dd1a006debb112b68e","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/upgrades/upgrades/namespace-migration/"},{"anchor":"#rolling-back","title":"Rolling Back","content":"Should something go wrong, follow the roll back instructions to restore the snapshot you took before you preformed the upgrade.","postref":"5c8efa6725c5486d35cbe4e4776fa8ad","objectID":"38d3fe1a8f03d44b2403b3a2d8c792d9","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/upgrades/upgrades/ha/helm2/"},{"anchor":"#first-log-in","title":"First Log In","content":"Available as of v2.3.0Rancher includes some features that are experimental and disabled by default. Feature flags were introduced to allow you to try these features. For more information, refer to the section about feature flags.Available as of v2.3.0With this feature, you can upgrade to the latest version of Kubernetes as soon as it is released, without upgrading Rancher. This feature allows you to easily upgrade Kubernetes patch versions (i.e. v1.15.X), but not intended to upgrade Kubernetes minor versions (i.e. v1.X.0) as Kubernetes tends to deprecate or add APIs between minor versions.The information that Rancher uses to provision RKE clusters is now located in the Rancher Kubernetes Metadata. For details on metadata configuration and how to change the Kubernetes version used for provisioning RKE clusters, see Rancher Kubernetes Metadata.Rancher Kubernetes Metadata contains Kubernetes version information which Rancher uses to provision RKE clusters.For more information on how metadata works and how to configure metadata config, see Rancher Kubernetes Metadata.Drivers in Rancher allow you to manage which providers can be used to provision hosted Kubernetes clusters or nodes in an infrastructure provider to allow Rancher to deploy and manage Kubernetes.For more information, see Provisioning Drivers.Pod Security Policies (or PSPs) are objects that control security-sensitive aspects of pod specification, e.g. root privileges. If a pod does not meet the conditions specified in the PSP, Kubernetes will not allow it to start, and Rancher will display an error message.For more information how to create and use PSPs, see Pod Security Policies.Within Rancher, each person authenticates as a user, which is a login that grants you access to Rancher. Once the user logs in to Rancher, their authorization, or their access rights within the system, is determined by the user’s role. Rancher provides built-in roles to allow you to easily configure a user’s permissions to resources, but Rancher also provides the ability to customize the roles for each Kubernetes resource.For more information how authorization works and how to customize roles, see Roles Based Access Control (RBAC).One of the key features that Rancher adds to Kubernetes is centralized user authentication. This feature allows to set up local users and/or connect to an external authentication provider. By connecting to an external authentication provider, you can leverage that provider’s user and groups.For more information how authentication works and how to configure each provider, see Authentication.After you log into Rancher for the first time, Rancher will prompt you for a Rancher Server URL.You should set the URL to the main entry point to the Rancher Server. When a load balancer sits in front a Rancher Server cluster, the URL should resolve to the load balancer. The system will automatically try to infer the Rancher Server URL from the IP address or host name of the host running the Rancher Server. This is only correct if you are running a single node Rancher Server installation. In most cases, therefore, you need to set the Rancher Server URL to the correct value yourself.\nImportant! After you set the Rancher Server URL, we do not support updating it. Set the URL with extreme care.\n","postref":"04321c954193562fa57f2f36bdadebf4","objectID":"a8e2d0f5dce2732e07690bf2bba03479","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/"},{"anchor":"#installing-with-the-custom-ca-certificate","title":"Installing with the custom CA Certificate","content":"For details on starting a Rancher container with your private CA certificates mounted, refer to the installation docs:\nDocker Install\n\nKubernetes Install\n","postref":"8c5fcabef34b6608c72d1f5d78a5bccf","objectID":"b76c75e2e4b516a27aa7350b643b5df4","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/custom-ca-root-certificate/"},{"anchor":"#adding-local-users","title":"Adding Local Users","content":"Regardless of whether you use external authentication, you should create a few local authentication users so that you can continue using Rancher if your external authentication service encounters issues.\nFrom the Global view, select Users from the navigation bar.\n\nClick Add User. Then complete the Add User form. Click Create when you’re done.\n","postref":"a138bb05697b48edac94b2a37cf4113b","objectID":"191ac92be6e682cf5abe9ac2384fe136","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/authentication/local/"},{"anchor":"#prerequisites","title":"Prerequisites","content":"If you are experiencing issues while testing the connection to the Active Directory server, first double-check the credentials entered for the service account as well as the search base configuration. You may also inspect the Rancher logs to help pinpointing the problem cause. Debug logs may contain more detailed information about the error. Please refer to How can I enable debug logging in this documentation.In order to successfully configure AD authentication it is crucial that you provide the correct configuration pertaining to the hierarchy and schema of your AD server.The ldapsearch tool allows you to query your AD server to learn about the schema used for user and group objects.For the purpose of the example commands provided below we will assume:\nThe Active Directory server has a hostname of ad.acme.com\nThe server is listening for unencrypted connections on port 389\nThe Active Directory domain is acme\nYou have a valid AD account with the username jdoe and password secret\nIdentify Search BaseFirst we will use ldapsearch to identify the Distinguished Name (DN) of the parent node(s) for users and groups:$ ldapsearch -x -D \"acme\\jdoe\" -w \"secret\" -p 389 \\\n-h ad.acme.com -b \"dc=acme,dc=com\" -s sub \"sAMAccountName=jdoe\"\nThis command performs an LDAP search with the search base set to the domain root (-b \"dc=acme,dc=com\") and a filter targeting the user account (sAMAccountNam=jdoe), returning the attributes for said user:Since in this case the user’s DN is CN=John Doe,CN=Users,DC=acme,DC=com [5], we should configure the User Search Base with the parent node DN CN=Users,DC=acme,DC=com.Similarly, based on the DN of the group referenced in the memberOf attribute [4], the correct value for the Group Search Base would be the parent node of that value, ie. OU=Groups,DC=acme,DC=com.Identify User SchemaThe output of the above ldapsearch query also allows to determine the correct values to use in the user schema configuration:\nObject Class: person [1]\nUsername Attribute: name [2]\nLogin Attribute: sAMAccountName [3]\nUser Member Attribute: memberOf [4]\n\nNote:\n\nIf the AD users in our organisation were to authenticate with their UPN (e.g. jdoe@acme.com) instead of the short logon name, then we would have to set the Login Attribute to userPrincipalName instead.\nWe’ll also set the Search Attribute parameter to sAMAccountName|name. That way users can be added to clusters/projects in the Rancher UI either by entering their username or full name.Identify Group SchemaNext, we’ll query one of the groups associated with this user, in this case CN=examplegroup,OU=Groups,DC=acme,DC=com:$ ldapsearch -x -D \"acme\\jdoe\" -w \"secret\" -p 389 \\\n-h ad.acme.com -b \"ou=groups,dc=acme,dc=com\" \\\n-s sub \"CN=examplegroup\"\nThis command will inform us on the attributes used for group objects:Again, this allows us to determine the correct values to enter in the group schema configuration:\nObject Class: group [1]\nName Attribute: name [2]\nGroup Member Mapping Attribute: member [3]\nSearch Attribute: sAMAccountName [4]\nLooking  at the value of the  member attribute, we can see that it contains the DN of the referenced user. This  corresponds to the distinguishedName attribute in our user object. Accordingly will have to set the value of the Group Member User Attribute parameter to this attribute.In the same way, we can observe that the value in the memberOf attribute in the user object corresponds to the distinguishedName [5] of the group. We therefore need to set the value for the Group DN Attribute parameter to this attribute.Open Active Directory Configuration\nLog into the Rancher UI using the initial local admin account.\nFrom the Global view, navigate to Security > Authentication\nSelect Active Directory. The Configure an AD server form will be displayed.\nConfigure Active Directory Server SettingsIn the section titled 1. Configure an Active Directory server,   complete the fields with the information specific to your Active Directory server. Please refer to the following table for detailed information on the required values for each parameter.\nNote:\n\nIf you are unsure about the correct values to enter in the  user/group Search Base field, please refer to Identify Search Base and Schema using ldapsearch.\nTable 1: AD Server parameters\n\n\nParameter\nDescription\n\n\n\n\n\nHostname\nSpecify the hostname or IP address of the AD server\n\n\n\nPort\nSpecify the port at which the Active Directory server is listening for connections. Unencrypted LDAP normally uses the standard port of 389, while LDAPS uses port 636.\n\n\n\nTLS\nCheck this box to enable LDAP over SSL/TLS (commonly known as LDAPS).\n\n\n\nServer Connection Timeout\nThe duration in number of seconds that Rancher waits before considering the AD server unreachable.\n\n\n\nService Account Username\nEnter the username of an AD account with read-only access to your domain partition (see Prerequisites). The username can be entered in NetBIOS format (e.g. “DOMAIN\\serviceaccount”) or UPN format (e.g. “serviceaccount@domain.com”).\n\n\n\nService Account Password\nThe password for the service account.\n\n\n\nDefault Login Domain\nWhen you configure this field with the NetBIOS name of your AD domain, usernames entered without a domain (e.g. “jdoe”) will automatically be converted to a slashed,  NetBIOS logon (e.g. “LOGIN_DOMAIN\\jdoe”) when binding to the AD server. If your users authenticate with the UPN (e.g. “jdoe@acme.com”) as username then this field must be left empty.\n\n\n\nUser Search Base\nThe Distinguished Name of the node in your directory tree from which to start searching for user objects. All users must be descendents of this base DN. For example: “ou=people,dc=acme,dc=com”.\n\n\n\nGroup Search Base\nIf your groups live under a different node than the one configured under User Search Base you will need to provide the Distinguished Name here. Otherwise leave it empty. For example: “ou=groups,dc=acme,dc=com”.\n\n\nConfigure User/Group SchemaIn the section titled 2. Customize Schema you must provide Rancher with a correct mapping of user and group attributes corresponding to the schema used in your directory.Rancher uses LDAP queries to search for and retrieve information about users and groups within the Active Directory. The attribute mappings configured in this section are used to construct search filters and resolve group membership. It is therefore paramount that the provided settings reflect the reality of your AD domain.\nNote:\n\nIf you are unfamiliar with the schema used in your Active Directory domain, please refer to Identify Search Base and Schema using ldapsearch to determine the correct configuration values.\nUser SchemaThe table below details the parameters for the user schema section configuration.Table 2: User schema configuration parameters\n\n\nParameter\nDescription\n\n\n\n\n\nObject Class\nThe name of the object class used for user objects in your domain. If defined, only specify the name of the object class - don’t include it in an LDAP wrapper such as &(objectClass=xxxx)\n\n\n\nUsername Attribute\nThe user attribute whose value is suitable as a display name.\n\n\n\nLogin Attribute\nThe attribute whose value matches the username part of credentials entered by your users when logging in to Rancher. If your users authenticate with their UPN (e.g. “jdoe@acme.com”) as username then this field must normally be set to userPrincipalName. Otherwise for the old, NetBIOS-style logon names (e.g. “jdoe”) it’s usually sAMAccountName.\n\n\n\nUser Member Attribute\nThe attribute containing the groups that a user is a member of.\n\n\n\nSearch Attribute\nWhen a user enters text to add users or groups in the UI, Rancher queries the AD server and attempts to match users by the attributes provided in this setting. Multiple attributes can be specified by separating them with the pipe (”|”) symbol. To match UPN usernames (e.g. jdoe@acme.com) you should usually set the value of this field to userPrincipalName.\n\n\n\nSearch Filter\nThis filter gets applied to the list of users that is searched when Rancher attempts to add users to a site access list or tries to add members to clusters or projects. For example, a user search filter could be (|(memberOf=CN=group1,CN=Users,DC=testad,DC=rancher,DC=io)(memberOf=CN=group2,CN=Users,DC=testad,DC=rancher,DC=io)). Note: If the search filter does not use valid AD search syntax, the list of users will be empty.\n\n\n\nUser Enabled Attribute\nThe attribute containing an integer value representing a bitwise enumeration of user account flags. Rancher uses this to determine if a user account is disabled. You should normally leave this set to the AD standard userAccountControl.\n\n\n\nDisabled Status Bitmask\nThis is the value of the User Enabled Attribute designating a disabled user account. You should normally leave this set to the default value of “2” as specified in the Microsoft Active Directory schema (see here).\n\n\nGroup SchemaThe table below details the parameters for the group schema configuration.Table 3: Group schema configuration parameters\n\n\nParameter\nDescription\n\n\n\n\n\nOb","postref":"69e6e26d042c2b353d976f05180cf832","objectID":"e3392cd4589f4822f06efa8b30f0b3d1","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/authentication/ad/"},{"anchor":"#openldap-authentication-flow","title":"OpenLDAP Authentication Flow","content":"If you are experiencing issues while testing the connection to the OpenLDAP server, first double-check the credentials entered for the service account as well as the search base configuration. You may also inspect the Rancher logs to help pinpointing the problem cause. Debug logs may contain more detailed information about the error. Please refer to How can I enable debug logging in this documentation.Open OpenLDAP Configuration\nLog into the Rancher UI using the initial local admin account.\nFrom the Global view, navigate to Security > Authentication\nSelect OpenLDAP. The Configure an OpenLDAP server form will be displayed.\nConfigure OpenLDAP Server SettingsIn the section titled 1. Configure an OpenLDAP server,   complete the fields with the information specific to your server. Please refer to the following table for detailed information on the required values for each parameter.\nNote:\n\nIf you are in doubt about the correct values to enter in the user/group Search Base configuration fields, consult your LDAP administrator or refer to the section Identify Search Base and Schema using ldapsearch in the Active Directory authentication documentation.\nTable 1: OpenLDAP server parameters\n\n\nParameter\nDescription\n\n\n\n\n\nHostname\nSpecify the hostname or IP address of the OpenLDAP server\n\n\n\nPort\nSpecify the port at which the OpenLDAP server is listening for connections. Unencrypted LDAP normally uses the standard port of 389, while LDAPS uses port 636.\n\n\n\nTLS\nCheck this box to enable LDAP over SSL/TLS (commonly known as LDAPS). You will also need to paste in the CA certificate if the server uses a self-signed/enterprise-signed certificate.\n\n\n\nServer Connection Timeout\nThe duration in number of seconds that Rancher waits before considering the server unreachable.\n\n\n\nService Account Distinguished Name\nEnter the Distinguished Name (DN) of the user that should be used to bind, search and retrieve LDAP entries. (see Prerequisites).\n\n\n\nService Account Password\nThe password for the service account.\n\n\n\nUser Search Base\nEnter the Distinguished Name of the node in your directory tree from which to start searching for user objects. All users must be descendents of this base DN. For example: “ou=people,dc=acme,dc=com”.\n\n\n\nGroup Search Base\nIf your groups live under a different node than the one configured under User Search Base you will need to provide the Distinguished Name here. Otherwise leave this field empty. For example: “ou=groups,dc=acme,dc=com”.\n\n\nConfigure User/Group SchemaIf your OpenLDAP directory deviates from the standard OpenLDAP schema, you must complete the Customize Schema section to match it.\nNote that the attribute mappings configured in this section are used by Rancher to construct search filters and resolve group membership. It is therefore always recommended to verify that the configuration here matches the schema used in your OpenLDAP.\nNote:\n\nIf you are unfamiliar with the user/group schema used in the OpenLDAP server, consult your LDAP administrator or refer to the section Identify Search Base and Schema using ldapsearch in the Active Directory authentication documentation.\nUser SchemaThe table below details the parameters for the user schema configuration.Table 2: User schema configuration parameters\n\n\nParameter\nDescription\n\n\n\n\n\nObject Class\nThe name of the object class used for user objects in your domain. If defined, only specify the name of the object class - don’t include it in an LDAP wrapper such as &(objectClass=xxxx)\n\n\n\nUsername Attribute\nThe user attribute whose value is suitable as a display name.\n\n\n\nLogin Attribute\nThe attribute whose value matches the username part of credentials entered by your users when logging in to Rancher. This is typically uid.\n\n\n\nUser Member Attribute\nThe user attribute containing the Distinguished Name of groups a user is member of. Usually this is one of memberOf or isMemberOf.\n\n\n\nSearch Attribute\nWhen a user enters text to add users or groups in the UI, Rancher queries the LDAP server and attempts to match users by the attributes provided in this setting. Multiple attributes can be specified by separating them with the pipe (”|”) symbol.\n\n\n\nUser Enabled Attribute\nIf the schema of your OpenLDAP server supports a user attribute whose value can be evaluated to determine if the account is disabled or locked, enter the name of that attribute. The default OpenLDAP schema does not support this and the field should usually be left empty.\n\n\n\nDisabled Status Bitmask\nThis is the value for a disabled/locked user account. The parameter is ignored if User Enabled Attribute is empty.\n\n\nGroup SchemaThe table below details the parameters for the group schema configuration.Table 3: Group schema configuration parameters\n\n\nParameter\nDescription\n\n\n\n\n\nObject Class\nThe name of the object class used for group entries in your domain. If defined, only specify the name of the object class - don’t include it in an LDAP wrapper such as &(objectClass=xxxx)\n\n\n\nName Attribute\nThe group attribute whose value is suitable for a display name.\n\n\n\nGroup Member User Attribute\nThe name of the user attribute whose format matches the group members in the Group Member Mapping Attribute.\n\n\n\nGroup Member Mapping Attribute\nThe name of the group attribute containing the members of a group.\n\n\n\nSearch Attribute\nAttribute used to construct search filters when adding groups to clusters or projects in the UI. See description of user schema Search Attribute.\n\n\n\nGroup DN Attribute\nThe name of the group attribute whose format matches the values in the user’s group membership attribute. See  User Member Attribute.\n\n\n\nNested Group Membership\nThis settings defines whether Rancher should resolve nested group memberships. Use only if your organisation makes use of these nested memberships (ie. you have groups that contain other groups as members).\n\n\nTest AuthenticationOnce you have completed the configuration, proceed by testing  the connection to the OpenLDAP server. Authentication with OpenLDAP will be enabled implicitly if the test is successful.\nNote:\n\nThe OpenLDAP user pertaining to the credentials entered in this step will be mapped to the local principal account and assigned administrator privileges in Rancher. You should therefore make a conscious decision on which LDAP account you use to perform this step.\n\nEnter the username and password for the OpenLDAP account that should be mapped to the local principal account.\nClick Authenticate With OpenLDAP to test the OpenLDAP connection and finalise the setup.\nResult:\nOpenLDAP authentication is configured.\nThe LDAP user pertaining to the entered credentials is mapped to the local principal (administrative) account.\n\nNote:\n\nYou will still be able to login using the locally configured admin account and password in case of a disruption of LDAP services.\nRancher must be configured with a LDAP bind account (aka service account) to search and retrieve LDAP entries pertaining to users and groups that should have access. It is recommended to not use an administrator account or personal account for this purpose and instead create a dedicated account in OpenLDAP with read-only access to users and groups under the configured search base (see below).\nUsing TLS?\n\nIf the certificate used by the OpenLDAP server is self-signed or not from a recognised certificate authority, make sure have at hand the CA certificate (concatenated with any intermediate certificates) in PEM format. You will have to paste in this certificate during the configuration so that Rancher is able to validate the certificate chain.\n\nWhen a user attempts to login with his LDAP credentials, Rancher creates an initial bind to the LDAP server using a service account with permissions to search the directory and read user/group attributes.\nRancher then searches the directory for the user by using a search filter based on the provided username and configured attribute mappings.\nOnce the user has been found, he is authenticated with another LDAP bind request using the user’s DN and provided password.\nOnce authentication succeeded, Rancher then resolves the group memberships both from the membership attribute in the user’s object and by performing a group search based on the configured user mapping attribute.\n\nNote:\n\nBefore you proceed with the configuration, please familiarise yourself with the concepts of External Authentication Configuration and Principal Users.\n","postref":"c4c87c575af3750bd0ec1b47d7bb8a8f","objectID":"5c85c0d8377af7b4993e98ac54eb4510","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/authentication/openldap/"},{"anchor":"#","title":"Configuring FreeIPA","content":"Available as of v2.0.5\n\nIf your organization uses FreeIPA for user authentication, you can configure Rancher to allow your users to login using their FreeIPA credentials.\n\n\nPrerequisites:\n\n\nYou must have a FreeIPA Server configured.\nCreate a service account in FreeIPA with read-only access. Rancher uses this account to verify group membership when a user makes a request using an API key.\nRead External Authentication Configuration and Principal Users.\n\n\n\n\nSign into Rancher using a local user assigned the administrator role (i.e., the local principal).\n\nFrom the Global view, select Security > Authentication from the main menu.\n\nSelect FreeIPA.\n\nComplete the Configure an FreeIPA server form.\n\nYou may need to log in to your domain controller to find the information requested in the form.\n\n\nUsing TLS?\nIf the certificate is self-signed or not from a recognized certificate authority, make sure you provide the complete chain. That chain is needed to verify the server’s certificate.\n\n\nUser Search Base vs. Group Search Base\n\nSearch base allows Rancher to search for users and groups that are in your FreeIPA.  These fields are only for search bases and not for search filters.\n\n\nIf your users and groups are in the same search base, complete only the User Search Base.\nIf your groups are in a different search base, you can optionally complete the Group Search Base. This field is dedicated to searching groups, but is not required.\n\n\n\nIf your FreeIPA deviates from the standard AD schema, complete the Customize Schema form to match it. Otherwise, skip this step.\n\n\nSearch Attribute The Search Attribute field defaults with three specific values: uid|sn|givenName. After FreeIPA is configured, when a user enters text to add users or groups, Rancher automatically queries the FreeIPA server and attempts to match fields by user id, last name, or first name. Rancher specifically searches for users/groups that begin with the text entered in the search field.\n\nThe default field value uid|sn|givenName, but you can configure this field to a subset of these fields. The pipe (|) between the fields separates these fields.\n\n\nuid: User ID\nsn: Last Name\ngivenName: First Name\n\n\nWith this search attribute, Rancher creates search filters for users and groups, but you cannot add your own search filters in this field.\n\n\nEnter your FreeIPA username and password in Authenticate with FreeIPA to confirm that Rancher is configured to use FreeIPA authentication.\n\n\nResult:\n\n\nFreeIPA authentication is configured.\nYou are signed into Rancher with your FreeIPA account (i.e., the external principal).\n\n","postref":"cde8b77eece27ba2388142ed94c9fcf0","objectID":"0219293661ee6c6fb024c5f865a0f3ff","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/authentication/freeipa/"},{"anchor":"#external-vs-local-authentication","title":"External vs. Local Authentication","content":"Configuration of external authentication requires:\nA local user assigned the administrator role, called hereafter the local principal.\nAn external user that can authenticate with your external authentication service, called hereafter the external principal.\nConfiguration of external authentication affects how principal users are managed within Rancher. Follow the list below to better understand these effects.\nSign into Rancher as the local principal and complete configuration of external authentication.\n\n\n\nRancher associates the external principal with the local principal. These two users share the local principal’s user ID.\n\n\n\nAfter you complete configuration, Rancher automatically signs out the local principal.\n\n\n\nThen, Rancher automatically signs you back in as the external principal.\n\n\n\nBecause the external principal and the local principal share an ID, no unique object for the external principal displays on the Users page.\n\n\n\nThe external principal and the local principal share the same access rights.\nAfter you configure Rancher to allow sign on using an external authentication service, you should configure who should be allowed to log in and use Rancher. The following options are available:\n\n\nAccess Level\nDescription\n\n\n\n\n\nAllow any valid Users\nAny user in the authorization service can access Rancher. We generally discourage use of this setting!\n\n\n\nAllow members of Clusters, Projects, plus Authorized Users and Organizations\nAny user in the authorization service and any group added as a Cluster Member or Project Member can log in to Rancher. Additionally, any user in the authentication service or group you add to the Authorized Users and Organizations list may log in to Rancher.\n\n\n\nRestrict access to only Authorized Users and Organizations\nOnly users in the authentication service or groups added to the Authorized Users and Organizations can log in to Rancher.\n\n\nTo set the Rancher access level for users in the authorization service, follow these steps:\nFrom the Global view, click Security > Authentication.\n\nUse the Site Access options to configure the scope of user authorization. The table above explains the access level for each option.\n\nOptional: If you choose an option other than Allow any valid Users, you can add users to the list of authorized users and organizations by searching for them in the text field that appears.\n\nClick Save.\nResult: The Rancher access configuration settings are applied.\nSAML Provider Caveats:\n\n\nSAML Protocol does not support search or lookup for users or groups. Therefore, there is no validation on users or groups when adding them to Rancher.\nWhen adding users, the exact user IDs (i.e. UID Field) must be entered correctly. As you type the user ID, there will be no search for other  user IDs that may match.\n\nWhen adding groups, you must select the group from the drop-down that is next to the text box. Rancher assumes that any input from the text box is a user.\n\n\nThe group drop-down shows only the groups that you are a member of. You will not be able to add groups that you are not a member of.\n\n\nRancher relies on users and groups to determine who is allowed to log in to Rancher and which resources they can access. When authenticating with an external provider, groups are provided from the external provider based on the user. These users and groups are given specific roles to resources like clusters, projects, multi-cluster apps, and global DNS providers and entries. When you give access to a group, all users who are a member of that group in the authentication provider will be able to access the resource with the permissions that you’ve specified. For more information on roles and permissions, see Role Based Access Control.\nNote: Local authentication does not support creating or managing groups.\nFor more information, see Users and GroupsThe Rancher authentication proxy integrates with the following external authentication services. The following table lists the first version of Rancher each service debuted.\n\n\nAuth Service\nAvailable as of\n\n\n\n\n\nMicrosoft Active Directory\nv2.0.0\n\n\n\nGitHub\nv2.0.0\n\n\n\nMicrosoft Azure AD\nv2.0.3\n\n\n\nFreeIPA\nv2.0.5\n\n\n\nOpenLDAP\nv2.0.5\n\n\n\nMicrosoft AD FS\nv2.0.7\n\n\n\nPingIdentity\nv2.0.7\n\n\n\nKeycloak\nv2.1.0\n\n\n\nOkta\nv2.2.0\n\n\n\nGoogle OAuth\nv2.3.0\n\n\n\nHowever, Rancher also provides local authentication.In most cases, you should use an external authentication service over local authentication, as external authentication allows user management from a central location. However, you may want a few local authentication users for managing Rancher under rare circumstances, such as if your external authentication provider is unavailable or undergoing maintenance.","postref":"d323f945970e5776cce8b41b5bea4a89","objectID":"50535ba2f0912b68f554cb1275a7d20e","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/authentication/"},{"anchor":"#azure-active-directory-configuration-outline","title":"Azure Active Directory Configuration Outline","content":"Configuring Rancher to allow your users to authenticate with their Azure AD accounts involves multiple procedures. Review the outline below before getting started.\nTip: Before you start, we recommend creating an empty text file. You can use this file to copy values from Azure that you’ll paste into Rancher later.\n\n1. Register Rancher with Azure\n2. Create an Azure API Key\n3. Set Required Permissions for Rancher\n4. Copy Azure Application Data\n5. Configure Azure AD in Rancher\n1. Register Rancher with AzureBefore enabling Azure AD within Rancher, you must register Rancher with Azure.\nLog in to Microsoft Azure as an administrative user. Configuration in future steps requires administrative access rights.\n\nUse search to open the App registrations service.\n\n\n\nClick New registrations and complete the Create form.\n\n\n\n\n\n\nEnter a Name (something like Rancher).\n\nFrom Supported account types, select “Accounts in this organizational directory only (AzureADTest only - Single tenant)” This corresponds to the legacy app registration options.\n\nIn the Redirect URI section, make sure Web is selected from the dropdown and enter the URL of your Rancher Server in the text box next to the dropdown. This Rancher server URL should be appended with the verification path: <MY_RANCHER_URL>/verify-auth-azure.\n\n\n\nTip: You can find your personalized Azure reply URL in Rancher on the Azure AD Authentication page (Global View > Security Authentication > Azure AD).\n\n\n\nClick Register.\n\n\nNote: It can take up to five minutes for this change to take affect, so don’t be alarmed if you can’t authenticate immediately after Azure AD configuration.\n2. Create a new client secretFrom the Azure portal, create a client secret. Rancher will use this key to authenticate with Azure AD.\nUse search to open App registrations services. Then open the entry for Rancher that you created in the last procedure.\n\n\n\nFrom the navigation pane on left, click Certificates and Secrets.\n\nClick New client secret.\n\n\n\n\nEnter a Description (something like Rancher).\n\nSelect duration for the key from the options under Expires. This drop-down sets the expiration date for the key. Shorter durations are more secure, but require you to create a new key after expiration.\n\nClick Add (you don’t need to enter a value—it will automatically populate after you save).\n\n\n\nCopy the key value and save it to an empty text file.\n\nYou’ll enter this key into the Rancher UI later as your Application Secret.\n\nYou won’t be able to access the key value again within the Azure UI.\n3. Set Required Permissions for RancherNext, set API permissions for Rancher within Azure.\nFrom the navigation pane on left, select API permissions.\n\n\n\nClick Add a permission.\n\nFrom the Azure Active Directory Graph, select the following Delegated Permissions:\n\n\n\n\n\n\n\nAccess the directory as the signed-in user\nRead directory data\nRead all groups\nRead all users’ full profiles\nRead all users’ basic profiles\nSign in and read user profile\n\n\nClick Add permissions.\n\nFrom API permissions, click Grant admin consent. Then click Yes.\n\n\nNote: You must be signed in as an Azure administrator to successfully save your permission settings.\n\n4. Copy Azure Application DataAs your final step in Azure, copy the data that you’ll use to configure Rancher for Azure AD authentication and paste it into an empty text file.\nObtain your Rancher Tenant ID.\n\n\nFrom App registrations select the app configured above.\n\nFrom the left navigation pane, open Overview.\n\nCopy the Directory ID and paste it into your text file.\n\nYou’ll paste this value into Rancher as your Tenant ID.\n\n\nObtain your Rancher Application ID.\n\n\nUse search to open App registrations.\n\n\n\nFind the entry you created for Rancher.\n\nCopy the Application ID and paste it to your text file.\n\n\nObtain your Rancher Graph Endpoint, Token Endpoint, and Auth Endpoint.\n\n\nFrom App registrations, click Endpoints.\n\n\n\nCopy the following endpoints to your clipboard and paste them into your text file (these values will be your Rancher endpoint values).\n\n\nMicrosoft Graph API endpoint (Graph Endpoint)\nOAuth 2.0 token endpoint (v1) (Token Endpoint)\nOAuth 2.0 authorization endpoint (v1) (Auth Endpoint)\n\n\n\n\nNote: Copy the v1 version of the endpoints\n5. Configure Azure AD in RancherFrom the Rancher UI, enter information about your AD instance hosted in Azure to complete configuration.Enter the values that you copied to your text file.\nLog into Rancher. From the Global view, select Security > Authentication.\n\nSelect Azure AD.\n\nComplete the Configure Azure AD Account form using the information you copied while completing Copy Azure Application Data.\n\n\nImportant: When entering your Graph Endpoint, remove the tenant ID from the URL, like below.\n\nhttps://graph.windows.net/abb5adde-bee8-4821-8b03-e63efdc7701c\n\n\nThe following table maps the values you copied in the Azure portal to the fields in Rancher.\n\n\n\n\nRancher Field\nAzure Value\n\n\n\n\n\nTenant ID\nDirectory ID\n\n\n\nApplication ID\nApplication ID\n\n\n\nApplication Secret\nKey Value\n\n\n\nEndpoint\nhttps://login.microsoftonline.com/\n\n\n\nGraph Endpoint\nMicrosoft Azure AD Graph API Endpoint\n\n\n\nToken Endpoint\nOAuth 2.0 Token Endpoint\n\n\n\nAuth Endpoint\nOAuth 2.0 Authorization Endpoint\n\n\n\n\nClick Authenticate with Azure.\nResult: Azure Active Directory authentication is configured.","postref":"80b65438a32bcb52ba4c32d1ba717886","objectID":"13016a762d7e5ca01c0ea3b6a8e3b538","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/authentication/azure-ad/"},{"anchor":"#","title":"Configuring GitHub","content":"In environments using GitHub, you can configure Rancher to allow sign on using GitHub credentials.\n\n\nPrerequisites: Read External Authentication Configuration and Principal Users.\n\n\n\nSign into Rancher using a local user assigned the administrator role (i.e., the local principal).\n\nFrom the Global view, select Security > Authentication from the main menu.\n\nSelect GitHub.\n\nFollow the directions displayed to Setup a GitHub Application. Rancher redirects you to GitHub to complete registration.\n\n\nWhat’s an Authorization Callback URL?\n\nThe Authorization Callback URL is the URL where users go to begin using your application (i.e. the splash screen).\n\nWhen you use external authentication, authentication does not actually take place in your application. Instead, authentication takes place externally (in this case, GitHub). After this external authentication completes successfully, the Authorization Callback URL is the location where the user re-enters your application.\n\n\nFrom GitHub, copy the Client ID and Client Secret. Paste them into Rancher.\n\n\nWhere do I find the Client ID and Client Secret?\n\nFrom GitHub, select Settings > Developer Settings > OAuth Apps. The Client ID and Client Secret are displayed prominently.\n\n\nClick Authenticate with GitHub.\n\nUse the Site Access options to configure the scope of user authorization.\n\n\nAllow any valid Users\n\nAny GitHub user can access Rancher. We generally discourage use of this setting!\n\nAllow members of Clusters, Projects, plus Authorized Users and Organizations\n\nAny GitHub user or group added as a Cluster Member or Project Member can log in to Rancher. Additionally, any GitHub user or group you add to the Authorized Users and Organizations list may log in to Rancher.\n\nRestrict access to only Authorized Users and Organizations\n\nOnly GitHub users or groups added to the Authorized Users and Organizations can log in to Rancher.\n\n\n\nClick Save.\n\n\nResult:\n\n\nGitHub authentication is configured.\nYou are signed into Rancher with your GitHub account (i.e., the external principal).\n\n","postref":"0209233339c809238af1cbf8b5e1915b","objectID":"09b6170b76ec58fd25b0bc836151902f","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/authentication/github/"},{"anchor":"#users-and-roles","title":"Users and Roles","content":"Once the user logs in to Rancher, their authorization, or their access rights within the system, is determined by global permissions, and cluster and project roles.\nGlobal Permissions:\n\nDefine user authorization outside the scope of any particular cluster.\n\nCluster and Project Roles:\n\nDefine user authorization inside the specific cluster or project where they are assigned the role.\nBoth global permissions and cluster and project roles are implemented on top of Kubernetes RBAC. Therefore, enforcement of permissions and roles is performed by Kubernetes.","postref":"dd60b3b30d25fc8835021eb79d4fd7ed","objectID":"808ceddd33b30b6250d054444436691c","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/rbac/"},{"anchor":"#","title":"Setting up Local System Charts for Air Gapped Installations","content":"The System Charts repository contains all the catalog items required for features such as monitoring, logging, alerting and global DNS.\n\nIn an air gapped installation of Rancher, you will need to configure Rancher to use a local copy of the system charts. This section describes how to use local system charts using a CLI flag in Rancher v2.3.0, and using a Git mirror for Rancher versions prior to v2.3.0.\n\nUsing Local System Charts in Rancher v2.3.0\n\nIn Rancher v2.3.0, a local copy of system-charts has been packaged into the rancher/rancher container. To be able to use these features in an air gap install, you will need to run the Rancher install command with an extra environment variable, CATTLE_SYSTEM_CATALOG=bundled, which tells Rancher to use the local copy of the charts instead of attempting to fetch them from GitHub.\n\nExample commands for a Rancher installation with a bundled system-charts are included in the air gap Docker installation instructions and the air gap Kubernetes installation instructions.\n\nSetting Up System Charts for Rancher Prior to v2.3.0\n\nA. Prepare System Charts\n\nThe System Charts repository contains all the catalog items required for features such as monitoring, logging, alerting and global DNS. To be able to use these features in an air gap install, you will need to mirror the system-charts repository to a location in your network that Rancher can reach and configure Rancher to use that repository.\n\nRefer to the release notes in the system-charts repository to see which branch corresponds to your version of Rancher.\n\nB. Configure System Charts\n\nRancher needs to be configured to use your Git mirror of the system-charts repository. You can configure the system charts repository either from the Rancher UI or from Rancher’s API view.\n\n\n  \n  \n  In the catalog management page in the Rancher UI, follow these steps:\n\n\nGo to the Global view.\n\nClick Tools > Catalogs.\n\nThe system chart is displayed under the name system-library. To edit the configuration of the system chart, click Ellipsis (…) > Edit.\n\nIn the Catalog URL field, enter the location of the Git mirror of the system-charts repository.\n\nClick Save.\n\n\nResult: Rancher is configured to download all the required catalog items from your system-charts repository.\n\n\n\n\n  \nLog into Rancher.\n\nOpen https://<your-rancher-server>/v3/catalogs/system-library in your browser.\n\n\n\nClick Edit on the upper right corner and update the value for url to the location of the Git mirror of the system-charts repository.\n\n\n\nClick Show Request\n\nClick Send Request\n\n\nResult: Rancher is configured to download all the required catalog items from your system-charts repository.\n\n\n\n\n\n","postref":"790cc957055f4a35ae059832577cb4a3","objectID":"9dc43c19864d1e3e18198ef5c277d948","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/local-system-charts/"},{"anchor":"#","title":"Upgrading Kubernetes without Upgrading Rancher","content":"Available as of v2.3.0\n\nThe RKE metadata feature allows you to provision clusters with new versions of Kubernetes as soon as they are released, without upgrading Rancher. This feature is useful for taking advantage of patch versions of Kubernetes, for example, if you want to upgrade to Kubernetes v1.14.7 when your Rancher server originally supported v1.14.6.\n\n\nNote: The Kubernetes API can change between minor versions. Therefore, we don’t support introducing minor Kubernetes versions, such as introducing v1.15 when Rancher currently supports v1.14. You would need to upgrade Rancher to add support for minor Kubernetes versions.\n\n\nRancher’s Kubernetes metadata contains information specific to the Kubernetes version that Rancher uses to provision RKE clusters. Rancher syncs the data periodically and creates custom resource definitions (CRDs) for system images, service options and addon templates. Consequently, when a new Kubernetes version is compatible with the Rancher server version, the Kubernetes metadata makes the new version available to Rancher for provisioning clusters. The metadata gives you an overview of the information that the Rancher Kubernetes Engine (RKE) uses for deploying various Kubernetes versions.\n\nThis table below describes the CRDs that are affected by the periodic data sync.\n\n\nNote: Only administrators can edit metadata CRDs. It is recommended not to update existing objects unless explicitly advised.\n\n\n\n\n\nResource\nDescription\nRancher API URL\n\n\n\n\n\nSystem Images\nList of system images used to deploy Kubernetes through RKE.\n<RANCHER_SERVER_URL>/v3/rkek8ssystemimages\n\n\n\nService Options\nDefault options passed to Kubernetes components like kube-api, scheduler, kubelet, kube-proxy, and kube-controller-manager\n<RANCHER_SERVER_URL>/v3/rkek8sserviceoptions\n\n\n\nAddon Templates\nYAML definitions used to deploy addon components like Canal, Calico, Flannel, Weave, Kube-dns, CoreDNS, metrics-server, nginx-ingress\n<RANCHER_SERVER_URL>/v3/rkeaddons\n\n\n\n\nAdministrators might configure the RKE metadata settings to do the following:\n\n\nRefresh the Kubernetes metadata, if a new patch version of Kubernetes comes out and they want Rancher to provision clusters with the latest version of Kubernetes without having to upgrade Rancher\nChange the metadata URL that Rancher uses to sync the metadata, which is useful for air gap setups if you need to sync Rancher locally instead of with GitHub\nPrevent Rancher from auto-syncing the metadata, which is one way to prevent new and unsupported Kubernetes versions from being available in Rancher\n\n\nRefresh Kubernetes Metadata\n\nThe option to refresh the Kubernetes metadata is available for administrators by default, or for any user who has the Manage Cluster Drivers global role.\n\nTo force Rancher to refresh the Kubernetes metadata, a manual refresh action is available under Tools > Drivers > Refresh Kubernetes Metadata on the right side corner.\n\nConfiguring the Metadata Synchronization\n\n\nOnly administrators can change these settings.\n\n\nThe RKE metadata config controls how often Rancher syncs metadata and where it downloads data from. You can configure the metadata from the settings in the Rancher UI, or through the Rancher API at the endpoint v3/settings/rke-metadata-config.\n\nTo edit the metadata config in Rancher,\n\n\nGo to the Global view and click the Settings tab.\nGo to the rke-metadata-config section. Click the Ellipsis (…) and click Edit.\n\nYou can optionally fill in the following parameters:\n\n\nrefresh-interval-minutes: This is the amount of time that Rancher waits to sync the metadata. To disable the periodic refresh, set refresh-interval-minutes to 0.\nurl: This is the HTTP path that Rancher fetches data from.\nbranch: This refers to the Git branch name if the URL is a Git URL.\n\n\n\nIf you don’t have an air gap setup, you don’t need to specify the URL or Git branch where Rancher gets the metadata, because the default setting is to pull from Rancher’s metadata Git repository.\n\nHowever, if you have an air gap setup, you will need to mirror the Kubernetes metadata repository in a location available to Rancher. Then you need to change the URL and Git branch in the rke-metadata-config settings to point to the new location of the repository.\n\nAir Gap Setups\n\nRancher relies on a periodic refresh of the rke-metadata-config to download new Kubernetes version metadata if it is supported with the current version of the Rancher server. For a table of compatible Kubernetes and Rancher versions, refer to the service terms section.\n\nIf you have an air gap setup, you might not be able to get the automatic periodic refresh of the Kubernetes metadata from Rancher’s Git repository. In that case, you should disable the periodic refresh to prevent your logs from showing errors. Optionally, you can configure your metadata settings so that Rancher can sync with a local copy of the RKE metadata.\n\nTo sync Rancher with a local mirror of the RKE metadata, an administrator would configure the rke-metadata-config settings by updating the url and branch to point to the mirror.\n\nAfter new Kubernetes versions are loaded into the Rancher setup, additional steps would be required in order to use them for launching clusters. Rancher needs access to updated system images. While the metadata settings can only be changed by administrators, any user can download the Rancher system images and prepare a private Docker registry for them.\n\n\nTo download the system images for the private registry, click the Rancher server version at the bottom left corner of the Rancher UI.\nDownload the OS specific image lists for Linux or Windows.\nDownload rancher-images.txt.\nPrepare the private registry using the same steps during the air gap install, but instead of using the rancher-images.txt from the releases page, use the one obtained from the previous steps.\n\n\nResult: The air gap installation of Rancher can now sync the Kubernetes metadata. If you update your private registry when new versions of Kubernetes are released, you can provision clusters with the new version without having to upgrade Rancher.\n","postref":"aba753b557bb8684bd93405ffec8c8dc","objectID":"3948728df800830df02a55cc43202a34","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/k8s-metadata/"},{"anchor":"#","title":"Global Permissions","content":"Permissions are individual access rights that you can assign when selecting a custom permission for a user.\n\nGlobal Permissions define user authorization outside the scope of any particular cluster. Out-of-the-box, there are two default global permissions: Administrator and Standard User.\n\n\nAdministrator: These users have full control over the entire Rancher system and all clusters within it.\n\nStandard User: These users can create new clusters and use them. Standard users can also assign other users permissions to their clusters.\n\n\nYou cannot update or delete the built-in Global Permissions.\n\nThis section covers the following topics:\n\n\nGlobal permission assignment\n\n\nGlobal permissions for new local users\nGlobal permissions for users with external authentication\n\nCustom global permissions\n\n\nCustom global permissions reference\nConfiguring default global permissions for new users\nConfiguring global permissions for existing individual users\nConfiguring global permissions for groups\nRefreshing group memberships\n\n\n\nGlobal Permission Assignment\n\nGlobal permissions for local users are assigned differently than users who log in to Rancher using external authentication.\n\nGlobal Permissions for New Local Users\n\nWhen you create a new local user, you assign them a global permission as you complete the Add User form.\n\nTo see the default permissions for new users, go to the Global view and click Security > Roles. On the Global tab, there is a column named New User Default. When adding a new local user, the user receives all  default global permissions that are marked as checked in this column. You can change the default global permissions to meet your needs.\n\nGlobal Permissions for Users with External Authentication\n\nWhen a user logs into Rancher using an external authentication provider for the first time, they are automatically assigned the  New User Default global permissions. By default, Rancher assigns the Standard User permission for new users.\n\nTo see the default permissions for new users, go to the Global view and click Security > Roles. On the Global tab, there is a column named New User Default. When adding a new local user, the user receives all default global permissions that are marked as checked in this column, and you can change them to meet your needs.\n\nPermissions can be assigned to an individual user with these steps.\n\nAs of Rancher v2.4.0-alpha1, you can assign a role to everyone in the group at the same time if the external authentication provider supports groups.\n\nCustom Global Permissions\n\nUsing custom permissions is convenient for providing users with narrow or specialized access to Rancher.\n\nWhen a user from an external authentication source signs into Rancher for the first time, they’re automatically assigned a set of global permissions (hereafter, permissions). By default, after a user logs in for the first time, they are created as a user and assigned the default user permission. The standard user permission allows users to login and create clusters.\n\nHowever, in some organizations, these permissions may extend too much access. Rather than assigning users the default global permissions of Administrator or Standard User, you can assign them a more restrictive set of custom global permissions.\n\nThe default roles, Administrator and Standard User, each come with multiple global permissions built into them. The Administrator role includes all global permissions, while the default user role includes three global permissions: Create Clusters, Use Catalog Templates, and User Base, which is equivalent to the minimum permission to log in to Rancher. In other words, the custom global permissions are modularized so that if you want to change the default user role permissions, you can choose which subset of global permissions are included in the new default user role.\n\nAdministrators can enforce custom global permissions in multiple ways:\n\n\nChanging the default permissions for new users\nEditing the permissions of an existing user\nAssigning a custom global permission to a group\n\n\nCustom Global Permissions Reference\n\nThe following table lists each custom global permission available and whether it is included in the default global permissions, Administrator and Standard User.\n\n\n\n\nCustom Global Permission\nAdministrator\nStandard User\n\n\n\n\n\nCreate Clusters\n✓\n✓\n\n\n\nCreate RKE Templates\n✓\n✓\n\n\n\nManage Authentication\n✓\n\n\n\n\nManage Catalogs\n✓\n\n\n\n\nManage Cluster Drivers\n✓\n\n\n\n\nManage Node Drivers\n✓\n\n\n\n\nManage PodSecurityPolicy Templates\n✓\n\n\n\n\nManage Roles\n✓\n\n\n\n\nManage Settings\n✓\n\n\n\n\nManage Users\n✓\n\n\n\n\nUse Catalog Templates\n✓\n✓\n\n\n\nUser Base* (Basic log-in access)\n✓\n✓\n\n\n\n\n\n*This role has two names:\n\n\nWhen you go to the Users tab and edit a user’s global role, this role is called Login Access in the custom global permissions list.\nWhen you go to the Security tab and edit the roles from the roles page, this role is called User Base.\n\n\n\nFor details on which Kubernetes resources correspond to each global permission, you can go to the Global view in the Rancher UI. Then click Security > Roles and go to the Global tab. If you click an individual role, you can refer to the Grant Resources table to see all of the operations and resources that are permitted by the role.\n\n\nNotes:\n\n\nEach permission listed above is comprised of multiple individual permissions not listed in the Rancher UI. For a full list of these permissions and the rules they are comprised of, access through the API at /v3/globalRoles.\nWhen viewing the resources associated with default roles created by Rancher, if there are multiple Kubernetes API resources on one line item, the resource will have (Custom) appended to it. These are not custom resources but just an indication that there are multiple Kubernetes API resources as one resource.\n\n\n\nConfiguring Default Global Permissions\n\nIf you want to restrict the default permissions for new users, you can remove the user permission as default role and then assign multiple individual permissions as default instead. Conversely, you can also add administrative permissions on top of a set of other standard permissions.\n\n\nNote: Default roles are only assigned to users added from an external authentication provider. For local users, you must explicitly assign global permissions when adding a user to Rancher. You can customize these global permissions when adding the user.\n\n\nTo change the default global permissions that are assigned to external users upon their first log in, follow these steps:\n\n\nFrom the Global view, select Security > Roles from the main menu. Make sure the Global tab is selected.\n\nFind the permissions set that you want to add or remove as a default. Then edit the permission by selecting Ellipsis > Edit.\n\nIf you want to add the permission as a default, Select Yes: Default role for new users and then click Save.\n\nIf you want to remove a default permission, edit the permission and select No from New User Default.\n\n\nResult: The default global permissions are configured based on your changes. Permissions assigned to new users display a check in the New User Default column.\n\nConfiguring Global Permissions for Existing Individual Users\n\nTo configure permission for a user,\n\n\nGo to the Users tab.\n\nOn this page, go to the user whose access level you want to change and click Ellipsis (…) > Edit.\n\nIn the Global Permissions section, click Custom.\n\nCheck the boxes for each subset of permissions you want the user to have access to.\n\nClick Save.\n\n\n\nResult: The user’s global permissions have been updated.\n\n\nConfiguring Global Permissions for Groups\n\nAvailable as of v2.4.0-alpha1\n\nIf you have a group of individuals that need the same level of access in Rancher, it can save time to assign permissions to the entire group at once, so that the users in the group have the appropriate level of access the first time they sign into Rancher.\n\nAfter you assign a custom global role to a group, the custom global role will be assigned to a user in the group when they log in to Rancher.\n\nFor existing users, the new permissions will take effect when the users log out of Rancher and back in again, or when an administrator refreshes the group memberships.\n\nFor new users, the new permissions take effect when the users log in to Rancher for the first time. New users from this group will receive the permissions from the custom global role in addition to the New User Default global permissions. By default, the New User Default permissions are equivalent to the Standard User global role, but the default permissions can be configured.\n\nIf a user is removed from the external authentication provider group, they would lose their permissions from the custom global role that was assigned to the group. They would continue to have any remaining roles that were assigned to them, which would typically include the roles marked as New User Default. Rancher will remove the permissions that are associated with the group when the user logs out, or when an administrator refreshes group me","postref":"8438d2e10528e421f75e037545c4bfb9","objectID":"e14df1af177b48572ef40231903c6ed3","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/rbac/global-permissions/"},{"anchor":"#","title":"Cluster and Project Roles","content":"Cluster and project roles define user authorization inside a cluster or project. You can manage these roles from the Global > Security > Roles page.\n\nMembership and Role Assignment\n\nThe projects and clusters accessible to non-administrative users is determined by membership. Membership is a list of users who have access to a specific cluster or project based on the roles they were assigned in that cluster or project. Each cluster and project includes a tab that a user with the appropriate permissions can use to manage membership.\n\nWhen you create a cluster or project, Rancher automatically assigns you as the Owner for it. Users assigned the Owner role can assign other users roles in the cluster or project.\n\n\nNote: Non-administrative users cannot access any existing projects/clusters by default. A user with appropriate permissions (typically the owner) must explicitly assign the project and cluster membership.\n\n\nCluster Roles\n\nCluster roles are roles that you can assign to users, granting them access to a cluster. There are two primary cluster roles: Owner and Member.\n\n\nCluster Owner:\n\nThese users have full control over the cluster and all resources in it.\n\nCluster Member:\n\nThese users can view most cluster level resources and create new projects.\n\n\nCustom Cluster Roles\n\nRancher lets you assign custom cluster roles to a standard user instead of the typical Owner or Member roles. These roles can be either a built-in custom cluster role or one defined by a Rancher administrator. They are convenient for defining narrow or specialized access for a standard user within a cluster. See the table below for a list of built-in custom cluster roles.\n\nCluster Role Reference\n\nThe following table lists each built-in custom cluster role available and whether that level of access is included in the default cluster-level permissions, Cluster Owner and Cluster Member.\n\n\n\n\nBuilt-in Cluster Role\nOwner\nMember \n\n\n\n\n\nCreate Projects\n✓\n✓\n\n\n\nManage Cluster Backups            \n✓\n\n\n\n\nManage Cluster Catalogs\n✓\n\n\n\n\nManage Cluster Members\n✓\n\n\n\n\nManage Nodes\n✓\n\n\n\n\nManage Storage\n✓\n\n\n\n\nView All Projects\n✓\n\n\n\n\nView Cluster Catalogs\n✓\n✓\n\n\n\nView Cluster Members\n✓\n✓\n\n\n\nView Nodes\n✓\n✓\n\n\n\n\nFor details on how each cluster role can access Kubernetes resources, you can go to the Global view in the Rancher UI. Then click Security > Roles and go to the Clusters tab. If you click an individual role, you can refer to the Grant Resources table to see all of the operations and resources that are permitted by the role.\n\n\nNote:\nWhen viewing the resources associated with default roles created by Rancher, if there are multiple Kubernetes API resources on one line item, the resource will have (Custom) appended to it. These are not custom resources but just an indication that there are multiple Kubernetes API resources as one resource.\n\n\nGiving a Custom Cluster Role to a Cluster Member\n\nAfter an administrator sets up a custom cluster role, cluster owners and admins can then assign those roles to cluster members.\n\nTo assign a custom role to a new cluster member, you can use the Rancher UI. To modify the permissions of an existing member, you will need to use the Rancher API view.\n\nTo assign the role to a new cluster member,\n\n\nGo to the Cluster view, then go to the Members tab.\nClick Add Member. Then in the Cluster Permissions section, choose the custom cluster role that should be assigned to the member.\nClick Create.\n\n\nResult: The member has the assigned role.\n\nTo assign any custom role to an existing cluster member,\n\n\nGo to the member you want to give the role to. Click the Ellipsis (…) > View in API.\nIn the roleTemplateId field, go to the drop-down menu and choose the role you want to assign to the member. Click Show Request and Send Request.\n\n\nResult: The member has the assigned role.\n\nProject Roles\n\nProject roles are roles that can be used to grant users access to a project. There are three primary project roles: Owner, Member, and Read Only.\n\n\nProject Owner:\n\nThese users have full control over the project and all resources in it.\n\nProject Member:\n\nThese users can manage project-scoped resources like namespaces and workloads, but cannot manage other project members.\n\nRead Only:\n\nThese users can view everything in the project but cannot create, update, or delete anything.\n\n\nCaveat:\n\nUsers assigned the Owner or Member role for a project automatically inherit the namespace creation role. However, this role is a Kubernetes ClusterRole, meaning its scope extends to all projects in the cluster. Therefore, users explicitly assigned the owner or member role for a project can create namespaces in other projects they’re assigned to, even with only the Read Only role assigned.\n\n\n\nCustom Project Roles\n\nRancher lets you assign custom project roles to a standard user instead of the typical Owner, Member, or Read Only roles. These roles can be either a built-in custom project role or one defined by a Rancher administrator. They are convenient for defining narrow or specialized access for a standard user within a project. See the table below for a list of built-in custom project roles.\n\nProject Role Reference\n\nThe following table lists each built-in custom project role available in Rancher and whether it is also granted by the Owner, Member, or Read Only role.\n\n\n\n\nBuilt-in Project Role\nOwner\nMember\nRead Only\n\n\n\n\n\nManage Project Members\n✓\n\n\n\n\n\nCreate Namespaces\n✓\n✓\n\n\n\n\nManage Config Maps\n✓\n✓\n\n\n\n\nManage Ingress\n✓\n✓\n\n\n\n\nManage Project Catalogs\n✓\n\n\n\n\n\nManage Secrets\n✓\n✓\n\n\n\n\nManage Service Accounts\n✓\n✓\n\n\n\n\nManage Services\n✓\n✓\n\n\n\n\nManage Volumes\n✓\n✓\n\n\n\n\nManage Workloads\n✓\n✓\n\n\n\n\nView Config Maps\n✓\n✓\n✓\n\n\n\nView Ingress\n✓\n✓\n✓\n\n\n\nView Project Members\n✓\n✓\n✓\n\n\n\nView Project Catalogs\n✓\n✓\n✓\n\n\n\nView Secrets\n✓\n✓\n✓\n\n\n\nView Service Accounts\n✓\n✓\n✓\n\n\n\nView Services\n✓\n✓\n✓\n\n\n\nView Volumes\n✓\n✓\n✓\n\n\n\nView Workloads\n✓\n✓\n✓\n\n\n\n\n\nNotes:\n\n\nEach project role listed above, including Owner, Member, and Read Only, is comprised of multiple rules granting access to various resources. You can view the roles and their rules on the Global > Security > Roles page.\nWhen viewing the resources associated with default roles created by Rancher, if there are multiple Kubernetes API resources on one line item, the resource will have (Custom) appended to it. These are not custom resources but just an indication that there are multiple Kubernetes API resources as one resource.\nThe Manage Project Members role allows the project owner to manage any members of the project and grant them any project scoped role regardless of their access to the project resources. Be cautious when assigning this role out individually.\n\n\n\nDefining Custom Roles\n\nAs previously mentioned, custom roles can be defined for use at the cluster or project level. The context field defines whether the role will appear on the cluster member page, project member page, or both.\n\nWhen defining a custom role, you can grant access to specific resources or specify roles from which the custom role should inherit. A custom role can be made up of a combination of specific grants and inherited roles. All grants are additive. This means that defining a narrower grant for a specific resource will not override a broader grant defined in a role that the custom role is inheriting from.\n\nDefault Cluster and Project Roles\n\nBy default, when a standard user creates a new cluster or project, they are automatically assigned an ownership role: either cluster owner or project owner. However, in some organizations, these roles may overextend administrative access. In this use case, you can change the default role to something more restrictive, such as a set of individual roles or a custom role.\n\nThere are two methods for changing default cluster/project roles:\n\n\nAssign Custom Roles: Create a custom role for either your cluster or project, and then set the custom role as default.\n\nAssign Individual Roles: Configure multiple cluster/project roles as default for assignment to the creating user.\n\nFor example, instead of assigning a role that inherits other roles (such as cluster owner), you can choose a mix of individual roles (such as manage nodes and manage storage).\n\n\n\nNote:\n\n\nAlthough you can lock a default role, the system still assigns the role to users who create a cluster/project.\nOnly users that create clusters/projects inherit their roles. Users added to the cluster/project membership afterward must be explicitly assigned their roles.\n\n\n\nConfiguring Default Roles for Cluster and Project Creators\n\nYou can change the cluster or project role(s) that are automatically assigned to the creating user.\n\n\nFrom the Global view, select Security > Roles from the main menu. Select either the Cluster or Project tab.\n\nFind the custom or individual role that you want to use as default. Then edit the role by selecting Ellipsis > Edit.\n\nEnable the role as default.\n\n  \n  For Clusters\n  \n    \nFrom Cluster Creator Default, choose Yes: Default role for new cluster creation.\nClick Save.\n\n\n  \n\n\n\n  \n  For Projects\n  \n  ","postref":"efd331a45e96ba80bd54b9889f120816","objectID":"2bff00dc7f0aaf330912b415b940fc73","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/rbac/cluster-project-roles/"},{"anchor":"#prerequisites","title":"Prerequisites","content":"Available as of v2.4.0-alpha1If you have a group of individuals that need the same level of access in Rancher, it can save time to create a custom global role. When the role is assigned to a group, the users in the group have the appropriate level of access the first time they sign into Rancher.When a user in the group logs in, they get the built-in Standard User global role by default. They will also get the permissions assigned to their groups.If a user is removed from the external authentication provider group, they would lose their permissions from the custom global role that was assigned to the group. They would continue to have their individual Standard User role.\nPrerequisites: You can only assign a global role to a group if:\n\n\nYou have set up an external authentication provider\nThe external authentication provider supports user groups\nYou have already set up at least one user group with the authentication provider\n\nTo assign a custom global role to a group, follow these steps:\nFrom the Global view, go to Security > Groups.\nClick Assign Global Role.\nIn the Select Group To Add field, choose the existing group that will be assigned the custom global role.\nIn the Custom section, choose any custom global role that will be assigned to the group.\nOptional: In the Global Permissions or Built-in sections, select any additional permissions that the group should have.\nClick Create.\nResult: The custom global role will take effect when the users in the group log into Rancher.Available as of v2.4.0-alpha1When deleting a custom global role, all global role bindings with this custom role are deleted.If a user is only assigned one custom global role, and the role is deleted, the user would lose access to Rancher. For the user to regain access, an administrator would need to edit the user and apply new global permissions.Custom global roles can be deleted, but built-in roles cannot be deleted.To delete a custom global role,\nGo to the Global view and click Security > Roles.\nOn the Global tab, go to the custom global role that should be deleted and click Ellipsis (…) > Delete.\nClick Delete.\nAvailable as of v2.4.0-alpha1Custom global roles don’t have to be based on existing roles. To create a custom global role by choosing the specific Kubernetes resource operations that should be allowed for the role, follow these steps:\nGo to the Global view and click Security > Roles.\nOn the Global tab, click Add Global Role.\nEnter a name for the role.\nOptional: To assign the custom role default for new users, go to the New User Default section and click Yes: Default role for new users.\nIn the Grant Resources section, select the Kubernetes resource operations that will be enabled for users with the custom role.\nClick Save.\nAvailable as of v2.4.0-alpha1If you have a group of individuals that need the same level of access in Rancher, it can save time to create a custom global role in which all of the rules from another role, such as the administrator role, are copied into a new role. This allows you to only configure the variations between the existing role and the new role.The custom global role can then be assigned to a user or group so that the custom global role takes effect the first time the user or users sign into Rancher.To create a custom global role based on an existing role,\nGo to the Global view and click Security > Roles.\nOn the Global tab, go to the role that the custom global role will be based on. Click Ellipsis (…) > Clone.\nEnter a name for the role.\nOptional: To assign the custom role default for new users, go to the New User Default section and click Yes: Default role for new users.\nIn the Grant Resources section, select the Kubernetes resource operations that will be enabled for users with the custom role.\nClick Save.\nWhile Rancher comes out-of-the-box with a set of default user roles, you can also create default custom roles to provide users with very specific permissions within Rancher.The steps to add custom roles differ depending on the version of Rancher.\n  \n  \n  \nFrom the Global view, select Security > Roles from the main menu.\n\nSelect a tab to determine the scope of the roles you’re adding. The tabs are:\n\n\nCluster: The role is valid for assignment when adding/managing members to only clusters.\nProject: The role is valid for assignment when adding/managing members to only projects.\n\n\nClick Add Cluster/Project Role.\n\nName the role.\n\nOptional: Choose the Cluster/Project Creator Default option to assign this role to a user when they create a new cluster or project. Using this feature, you can expand or restrict the default roles for cluster/project creators.\n\n\nOut of the box, the Cluster Creator Default and the Project Creator Default roles are Cluster Owner and Project Owner respectively.\n\n\nUse the Grant Resources options to assign individual Kubernetes API endpoints to the role.\n\n\nWhen viewing the resources associated with default roles created by Rancher, if there are multiple Kubernetes API resources on one line item, the resource will have (Custom) appended to it. These are not custom resources but just an indication that there are multiple Kubernetes API resources as one resource.\n\n\nYou can also choose the individual cURL methods (Create, Delete, Get, etc.) available for use with each endpoint you assign.\n\nUse the Inherit from a Role options to assign individual Rancher roles to your custom roles. Note: When a custom role inherits from a parent role, the parent role cannot be deleted until the child role is deleted.\n\nClick Create.\n\n\n\n\n\n  \nFrom the Global view, select Security > Roles from the main menu.\n\nClick Add Role.\n\nName the role.\n\nChoose whether to set the role to a status of locked.\n\n\nNote: Locked roles cannot be assigned to users.\n\n\nIn the Context dropdown menu, choose the scope of the role assigned to the user. The contexts are:\n\n\nAll: The user can use their assigned role regardless of context. This role is valid for assignment when adding/managing members to clusters or projects.\n\nCluster: This role is valid for assignment when adding/managing members to only clusters.\n\nProject: This role is valid for assignment when adding/managing members to only projects.\n\n\nUse the Grant Resources options to assign individual Kubernetes API endpoints to the role.\n\n\nWhen viewing the resources associated with default roles created by Rancher, if there are multiple Kubernetes API resources on one line item, the resource will have (Custom) appended to it. These are not custom resources but just an indication that there are multiple Kubernetes API resources as one resource.\n\n\nYou can also choose the individual cURL methods (Create, Delete, Get, etc.) available for use with each endpoint you assign.\n\nUse the Inherit from a Role options to assign individual Rancher roles to your custom roles. Note: When a custom role inherits from a parent role, the parent role cannot be deleted until the child role is deleted.\n\nClick Create.\n\n\n\n\nTo complete the tasks on this page, one of the following permissions are required:\nAdministrator Global Permissions.\nCustom Global Permissions with the Manage Roles role assigned.\n","postref":"42165d7fdeedb5921a546c4e7f0ba966","objectID":"3697d6b5b3fb2ae6cba3cf59359aadb1","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/rbac/default-custom-roles/"},{"anchor":"#locking-unlocking-roles","title":"Locking/Unlocking Roles","content":"If you want to prevent a role from being assigned to users, you can set it to a status of locked.You can lock roles in two contexts:\nWhen you’re adding a custom role.\nWhen you editing an existing role (see below).\n\nFrom the Global view, select Security > Roles.\n\nFrom the role that you want to lock (or unlock), select Vertical Ellipsis (…) > Edit.\n\nFrom the Locked option, choose the Yes or No radio button. Then click Save.\n","postref":"1268d2c894d46f0e6c9439a8c729d757","objectID":"15f2576c22b879c0a68e987ae03b3e58","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/rbac/locked-roles/"},{"anchor":"#default-pod-security-policies","title":"Default Pod Security Policies","content":"\nFrom the Global view, select Security > Pod Security Policies from the main menu. Then click Add Policy.\n\nStep Result: The Add Policy form opens.\n\nName the policy.\n\nComplete each section of the form. Refer to the Kubernetes documentation linked below for more information on what each policy does.\n\n\nBasic Policies:\n\n\nPrivilege Escalation\nHost Namespaces\nRead Only Root Filesystems\n\n\nCapability Policies\n\nVolume Policy\n\nAllowed Host Paths Policy\n\nFS Group Policy\n\nHost Ports Policy\n\nRun As User Policy\n\nSELinux Policy\n\nSupplemental Groups Policy\n\nWhat’s Next?You can add a Pod Security Policy (PSPs hereafter) in the following contexts:\nWhen creating a cluster\nWhen editing an existing cluster\nWhen creating a project\nWhen editing an existing project\n\nNote: We recommend adding PSPs during cluster and project creation instead of adding it to an existing one.\nAvailable as of v2.0.7Rancher ships with two default Pod Security Policies (PSPs): the restricted and unrestricted policies.\nrestricted\n\nThis policy is based on the Kubernetes example restricted policy. It significantly restricts what types of pods can be deployed to a cluster or project. This policy:\n\n\nPrevents pods from running as a privileged user and prevents escalation of privileges.\nValidates that server-required security mechanisms are in place (such as restricting what volumes can be mounted to only the core volume types and preventing root supplemental groups from being added).\n\n\nunrestricted\n\nThis policy is equivalent to running Kubernetes with the PSP controller disabled. It has no restrictions on what pods can be deployed into a cluster or project.\n","postref":"fb3267c9309282ce8a4aa6d9308c3a77","objectID":"3b1caedb44f5adaf76fda9bb8e3378e1","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/pod-security-policies/"},{"anchor":"#cluster-drivers","title":"Cluster Drivers","content":"Node drivers are used to provision hosts, which Rancher uses to launch and manage Kubernetes clusters. A node driver is the same as a Docker Machine driver. The availability of which node driver to display when creating node templates is defined based on the node driver’s status. Only active node drivers will be displayed as an option for creating node templates. By default, Rancher is packaged with many existing Docker Machine drivers, but you can also create custom node drivers to add to Rancher.If there are specific node drivers that you don’t want to show to your users, you would need to de-activate these node drivers.Rancher supports several major cloud providers, but by default, these node drivers are active and available for deployment:\nAmazon EC2\nAzure\nDigital Ocean\nvSphere\nAvailable as of v2.2.0Cluster drivers are used to provision hosted Kubernetes clusters, such as GKE, EKS, AKS, etc.. The availability of which cluster driver to display when creating a cluster is defined based on the cluster driver’s status. Only active cluster drivers will be displayed as an option for creating clusters for hosted Kubernetes clusters. By default, Rancher is packaged with several existing cluster drivers, but you can also create custom cluster drivers to add to Rancher.By default, Rancher has activated several hosted Kubernetes cloud providers including:\nAmazon EKS\nGoogle GKE\nAzure AKS\nThere are several other hosted Kubernetes cloud providers that are disabled by default, but are packaged in Rancher:\nAlibaba ACK\nHuawei CCE\nTencent\n","postref":"73e28b7be8a8732e547ae8883e97b3e5","objectID":"c4e6bc33ef3b9217f0370f2cf21c19b3","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/drivers/"},{"anchor":"#prerequisites","title":"Prerequisites","content":"If you are experiencing issues while testing the connection to the Keycloak server, first double-check the configuration option of your SAML client. You may also inspect the Rancher logs to help pinpointing the problem cause. Debug logs may contain more detailed information about the error. Please refer to How can I enable debug logging in this documentation.You are not redirected to KeycloakWhen you click on Authenticate with Keycloak, your are not redirected to your IdP.\nVerify your Keycloak client configuration.\nMake sure Force Post Binding set to OFF.\nForbidden message displayed after IdP loginYou are correctly redirected to your IdP login page and you are able to enter your credentials, however you get a Forbidden message afterwards.\nCheck the Rancher debug log.\nIf the log displays ERROR: either the Response or Assertion must be signed, make sure either Sign Documents or Sign assertions is set to ON in your Keycloak client.\nHTTP 502 when trying to access /v1-saml/keycloak/saml/metadataThis is usually due to the metadata not being created until a SAML provider is configured.\nTry configuring and saving keycloak as your SAML provider and then accessing the metadata.Keycloak Error: “We’re sorry, failed to process response”\nCheck your Keycloak log.\nIf the log displays failed: org.keycloak.common.VerificationException: Client does not have a public key, set Encrypt Assertions to OFF in your Keycloak client.\nKeycloak Error: “We’re sorry, invalid requester”\nCheck your Keycloak log.\nIf the log displays request validation failed: org.keycloak.common.VerificationException: SigAlg was null, set Client Signature Required to OFF in your Keycloak client.\nKeycloak 6.0.0+: IDPSSODescriptor missing from optionsKeycloak versions 6.0.0 and up no longer provide the IDP metadata under the Installation tab.\nYou can still get the XML from the following url:https://{KEYCLOAK-URL}/auth/realms/{REALM-NAME}/protocol/saml/descriptorThe XML obtained from this URL contains EntitiesDescriptor as the root element. Rancher expects the root element to be EntityDescriptor rather than EntitiesDescriptor. So before passing this XML to Rancher, follow these steps to adjust it:\nCopy all the tags from EntitiesDescriptor to the EntityDescriptor.\nRemove the <EntitiesDescriptor> tag from the beginning.\nRemove the </EntitiesDescriptor> from the end of the xml.\nYou are left with something similar as the example below:<EntityDescriptor xmlns=\"urn:oasis:names:tc:SAML:2.0:metadata\" xmlns:dsig=\"http://www.w3.org/2000/09/xmldsig#\" entityID=\"https://{KEYCLOAK-URL}/auth/realms/{REALM-NAME}\">\n  ....\n\n</EntityDescriptor>\n\nFrom the Global view, select Security > Authentication from the main menu.\n\nSelect Keycloak.\n\nComplete the Configure Keycloak Account form. Keycloak IdP lets you specify what data store you want to use. You can either add a database or use an existing LDAP server. For example, if you select your Active Directory (AD) server, the examples below describe how you can map AD attributes to fields within Rancher.\n\n\n\n\nField\nDescription\n\n\n\n\n\nDisplay Name Field\nThe AD attribute that contains the display name of users.\n\n\n\nUser Name Field\nThe AD attribute that contains the user name/given name.\n\n\n\nUID Field\nAn AD attribute that is unique to every user.\n\n\n\nGroups Field\nMake entries for managing group memberships.\n\n\n\nRancher API Host\nThe URL for your Rancher Server.\n\n\n\nPrivate Key / Certificate\nA key/certificate pair to create a secure shell between Rancher and your IdP.\n\n\n\nIDP-metadata\nThe metadata.xml file that you exported from your IdP server.\n\n\n\n\n\nTip: You can generate a key/certificate pair using an openssl command. For example:\n\n   openssl req -x509 -sha256 -nodes -days 365 -newkey rsa:2048 -keyout myservice.key -out myservice.cert\n\n\n\nAfter you complete the Configure Keycloak Account form, click Authenticate with Keycloak, which is at the bottom of the page.\n\nRancher redirects you to the IdP login page. Enter credentials that authenticate with Keycloak IdP to validate your Rancher Keycloak configuration.\n\n\nNote: You may have to disable your popup blocker to see the IdP login page.\n\nResult: Rancher is configured to work with Keycloak. Your users can now sign into Rancher using their Keycloak logins.\nSAML Provider Caveats:\n\n\nSAML Protocol does not support search or lookup for users or groups. Therefore, there is no validation on users or groups when adding them to Rancher.\nWhen adding users, the exact user IDs (i.e. UID Field) must be entered correctly. As you type the user ID, there will be no search for other  user IDs that may match.\n\nWhen adding groups, you must select the group from the drop-down that is next to the text box. Rancher assumes that any input from the text box is a user.\n\n\nThe group drop-down shows only the groups that you are a member of. You will not be able to add groups that you are not a member of.\n\n\n\nYou must have a Keycloak IdP Server configured.\n\nIn Keycloak, create a new SAML client, with the settings below. See the Keycloak documentation for help.\n\n\n\n\nSetting\nValue\n\n\n\n\n\nSign Documents\nON 1\n\n\n\nSign Assertions\nON 1\n\n\n\nAll other ON/OFF Settings\nOFF\n\n\n\nClient ID\nhttps://yourRancherHostURL/v1-saml/keycloak/saml/metadata2\n\n\n\nClient Name\n (e.g. rancher)\n\n\n\nClient Protocol\nSAML\n\n\n\nValid Redirect URI\nhttps://yourRancherHostURL/v1-saml/keycloak/saml/acs\n\n\n\n\n\n1: Optionally, you can enable either one or both of these settings.\n2: Rancher SAML metadata won’t be generated until a SAML provider is configured and saved.\n\n\nExport a metadata.xml file from your Keycloak client:\nFrom the Installation tab, choose the SAML Metadata IDPSSODescriptor format option and download your file.\n","postref":"091e999ff095aa1bd96ce7d9fc10c43b","objectID":"887ad0be853ec8649646ed19bac2ab91","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/authentication/keycloak/"},{"anchor":"#","title":"Configuring PingIdentity (SAML)","content":"Available as of v2.0.7\n\nIf your organization uses Ping Identity Provider (IdP) for user authentication, you can configure Rancher to allow your users to log in using their IdP credentials.\n\n\nPrerequisites:\n\n\nYou must have a Ping IdP Server configured.\nFollowing are the Rancher Service Provider URLs needed for configuration:\nMetadata URL: https://<rancher-server>/v1-saml/ping/saml/metadata\nAssertion Consumer Service (ACS) URL: https://<rancher-server>/v1-saml/ping/saml/acs\nExport a metadata.xml file from your IdP Server. For more information, see the PingIdentity documentation.\n\n\n\n\nFrom the Global view, select Security > Authentication from the main menu.\n\nSelect PingIdentity.\n\nComplete the Configure Ping Account form. Ping IdP lets you specify what data store you want to use. You can either add a database or use an existing ldap server. For example, if you select your Active Directory (AD) server, the examples below describe how you can map AD attributes to fields within Rancher.\n\n\nDisplay Name Field: Enter the AD attribute that contains the display name of users (example: displayName).\n\nUser Name Field: Enter the AD attribute that contains the user name/given name (example: givenName).\n\nUID Field: Enter an AD attribute that is unique to every user (example: sAMAccountName, distinguishedName).\n\nGroups Field: Make entries for managing group memberships (example: memberOf).\n\nRancher API Host: Enter the URL for your Rancher Server.\n\nPrivate Key and Certificate: This is a key-certificate pair to create a secure shell between Rancher and your IdP.\n\nYou can generate one using an openssl command. For example:\n\nopenssl req -x509 -newkey rsa:2048 -keyout myservice.key -out myservice.cert -days 365 -nodes -subj \"/CN=myservice.example.com\"\n\n\nIDP-metadata: The metadata.xml file that you exported from your IdP server.\n\n\nAfter you complete the Configure Ping Account form, click Authenticate with Ping, which is at the bottom of the page.\n\nRancher redirects you to the IdP login page. Enter credentials that authenticate with Ping IdP to validate your Rancher PingIdentity configuration.\n\n\nNote: You may have to disable your popup blocker to see the IdP login page.\n\n\n\nResult: Rancher is configured to work with PingIdentity. Your users can now sign into Rancher using their PingIdentity logins.\n\n\n\nSAML Provider Caveats:\n\n\nSAML Protocol does not support search or lookup for users or groups. Therefore, there is no validation on users or groups when adding them to Rancher.\nWhen adding users, the exact user IDs (i.e. UID Field) must be entered correctly. As you type the user ID, there will be no search for other  user IDs that may match.\n\nWhen adding groups, you must select the group from the drop-down that is next to the text box. Rancher assumes that any input from the text box is a user.\n\n\nThe group drop-down shows only the groups that you are a member of. You will not be able to add groups that you are not a member of.\n\n\n\n\n","postref":"fb92d49fcdc90ec6bc85ec9dc8018798","objectID":"9f8d9d13d03c72cf6fe53abb06e1d0d7","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/authentication/ping-federate/"},{"anchor":"#","title":"1 — Configuring Microsoft AD FS for Rancher","content":"Before configuring Rancher to support AD FS users, you must add Rancher as a relying party trust in AD FS.\n\n\nLog into your AD server as an administrative user.\n\nOpen the AD FS Management console. Select Add Relying Party Trust… from the Actions menu and click Start.\n\n\n\n\n    \n    \n    \n    \n    \n    \n\n\n\nSelect Enter data about the relying party manually as the option for obtaining data about the relying party.\n\n\n\n\n    \n    \n    \n    \n    \n    \n\n\n\nEnter your desired Display name for your Relying Party Trust. For example, Rancher.\n\n\n\n\n    \n    \n    \n    \n    \n    \n\n\n\nSelect AD FS profile as the configuration profile for your relying party trust.\n\n\n\n\n    \n    \n    \n    \n    \n    \n\n\n\nLeave the optional token encryption certificate empty, as Rancher AD FS will not be using one.\n\n\n\n\n\n    \n    \n    \n    \n    \n    \n\n\n\n\nSelect Enable support for the SAML 2.0 WebSSO protocol\nand enter https://<rancher-server>/v1-saml/adfs/saml/acs for the service URL.\n\n\n\n\n\n\n    \n    \n    \n    \n    \n    \n\n\n\n\nAdd https://<rancher-server>/v1-saml/adfs/saml/metadata as the Relying party trust identifier.\n\n\n\n\n    \n    \n    \n    \n    \n    \n\n\n\nThis tutorial will not cover multi-factor authentication; please refer to the Microsoft documentation if you would like to configure multi-factor authentication.\n\n\n\n\n    \n    \n    \n    \n    \n    \n\n\n\nFrom Choose Issuance Authorization RUles, you may select either of the options available according to use case. However, for the purposes of this guide, select Permit all users to access this relying party.\n\n\n\n\n    \n    \n    \n    \n    \n    \n\n\n\nAfter reviewing your settings, select Next to add the relying party trust.\n\n\n\n\n    \n    \n    \n    \n    \n    \n\n\n\nSelect Open the Edit Claim Rules… and click Close.\n\n\n\n\n    \n    \n    \n    \n    \n    \n\n\n\nOn the Issuance Transform Rules tab, click Add Rule….\n\n\n\n\n    \n    \n    \n    \n    \n    \n\n\n\nSelect Send LDAP Attributes as Claims as the Claim rule template.\n\n\n\n\n    \n    \n    \n    \n    \n    \n\n\n\nSet the Claim rule name to your desired name (for example, Rancher Attributes) and select Active Directory as the Attribute store. Create the following mapping to reflect the table below:\n\n\n\n\nLDAP Attribute\nOutgoing Claim Type\n\n\n\n\n\nGiven-Name\nGiven Name\n\n\n\nUser-Principal-Name\nUPN\n\n\n\nToken-Groups - Qualified by Long Domain Name\nGroup\n\n\n\nSAM-Account-Name\nName\n\n\n\n\n\n\n\n\n    \n    \n    \n    \n    \n    \n\n\n\nDownload the federationmetadata.xml from your AD server at:\n\nhttps://<AD_SERVER>/federationmetadata/2007-06/federationmetadata.xml\n\n\n\nResult: You’ve added Rancher as a relying trust party. Now you can configure Rancher to leverage AD.\n\nNext: Configuring Rancher for Microsoft AD FS\n","postref":"f4adc603d49a3cf23d0ee4b0af4bdb85","objectID":"1b8d9528dc2fa94c32730c0d52555f94","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/authentication/microsoft-adfs/microsoft-adfs-setup/"},{"anchor":"#","title":"2 — Configuring Rancher for Microsoft AD FS","content":"Available as of v2.0.7\n\nAfter you complete Configuring Microsoft AD FS for Rancher, enter your AD FS information into Rancher to allow AD FS users to authenticate with Rancher.\n\n\nImportant Notes For Configuring Your AD FS Server:\n\n\nThe SAML 2.0 WebSSO Protocol Service URL is: https://<RANCHER_SERVER>/v1-saml/adfs/saml/acs\nThe Relying Party Trust identifier URL is: https://<RANCHER_SERVER>/v1-saml/adfs/saml/metadata\nYou must export the federationmetadata.xml file from your AD FS server. This can be found at: https://<AD_SERVER>/federationmetadata/2007-06/federationmetadata.xml\n\n\n\n\nFrom the Global view, select Security > Authentication from the main menu.\n\nSelect Microsoft Active Directory Federation Services.\n\nComplete the Configure AD FS Account form. Microsoft AD FS lets you specify an existing Active Directory (AD) server. The examples below describe how you can map AD attributes to fields within Rancher.\n\n\n\n\nField\nDescription\n\n\n\n\n\nDisplay Name Field\nThe AD attribute that contains the display name of users. Example: http://schemas.xmlsoap.org/ws/2005/05/identity/claims/givenname\n\n\n\nUser Name Field\nThe AD attribute that contains the user name/given name. Example: http://schemas.xmlsoap.org/ws/2005/05/identity/claims/name\n\n\n\nUID Field\nAn AD attribute that is unique to every user. Example: http://schemas.xmlsoap.org/ws/2005/05/identity/claims/upn\n\n\n\nGroups Field\nMake entries for managing group memberships. Example: http://schemas.xmlsoap.org/claims/Group\n\n\n\nRancher API Host\nThe URL for your Rancher Server.\n\n\n\nPrivate Key / Certificate\nThis is a key-certificate pair to create a secure shell between Rancher and your AD FS. Ensure you set the Common Name (CN) to your Rancher Server URL.Certificate creation command\n\n\n\nMetadata XML\nThe federationmetadata.xml file exported from your AD FS server. You can find this file at https://<AD_SERVER>/federationmetadata/2007-06/federationmetadata.xml.\n\n\n\n\n\n\n\nTip: You can generate a certificate using an openssl command. For example:\n\n   openssl req -x509 -newkey rsa:2048 -keyout myservice.key -out myservice.cert -days 365 -nodes -subj \"/CN=myservice.example.com\"\n\n\n\nAfter you complete the Configure AD FS Account form, click Authenticate with AD FS, which is at the bottom of the page.\n\nRancher redirects you to the AD FS login page. Enter credentials that authenticate with Microsoft AD FS to validate your Rancher AD FS configuration.\n\n\nNote: You may have to disable your popup blocker to see the AD FS login page.\n\n\n\nResult: Rancher is configured to work with MS FS. Your users can now sign into Rancher using their MS FS logins.\n","postref":"8f1dcfc1458558ab698474e989fde90f","objectID":"920f61e276e34b02df677d97278e9f7a","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/authentication/microsoft-adfs/rancher-adfs-setup/"},{"anchor":"#prerequisites","title":"Prerequisites","content":"Setting up Microsoft AD FS with Rancher Server requires configuring AD FS on your Active Directory server, and configuring Rancher to utilize your AD FS server. The following pages serve as guides for setting up Microsoft AD FS authentication on your Rancher installation.\n1 — Configuring Microsoft AD FS for Rancher\n2 — Configuring Rancher for Microsoft AD FS\n\nSAML Provider Caveats:\n\n\nSAML Protocol does not support search or lookup for users or groups. Therefore, there is no validation on users or groups when adding them to Rancher.\nWhen adding users, the exact user IDs (i.e. UID Field) must be entered correctly. As you type the user ID, there will be no search for other  user IDs that may match.\n\nWhen adding groups, you must select the group from the drop-down that is next to the text box. Rancher assumes that any input from the text box is a user.\n\n\nThe group drop-down shows only the groups that you are a member of. You will not be able to add groups that you are not a member of.\n\n\nNext: Configuring Microsoft AD FS for Rancher\nYou must have Rancher installed.\n\n\nObtain your Rancher Server URL. During AD FS configuration, substitute this URL for the <RANCHER_SERVER> placeholder.\n\nYou must have a global administrator account on your Rancher installation.\n\n\nYou must have a Microsoft AD FS Server configured.\n\n\nObtain your AD FS Server IP/DNS name. During AD FS configuration, substitute this IP/DNS name for the <AD_SERVER> placeholder.\n\nYou must have access to add Relying Party Trusts on your AD FS Server.\n\n","postref":"2a64df5a1cb589bc0966765b7d912d04","objectID":"22c3b5fb7a888807c7557988ffd61ade","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/authentication/microsoft-adfs/"},{"anchor":"#prerequisites","title":"Prerequisites","content":"\nFrom the Global view, select Security > Authentication from the main menu.\n\nSelect Okta.\n\nComplete the Configure Okta Account form. The examples below describe how you can map Okta attributes to fields within Rancher.\n\n\n\n\nField\nDescription\n\n\n\n\n\nDisplay Name Field\nThe attribute that contains the display name of users.\n\n\n\nUser Name Field\nThe attribute that contains the user name/given name.\n\n\n\nUID Field\nAn attribute that is unique to every user.\n\n\n\nGroups Field\nMake entries for managing group memberships.\n\n\n\nRancher API Host\nThe URL for your Rancher Server.\n\n\n\nPrivate Key / Certificate\nA key/certificate pair to create a secure shell between Rancher and your IdP.\n\n\n\nMetadata XML\nThe Identity Provider metadata file that you find in the application Sign On section.\n\n\n\n\n\nTip: You can generate a key/certificate pair using an openssl command. For example:\n\n   openssl req -x509 -sha256 -nodes -days 365 -newkey rsa:2048 -keyout myservice.key -out myservice.crt\n\n\n\nAfter you complete the Configure Okta Account form, click Authenticate with Okta, which is at the bottom of the page.\n\nRancher redirects you to the IdP login page. Enter credentials that authenticate with Okta IdP to validate your Rancher Okta configuration.\n\n\nNote: If nothing seems to happen, it’s likely because your browser blocked the pop-up. Make sure you disable the pop-up blocker for your rancher domain and whitelist it in any other extensions you might utilize.\n\nResult: Rancher is configured to work with Okta. Your users can now sign into Rancher using their Okta logins.\nSAML Provider Caveats:\n\n\nSAML Protocol does not support search or lookup for users or groups. Therefore, there is no validation on users or groups when adding them to Rancher.\nWhen adding users, the exact user IDs (i.e. UID Field) must be entered correctly. As you type the user ID, there will be no search for other  user IDs that may match.\n\nWhen adding groups, you must select the group from the drop-down that is next to the text box. Rancher assumes that any input from the text box is a user.\n\n\nThe group drop-down shows only the groups that you are a member of. You will not be able to add groups that you are not a member of.\n\n\nIn Okta, create a SAML Application with the settings below. See the Okta documentation for help.\n\n\nSetting\nValue\n\n\n\n\n\nSingle Sign on URL\nhttps://yourRancherHostURL/v1-saml/okta/saml/acs\n\n\n\nAudience URI (SP Entity ID)\nhttps://yourRancherHostURL/v1-saml/okta/saml/metadata\n\n\n","postref":"a826c89179cb72eae424c517f0228e66","objectID":"3ad7a98f828a15268d7e352279dd25a2","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/authentication/okta/"},{"anchor":"#","title":"Setting up Kubernetes Clusters in Rancher","content":"Rancher simplifies the creation of clusters by allowing you to create them through the Rancher UI rather than more complex alternatives. Rancher provides multiple options for launching a cluster. Use the option that best fits your use case.\n\nThis section assumes a basic familiarity with Docker and Kubernetes. For a brief explanation of how Kubernetes components work together, refer to the concepts page.\n\nFor a conceptual overview of how the Rancher server provisions clusters and what tools it uses to provision them, refer to the architecture page.\n\nThis section covers the following topics:\n\n\n\n\nSetting up clusters in a hosted Kubernetes provider\nLaunching Kubernetes with Rancher\n\n\nLaunching Kubernetes and Provisioning Nodes in an Infrastructure Provider\nLaunching Kubernetes on Existing Custom Nodes\n\nImporting Existing Cluster\n\n\n\nThe following table summarizes the options and settings available for each cluster type:\n\n\n\n\nRancher Capability\nRKE Launched\nHosted Kubernetes Cluster\nImported Cluster\n\n\n\n\n\nManage member roles\n✓\n✓\n✓\n\n\n\nEdit cluster options\n✓\n\n\n\n\n\nManage node pools\n✓\n\n\n\n\n\n\nSetting up Clusters in a Hosted Kubernetes Provider\n\nIn this scenario, Rancher does not provision Kubernetes because it is installed by providers such as Google Kubernetes Engine (GKE), Amazon Elastic Container Service for Kubernetes, or Azure Kubernetes Service.\n\nIf you use a Kubernetes provider such as Google GKE, Rancher integrates with its cloud APIs, allowing you to create and manage role-based access control for the hosted cluster from the Rancher UI.\n\nFor more information, refer to the section on hosted Kubernetes clusters.\n\nLaunching Kubernetes with Rancher\n\nRancher uses the Rancher Kubernetes Engine (RKE)as a library when provisioning Kubernetes on your own nodes. RKE is Rancher’s own lightweight Kubernetes installer.\n\nIn RKE clusters, Rancher manages the deployment of Kubernetes. These clusters can be deployed on any bare metal server, cloud provider, or virtualization platform.\n\nThese nodes can be dynamically provisioned through Rancher’s UI, which calls Docker Machine to launch nodes on various cloud providers.\n\nIf you already have a node that you want to add to an RKE cluster, you can add it to the cluster by running a Rancher agent container on it.\n\nFor more information, refer to the section on RKE clusters.\n\nLaunching Kubernetes and Provisioning Nodes in an Infrastructure Provider\n\nRancher can dynamically provision nodes in infrastructure providers such as Amazon EC2, DigitalOcean, Azure, or vSphere, then install Kubernetes on them.\n\nUsing Rancher, you can create pools of nodes based on a node template. This template defines the parameters used to launch nodes in your cloud providers.\n\nOne benefit of using nodes hosted by an infrastructure provider is that if a node loses connectivity with the cluster, Rancher can automatically replace it, thus maintaining the expected cluster configuration.\n\nThe cloud providers available for creating a node template are decided based on the node drivers active in the Rancher UI.\n\nFor more information, refer to the section on nodes hosted by an infrastructure provider\n\nLaunching Kubernetes on Existing Custom Nodes\n\nWhen setting up this type of cluster, Rancher installs Kubernetes on existing custom nodes, which creates a custom cluster.\n\nYou can bring any nodes you want to Rancher and use them to create a cluster.\n\nThese nodes include on-premise bare metal servers, cloud-hosted virtual machines, or on-premise virtual machines.\n\nImporting Existing Clusters\n\nIn this type of cluster, Rancher connects to a Kubernetes cluster that has already been set up. Therefore, Rancher does not provision Kubernetes, but only sets up the Rancher agents to communicate with the cluster.\n\nNote that Rancher does not automate the provisioning, scaling, or upgrade of imported clusters. All other Rancher features, including management of cluster, policy, and workloads, are available for imported clusters.\n\nFor more information, refer to the section on importing existing clusters.\n","postref":"a53656025e59ba11520a6ed68fb266d9","objectID":"96d2e1910407ad3af6467f54e56b97b7","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/"},{"anchor":"#","title":"Checklist for Production-Ready Clusters","content":"In this section, we recommend best practices for creating the production-ready Kubernetes clusters that will run your apps and services.\n\nFor a list of requirements for your cluster, including the requirements for OS/Docker, hardware, and networking, refer to the section on node requirements.\n\nThis is a shortlist of best practices that we strongly recommend for all production clusters.\n\nFor a full list of all the best practices that we recommend, refer to the best practices section.\n\nNode Requirements\n\n\nMake sure your nodes fulfill all of the node requirements, including the port requirements.\n\n\nBack up etcd\n\n\nEnable etcd snapshots. Verify that snapshots are being created, and run a disaster recovery scenario to verify the snapshots are valid. etcd is the location where the state of your cluster is stored, and losing etcd data means losing your cluster. Make sure you configure etcd Recurring Snapshots for your cluster(s), and make sure the snapshots are stored externally (off the node) as well.\n\n\nCluster Architecture\n\n\nNodes should have one of the following role configurations:\n\n\netcd\ncontrolplane\netcd and controlplane\nworker (the worker role should not be used or added on nodes with the etcd or controlplane role)\n\nHave at least three nodes with the role etcd to survive losing one node. Increase this count for higher node fault toleration, and spread them across (availability) zones to provide even better fault tolerance.\nAssign two or more nodes the controlplane role for master component high availability.\nAssign two or more nodes the worker role for workload rescheduling upon node failure.\n\n\nFor more information on what each role is used for, refer to the section on roles for nodes in Kubernetes.\n\nFor more information about the\nnumber of nodes for each Kubernetes role, refer to the section on recommended architecture.\n\nLogging and Monitoring\n\n\nConfigure alerts/notifiers for Kubernetes components (System Service).\nConfigure logging for cluster analysis and post-mortems.\n\n\nReliability\n\n\nPerform load tests on your cluster to verify that its hardware can support your workloads.\n\n\nNetworking\n\n\nMinimize network latency. Rancher recommends minimizing latency between the etcd nodes. The default setting for heartbeat-interval is 500, and the default setting for election-timeout is 5000. These settings for etcd tuning allow etcd to run in most networks (except really high latency networks).\nCluster nodes should be located within a single region. Most cloud providers provide multiple availability zones within a region, which can be used to create higher availability for your cluster. Using multiple availability zones is fine for nodes with any role. If you are using Kubernetes Cloud Provider resources, consult the documentation for any restrictions (i.e. zone storage restrictions).\n\n","postref":"63a5ab15a31512b0c29ba7e06b606d99","objectID":"9d75f3e840b82857ec921377b58b4f70","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/production/"},{"anchor":"#switching-between-clusters","title":"Switching between Clusters","content":"Rancher contains a variety of tools that aren’t included in Kubernetes to assist in your DevOps operations. Rancher can integrate with external services to help your clusters run more efficiently. Tools are divided into following categories:\nAlerts\nNotifiers\nLogging\nMonitoring\nFor more information, see ToolsAfter clusters have been provisioned into Rancher, cluster owners will need to manage these clusters. There are many different options of how to manage your cluster.\n\n\nAction\nRancher launched Kubernetes Clusters\nHosted Kubernetes Clusters\nImported Clusters\n\n\n\n\n\nUsing kubectl and a kubeconfig file to Access a Cluster\n*\n*\n*\n\n\n\nAdding Cluster Members\n*\n*\n*\n\n\n\nEditing Clusters\n*\n*\n*\n\n\n\nManaging Nodes\n*\n*\n*\n\n\n\nManaging Persistent Volumes and Storage Classes\n*\n*\n*\n\n\n\nManaging Projects and Namespaces\n*\n*\n*\n\n\n\nConfiguring Tools\n*\n*\n*\n\n\n\nCloning Clusters\n\n*\n*\n\n\n\nAbility to rotate certificates\n*\n\n\n\n\n\nAbility to back up your Kubernetes Clusters\n*\n\n\n\n\n\nAbility to recover and restore etcd\n*\n\n\n\n\n\nCleaning Kubernetes components when clusters are no longer reachable from Rancher\n*\n\n\n\n\nTo switch between clusters, use the drop-down available in the navigation bar.Alternatively, you can switch between projects and clusters directly in the navigation bar. Open the Global view and select Clusters from the main menu. Then select the name of the cluster you want to open.","postref":"bfd0cc60c3a83b81af3257c929141efe","objectID":"e4cad93caf3469829e8f8ed61d852ae2","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/"},{"anchor":"#","title":"Access a Cluster with Kubectl and kubeconfig","content":"This section describes how to manipulate your downstream Kubernetes cluster with kubectl from the Rancher UI or from your workstation.\n\nFor more information on using kubectl, see Kubernetes Documentation: Overview of kubectl.\n\n\nAccessing clusters with kubectl shell in the Rancher UI\nAccessing clusters with kubectl from your workstation\nNote on Resources created using kubectl\nAuthenticating Directly with a Downstream Cluster\n\n\nConnecting Directly to Clusters with FQDN Defined\nConnecting Directly to Clusters without FQDN Defined\n\n\n\nAccessing Clusters with kubectl Shell in the Rancher UI\n\nYou can access and manage your clusters by logging into Rancher and opening the kubectl shell in the UI. No further configuration necessary.\n\n\nFrom the Global view, open the cluster that you want to access with kubectl.\n\nClick Launch kubectl. Use the window that opens to interact with your Kubernetes cluster.\n\n\nAccessing Clusters with kubectl from Your Workstation\n\nThis section describes how to download your cluster’s kubeconfig file, launch kubectl from your workstation, and access your downstream cluster.\n\nThis alternative method of accessing the cluster allows you to authenticate with Rancher and manage your cluster without using the Rancher UI.\n\n\nPrerequisites: These instructions assume that you have already created a Kubernetes cluster, and that kubectl is installed on your workstation. For help installing kubectl, refer to the official Kubernetes documentation.\n\n\n\nLog into Rancher. From the Global view, open the cluster that you want to access with kubectl.\nClick Kubeconfig File.\nCopy the contents displayed to your clipboard.\nPaste the contents into a new file on your local computer. Move the file to ~/.kube/config. Note: The default location that kubectl uses for the kubeconfig file is ~/.kube/config, but you can use any directory and specify it using the --kubeconfig flag, as in this command:\n\nkubectl --kubeconfig /custom/path/kube.config get pods\n\nFrom your workstation, launch kubectl. Use it to interact with your kubernetes cluster.\n\n\nNote on Resources Created Using kubectl\n\nRancher will discover and show resources created by kubectl. However, these resources might not have all the necessary annotations on discovery. If an operation (for instance, scaling the workload) is done to the resource using the Rancher UI/API, this may trigger recreation of the resources due to the missing annotations. This should only happen the first time an operation is done to the discovered resource.\n\nAuthenticating Directly with a Downstream Cluster\n\nThis section intended to help you set up an alternative method to access an RKE cluster.\n\nThis method is only available for RKE clusters that have the authorized cluster endpoint enabled. When Rancher creates this RKE cluster, it generates a kubeconfig file that includes additional kubectl context(s) for accessing your cluster. This additional context allows you to use kubectl to authenticate with the downstream cluster without authenticating through Rancher. For a longer explanation of how the authorized cluster endpoint works, refer to this page.\n\nWe recommend that as a best practice, you should set up this method to access your RKE cluster, so that just in case you can’t connect to Rancher, you can still access the cluster.\n\n\nPrerequisites: The following steps assume that you have created a Kubernetes cluster and followed the steps to connect to your cluster with kubectl from your workstation.\n\n\nTo find the name of the context(s) in your downloaded kubeconfig file, run:\n\nkubectl config get-contexts --kubeconfig /custom/path/kube.config\nCURRENT   NAME                        CLUSTER                     AUTHINFO     NAMESPACE\n*         my-cluster                  my-cluster                  user-46tmn\n          my-cluster-controlplane-1   my-cluster-controlplane-1   user-46tmn\n\n\nIn this example, when you use kubectl with the first context, my-cluster, you will be authenticated through the Rancher server.\n\nWith the second context, my-cluster-controlplane-1, you would authenticate with the authorized cluster endpoint, communicating with an downstream RKE cluster directly.\n\nWe recommend using a load balancer with the authorized cluster endpoint. For details, refer to the recommended architecture section.\n\nNow that you have the name of the context needed to authenticate directly with the cluster, you can pass the name of the context in as an option when running kubectl commands. The commands will differ depending on whether your cluster has an FQDN defined. Examples are provided in the sections below.\n\nWhen kubectl works normally, it confirms that you can access your cluster while bypassing Rancher’s authentication proxy.\n\nConnecting Directly to Clusters with FQDN Defined\n\nIf an FQDN is defined for the cluster, a single context referencing the FQDN will be created. The context will be named <CLUSTER_NAME>-fqdn. When you want to use kubectl to access this cluster without Rancher, you will need to use this context.\n\nAssuming the kubeconfig file is located at ~/.kube/config:\n\nkubectl --context <CLUSTER_NAME>-fqdn get nodes\n\n\nDirectly referencing the location of the kubeconfig file:\n\nkubectl --kubeconfig /custom/path/kube.config --context <CLUSTER_NAME>-fqdn get pods\n\n\nConnecting Directly to Clusters without FQDN Defined\n\nIf there is no FQDN defined for the cluster, extra contexts will be created referencing the IP address of each node in the control plane. Each context will be named <CLUSTER_NAME>-<NODE_NAME>. When you want to use kubectl to access this cluster without Rancher, you will need to use this context.\n\nAssuming the kubeconfig file is located at ~/.kube/config:\n\nkubectl --context <CLUSTER_NAME>-<NODE_NAME> get nodes\n\n\nDirectly referencing the location of the kubeconfig file:\n\nkubectl --kubeconfig /custom/path/kube.config --context <CLUSTER_NAME>-<NODE_NAME> get pods\n\n","postref":"53abbb89333c81f86c599c4e6650f565","objectID":"43685117f69d3a01db1bdccbd01d4a95","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/cluster-access/kubectl/"},{"anchor":"#","title":"How the Authorized Cluster Endpoint Works","content":"This section describes how the kubectl CLI, the kubeconfig file, and the authorized cluster endpoint work together to allow you to access a downstream Kubernetes cluster directly, without authenticating through the Rancher server. It is intended to provide background information and context to the instructions for how to set up kubectl to directly access a cluster.\n\nAbout the kubeconfig File\n\nThe kubeconfig file is a file used to configure access to Kubernetes when used in conjunction with the kubectl command line tool (or other clients).\n\nThis kubeconfig file and its contents are specific to the cluster you are viewing. It can be downloaded from the cluster view in Rancher. You will need a separate kubeconfig file for each cluster that you have access to in Rancher.\n\nAfter you download the kubeconfig file, you will be able to use the kubeconfig file and its Kubernetes contexts to access your downstream cluster.\n\nTwo Authentication Methods for RKE Clusters\n\nIf the cluster is not an RKE cluster, the kubeconfig file allows you to access the cluster in only one way: it lets you be authenticated with the Rancher server, then Rancher allows you to run kubectl commands on the cluster.\n\nFor RKE clusters, the kubeconfig file allows you to be authenticated in two ways:\n\n\nThrough the Rancher server authentication proxy: Rancher’s authentication proxy validates your identity, then connects you to the downstream cluster that you want to access.\nDirectly with the downstream cluster’s API server: RKE clusters have an authorized cluster endpoint enabled by default. This endpoint allows you to access your downstream Kubernetes cluster with the kubectl CLI and a kubeconfig file, and it is enabled by default for RKE clusters. In this scenario, the downstream cluster’s Kubernetes API server authenticates you by calling a webhook (the kube-api-auth microservice) that Rancher set up.\n\n\nThis second method, the capability to connect directly to the cluster’s Kubernetes API server, is important because it lets you access your downstream cluster if you can’t connect to Rancher.\n\nTo use the authorized cluster endpoint, you will need to configure kubectl to use the extra kubectl context in the kubeconfig file that Rancher generates for you when the RKE cluster is created. This file can be downloaded from the cluster view in the Rancher UI, and the instructions for configuring kubectl are on this page.\n\nThese methods of communicating with downstream Kubernetes clusters are also explained in the architecture page in the larger context of explaining how Rancher works and how Rancher communicates with downstream clusters.\n\nAbout the kube-api-auth Authentication Webhook\n\nThe kube-api-auth microservice is deployed to provide the user authentication functionality for the authorized cluster endpoint, which is only available for RKE clusters. When you access the user cluster using kubectl, the cluster’s Kubernetes API server authenticates you by using the kube-api-auth service as a webhook.\n\nDuring cluster provisioning, the file /etc/kubernetes/kube-api-authn-webhook.yaml is deployed and kube-apiserver is configured with --authentication-token-webhook-config-file=/etc/kubernetes/kube-api-authn-webhook.yaml. This configures the kube-apiserver to query http://127.0.0.1:6440/v1/authenticate to determine authentication for bearer tokens.\n\nThe scheduling rules for kube-api-auth are listed below:\n\nApplies to v2.3.0 and higher\n\n\n\n\nComponent\nnodeAffinity nodeSelectorTerms\nnodeSelector\nTolerations\n\n\n\n\n\nkube-api-auth\nbeta.kubernetes.io/os:NotIn:windowsnode-role.kubernetes.io/controlplane:In:\"true\"\nnone\noperator:Exists\n\n\n\n","postref":"93148bbf5c92e4c89af5d3d5c81d4b27","objectID":"65810cfe98d2bb3de7ecff87d44dde88","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/cluster-access/ace/"},{"anchor":"#editing-cluster-membership","title":"Editing Cluster Membership","content":"Cluster administrators can edit the membership for a cluster, controlling which Rancher users can access the cluster and what features they can use.\nFrom the Global view, open the cluster that you want to add members to.\n\nFrom the main menu, select Members. Then click Add Member.\n\nSearch for the user or group that you want to add to the cluster.\n\nIf external authentication is configured:\n\n\nRancher returns users from your external authentication source as you type.\n\n\nUsing AD but can’t find your users?\nThere may be an issue with your search attribute configuration. See Configuring Active Directory Authentication: Step 5.\n\n\nA drop-down allows you to add groups instead of individual users. The drop-down only lists groups that you, the logged in user, are part of.\n\n\nNote: If you are logged in as a local user, external users do not display in your search results. For more information, see External Authentication Configuration and Principal Users.\n\n\n\nAssign the user or group Cluster roles.\n\nWhat are Cluster Roles?\n\n\nTip: For Custom Roles, you can modify the list of individual roles available for assignment.\n\n\nTo add roles to the list, Add a Custom Role.\nTo remove roles from the list, Lock/Unlock Roles.\n\n\nResult: The chosen users are added to the cluster.\nTo revoke cluster membership, select the user and click Delete. This action deletes membership, not the user.\nTo modify a user’s roles in the cluster, delete them from the cluster, and then re-add them with modified roles.\n","postref":"6cb0f2551dfa8c2f05d05387c2f9d08b","objectID":"1ad9e64ebf82070ed75466ee22348d1d","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/cluster-access/cluster-members/"},{"anchor":"#editing-cluster-membership","title":"Editing Cluster Membership","content":"When editing clusters, clusters that are launched using RKE feature more options than clusters that are imported or hosted by a Kubernetes provider. The headings that follow document options available only for RKE clusters.Updating ingress-nginxClusters that were created before Kubernetes 1.16 will have an ingress-nginx updateStrategy of OnDelete. Clusters that were created with Kubernetes 1.16 or newer will have RollingUpdate.If the updateStrategy of ingress-nginx is OnDelete, you will need to delete these pods to get the correct version for your deployment.Editing Other Cluster OptionsIn clusters launched by RKE, you can edit any of the remaining options that follow.\nNote: These options are not available for imported clusters or hosted Kubernetes clusters.\nOptions for RKE Clusters\n\n\n\nOption\nDescription\n\n\n\n\n\nKubernetes Version\nThe version of Kubernetes installed on each cluster node. For more detail, see Upgrading Kubernetes.\n\n\n\nNetwork Provider\nThe container networking interface that powers networking for your cluster.Note: You can only choose this option while provisioning your cluster. It cannot be edited later.\n\n\n\nProject Network Isolation\nAs of Rancher v2.0.7, if you’re using the Canal network provider, you can choose whether to enable or disable inter-project communication.\n\n\n\nNginx Ingress\nIf you want to publish your applications in a high-availability configuration, and you’re hosting your nodes with a cloud-provider that doesn’t have a native load-balancing feature, enable this option to use Nginx ingress within the cluster.\n\n\n\nMetrics Server Monitoring\nEach cloud provider capable of launching a cluster using RKE can collect metrics and monitor for your cluster nodes. Enable this option to view your node metrics from your cloud provider’s portal.\n\n\n\nPod Security Policy Support\nEnables pod security policies for the cluster. After enabling this option, choose a policy using the Default Pod Security Policy drop-down.\n\n\n\nDocker version on nodes\nConfigures whether nodes are allowed to run versions of Docker that Rancher doesn’t officially support. If you choose to require a supported Docker version, Rancher will stop pods from running on nodes that don’t have a supported Docker version installed.\n\n\n\nDocker Root Directory\nThe directory on your cluster nodes where you’ve installed Docker. If you install Docker on your nodes to a non-default directory, update this path.\n\n\n\nDefault Pod Security Policy\nIf you enable Pod Security Policy Support, use this drop-down to choose the pod security policy that’s applied to the cluster.\n\n\n\nCloud Provider\nIf you’re using a cloud provider to host cluster nodes launched by RKE, enable this option so that you can use the cloud provider’s native features. If you want to store persistent data for your cloud-hosted cluster, this option is required.\n\n\nEditing Cluster as YAML\nNote: In Rancher v2.0.5 and v2.0.6, the names of services in the Config File (YAML) should contain underscores only: kube_api and kube_controller.\nInstead of using the Rancher UI to choose Kubernetes options for the cluster, advanced users can create an RKE config file. Using a config file allows you to set any of the options available in an RKE installation, except for system_images configuration, by specifying them in YAML.\nTo edit an RKE config file directly from the Rancher UI, click Edit as YAML.\nTo read from an existing RKE file, click Read from File.\nIn Rancher v2.0.0-v2.2.x, the config file is identical to the  cluster config file for the Rancher Kubernetes Engine, which is the tool Rancher uses to provision clusters. In Rancher v2.3.0, the RKE information is still included in the config file, but it is separated from other options, so that the RKE cluster config options are nested under the rancher_kubernetes_engine_config directive. For more information, see the cluster configuration reference.For an example of RKE config file syntax, see the RKE documentation.Cluster administrators can edit the membership for a cluster, controlling which Rancher users can access the cluster and what features they can use.","postref":"2f9bd043c8f2cc83115af27131a6b98f","objectID":"ca7d6b19f67bf73dcfce5e273bb081fd","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/editing-clusters/"},{"anchor":"#","title":"Nodes and Node Pools","content":"After you launch a Kubernetes cluster in Rancher, you can manage individual nodes from the cluster’s Node tab. Depending on the option used to provision the cluster, there are different node options available.\n\nThis page covers the following topics:\n\n\nNode options for each type of cluster\nCordoning and draining nodes\nEditing a node\nViewing a node API\nDeleting a node\nScaling nodes\nSSH into a node hosted by an infrastructure provider\nManaging node pools\n\n\nTo manage individual nodes, browse to the cluster that you want to manage and then select Nodes from the main menu. You can open the options menu for a node by clicking its Ellipsis icon (…).\n\n\nNote: If you want to manage the cluster and not individual nodes, see Editing Clusters.\n\n\nNode Options for Each Type of Cluster\n\nThe following table lists which node options are available for each type of cluster in Rancher. Click the links in the Option column for more detailed information about each feature.\n\n\n\n\nOption\nNodes Hosted by an Infrastructure Provider\nCustom Node\nHosted Cluster\nImported Nodes\nDescription\n\n\n\n\n\nCordon\n✓\n✓\n✓\n\nMarks the node as unschedulable.\n\n\n\nDrain\n✓\n✓\n✓\n\nMarks the node as unschedulable and evicts all pods.\n\n\n\nEdit\n✓\n✓\n✓\n\nEnter a custom name, description, label, or taints for a node.\n\n\n\nView API\n✓\n✓\n✓\n\nView API data.\n\n\n\nDelete\n✓\n✓\n\n\nDeletes defective nodes from the cluster.\n\n\n\nDownload Keys\n✓\n\n\n\nDownload SSH key for in order to SSH into the node.\n\n\n\nNode Scaling\n✓\n\n\n\nScale the number of nodes in the node pool up or down.\n\n\n\n\nNotes for Node Pool Nodes\n\nClusters provisioned using one of the node pool options automatically maintain the node scale that’s set during the initial cluster provisioning. This scale determines the number of active nodes that Rancher maintains for the cluster.\n\nNotes for Nodes Provisioned by Hosted Kubernetes Providers\n\nOptions for managing nodes hosted by a Kubernetes provider are somewhat limited in Rancher. Rather than using the Rancher UI to make edits such as scaling the number of nodes up or down, edit the cluster directly.\n\nNotes for Imported Nodes\n\nAlthough you can deploy workloads to an imported cluster using Rancher, you cannot manage individual cluster nodes. All management of imported cluster nodes must take place outside of Rancher.\n\nCordoning and Draining Nodes\n\nCordoning a node marks it as unschedulable. This feature is useful for performing short tasks on the node during small maintenance windows, like reboots, upgrades, or decommissions.  When you’re done, power back on and make the node schedulable again by uncordoning it.\n\nDraining is the process of first cordoning the node, and then evicting all its pods. This feature is useful for performing node maintenance (like kernel upgrades or hardware maintenance). It prevents new pods from deploying to the node while redistributing existing pods so that users don’t experience service interruption.\n\nWhen nodes are drained, pods are handled with the following rules:\n\n\nFor pods with a replica set, the pod is replaced by a new pod that will be scheduled to a new node. Additionally, if the pod is part of a service, then clients will automatically be redirected to the new pod.\n\nFor pods with no replica set, you need to bring up a new copy of the pod, and assuming it is not part of a service, redirect clients to it.\n\n\nYou can drain nodes that are in either a cordoned or active state. When you drain a node, the node is cordoned, the nodes are evaluated for conditions they must meet to be drained, and then (if it meets the conditions) the node evicts its pods.\n\nHowever, you can override the conditions draining when you initiate the drain. You’re also given an opportunity to set a grace period and timeout value.\n\nThe node draining options are different based on your version of Rancher.\n\nAggressive and Safe Draining Options for Rancher v2.2.x+\n\nThere are two drain modes: aggressive and safe.\n\n\nAggressive Mode\n\nIn this mode, pods won’t get rescheduled to a new node, even if they do not have a controller. Kubernetes expects you to have your own logic that handles the deletion of these pods.\n\nKubernetes also expects the implementation to decide what to do with pods using emptyDir. If a pod uses emptyDir to store local data, you might not be able to safely delete it, since the data in the emptyDir will be deleted once the pod is removed from the node. Choosing aggressive mode will delete these pods.\n\nSafe Mode\n\nIf a node has standalone pods or ephemeral data it will be cordoned but not drained.\n\n\nAggressive and Safe Draining Options for Rancher Prior to v2.2.x\n\nThe following list describes each drain option:\n\n\nEven if there are pods not managed by a ReplicationController, ReplicaSet, Job, DaemonSet or StatefulSet\n\nThese types of pods won’t get rescheduled to a new node, since they do not have a controller. Kubernetes expects you to have your own logic that handles the deletion of these pods. Kubernetes forces you to choose this option (which will delete/evict these pods) or drain won’t proceed.\n\nEven if there are DaemonSet-managed pods\n\nSimilar to above, if you have any daemonsets, drain would proceed only if this option is selected. Even when this option is on, pods won’t be deleted since they’ll immediately be replaced. On startup, Rancher currently has a few daemonsets running by default in the system, so this option is turned on by default.\n\nEven if there are pods using emptyDir\n\nIf a pod uses emptyDir to store local data, you might not be able to safely delete it, since the data in the emptyDir will be deleted once the pod is removed from the node. Similar to the first option, Kubernetes expects the implementation to decide what to do with these pods. Choosing this option will delete these pods.\n\n\nGrace Period\n\nThe timeout given to each pod for cleaning things up, so they will have chance to exit gracefully. For example, when pods might need to finish any outstanding requests, roll back transactions or save state to some external storage. If negative, the default value specified in the pod will be used.\n\nTimeout\n\nThe amount of time drain should continue to wait before giving up.\n\n\nKubernetes Known Issue: Currently, the timeout setting is not enforced while draining a node. This issue will be corrected as of Kubernetes 1.12.\n\n\nDrained and Cordoned State\n\nIf there’s any error related to user input, the node enters a cordoned state because the drain failed. You can either correct the input and attempt to drain the node again, or you can abort by uncordoning the node.\n\nIf the drain continues without error, the node enters a draining state. You’ll have the option to stop the drain when the node is in this state, which will stop the drain process and change the node’s state to cordoned.\n\nOnce drain successfully completes, the node will be in a state of drained. You can then power off or delete the node.\n\n\nWant to know more about cordon and drain? See the Kubernetes documentation.\n\n\nEditing a Node\n\nEditing a node lets you:\n\n\nChange its name\nChange its description\nAdd labels\nAdd/Remove taints\n\n\nViewing a Node API\n\nSelect this option to view the node’s API endpoints.\n\nDeleting a Node\n\nUse Delete to remove defective nodes from the cloud provider. When you the delete a defective node, Rancher automatically replaces it with an identically provisioned node.\n\n\nTip: If your cluster is hosted by an infrastructure provider, and you want to scale your cluster down instead of deleting a defective node, scale down rather than delete.\n\n\nScaling Nodes\n\nFor nodes hosted by an infrastructure provider, you can scale the number of nodes in each node pool by using the scale controls. This option isn’t available for other cluster types.\n\nSSH into a Node Hosted by an Infrastructure Provider\n\nFor nodes hosted by an infrastructure provider, you have the option of downloading its SSH key so that you can connect to it remotely from your desktop.\n\n\nFrom the cluster hosted by an infrastructure provider, select Nodes from the main menu.\n\nFind the node that you want to remote into. Select Ellipsis (…) > Download Keys.\n\nStep Result: A ZIP file containing files used for SSH is downloaded.\n\nExtract the ZIP file to any location.\n\nOpen Terminal. Change your location to the extracted ZIP file.\n\nEnter the following command:\n\nssh -i id_rsa root@<IP_OF_HOST>\n\n\n\nManaging Node Pools\n\n\nPrerequisite: The options below are available only for clusters that are launched using RKE. The node pool features are not available for imported clusters or clusters hosted by a Kubernetes provider.\n\n\nIn clusters launched by RKE, you can:\n\n\nAdd new pools of nodes to your cluster. The nodes added to the pool are provisioned according to the node template that you use.\n\n\nClick + and follow the directions on screen to create a new template.\n\nYou can also reuse existing templates by selecting one from the Template drop-down.\n\n\nRedistribute Kubernetes roles amongst your node pools by making different checkbox selections\n\nScale the number of nodes in a pool u","postref":"414a272377e66ab9b8dcfc1f176de815","objectID":"aef0e58679e0717cef1ffdd14f422fe7","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/nodes/"},{"anchor":"#","title":"Kubernetes Persistent Storage: Volumes and Storage Classes","content":"When deploying an application that needs to retain data, you’ll need to create persistent storage. Persistent storage allows you to store application data external from the pod running your application. This storage practice allows you to maintain application data, even if the application’s pod fails.\n\nThe documents in this section assume that you understand the Kubernetes concepts of persistent volumes, persistent volume claims, and storage classes. For more information, refer to the section on how storage works.\n\nPrerequisites\n\nTo set up persistent storage, the Manage Volumes role is required.\n\nIf you are provisioning storage for a cluster hosted in the cloud, the storage and cluster hosts must have the same cloud provider.\n\nFor provisioning new storage with Rancher, the cloud provider must be enabled. For details on enabling cloud providers, refer to this page.\n\nFor attaching existing persistent storage to a cluster, the cloud provider does not need to be enabled.\n\nSetting up Existing Storage\n\nThe overall workflow for setting up existing storage is as follows:\n\n\nSet up persistent storage in an infrastructure provider.\nAdd a persistent volume (PV) that refers to the persistent storage.\nAdd a persistent volume claim (PVC) that refers to the PV.\nMount the PVC as a volume in your workload.\n\n\nFor details and prerequisites, refer to this page.\n\nDynamically Provisioning New Storage in Rancher\n\nThe overall workflow for provisioning new storage is as follows:\n\n\nAdd a storage class and configure it to use your storage provider.\nAdd a persistent volume claim (PVC) that refers to the storage class.\nMount the PVC as a volume for your workload.\n\n\nFor details and prerequisites, refer to this page.\n\nProvisioning Storage Examples\n\nWe provide examples of how to provision storage with NFS, vSphere, and Amazon’s EBS.\n\nGlusterFS Volumes\n\nIn clusters that store data on GlusterFS volumes, you may experience an issue where pods fail to mount volumes after restarting the kubelet. For details on preventing this from happening, refer to this page.\n\niSCSI Volumes\n\nIn Rancher Launched Kubernetes clusters that store data on iSCSI volumes, you may experience an issue where kubelets fail to automatically connect with iSCSI volumes. For details on resolving this issue, refer to this page.\n\nRelated Links\n\n\nKubernetes Documentation: Storage\n\n","postref":"9ecf39af1d9ce62f0ee4da1cbe6dae7b","objectID":"6d284ab704ebe8f14bf384c7b304fa99","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/volumes-and-storage/"},{"anchor":"#","title":"Projects and Kubernetes Namespaces with Rancher","content":"A namespace is a Kubernetes concept that allows a virtual cluster within a cluster, which is useful for dividing the cluster into separate “virtual clusters” that each have their own access control and resource quotas.\n\nA project is a group of namespaces, and it is a concept introduced by Rancher. Projects allow you to manage multiple namespaces as a group and perform Kubernetes operations in them. You can use projects to support multi-tenancy, so that a team can access a project within a cluster without having access to other projects in the same cluster.\n\nThis section describes how projects and namespaces work with Rancher. It covers the following topics:\n\n\nAbout namespaces\nAbout projects\n\n\nThe cluster’s default project\nThe system project\n\nProject authorization\nPod security policies\nCreating projects\nSwitching between clusters and projects\n\n\nAbout Namespaces\n\nA namespace is a concept introduced by Kubernetes. According to the official Kubernetes documentation on namespaces,\n\n\nKubernetes supports multiple virtual clusters backed by the same physical cluster. These virtual clusters are called namespaces. […] Namespaces are intended for use in environments with many users spread across multiple teams, or projects. For clusters with a few to tens of users, you should not need to create or think about namespaces at all.\n\n\nNamespaces provide the following functionality:\n\n\nProviding a scope for names: Names of resources need to be unique within a namespace, but not across namespaces. Namespaces can not be nested inside one another and each Kubernetes resource can only be in one namespace.\nResource quotas: Namespaces provide a way to divide cluster resources between multiple users.\n\n\nYou can assign resources at the project level so that each namespace in the project can use them. You can also bypass this inheritance by assigning resources explicitly to a namespace.\n\nYou can assign the following resources directly to namespaces:\n\n\nWorkloads\nLoad Balancers/Ingress\nService Discovery Records\nPersistent Volume Claims\nCertificates\nConfigMaps\nRegistries\nSecrets\n\n\nTo manage permissions in a vanilla Kubernetes cluster, cluster admins configure role-based access policies for each namespace. With Rancher, user permissions are assigned on the project level instead, and permissions are automatically inherited by any namespace owned by the particular project.\n\nFor more information on creating and moving namespaces, see Namespaces.\n\nRole-based access control issues with namespaces and kubectl\n\nBecause projects are a concept introduced by Rancher, kubectl does not have the capability to restrict the creation of namespaces to a project the creator has access to.\n\nThis means that when standard users with project-scoped permissions create a namespaces with kubectl, it may be unusable because kubectl doesn’t require the new namespace to be scoped within a certain project.\n\nIf your permissions are restricted to the project level, it is better to create a namespace through Rancher to ensure that you will have permission to access the namespace.\n\nIf a standard user is a project owner, the user will be able to create namespaces within that project. The Rancher UI will prevent that user from creating namespaces outside the scope of the projects they have access to.\n\nAbout Projects\n\nIn terms of hierarchy:\n\n\nClusters contain projects\nProjects contain namespaces\n\n\nYou can use projects to support multi-tenancy, so that a team can access a project within a cluster without having access to other projects in the same cluster.\n\nIn the base version of Kubernetes, features like role-based access rights or cluster resources are assigned to individual namespaces. A project allows you to save time by giving an individual or a team access to multiple namespaces simultaneously.\n\nYou can use projects to perform actions such as:\n\n\nAssign users to a group of namespaces (i.e., project membership).\nAssign users specific roles in a project. A role can be owner, member, read-only, or custom.\nAssign resources to the project.\nAssign Pod Security Policies.\n\n\nWhen you create a cluster, two projects are automatically created within it:\n\n\nDefault Project\nSystem Project\n\n\nThe Cluster’s Default Project\n\nWhen you provision a cluster with Rancher, it automatically creates a default project for the cluster. This is a project you can use to get started with your cluster, but you can always delete it and replace it with projects that have more descriptive names.\n\nIf you don’t have a need for more than the default namespace, you also do not need more than the Default project in Rancher.\n\nIf you require another level of organization beyond the Default project, you can create more projects in Rancher to isolate namespaces, applications and resources.\n\nThe System Project\n\nAvailable as of v2.0.7\n\nWhen troubleshooting, you can view the system project to check if important namespaces in the Kubernetes system are working properly. This easily accessible project saves you from troubleshooting individual system namespace containers.\n\nTo open it, open the Global menu, and then select the system project for your cluster.\n\nThe system project:\n\n\nIs automatically created when you provision a cluster.\nLists all namespaces that exist in v3/settings/system-namespaces, if they exist.\nAllows you to add more namespaces or move its namespaces to other projects.\nCannot be deleted because it’s required for cluster operations.\n\n\n\nNote: In clusters where both:\n\n\nThe Canal network plug-in is in use.\nThe Project Network Isolation option is enabled.\n\n\nThe system project overrides the Project Network Isolation option so that it can communicate with other projects, collect logs, and check health.\n\n\nProject Authorization\n\nStandard users are only authorized for project access in two situations:\n\n\nAn administrator, cluster owner or cluster member explicitly adds the standard user to the project’s Members tab.\nStandard users can access projects that they create themselves.\n\n\nPod Security Policies\n\nRancher extends Kubernetes to allow the application of Pod Security Policies at the project level in addition to the cluster level. However, as a best practice, we recommend applying Pod Security Policies at the cluster level.\n\nCreating Projects\n\nThis section describes how to create a new project with a name and with optional pod security policy, members, and resource quotas.\n\n\nName a new project.\nOptional: Select a pod security policy.\nRecommended: Add project members.\nOptional: Add resource quotas.\n\n\n1. Name a New Project\n\n\nFrom the Global view, choose Clusters from the main menu. From the Clusters page, open the cluster from which you want to create a project.\n\nFrom the main menu, choose Projects/Namespaces. Then click Add Project.\n\nEnter a Project Name.\n\n\n2. Optional: Select a Pod Security Policy\n\nThis option is only available if you’ve already created a Pod Security Policy. For instruction, see Creating Pod Security Policies.\n\nAssigning a PSP to a project will:\n\n\nOverride the cluster’s default PSP.\nApply the PSP to the project.\nApply the PSP to any namespaces you add to the project later.\n\n\n3. Recommended: Add Project Members\n\nUse the Members section to provide other users with project access and roles.\n\nBy default, your user is added as the project Owner.\n\n\nNotes on Permissions:\n\n\nUsers assigned the Owner or Member role for a project automatically inherit the namespace creation role. However, this role is a Kubernetes ClusterRole, meaning its scope extends to all projects in the cluster. Therefore, users explicitly assigned the Owner or Member role for a project can create namespaces in other projects they’re assigned to, even with only the Read Only role assigned.\nChoose Custom to create a custom role on the fly: Custom Project Roles.\n\n\n\nTo add members:\n\n\nClick Add Member.\nFrom the Name combo box, search for a user or group that you want to assign project access. Note: You can only search for groups if external authentication is enabled.\nFrom the Role drop-down, choose a role. For more information, refer to the documentation on project roles.\n\n\n4. Optional: Add Resource Quotas\n\nAvailable as of v2.1.0\n\nResource quotas limit the resources that a project (and its namespaces) can consume. For more information, see Resource Quotas.\n\nTo add a resource quota,\n\n\nClick Add Quota.\nSelect a Resource Type.\nEnter values for the Project Limit and the Namespace Default Limit.\nOptional: Specify Container Default Resource Limit, which will be applied to every container started in the project. The parameter is recommended if you have CPU or Memory limits set by the Resource Quota. It can be overridden on per an individual namespace or a container level. For more information, see Container Default Resource Limit Note: This option is available as of v2.2.0.\nClick Create.\n\n\nResult: Your project is created. You can view it from the cluster’s Projects/Namespaces view.\n\n\n\n\nField\nDescription\n\n\n\n\n\nProject Limit\nThe overall resource limit for the project","postref":"458a6ad658a84b66e99f1f3f79164e9e","objectID":"eb970444dd7985c52d67e1fd497fb0b1","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/projects-and-namespaces/"},{"anchor":"#notifiers-and-alerts","title":"Notifiers and Alerts","content":"Istio is an open-source tool that makes it easier for DevOps teams to observe, control, troubleshoot, and secure the traffic within a complex network of microservices. For details on how to enable Istio in Rancher, refer to the Istio section.Available as of v2.2.0Using Rancher, you can monitor the state and processes of your cluster nodes, Kubernetes components, and software deployments through integration with Prometheus, a leading open-source monitoring solution. For details, refer to the monitoring section.Logging is helpful because it allows you to:\nCapture and analyze the state of your cluster\nLook for trends in your environment\nSave your logs to a safe location outside of your cluster\nStay informed of events like a container crashing, a pod eviction, or a node dying\nMore easily debugg and troubleshoot problems\nRancher can integrate with Elasticsearch, splunk, kafka, syslog, and fluentd.For details, refer to the logging section.Notifiers and alerts are two features that work together to inform you of events in the Rancher system.Notifiers are services that inform you of alert events. You can configure notifiers to send alert notifications to staff best suited to take corrective action. Notifications can be sent with Slack, email, PagerDuty, WeChat, and webhooks.Alerts are rules that trigger those notifications. Before you can receive alerts, you must configure one or more notifier in Rancher. The scope for alerts can be set at either the cluster or project level.","postref":"b19a8c69435b02b7918dfc448b8c8f24","objectID":"831acba6fe8f1ca690089fda2a6ec86a","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/tools/"},{"anchor":"#prerequisites","title":"Prerequisites","content":"Move cluster-template.yml into the same directory as the Rancher CLI binary. Then run this command:./rancher up --file cluster-template.yml\nResult: Your cloned cluster begins provisioning. Enter ./rancher cluster ls to confirm. You can also log into the Rancher UI and open the Global view to watch your provisioning cluster’s progress.Use your favorite text editor to modify the cluster configuration in cluster-template.yml for your cloned cluster.\nNote: As of Rancher v2.3.0, cluster configuration directives must be nested under the rancher_kubernetes_engine_config directive in cluster.yml. For more information, refer to the section on the config file structure in Rancher v2.3.0+.\n\nOpen cluster-template.yml (or whatever you named your config) in your favorite text editor.\n\n\nWarning: Only edit the cluster config values explicitly called out below. Many of the values listed in this file are used to provision your cloned cluster, and editing their values may break the provisioning process.\n\n\nAs depicted in the example below, at the <CLUSTER_NAME> placeholder, replace your original cluster’s name with a unique name (<CLUSTER_NAME>). If your cloned cluster has a duplicate name, the cluster will not provision successfully.\nVersion: v3\nclusters:\n    <CLUSTER_NAME>: # ENTER UNIQUE NAME\n    dockerRootDir: /var/lib/docker\n    enableNetworkPolicy: false\n    rancherKubernetesEngineConfig:\n    addonJobTimeout: 30\n    authentication:\n        strategy: x509\n    authorization: {}\n    bastionHost: {}\n    cloudProvider: {}\n    ignoreDockerVersion: true\n\nFor each nodePools section, replace the original nodepool name with a unique name at the <NODEPOOL_NAME> placeholder.  If your cloned cluster has a duplicate nodepool name, the cluster will not provision successfully.\nnodePools:\n    <NODEPOOL_NAME>:\n    clusterId: do\n    controlPlane: true\n    etcd: true\n    hostnamePrefix: mark-do\n    nodeTemplateId: do\n    quantity: 1\n    worker: true\n\nWhen you’re done, save and close the configuration.\nBegin by using Rancher CLI to export the configuration for the cluster that you want to clone.\nOpen Terminal and change your directory to the location of the Rancher CLI binary, rancher.\n\nEnter the following command to list the clusters managed by Rancher.\n\n./rancher cluster ls\n\n\nFind the cluster that you want to clone, and copy either its resource ID or NAME to your clipboard. From this point on, we’ll refer to the resource ID or NAME as <RESOURCE_ID>, which is used as a placeholder in the next step.\n\nEnter the following command to export the configuration for your cluster.\n\n./rancher clusters export <RESOURCE_ID>\n\n\nStep Result: The YAML for a cloned cluster prints to Terminal.\n\nCopy the YAML to your clipboard and paste it in a new file. Save the file as cluster-template.yml (or any other name, as long as it has a .yml extension).\nDownload and install Rancher CLI. Remember to create an API bearer token if necessary.","postref":"5dfc2401ce49b534e55df64ca42f67c2","objectID":"7d16c87f5ce85fafafb498979635dd46","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/cloning-clusters/"},{"anchor":"#","title":"Certificate Rotation","content":"\nWarning: Rotating Kubernetes certificates may result in your cluster being temporarily unavailable as components are restarted. For production environments, it’s recommended to perform this action during a maintenance window.\n\n\nBy default, Kubernetes clusters require certificates and Rancher launched Kubernetes clusters automatically generate  certificates for the Kubernetes components. Rotating these certificates is important before the certificates expire as well as if a certificate is compromised. After the certificates are rotated, the Kubernetes components are automatically restarted.\n\nCertificates can be rotated for the following services:\n\n\netcd\nkubelet\nkube-apiserver\nkube-proxy\nkube-scheduler\nkube-controller-manager\n\n\nCertificate Rotation in Rancher v2.2.x\n\nAvailable as of v2.2.0\n\nRancher launched Kubernetes clusters have the ability to rotate the auto-generated certificates through the UI.\n\n\nIn the Global view, navigate to the cluster that you want to rotate certificates.\n\nSelect the Ellipsis (…) > Rotate Certificates.\n\nSelect which certificates that you want to rotate.\n\n\nRotate all Service certificates (keep the same CA)\nRotate an individual service and choose one of the services from the drop down menu\n\n\nClick Save.\n\n\nResults: The selected certificates will be rotated and the related services will be restarted to start using the new certificate.\n\n\nNote: Even though the RKE CLI can use custom certificates for the Kubernetes cluster components, Rancher currently doesn’t allow the ability to upload these in Rancher Launched Kubernetes clusters.\n\n\nCertificate Rotation in Rancher v2.1.x and v2.0.x\n\nAvailable as of v2.0.14 and v2.1.9\n\nRancher launched Kubernetes clusters have the ability to rotate the auto-generated certificates through the API.\n\n\nIn the Global view, navigate to the cluster that you want to rotate certificates.\n\nSelect the Ellipsis (…) > View in API.\n\nClick on RotateCertificates.\n\nClick on Show Request.\n\nClick on Send Request.\n\n\nResults: All Kubernetes certificates will be rotated.\n\nRotating Expired Certificates After Upgrading Older Rancher Versions\n\nIf you are upgrading from Rancher v2.0.13 or earlier, or v2.1.8 or earlier, and your clusters have expired certificates, some manual steps are required to complete the certificate rotation.\n\n\nFor the controlplane and etcd nodes, log in to each corresponding host and check if the certificate kube-apiserver-requestheader-ca.pem is in the following directory:\n\ncd /etc/kubernetes/.tmp\n\n\nIf the certificate is not in the directory, perform the following commands:\n\ncp kube-ca.pem kube-apiserver-requestheader-ca.pem\ncp kube-ca-key.pem kube-apiserver-requestheader-ca-key.pem\ncp kube-apiserver.pem kube-apiserver-proxy-client.pem\ncp kube-apiserver-key.pem kube-apiserver-proxy-client-key.pem\n\n\nIf the .tmp directory does not exist, you can copy the entire SSL certificate to .tmp:\n\ncp -r /etc/kubernetes/ssl /etc/kubernetes/.tmp\n\n\nRotate the certificates. For Rancher v2.0.x and v2.1.x, use the Rancher API. For Rancher 2.2.x, use the UI.\n\nAfter the command is finished, check if the worker nodes are Active. If not, log in to each worker node and restart the kubelet and proxy.\n\n","postref":"8718d294898fff5a9ba6e6f712b68430","objectID":"c096e1645f8ede495ca32615db372f8d","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/certificate-rotation/"},{"anchor":"#upgrade-cert-manager-only","title":"Upgrade Cert-Manager Only","content":"Cert-manager has deprecated the use of the certificate.spec.acme.solvers field and will drop support for it completely in an upcoming release.Per the cert-manager documentation, a new format for configuring ACME certificate resources was introduced in v0.8. Specifically, the challenge solver configuration field was moved. Both the old format and new are supported as of v0.9, but support for the old format will be dropped in an upcoming release of cert-manager. The cert-manager documentation strongly recommends that after upgrading you update your ACME Issuer and Certificate resources to the new format.Details about the change and migration instructions can be found in the cert-manager v0.7 to v0.8 upgrade instructions.The v0.11 release marks the removal of the v1alpha1 API that was used in previous versions of cert-manager, as well as our API group changing to be cert-manager.io instead of certmanager.k8s.io.We have also removed support for the old configuration format that was deprecated in the v0.8 release. This means you must transition to using the new solvers style configuration format for your ACME issuers before upgrading to v0.11. For more information, see the upgrading to v0.8 guide.Details about the change and migration instructions can be found in the cert-manager v0.10 to v0.11 upgrade instructions.More info about cert-manager upgrade information.\nNote:\nThese instructions are applied if you have no plan to upgrade Rancher.\nThe namespace used in these instructions depends on the namespace cert-manager is currently installed in. If it is in kube-system use that in the instructions below. You can verify by running kubectl get pods --all-namespaces and checking which namespace the cert-manager-* pods are listed in. Do not change the namespace cert-manager is running in or this can cause issues.\nThese instructions have been updated for Helm 3. If you are still using Helm 2, refer to these instructions.\nIn order to upgrade cert-manager, follow these instructions:\n  \n  Upgrading cert-manager with Internet access\n  \n    \nBack up existing resources as a precaution\nkubectl get -o yaml --all-namespaces \\\nissuer,clusterissuer,certificates,certificaterequests > cert-manager-backup.yaml\n\nImportant:\nIf you are upgrading from a version older than 0.11.0, Update the apiVersion on all your backed up resources from certmanager.k8s.io/v1alpha1 to cert-manager.io/v1alpha2. If you use any cert-manager annotations on any of your other resources, you will need to update them to reflect the new API group. For details, refer to the documentation on additional annotation changes.\n\n\nUninstall existing deployment\nhelm uninstall cert-manager\nDelete the CustomResourceDefinition using the link to the version vX.Y you installed\nkubectl delete -f https://raw.githubusercontent.com/jetstack/cert-manager/release-X.Y/deploy/manifests/00-crds.yaml\n\nInstall the CustomResourceDefinition resources separately\nkubectl apply -f https://raw.githubusercontent.com/jetstack/cert-manager/release-0.12/deploy/manifests/00-crds.yaml\n\nNote:\nIf you are running Kubernetes v1.15 or below, you will need to add the --validate=false flag to your kubectl apply command above. Otherwise, you will receive a validation error relating to the x-kubernetes-preserve-unknown-fields field in cert-manager’s CustomResourceDefinition resources. This is a benign error and occurs due to the way kubectl performs resource validation.\n\n\nCreate the namespace for cert-manager if needed\nkubectl create namespace cert-manager\n\nAdd the Jetstack Helm repository\nhelm repo add jetstack https://charts.jetstack.io\n\nUpdate your local Helm chart repository cache\nhelm repo update\n\nInstall the new version of cert-manager\nhelm install \\\n  cert-manager jetstack/cert-manager \\\n  --namespace cert-manager \\\n  --version v0.12.0 \n\nRestore back up resources\nkubectl apply -f cert-manager-backup.yaml\n\n\n  \n\n  \n  Upgrading cert-manager in an airgapped environment\n  \n    Prerequisites\n\nBefore you can perform the upgrade, you must prepare your air gapped environment by adding the necessary container images to your private registry and downloading or rendering the required Kubernetes manifest files.\n\n\nFollow the guide to Prepare your Private Registry with the images needed for the upgrade.\n\nFrom a system connected to the internet, add the cert-manager repo to Helm\nhelm repo add jetstack https://charts.jetstack.io\nhelm repo update\n\nFetch the latest cert-manager chart available from the Helm chart repository.\nhelm fetch jetstack/cert-manager --version v0.12.0\n\nRender the cert manager template with the options you would like to use to install the chart. Remember to set the image.repository option to pull the image from your private registry. This will create a cert-manager directory with the Kubernetes manifest files.\n\nThe Helm 3 command is as follows:\nhelm template cert-manager ./cert-manager-v0.12.0.tgz --output-dir . \\\n--namespace cert-manager \\\n--set image.repository=<REGISTRY.YOURDOMAIN.COM:PORT>/quay.io/jetstack/cert-manager-controller\n--set webhook.image.repository=<REGISTRY.YOURDOMAIN.COM:PORT>/quay.io/jetstack/cert-manager-webhook\n--set cainjector.image.repository=<REGISTRY.YOURDOMAIN.COM:PORT>/quay.io/jetstack/cert-manager-cainjector\nThe Helm 2 command is as follows:\nhelm template ./cert-manager-v0.12.0.tgz --output-dir . \\\n--name cert-manager --namespace cert-manager \\\n--set image.repository=<REGISTRY.YOURDOMAIN.COM:PORT>/quay.io/jetstack/cert-manager-controller\n--set webhook.image.repository=<REGISTRY.YOURDOMAIN.COM:PORT>/quay.io/jetstack/cert-manager-webhook\n--set cainjector.image.repository=<REGISTRY.YOURDOMAIN.COM:PORT>/quay.io/jetstack/cert-manager-cainjector\n\nDownload the required CRD file for cert-manager (old and new)\ncurl -L -o cert-manager/cert-manager-crd.yaml https://raw.githubusercontent.com/jetstack/cert-manager/release-0.12/deploy/manifests/00-crds.yaml\ncurl -L -o cert-manager/cert-manager-crd-old.yaml https://raw.githubusercontent.com/jetstack/cert-manager/release-X.Y/deploy/manifests/00-crds.yaml\n\n\nInstall cert-manager\n\n\nBack up existing resources as a precaution\nkubectl get -o yaml --all-namespaces \\\nissuer,clusterissuer,certificates,certificaterequests > cert-manager-backup.yaml\n\nImportant:\nIf you are upgrading from a version older than 0.11.0, Update the apiVersion on all your backed up resources from certmanager.k8s.io/v1alpha1 to cert-manager.io/v1alpha2. If you use any cert-manager annotations on any of your other resources, you will need to update them to reflect the new API group. For details, refer to the documentation on additional annotation changes.\n\n\nDelete the existing cert-manager installation\nkubectl -n cert-manager \\\ndelete deployment,sa,clusterrole,clusterrolebinding \\\n-l 'app=cert-manager' -l 'chart=cert-manager-v0.5.2'\nDelete the CustomResourceDefinition using the link to the version vX.Y you installed\nkubectl delete -f cert-manager/cert-manager-crd-old.yaml\n\nInstall the CustomResourceDefinition resources separately\nkubectl apply -f cert-manager/cert-manager-crd.yaml\n\nNote:\nIf you are running Kubernetes v1.15 or below, you will need to add the --validate=false flag to your kubectl apply command above. Otherwise, you will receive a validation error relating to the x-kubernetes-preserve-unknown-fields field in cert-manager’s CustomResourceDefinition resources. This is a benign error and occurs due to the way kubectl performs resource validation.\n\n\nCreate the namespace for cert-manager\nkubectl create namespace cert-manager\n\nInstall cert-manager\nkubectl -n cert-manager apply -R -f ./cert-manager\n\nRestore back up resources\nkubectl apply -f cert-manager-backup.yaml\n\n\n  \nOnce you’ve installed cert-manager, you can verify it is deployed correctly by checking the kube-system namespace for running pods:kubectl get pods --namespace cert-manager\n\nNAME                                       READY   STATUS    RESTARTS   AGE\ncert-manager-5c6866597-zw7kh               1/1     Running   0          2m\ncert-manager-cainjector-577f6d9fd7-tr77l   1/1     Running   0          2m\ncert-manager-webhook-787858fcdb-nlzsq      1/1     Running   0          2m\n","postref":"785051ab1a1db630f372bb5eb6e15b8d","objectID":"6c4336e313c855c4ca3344648690d02a","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/upgrading-cert-manager/"},{"anchor":"#upgrade-cert-manager-only","title":"Upgrade Cert-Manager Only","content":"Cert-manager has deprecated the use of the certificate.spec.acme.solvers field and will drop support for it completely in an upcoming release.Per the cert-manager documentation, a new format for configuring ACME certificate resources was introduced in v0.8. Specifically, the challenge solver configuration field was moved. Both the old format and new are supported as of v0.9, but support for the old format will be dropped in an upcoming release of cert-manager. The cert-manager documentation strongly recommends that after upgrading you update your ACME Issuer and Certificate resources to the new format.Details about the change and migration instructions can be found in the cert-manager v0.7 to v0.8 upgrade instructions.The v0.11 release marks the removal of the v1alpha1 API that was used in previous versions of cert-manager, as well as our API group changing to be cert-manager.io instead of certmanager.k8s.io.We have also removed support for the old configuration format that was deprecated in the v0.8 release. This means you must transition to using the new solvers style configuration format for your ACME issuers before upgrading to v0.11. For more information, see the upgrading to v0.8 guide.Details about the change and migration instructions can be found in the cert-manager v0.10 to v0.11 upgrade instructions.For information on upgrading from all other versions of cert-manager, refer to the official documentation.\nNote:\nThese instructions are applied if you have no plan to upgrade Rancher.\nThe namespace used in these instructions depends on the namespace cert-manager is currently installed in. If it is in kube-system use that in the instructions below. You can verify by running kubectl get pods --all-namespaces and checking which namespace the cert-manager-* pods are listed in. Do not change the namespace cert-manager is running in or this can cause issues.In order to upgrade cert-manager, follow these instructions:\n  \n  Upgrading cert-manager with Internet access\n  \n    \nBack up existing resources as a precaution\nkubectl get -o yaml --all-namespaces issuer,clusterissuer,certificates > cert-manager-backup.yaml\n\nDelete the existing deployment\nhelm delete --purge cert-manager\n\nInstall the CustomResourceDefinition resources separately\nkubectl apply -f https://raw.githubusercontent.com/jetstack/cert-manager/release-0.9/deploy/manifests/00-crds.yaml\n\nLabel the kube-system namespace to disable resource validation\nkubectl label namespace kube-system certmanager.k8s.io/disable-validation=true\n\nAdd the Jetstack Helm repository\nhelm repo add jetstack https://charts.jetstack.io\n\nUpdate your local Helm chart repository cache\nhelm repo update\n\nInstall the new version of cert-manager\nhelm install --version 0.12.0 --name cert-manager --namespace kube-system jetstack/cert-manager\n\n\n  \n\n  \n  Upgrading cert-manager in an airgapped environment\n  \n    Prerequisites\n\nBefore you can perform the upgrade, you must prepare your air gapped environment by adding the necessary container images to your private registry and downloading or rendering the required Kubernetes manifest files.\n\n\nFollow the guide to Prepare your Private Registry with the images needed for the upgrade.\n\nFrom a system connected to the internet, add the cert-manager repo to Helm\nhelm repo add jetstack https://charts.jetstack.io\nhelm repo update\n\nFetch the latest cert-manager chart available from the Helm chart repository.\nhelm fetch jetstack/cert-manager --version v0.12.0\n\nRender the cert manager template with the options you would like to use to install the chart. Remember to set the image.repository option to pull the image from your private registry. This will create a cert-manager directory with the Kubernetes manifest files.\nhelm template ./cert-manager-v0.12.0.tgz --output-dir . \\\n--name cert-manager --namespace kube-system \\\n--set image.repository=<REGISTRY.YOURDOMAIN.COM:PORT>/quay.io/jetstack/cert-manager-controller\n--set webhook.image.repository=<REGISTRY.YOURDOMAIN.COM:PORT>/quay.io/jetstack/cert-manager-webhook\n--set cainjector.image.repository=<REGISTRY.YOURDOMAIN.COM:PORT>/quay.io/jetstack/cert-manager-cainjector\n\nDownload the required CRD file for cert-manager\ncurl -L -o cert-manager/cert-manager-crd.yaml https://raw.githubusercontent.com/jetstack/cert-manager/release-0.9/deploy/manifests/00-crds.yaml\n\n\nInstall cert-manager\n\n\nBack up existing resources as a precaution\nkubectl get -o yaml --all-namespaces issuer,clusterissuer,certificates > cert-manager-backup.yaml\n\nDelete the existing cert-manager installation\nkubectl -n kube-system delete deployment,sa,clusterrole,clusterrolebinding -l 'app=cert-manager' -l 'chart=cert-manager-v0.5.2'\n\nInstall the CustomResourceDefinition resources separately\nkubectl apply -f cert-manager/cert-manager-crd.yaml\n\nLabel the kube-system namespace to disable resource validation\nkubectl label namespace kube-system certmanager.k8s.io/disable-validation=true\n\nInstall cert-manager\nkubectl -n kube-system apply -R -f ./cert-manager\n\n\n  \nOnce you’ve installed cert-manager, you can verify it is deployed correctly by checking the kube-system namespace for running pods:kubectl get pods --namespace kube-system\n\nNAME                                            READY   STATUS      RESTARTS   AGE\ncert-manager-7cbdc48784-rpgnt                   1/1     Running     0          3m\ncert-manager-webhook-5b5dd6999-kst4x            1/1     Running     0          3m\ncert-manager-cainjector-3ba5cd2bcd-de332x       1/1     Running     0          3m\nIf the ‘webhook’ pod (2nd line) is in a ContainerCreating state, it may still be waiting for the Secret to be mounted into the pod. Wait a couple of minutes for this to happen but if you experience problems, please check cert-manager’s troubleshooting guide.\nNote: The above instructions ask you to add the disable-validation label to the kube-system namespace. Here are additional resources that explain why this is necessary:\n\n\nInformation on the disable-validation label\nInformation on webhook validation for certificates\n\n","postref":"459306a2f412a0ba2f7afa97ab440ea4","objectID":"8bf7ad8e3dcaaee467aea1a82c75706b","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/upgrading-cert-manager/helm-2-instructions/"},{"anchor":"#","title":"Backing up etcd","content":"Available as of v2.2.0\n\nIn the Rancher UI, etcd backup and recovery for Rancher launched Kubernetes clusters can be easily performed. Snapshots of the etcd database are taken and saved either locally onto the etcd nodes or to a S3 compatible target. The advantages of configuring S3 is that if all etcd nodes are lost, your snapshot is saved remotely and can be used to restore the cluster.\n\nRancher recommends configuring recurrent etcd snapshots for all production clusters. Additionally, one-time snapshots can easily be taken as well.\n\n\nNote: If you have any Rancher launched Kubernetes clusters that were created prior to v2.2.0, after upgrading Rancher, you must edit the cluster and save it, in order to enable the updated snapshot features. Even if you were already creating snapshots prior to v2.2.0, you must do this step as the older snapshots will not be available to use to back up and restore etcd through the UI.\n\n\nSnapshot Creation Period and Retention Count\n\nSelect how often you want recurring snapshots to be taken as well as how many snapshots to keep. The amount of time is measured in hours. With timestamped snapshots, the user has the ability to do a point-in-time recovery.\n\nConfiguring Recurring Snapshots for the Cluster\n\nBy default, Rancher launched Kubernetes clusters are configured to take recurring snapshots (saved to local disk). To protect against local disk failure, using the S3 Target or replicating the path on disk is advised.\n\nDuring cluster provisioning or editing the cluster, the configuration for snapshots can be found in the advanced section for Cluster Options. Click on Show advanced options.\n\nIn the Advanced Cluster Options section, there are several options available to configure:\n\n\n\n\nOption\nDescription\nDefault Value\n\n\n\n\n\netcd Snapshot Backup Target\nSelect where you want the snapshots to be saved. Options are either local or in S3\nlocal\n\n\n\nRecurring etcd Snapshot Enabled\nEnable/Disable recurring snapshots\nYes\n\n\n\nRecurring etcd Snapshot Creation Period\nTime in hours between recurring snapshots\n12 hours\n\n\n\nRecurring etcd Snapshot Retention Count\nNumber of snapshots to retain\n6\n\n\n\n\nOne-Time Snapshots\n\nIn addition to recurring snapshots, you may want to take a “one-time” snapshot. For example, before upgrading the Kubernetes version of a cluster it’s best to backup the state of the cluster to protect against upgrade failure.\n\n\nIn the Global view, navigate to the cluster that you want to take a one-time snapshot.\n\nClick the Vertical Ellipsis (…) > Snapshot Now.\n\n\nResult: Based on your snapshot backup target, a one-time snapshot will be taken and saved in the selected backup target.\n\nSnapshot Backup Targets\n\nRancher supports two different backup targets:\n\n\nLocal Target\nS3 Target\n\n\nLocal Backup Target\n\nBy default, the local backup target is selected. The benefits of this option is that there is no external configuration. Snapshots are automatically saved locally to the etcd nodes in the Rancher launched Kubernetes clusters in /opt/rke/etcd-snapshots. All recurring snapshots are taken at configured intervals. The downside of using the local backup target is that if there is a total disaster and all etcd nodes are lost, there is no ability to restore the cluster.\n\nSafe Timestamps\n\nAvailable as of v2.3.0\n\nAs of v2.2.6, snapshot files are timestamped to simplify processing the files using external tools and scripts, but in some S3 compatible backends, these timestamps were unusable. As of Rancher v2.3.0, the option safe_timestamp is added to support compatible file names. When this flag is set to true, all special characters in the snapshot filename timestamp are replaced.\n\n\n\nNote: This option is not available directly in the UI, and is only available through the Edit as Yaml interface.\n\n\n\nS3 Backup Target\n\nThe S3 backup target allows users to configure a S3 compatible backend to store the snapshots. The primary benefit of this option is that if the cluster loses all the etcd nodes, the cluster can still be restored as the snapshots are stored externally. Rancher recommends external targets like S3 backup, however its configuration requirements do require additional effort that should be considered.\n\n\n\n\nOption\nDescription\nRequired\n\n\n\n\n\nS3 Bucket Name\nS3 bucket name where backups will be stored\n*\n\n\n\nS3 Region\nS3 region for the backup bucket\n\n\n\n\nS3 Region Endpoint\nS3 regions endpoint for the backup bucket\n*\n\n\n\nS3 Access Key\nS3 access key with permission to access the backup bucket\n*\n\n\n\nS3 Secret Key\nS3 secret key with permission to access the backup bucket\n*\n\n\n\nCustom CA Certificate\nA custom certificate used to access private S3 backends Available as of v2.2.5\n\n\n\n\n\nUsing a custom CA certificate for S3\n\nAvailable as of v2.2.5\n\nThe backup snapshot can be stored on a custom S3 backup like minio. If the S3 back end uses a self-signed or custom certificate, provide a custom certificate using the Custom CA Certificate option to connect to the S3 backend.\n\nIAM Support for Storing Snapshots in S3\n\nThe S3 backup target supports using IAM authentication to AWS API in addition to using API credentials. An IAM role gives temporary permissions that an application can use when making API calls to S3 storage. To use IAM authentication, the following requirements must be met:\n\n\nThe cluster etcd nodes must have an instance role that has read/write access to the designated backup bucket.\nThe cluster etcd nodes must have network access to the specified S3 endpoint.\nThe Rancher Server worker node(s) must have an instance role that has read/write to the designated backup bucket.\nThe Rancher Server worker node(s) must have network access to the specified S3 endpoint.\n\n\nTo give an application access to S3, refer to the AWS documentation on Using an IAM Role to Grant Permissions to Applications Running on Amazon EC2 Instances.\n\nViewing Available Snapshots\n\nThe list of all available snapshots for the cluster is available.\n\n\nIn the Global view, navigate to the cluster that you want to view snapshots.\n\nClick Tools > Snapshots from the navigation bar to view the list of saved snapshots. These snapshots include a timestamp of when they were created.\n\n","postref":"9bab843efe5eb0ad70753609e5003e43","objectID":"f7412e8203d595d8ae57cf517f36e70f","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/backing-up-etcd/"},{"anchor":"#viewing-available-snapshots","title":"Viewing Available Snapshots","content":"If the group of etcd nodes loses quorum, the Kubernetes cluster will report a failure because no operations, e.g. deploying workloads, can be executed in the Kubernetes cluster. Please review the best practices for the what the number of etcd nodes should be in a Kubernetes cluster. If you want to recover your set of etcd nodes, follow these instructions:\nKeep only one etcd node in the cluster by removing all other etcd nodes.\n\nOn the single remaining etcd node, run the following command:\n\n$ docker run --rm -v /var/run/docker.sock:/var/run/docker.sock assaflavie/runlike etcd\n\n\nThis command outputs the running command for etcd, save this command to use later.\n\nStop the etcd container that you launched in the previous step and rename it to etcd-old.\n\n$ docker stop etcd\n$ docker rename etcd etcd-old\n\n\nTake the saved command from Step 2 and revise it:\n\n\nIf you originally had more than 1 etcd node, then you need to change --initial-cluster to only contain the node that remains.\nAdd --force-new-cluster to the end of the command.\n\n\nRun the revised command.\n\nAfter the single nodes is up and running, Rancher recommends adding additional etcd nodes to your cluster. If you have a custom cluster and you want to reuse an old node, you are required to clean up the nodes before attempting to add them back into a cluster.\nIf your Kubernetes cluster is broken, you can restore the cluster from a snapshot.\nIn the Global view, navigate to the cluster that you want to view snapshots.\n\nClick the Vertical Ellipsis (…) > Restore Snapshot.\n\nSelect the snapshot that you want to use for restoring your cluster from the dropdown of available snapshots. Click Save.\n\n\nNote: Snapshots from S3 can only be restored from if the cluster is configured to take recurring snapshots on S3.\n\nResult: The cluster will go into updating state and the process of restoring the etcd nodes from the snapshot will start. The cluster is restored when it returns to an active state.\nNote: If you are restoring a cluster with unavailable etcd nodes, it’s recommended that all etcd nodes are removed from Rancher before attempting to restore. For clusters that were provisioned using nodes hosted in an infrastructure provider, new etcd nodes will automatically be created. For custom clusters, please ensure that you add new etcd nodes to the cluster.\nThe list of all available snapshots for the cluster is available.\nIn the Global view, navigate to the cluster that you want to view snapshots.\n\nClick Tools > Snapshots from the navigation bar to view the list of saved snapshots. These snapshots include a timestamp of when they were created.\n","postref":"d592744334773a05a91f3a3867cce400","objectID":"a40ade082e4602992b0a44b7aa43c690","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/restoring-etcd/"},{"anchor":"#what-gets-removed","title":"What Gets Removed?","content":"When a node is unreachable and removed from the cluster, the automatic cleaning process can’t be triggered because the node is unreachable. Please follow the steps below to manually remove the Rancher components.\nWarning: The commands listed below will remove data from the node. Make sure you have created a backup of files you want to keep before executing any of the commands as data will be lost.\nRemoving Rancher Components from Imported ClustersFor imported clusters, the process for removing Rancher is a little different. You have the option of simply deleting the cluster in the Rancher UI, or your can run a script that removes Rancher components from the nodes. Both options make the same deletions.After the imported cluster is detached from Rancher, the cluster’s workloads will be unaffected and you can access the cluster using the same methods that you did before the cluster was imported into Rancher.\n  \n  \n  \nWarning: This process will remove data from your cluster. Make sure you have created a backup of files you want to keep before executing the command, as data will be lost.\n\n\nAfter you initiate the removal of an imported cluster using the Rancher UI (or API), the following events occur.\n\n\nRancher creates a serviceAccount that it uses to remove the Rancher components from the cluster. This account is assigned the clusterRole and clusterRoleBinding permissions, which are required to remove the Rancher components.\n\nUsing the serviceAccount, Rancher schedules and runs a job that cleans the Rancher components off of the cluster. This job also references the serviceAccount and its roles as dependencies, so the job deletes them before its completion.\n\nRancher is removed from the cluster. However, the cluster persists, running the native version of Kubernetes.\n\n\nResult: All components listed for imported clusters in What Gets Removed? are deleted.\n\n\n\n\n  Rather than cleaning imported cluster nodes using the Rancher UI, you can run a script instead. This functionality is available since v2.1.0.\n\n\nPrerequisite:\n\nInstall kubectl.\n\n\n\nOpen a web browser, navigate to GitHub, and download user-cluster.sh.\n\nMake the script executable by running the following command from the same directory as user-cluster.sh:\n\nchmod +x user-cluster.sh\n\n\nAir Gap Environments Only: Open user-cluster.sh and replace yaml_url with the URL in user-cluster.yml.\n\nIf you don’t have an air gap environment, skip this step.\n\nFrom the same directory, run the script and provide the rancher/rancher-agent image version which should be equal to the version of Rancher used to manage the cluster. (<RANCHER_VERSION>):\n\n\nTip:\n\nAdd the -dry-run flag to preview the script’s outcome without making changes.\n\n./user-cluster.sh rancher/rancher-agent:<RANCHER_VERSION>\n\n\n\n\nResult: The script runs. All components listed for imported clusters in What Gets Removed? are deleted.\n\n\n\nWindows NodesTo clean up a Windows node, you can run a cleanup script located in c:\\etc\\rancher. The script deletes Kubernetes generated resources and the execution binary. It also drops the firewall rules and network settings.To run the script, you can use this command in the PowerShell:pushd c:\\etc\\rancher\n.\\cleanup.ps1\npopd\nResult: The node is reset and can be re-added to a Kubernetes cluster.Docker Containers, Images, and VolumesBased on what role you assigned to the node, there are Kubernetes components in containers, containers belonging to overlay networking, DNS, ingress controller and Rancher agent. (and pods you created that have been scheduled to this node)To clean all Docker containers, images and volumes:docker rm -f $(docker ps -qa)\ndocker rmi -f $(docker images -q)\ndocker volume rm $(docker volume ls -q)\nMountsKubernetes components and secrets leave behind mounts on the system that need to be unmounted.\n\n\nMounts\n\n\n\n\n\n/var/lib/kubelet/pods/XXX (miscellaneous mounts)\n\n\n\n/var/lib/kubelet\n\n\n\n/var/lib/rancher\n\n\nTo unmount all mounts:for mount in $(mount | grep tmpfs | grep '/var/lib/kubelet' | awk '{ print $3 }') /var/lib/kubelet /var/lib/rancher; do umount $mount; done\nDirectories and FilesThe following directories are used when adding a node to a cluster, and should be removed. You can remove a directory using rm -rf /directory_name.\nNote: Depending on the role you assigned to the node, some of the directories will or won’t be present on the node.\n\n\n\nDirectories\n\n\n\n\n\n/etc/ceph\n\n\n\n/etc/cni\n\n\n\n/etc/kubernetes\n\n\n\n/opt/cni\n\n\n\n/opt/rke\n\n\n\n/run/secrets/kubernetes.io\n\n\n\n/run/calico\n\n\n\n/run/flannel\n\n\n\n/var/lib/calico\n\n\n\n/var/lib/etcd\n\n\n\n/var/lib/cni\n\n\n\n/var/lib/kubelet\n\n\n\n/var/lib/rancher/rke/log\n\n\n\n/var/log/containers\n\n\n\n/var/log/pods\n\n\n\n/var/run/calico\n\n\nTo clean the directories:rm -rf /etc/ceph \\\n       /etc/cni \\\n       /etc/kubernetes \\\n       /opt/cni \\\n       /opt/rke \\\n       /run/secrets/kubernetes.io \\\n       /run/calico \\\n       /run/flannel \\\n       /var/lib/calico \\\n       /var/lib/etcd \\\n       /var/lib/cni \\\n       /var/lib/kubelet \\\n       /var/lib/rancher/rke/log \\\n       /var/log/containers \\\n       /var/log/pods \\\n       /var/run/calico\nNetwork Interfaces and IptablesThe remaining two components that are changed/configured are (virtual) network interfaces and iptables rules. Both are non-persistent to the node, meaning that they will be cleared after a restart of the node. To remove these components, a restart is recommended.To restart a node:# using reboot\n$ sudo reboot\n\n# using shutdown\n$ sudo shutdown -r now\nIf you want to know more on (virtual) network interfaces or iptables rules, please see the specific subjects below.Network Interfaces\nNote: Depending on the network provider configured for the cluster the node was part of, some of the interfaces will or won’t be present on the node.\n\n\n\nInterfaces\n\n\n\n\n\nflannel.1\n\n\n\ncni0\n\n\n\ntunl0\n\n\n\ncaliXXXXXXXXXXX (random interface names)\n\n\n\nvethXXXXXXXX (random interface names)\n\n\nTo list all interfaces:# Using ip\nip address show\n\n# Using ifconfig\nifconfig -a\nTo remove an interface:ip link delete interface_name\nIptables\nNote: Depending on the network provider configured for the cluster the node was part of, some of the chains will or won’t be present on the node.\nIptables rules are used to route traffic from and to containers. The created rules are not persistent, so restarting the node will restore iptables to its original state.\n\n\nChains\n\n\n\n\n\ncali-failsafe-in\n\n\n\ncali-failsafe-out\n\n\n\ncali-fip-dnat\n\n\n\ncali-fip-snat\n\n\n\ncali-from-hep-forward\n\n\n\ncali-from-host-endpoint\n\n\n\ncali-from-wl-dispatch\n\n\n\ncali-fw-caliXXXXXXXXXXX (random chain names)\n\n\n\ncali-nat-outgoing\n\n\n\ncali-pri-kns.NAMESPACE (chain per namespace)\n\n\n\ncali-pro-kns.NAMESPACE (chain per namespace)\n\n\n\ncali-to-hep-forward\n\n\n\ncali-to-host-endpoint\n\n\n\ncali-to-wl-dispatch\n\n\n\ncali-tw-caliXXXXXXXXXXX (random chain names)\n\n\n\ncali-wl-to-host\n\n\n\nKUBE-EXTERNAL-SERVICES\n\n\n\nKUBE-FIREWALL\n\n\n\nKUBE-MARK-DROP\n\n\n\nKUBE-MARK-MASQ\n\n\n\nKUBE-NODEPORTS\n\n\n\nKUBE-SEP-XXXXXXXXXXXXXXXX (random chain names)\n\n\n\nKUBE-SERVICES\n\n\n\nKUBE-SVC-XXXXXXXXXXXXXXXX (random chain names)\n\n\nTo list all iptables rules:iptables -L -t nat\niptables -L -t mangle\niptables -L\nWhen the node is in Active state, removing the node from a cluster will trigger a process to clean up the node. Please restart the node after the automatic cleanup process is done to make sure any non-persistent data is properly removed.To restart a node:# using reboot\n$ sudo reboot\n\n# using shutdown\n$ sudo shutdown -r now\nWhen cleaning nodes provisioned using Rancher, the following components are deleted based on the type of cluster node you’re removing.\n\n\nRemoved Component\nNodes Hosted by Infrastructure Provider\nCustom Nodes\nHosted Cluster\nImported Nodes\n\n\n\n\n\nThe Rancher deployment namespace (cattle-system by default)\n✓\n✓\n✓\n✓\n\n\n\nserviceAccount, clusterRoles, and clusterRoleBindings labeled by Rancher\n✓\n✓\n✓\n✓\n\n\n\nLabels, Annotations, and Finalizers\n✓\n✓\n✓\n✓\n\n\n\nRancher Deployment\n✓\n✓\n✓\n\n\n\n\nMachines, clusters, projects, and user custom resource definitions (CRDs)\n✓\n✓\n✓\n\n\n\n\nAll resources create under the management.cattle.io API Group\n✓\n✓\n✓\n\n\n\n\nAll CRDs created by Rancher v2.x\n✓\n✓\n✓\n\n\n\n","postref":"acba3909aeca8be197fd562b90be1711","objectID":"588f7f0345aef92744eab0847b37310e","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/cleaning-cluster-nodes/"},{"anchor":"#hosted-kubernetes-provider-authentication","title":"Hosted Kubernetes Provider Authentication","content":"When using Rancher to create a cluster hosted by a provider, you are prompted for authentication information. This information is required to access the provider’s API. For more information on how to obtain this information, see the following procedures:\nCreating a GKE Cluster\nCreating an EKS Cluster\nCreating an AKS Cluster\nCreating an ACK Cluster\nCreating a TKE Cluster\nCreating a CCE Cluster\n","postref":"a515306da7b79aec5d1eca6a3da7d0d0","objectID":"0010305e83a554404031acea90bfd41d","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/hosted-kubernetes-clusters/"},{"anchor":"#prerequisites-in-google-kubernetes-engine","title":"Prerequisites in Google Kubernetes Engine","content":"Use Rancher\n to set up and configure your Kubernetes cluster.\nFrom the Clusters page, click Add Cluster.\n\nChoose Google Kubernetes Engine.\n\nEnter a Cluster Name.\n\n\nUse Member Roles to configure user authorization for the cluster.\n\t\n\t\tClick Add Member to add users that can access the cluster.\n\t\tUse the Role drop-down to set permissions for each user.\n\t\n\t\n\n\n\nEither paste your service account private key in the Service Account text box or Read from a file. Then click Next: Configure Nodes.\n\n\nNote: After submitting your private key, you may have to enable the Google Kubernetes Engine API. If prompted, browse to the URL displayed in the Rancher UI to enable the API.\n\n\nSelect your Cluster Options, customize your Nodes and customize the Security for the GKE cluster. Review your options to confirm they’re correct. Then click Create.\n\n\tResult:\n\t\n\t\tYour cluster is created and assigned a state of Provisioning. Rancher is standing up your cluster.\n\t\tYou can access your cluster after its state is updated to Active.\n\t\tActive clusters are assigned two Projects, Default (containing the namespace default) and System (containing the namespaces cattle-system,ingress-nginx,kube-public and kube-system, if present).\n\t\n\nNote\nDeploying to GKE will incur charges.\nCreate a service account using Google Kubernetes Engine. GKE uses this account to operate your cluster. Creating this account also generates a private key used for authentication.The service account requires the following roles:\nCompute Viewer: roles/compute.viewer\nProject Viewer: roles/viewer\nKubernetes Engine Admin: roles/container.admin\nService Account User: roles/iam.serviceAccountUser\nGoogle Documentation: Creating and Enabling Service Accounts","postref":"16cea34323de6d4ff172da25a69a3717","objectID":"df882b14bd2221cf50eb1cb6d061a6ef","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/hosted-kubernetes-clusters/gke/"},{"anchor":"#prerequisites-in-amazon-web-services","title":"Prerequisites in Amazon Web Services","content":"This tutorial on the AWS Open Source Blog will walk you through how to set up an EKS cluster with Rancher, deploy a publicly accessible app to test the cluster, and deploy a sample project to track real-time geospatial data using a combination of other open-source software such as Grafana and InfluxDB.For more information on security and compliance with your Amazon EKS Kubernetes cluster, please see this documentation.To find information on any AWS Service events, please see this page.For any issues or troubleshooting details for your Amazon EKS Kubernetes cluster, please see this documentation.Use Rancher to set up and configure your Kubernetes cluster.\nFrom the Clusters page, click Add Cluster.\n\nChoose Amazon EKS.\n\nEnter a Cluster Name.\n\n\nUse Member Roles to configure user authorization for the cluster.\n\t\n\t\tClick Add Member to add users that can access the cluster.\n\t\tUse the Role drop-down to set permissions for each user.\n\t\n\t\n\n\n\nConfigure Account Access for the EKS cluster. Complete each drop-down and field using the information obtained in 2. Create Access Key and Secret Key.\n\n\n\n\nSetting\nDescription\n\n\n\n\n\nRegion\nFrom the drop-down choose the geographical region in which to build your cluster.\n\n\n\nAccess Key\nEnter the access key that you created in 2. Create Access Key and Secret Key.\n\n\n\nSecret Key\nEnter the secret key that you created in 2. Create Access Key and Secret Key.\n\n\n\n\nClick Next: Select Service Role. Then choose a service role.\n\n\n\n\nService Role\nDescription\n\n\n\n\n\nStandard: Rancher generated service role\nIf you choose this role, Rancher automatically adds a service role for use with the cluster.\n\n\n\nCustom: Choose from your existing service roles\nIf you choose this role, Rancher lets you choose from service roles that you’re already created within AWS. For more information on creating a custom service role in AWS, see the Amazon documentation.\n\n\n\n\nClick Next: Select VPC and Subnet.\n\nChoose an option for Public IP for Worker Nodes. Your selection for this option determines what options are available for VPC & Subnet.\n\n\n\n\nOption\nDescription\n\n\n\n\n\nYes\nWhen your cluster nodes are provisioned, they’re assigned a both a private and public IP address.\n\n\n\nNo: Private IPs only\nWhen your cluster nodes are provisioned, they’re assigned only a private IP address.If you choose this option, you must also choose a VPC & Subnet that allow your instances to access the internet. This access is required so that your worker nodes can connect to the Kubernetes control plane.\n\n\n\n\nNow choose a VPC & Subnet. For more information, refer to the AWS documentation for Cluster VPC Considerations. Follow one of the sets of instructions below based on your selection from the previous step.\n\n\nWhat Is Amazon VPC?\nVPCs and Subnets\n\n\n\n  \n  Public IP for Worker Nodes—Yes\n  \n    If you choose to assign a public IP address to your cluster’s worker nodes, you have the option of choosing between a VPC that’s automatically generated by Rancher (i.e., Standard: Rancher generated VPC and Subnet), or a VPC that you’re already created with AWS (i.e., Custom: Choose from your existing VPC and Subnets). Choose the option that best fits your use case.\n\n\nChoose a VPC and Subnet option.\n\n\n\n\nOption\nDescription\n\n\n\n\n\nStandard: Rancher generated VPC and Subnet\nWhile provisioning your cluster, Rancher generates a new VPC and Subnet.\n\n\n\nCustom: Choose from your exiting VPC and Subnets\nWhile provisioning your cluster, Rancher configures your nodes to use a VPC and Subnet that you’ve already created in AWS. If you choose this option, complete the remaining steps below.\n\n\n\n\nIf you’re using Custom: Choose from your existing VPC and Subnets:\n\n(If you’re using Standard, skip to step 11)\n\n\nMake sure Custom: Choose from your existing VPC and Subnets is selected.\n\nFrom the drop-down that displays, choose a VPC.\n\nClick Next: Select Subnets. Then choose one of the Subnets that displays.\n\nClick Next: Select Security Group.\n\n\n\n  \n\n\n\n  \n  Public IP for Worker Nodes—No: Private IPs only\n  \n    If you chose this option, you must also choose a VPC & Subnet that allow your instances to access the internet. This access is required so that your worker nodes can connect to the Kubernetes control plane. Follow the steps below.\n\n\nTip: When using only private IP addresses, you can provide your nodes internet access by creating a VPC constructed with two subnets, a private set and a public set.  The private set should have its route tables configured to point toward a NAT in the public set.  For more information on routing traffic from private subnets, please see the official AWS documentation.\n\n\n1. From the drop-down that displays, choose a VPC.\n\n1. Click **Next: Select Subnets**. Then choose one of the **Subnets** that displays.\n\n1. Click **Next: Select Security Group**.\n\n\n  \n\n\n\nChoose a Security Group. See the documentation below on how to create one.\n\nAmazon Documentation:\n\n\nCluster Security Group Considerations\nSecurity Groups for Your VPC\nCreate a Security Group\n\n\nClick Select Instance Options, and then edit the node options available. Instance type and size of your worker nodes affects how many IP addresses each worker node will have available. See this documentation for more information.\n\n\n\n\nOption\nDescription\n\n\n\n\n\nInstance Type\nChoose the hardware specs for the instance you’re provisioning.\n\n\n\nCustom AMI Override\nIf you want to use a custom Amazon Machine Image (AMI), specify it here. By default, Rancher will use the EKS-optimized AMI for the EKS version that you chose.\n\n\n\nMinimum ASG Size\nThe minimum number of instances that your cluster will scale to during low traffic, as controlled by Amazon Auto Scaling.\n\n\n\nMaximum ASG Size\nThe maximum number of instances that your cluster will scale to during high traffic, as controlled by Amazon Auto Scaling.\n\n\n\nUser Data\nCustom commands can to be passed to perform automated configuration tasks WARNING: Modifying this may cause your nodes to be unable to join the cluster. Note: Available as of v2.2.0\n\n\n\n\nClick Create.\n\n\tResult:\n\t\n\t\tYour cluster is created and assigned a state of Provisioning. Rancher is standing up your cluster.\n\t\tYou can access your cluster after its state is updated to Active.\n\t\tActive clusters are assigned two Projects, Default (containing the namespace default) and System (containing the namespaces cattle-system,ingress-nginx,kube-public and kube-system, if present).\n\t\nThe figure below illustrates the high-level architecture of Rancher 2.x. The figure depicts a Rancher Server installation that manages two Kubernetes clusters: one created by RKE and another created by EKS.\nNote\nDeploying to Amazon AWS will incur charges. For more information, refer to the EKS pricing page.\nTo set up a cluster on EKS, you will need to set up an Amazon VPC (Virtual Private Cloud). You will also need to make sure that the account you will be using to create the EKS cluster has the appropriate permissions. For details, refer to the official guide on Amazon EKS Prerequisites.Amazon VPCYou need to set up an Amazon VPC to launch the EKS cluster. The VPC enables you to launch AWS resources into a virtual network that you’ve defined. For more information, refer to the Tutorial: Creating a VPC with Public and Private Subnets for Your Amazon EKS Cluster.IAM PoliciesRancher needs access to your AWS account in order to provision and administer your Kubernetes clusters in Amazon EKS. You’ll need to create a user for Rancher in your AWS account and define what that user can access.\nCreate a user with programmatic access by following the steps here.\n\nNext, create an IAM policy that defines what this user has access to in your AWS account. It’s important to only grant this user minimal access within your account. Follow the steps here to create an IAM policy and attach it to your user.\n\nFinally, follow the steps here to create an access key and secret key for this user.\n\nNote: It’s important to regularly rotate your access and secret keys. See this documentation for more information.\nFor more detailed information on IAM policies for EKS, refer to the official documentation on Amazon EKS IAM Policies, Roles, and Permissions.","postref":"5f4e527208564c52965aa6fa9a7aafb2","objectID":"63b4c6c0a8809be28a9ed43216cf6740","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/hosted-kubernetes-clusters/eks/"},{"anchor":"#prerequisites-in-microsoft-azure","title":"Prerequisites in Microsoft Azure","content":"Use Rancher to set up and configure your Kubernetes cluster.\nFrom the Clusters page, click Add Cluster.\n\nChoose Azure Kubernetes Service.\n\nEnter a Cluster Name.\n\n\nUse Member Roles to configure user authorization for the cluster.\n\t\n\t\tClick Add Member to add users that can access the cluster.\n\t\tUse the Role drop-down to set permissions for each user.\n\t\n\t\n\n\n\nUse your subscription ID, tenant ID, app ID, and client secret to give your cluster access to AKS. If you don’t have all of that information, you can retrieve it using these instructions:\n\n\nApp ID and tenant ID: To get the app ID and tenant ID, you can go to the Azure Portal, then click Azure Active Directory, then click App registrations, then click the name of the service principal. The app ID and tenant ID are both on the app registration detail page.\nClient secret: If you didn’t copy the client secret when creating the service principal, you can get a new one if you go to the app registration detail page, then click Certificates & secrets, then click New client secret.\nSubscription ID: You can get the subscription ID is available in the portal from All services > Subscriptions.\n\n\nUse Cluster Options to choose the version of Kubernetes, what network provider will be used and if you want to enable project network isolation. To see more cluster options, click on Show advanced options.\n\n\nComplete the Account Access form using the output from your Service Principal. This information is used to authenticate with Azure.\n\nUse Nodes to provision each node in your cluster and choose a geographical region.\n\nMicrosoft Documentation: How to create and use an SSH public and private key pair\n\n\nClick Create.\n\n\nReview your options to confirm they’re correct. Then click Create.\n\n\tResult:\n\t\n\t\tYour cluster is created and assigned a state of Provisioning. Rancher is standing up your cluster.\n\t\tYou can access your cluster after its state is updated to Active.\n\t\tActive clusters are assigned two Projects, Default (containing the namespace default) and System (containing the namespaces cattle-system,ingress-nginx,kube-public and kube-system, if present).\n\t\n\nNote\nDeploying to AKS will incur charges.\nTo interact with Azure APIs, an AKS cluster requires an Azure Active Directory (AD) service principal. The service principal is needed to dynamically create and manage other Azure resources, and it provides credentials for your cluster to communicate with AKS. For more information about the service principal, refer to the AKS documentation.Before creating the service principal, you need to obtain the following information from the Microsoft Azure Portal:\nYour subscription ID\nYour tenant ID\nAn app ID (also called a client ID)\nClient secret\nA resource group\nThe below sections describe how to set up these prerequisites using either the Azure command line tool or the Azure portal.Setting Up the Service Principal with the Azure Command Line ToolYou can create the service principal by running this command:az ad sp create-for-rbac --skip-assignment\nThe result should show information about the new service principal:{\n  \"appId\": \"xxxx--xxx\",\n  \"displayName\": \"<SERVICE-PRINCIPAL-NAME>\",\n  \"name\": \"http://<SERVICE-PRINCIPAL-NAME>\",\n  \"password\": \"<SECRET>\",\n  \"tenant\": \"<TENANT NAME>\"\n}\nYou also need to add roles to the service principal so that it has privileges for communication with the AKS API. It also needs access to create and list virtual networks.Below is an example command for assigning the Contributor role to a service principal. Contributors can manage anything on AKS but cannot give access to others:az role assignment create \\\n  --assignee $appId \\\n  --scope /subscriptions/$<SUBSCRIPTION-ID>/resourceGroups/$<GROUP> \\\n  --role Contributor\nYou can also create the service principal and give it Contributor privileges by combining the two commands into one. In this command, the scope needs to provide a full path to an Azure resource:az ad sp create-for-rbac \\\n  --scope /subscriptions/$<SUBSCRIPTION-ID>/resourceGroups/$<GROUP> \\\n  --role Contributor\nSetting Up the Service Principal from the Azure PortalYou can also follow these instructions to set up a service principal and give it role-based access from the Azure Portal.\nGo to the Microsoft Azure Portal home page.\n\nClick Azure Active Directory.\n\nClick App registrations.\n\nClick New registration.\n\nEnter a name. This will be the name of your service principal.\n\nOptional: Choose which accounts can use the service principal.\n\nClick Register.\n\nYou should now see the name of your service principal under Azure Active Directory > App registrations.\n\nClick the name of your service principal. Take note of the tenant ID and application ID (also called app ID or client ID) so that you can use it when provisioning your AKS cluster. Then click Certificates & secrets.\n\nClick New client secret.\n\nEnter a short description, pick an expiration time, and click Add. Take note of the client secret so that you can use it when provisioning the AKS cluster.\nResult: You have created a service principal and you should be able to see it listed in the Azure Active Directory section under App registrations. You still need to give the service principal access to AKS.To give role-based access to your service principal,\nClick All Services in the left navigation bar. Then click Subscriptions.\n\nClick the name of the subscription that you want to associate with your Kubernetes cluster. Take note of the subscription ID so that you can use it when provisioning your AKS cluster.\n\nClick Access Control (IAM).\n\nIn the Add role assignment section, click Add.\n\nIn the Role field, select a role that will have access to AKS. For example, you can use the Contributor role, which has permission to manage everything except for giving access to other users.\n\nIn the Assign access to field, select Azure AD user, group, or service principal.\n\nIn the Select field, select the name of your service principal and click Save.\nResult: Your service principal now has access to AKS.","postref":"55fd214b94a3ff5e5f4230746f4675e6","objectID":"d916c3446ab6bb34dcad9710fb242ef2","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/hosted-kubernetes-clusters/aks/"},{"anchor":"#prerequisites","title":"Prerequisites","content":"\nFrom the Clusters page, click Add Cluster.\n\nChoose Alibaba ACK.\n\nEnter a Cluster Name.\n\n\nUse Member Roles to configure user authorization for the cluster.\n\t\n\t\tClick Add Member to add users that can access the cluster.\n\t\tUse the Role drop-down to set permissions for each user.\n\t\n\t\n\n\n\nConfigure Account Access for the ACK cluster. Choose the geographical region in which to build your cluster, and input the access key that was created as part of the prerequisite steps.\n\nClick Next: Configure Cluster, then choose cluster type, the version of Kubernetes and the availability zone.\n\nIf you choose Kubernetes as the cluster type, Click Next: Configure Master Nodes, then complete the Master Nodes form.\n\nClick Next: Configure Worker Nodes, then complete the Worker Nodes form.\n\nReview your options to confirm they’re correct. Then click Create.\n\n\tResult:\n\t\n\t\tYour cluster is created and assigned a state of Provisioning. Rancher is standing up your cluster.\n\t\tYou can access your cluster after its state is updated to Active.\n\t\tActive clusters are assigned two Projects, Default (containing the namespace default) and System (containing the namespaces cattle-system,ingress-nginx,kube-public and kube-system, if present).\n\t\n\nNote\nDeploying to ACK will incur charges.\n\nIn Aliyun, activate the following services in their respective consoles.\n\n\nContainer Service\nResource Orchestration Service\nRAM\n\n\nMake sure that the account you will be using to create the ACK cluster has the appropriate permissions. Referring to the official Alibaba Cloud documentation about Role authorization and Use the Container Service console as a RAM user for details.\n\nIn Alibaba Cloud, create an access key.\n\nIn Alibaba Cloud, create an SSH key pair. This key is used to access nodes in the Kubernetes cluster.\n","postref":"fb8f533b039c7b99ca5433a9b81b2b1b","objectID":"6bd977934d93bdfe23fc178ff6969e22","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/hosted-kubernetes-clusters/ack/"},{"anchor":"#prerequisites-in-tencent","title":"Prerequisites in Tencent","content":"\nFrom the Clusters page, click Add Cluster.\n\nChoose Tencent TKE.\n\nEnter a Cluster Name.\n\n\nUse Member Roles to configure user authorization for the cluster.\n\t\n\t\tClick Add Member to add users that can access the cluster.\n\t\tUse the Role drop-down to set permissions for each user.\n\t\n\t\n\n\n\nConfigure Account Access for the TKE cluster. Complete each drop-down and field using the information obtained in Prerequisites.\n\n\n\n\nOption\nDescription\n\n\n\n\n\nRegion\nFrom the drop-down chooses the geographical region in which to build your cluster.\n\n\n\nSecret ID\nEnter the Secret ID that you obtained from the Tencent Cloud Console.\n\n\n\nSecret Key\nEnter the Secret key that you obtained from Tencent Cloud Console.\n\n\n\n\nClick Next: Configure Cluster to set your TKE cluster configurations.\n\n\n\n\nOption\nDescription\n\n\n\n\n\nKubernetes Version\nThe TKE only supports Kubernetes version 1.10.5 now.\n\n\n\nNode Count\nEnter the amount of worker node you want to purchase for your Kubernetes cluster, up to 100.\n\n\n\nVPC\nSelect the VPC name that you have created in the Tencent Cloud Console.\n\n\n\nContainer Network CIDR\nEnter the CIDR range of your Kubernetes cluster, you may check the available range of the CIDR in the VPC service of the Tencent Cloud Console. Default to 172.16.0.0/16.\n\n\n\n\nNote: If you are editing the cluster in the cluster.yml instead of the Rancher UI, note that as of Rancher v2.3.0, cluster configuration directives must be nested under the rancher_kubernetes_engine_config directive in cluster.yml. For more information, refer to the section on the config file structure in Rancher v2.3.0+.\n\nClick Next: Select Instance Type to choose the instance type that will use for your TKE cluster.\n\n\n\n\nOption\nDescription\n\n\n\n\n\nAvailability Zone\nChoose the availability zone of the VPC region.\n\n\n\nSubnet\nSelect the Subnet that you have created within the VPC, and add a new one if you don’t have it in the chosen availability zone.\n\n\n\nInstance Type\nFrom the drop-down chooses the VM instance type that you want to use for the TKE cluster, default to S2.MEDIUM4 (CPU 2 Memory 4 GiB).\n\n\n\n\nClick Next: Configure Instance to configure the VM instance that will use for your TKE cluster.\n\n\n\n\nOption\nDescription\n\n\n\n\n\nOperating System\nThe name of the operating system, currently supports Centos7.2x86_64 or ubuntu16.04.1 LTSx86_64\n\n\n\nSecurity Group\nSecurity group ID, default does not bind any security groups.\n\n\n\nRoot Disk Type\nSystem disk type. System disk type restrictions are detailed in the CVM instance configuration.\n\n\n\nRoot Disk Size\nSystem disk size. Linux system adjustment range is 20 - 50G, step size is 1.\n\n\n\nData Disk Type\nData disk type, default value to the SSD cloud drive\n\n\n\nData Disk Size\nData disk size (GB), the step size is 10\n\n\n\nBand Width Type\nType of bandwidth, PayByTraffic or PayByHour\n\n\n\nBand Width\nPublic network bandwidth (Mbps)\n\n\n\nKey Pair\nKey id, after associating the key can be used to logging to the VM node\n\n\n\n\nClick Create.\n\n\tResult:\n\t\n\t\tYour cluster is created and assigned a state of Provisioning. Rancher is standing up your cluster.\n\t\tYou can access your cluster after its state is updated to Active.\n\t\tActive clusters are assigned two Projects, Default (containing the namespace default) and System (containing the namespaces cattle-system,ingress-nginx,kube-public and kube-system, if present).\n\t\n\nNote\nDeploying to TKE will incur charges.\n\nMake sure that the account you will be using to create the TKE cluster has the appropriate permissions by referring to the  Cloud Access Management documentation for details.\n\nCreate a Cloud API Secret ID and Secret Key.\n\nCreate a Private Network and Subnet in the region that you want to deploy your Kubernetes cluster.\n\nCreate a SSH key pair. This key is used to access the nodes in the Kubernetes cluster.\n","postref":"cdbcac3e22dacce35b7ef6e51f4b47b6","objectID":"45001ada3a6cdd102374584a924c3f5b","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/hosted-kubernetes-clusters/tke/"},{"anchor":"#prerequisites-in-huawei","title":"Prerequisites in Huawei","content":"\nFrom the Clusters page, click Add Cluster.\n\nChoose Huawei CCE.\n\nEnter a Cluster Name.\n\n\nUse Member Roles to configure user authorization for the cluster.\n\t\n\t\tClick Add Member to add users that can access the cluster.\n\t\tUse the Role drop-down to set permissions for each user.\n\t\n\t\n\n\n\nEnter Project Id, Access Key ID as Access Key and Secret Access Key Secret Key. Then Click Next: Configure cluster.\n\nFill the following cluster configuration:\n\n\n\n\nSettings\nDescription\n\n\n\n\n\nCluster Type\nWhich type or node you want to include into the cluster, VirtualMachine or BareMetal.\n\n\n\nDescription\nThe description of the cluster.\n\n\n\nMaster Version\nThe Kubernetes version.\n\n\n\nManagement Scale Count\nThe max node count of the cluster. The options are 50, 200 and 1000. The larger of the scale count, the more the cost.\n\n\n\nHigh Availability\nEnable master node high availability. The cluster with high availability enabled will have more cost.\n\n\n\nContainer Network Mode\nThe network mode used in the cluster. overlay_l2 and vpc-router is supported in VirtualMachine type and underlay_ipvlan is supported in BareMetal type\n\n\n\nContainer Network CIDR\nNetwork CIDR for the cluster.\n\n\n\nVPC Name\nThe VPC name which the cluster is going to deploy into. Rancher will create one if it is blank.\n\n\n\nSubnet Name\nThe Subnet name which the cluster is going to deploy into. Rancher will create one if it is blank.\n\n\n\nExternal Server\nThis option is reserved for the future we can enable CCE cluster public access via API. For now, it is always disabled.\n\n\n\nCluster Label\nThe labels for the cluster.\n\n\n\nHighway Subnet\nThis option is only supported in BareMetal type. It requires you to select a VPC with high network speed for the bare metal machines.\n\n\n\n\nNote: If you are editing the cluster in the cluster.yml instead of the Rancher UI, note that as of Rancher v2.3.0, cluster configuration directives must be nested under the rancher_kubernetes_engine_config directive in cluster.yml. For more information, refer to the section on the config file structure in Rancher v2.3.0+.\n\nFill the following node configuration of the cluster:\n\n\n\n\nSettings\nDescription\n\n\n\n\n\nZone\nThe available zone at where the node(s) of the cluster is deployed.\n\n\n\nBilling Mode\nThe bill mode for the cluster node(s). In VirtualMachine type, only Pay-per-use is supported. in BareMetal, you can choose Pay-per-use or Yearly/Monthly.\n\n\n\nValidity Period\nThis option only shows in Yearly/Monthly bill mode. It means how long you want to pay for the cluster node(s).\n\n\n\nAuto Renew\nThis option only shows in Yearly/Monthly bill mode. It means that the cluster node(s) will renew the Yearly/Monthly payment automatically or not.\n\n\n\nData Volume Type\nData volume type for the cluster node(s). SATA, SSD or SAS for this option.\n\n\n\nData Volume Size\nData volume size for the cluster node(s)\n\n\n\nRoot Volume Type\nRoot volume type for the cluster node(s). SATA, SSD or SAS for this option.\n\n\n\nRoot Volume Size\nRoot volume size for the cluster node(s)\n\n\n\nNode Flavor\nThe node flavor of the cluster node(s). The flavor list in Rancher UI is fetched from Huawei Cloud. It includes all the supported node flavors.\n\n\n\nNode Count\nThe node count of the cluster\n\n\n\nNode Operating System\nThe operating system for the cluster node(s). Only EulerOS 2.2 and CentOS 7.4 are supported right now.\n\n\n\nSSH Key Name\nThe ssh key for the cluster node(s)\n\n\n\nEIP\nThe public IP options for the cluster node(s). Disabled means that the cluster node(s) are not going to bind a public IP. Create EIP means that the cluster node(s) will bind one or many newly created Eips after provisioned and more options will be shown in the UI to set the to-create EIP parameters. And Select Existed EIP means that the node(s) will bind to the EIPs you select.\n\n\n\nEIP Count\nThis option will only be shown when Create EIP is selected. It means how many EIPs you want to create for the node(s).\n\n\n\nEIP Type\nThis option will only be shown when Create EIP is selected. The options are 5_bgp and 5_sbgp.\n\n\n\nEIP Share Type\nThis option will only be shown when Create EIP is selected. The only option is PER.\n\n\n\nEIP Charge Mode\nThis option will only be shown when Create EIP is selected. The options are pay by BandWidth and pay by Traffic.\n\n\n\nEIP Bandwidth Size\nThis option will only be shown when Create EIP is selected. The BandWidth of the EIPs.\n\n\n\nAuthentication Mode\nIt means enabling RBAC or also enabling Authenticating Proxy. If you select Authenticating Proxy, the certificate which is used for authenticating proxy will be also required.\n\n\n\nNode Label\nThe labels for the cluster node(s).\n\n\n\n\nClick Create to create the CCE cluster.\n\n\tResult:\n\t\n\t\tYour cluster is created and assigned a state of Provisioning. Rancher is standing up your cluster.\n\t\tYou can access your cluster after its state is updated to Active.\n\t\tActive clusters are assigned two Projects, Default (containing the namespace default) and System (containing the namespaces cattle-system,ingress-nginx,kube-public and kube-system, if present).\n\t\nHuawei CCE service doesn’t support the ability to create clusters with public access through their API. You are required to run Rancher in the same VPC as the CCE clusters that you want to provision.\nNote\nDeploying to CCE will incur charges.\n\nFind your project ID in Huawei CCE portal. See the CCE documentation on how to manage your projects.\n\nCreate an Access Key ID and Secret Access Key.\n","postref":"526b9aa50b37332213cca09b38241a22","objectID":"243de59846d9bd0a7b7198ba9094935b","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/hosted-kubernetes-clusters/cce/"},{"anchor":"#","title":"Launching Kubernetes with Rancher","content":"You can have Rancher launch a Kubernetes cluster using any nodes you want. When Rancher deploys Kubernetes onto these nodes, it uses Rancher Kubernetes Engine (RKE), which is Rancher’s own lightweight Kubernetes installer. It can launch Kubernetes on any computers, including:\n\n\nBare-metal servers\nOn-premise virtual machines\nVirtual machines hosted by an infrastructure provider\n\n\nRancher can install Kubernetes on existing nodes, or it can dynamically provision nodes in an infrastructure provider and install Kubernetes on them.\n\nRKE clusters include clusters that Rancher launched on Windows nodes or other existing custom nodes, as well as clusters that Rancher launched with new nodes on Azure, Digital Ocean, EC2, or vSphere.\n\nRequirements\n\nIf you use RKE to set up a cluster, your nodes must meet the requirements for nodes in downstream user clusters.\n\nLaunching Kubernetes on New Nodes in an Infrastructure Provider\n\nUsing Rancher, you can create pools of nodes based on a node template. This node template defines the parameters you want to use to launch nodes in your cloud providers.\n\nOne benefit of installing Kubernetes on node pools hosted by an infrastructure provider is that if a node loses connectivity with the cluster, Rancher can automatically create another node to join the cluster to ensure that the count of the node pool is as expected.\n\nFor more information, refer to the section on launching Kubernetes on new nodes.\n\nLaunching Kubernetes on Existing Custom Nodes\n\nIn this scenario, you want to install Kubernetes on bare-metal servers, on-premise virtual machines, or virtual machines that already exist in a cloud provider. With this option, you will run a Rancher agent Docker container on the machine.\n\nIf you want to reuse a node from a previous custom cluster, clean the node before using it in a cluster again. If you reuse a node that hasn’t been cleaned, cluster provisioning may fail.\n\nFor more information, refer to the section on custom nodes.\n","postref":"a2421df752ee5b187a4733e45918651d","objectID":"063d6d4a65e8330d2e017d51f02bd36b","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/"},{"anchor":"#","title":"Launching Kubernetes on New Nodes in an Infrastructure Provider","content":"Using Rancher, you can create pools of nodes based on a node template. This node template defines the parameters you want to use to launch nodes in your infrastructure providers or cloud providers.\n\nOne benefit of installing Kubernetes on node pools hosted by an infrastructure provider is that if a node loses connectivity with the cluster, Rancher can automatically create another node to join the cluster to ensure that the count of the node pool is as expected.\n\nThe available cloud providers to create a node template are decided based on active node drivers.\n\nThis section covers the following topics:\n\n\nNode templates\n\n\nNode labels\nNode taints\n\nNode pools\n\n\nNode pool taints\nAbout node auto-replace\nEnabling node auto-replace\nDisabling node auto-replace\n\nCloud credentials\nNode drivers\n\n\nNode Templates\n\nA node template is the saved configuration for the parameters to use when provisioning nodes in a specific cloud provider. These nodes can be launched from the UI. Rancher uses Docker Machine to provision these nodes. The available cloud providers to create node templates are based on the active node drivers in Rancher.\n\nAfter you create a node template in Rancher, it’s saved so that you can use this template again to create node pools. Node templates are bound to your login. After you add a template, you can remove them from your user profile.\n\nNode Labels\n\nYou can add labels on each node template, so that any nodes created from the node template will automatically have these labels on them.\n\nNode Taints\n\nAvailable as of Rancher v2.3.0\n\nYou can add taints on each node template, so that any nodes created from the node template will automatically have these taints on them.\n\nSince taints can be added at a node template and node pool, if there is no conflict with the same key and effect of the taints, all taints will be added to the nodes. If there are taints with the same key and different effect, the taints from the node pool will override the taints from the node template.\n\nNode Pools\n\nUsing Rancher, you can create pools of nodes based on a node template. The benefit of using a node pool is that if a node is destroyed or deleted, you can increase the number of live nodes to compensate for the node that was lost. The node pool helps you ensure that the count of the node pool is as expected.\n\nEach node pool is assigned with a node component to specify how these nodes should be configured for the Kubernetes cluster.\n\nNode Pool Taints\n\nAvailable as of Rancher v2.3.0\n\nIf you haven’t defined taints on your node template, you can add taints for each node pool. The benefit of adding taints at a node pool is beneficial over adding it at a node template is that you can swap out the node templates without worrying if the taint is on the node template.\n\nFor each taint, they will automatically be added to any created node in the node pool. Therefore, if you add taints to a node pool that have existing nodes, the taints won’t apply to existing nodes in the node pool, but any new node added into the node pool will get the taint.\n\nWhen there are taints on the node pool and node template, if there is no conflict with the same key and effect of the taints, all taints will be added to the nodes. If there are taints with the same key and different effect, the taints from the node pool will override the taints from the node template.\n\nAbout Node Auto-replace\n\nAvailable as of Rancher v2.3.0\n\nIf a node is in a node pool, Rancher can automatically replace unreachable nodes. Rancher will use the existing node template for the given node pool to recreate the node if it becomes inactive for a specified number of minutes.\n\n\nImportant: Self-healing node pools are designed to help you replace worker nodes for stateless applications. It is not recommended to enable node auto-replace on a node pool of master nodes or nodes with persistent volumes attached, because VMs are treated ephemerally. When a node in a node pool loses connectivity with the cluster, its persistent volumes are destroyed, resulting in data loss for stateful applications.\n\n\n\n  \n  How does Node Auto-replace Work?\n  \n    Node auto-replace works on top of the Kubernetes node controller. The node controller periodically checks the status of all the nodes (configurable via the --node-monitor-period flag of the kube-controller). When a node is unreachable, the node controller will taint that node. When this occurs, Rancher will begin its deletion countdown. You can configure the amount of time Rancher waits to delete the node. If the taint is not removed before the deletion countdown ends, Rancher will proceed to delete the node object. Rancher will then provision a node in accordance with the set quantity of the node pool.\n\n  \n\n\n\nEnabling Node Auto-replace\n\nWhen you create the node pool, you can specify the amount of time in minutes that Rancher will wait to replace an unresponsive node.\n\n\nIn the form for creating a cluster, go to the Node Pools section.\nGo to the node pool where you want to enable node auto-replace. In the Recreate Unreachable After field, enter the number of minutes that Rancher should wait for a node to respond before replacing the node.\nFill out the rest of the form for creating a cluster.\n\n\nResult: Node auto-replace is enabled for the node pool.\n\nYou can also enable node auto-replace after the cluster is created with the following steps:\n\n\nFrom the Global view, click the Clusters tab.\nGo to the cluster where you want to enable node auto-replace, click the vertical ellipsis (…), and click Edit.\nIn the Node Pools section, go to the node pool where you want to enable node auto-replace. In the Recreate Unreachable After field, enter the number of minutes that Rancher should wait for a node to respond before replacing the node.\nClick Save.\n\n\nResult: Node auto-replace is enabled for the node pool.\n\nDisabling Node Auto-replace\n\nYou can disable node auto-replace from the Rancher UI with the following steps:\n\n\nFrom the Global view, click the Clusters tab.\nGo to the cluster where you want to enable node auto-replace, click the vertical ellipsis (…), and click Edit.\nIn the Node Pools section, go to the node pool where you want to enable node auto-replace. In the Recreate Unreachable After field, enter 0.\nClick Save.\n\n\nResult: Node auto-replace is disabled for the node pool.\n\nCloud Credentials\n\nAvailable as of v2.2.0\n\nNode templates can use cloud credentials to store credentials for launching nodes in your cloud provider, which has some benefits:\n\n\nCredentials are stored as a Kubernetes secret, which is not only more secure, but it also allows you to edit a node template without having to enter your credentials every time.\n\nAfter the cloud credential is created, it can be re-used to create additional node templates.\n\nMultiple node templates can share the same cloud credential to create node pools. If your key is compromised or expired, the cloud credential can be updated in a single place, which allows all node templates that are using it to be updated at once.\n\n\n\nNote: As of v2.2.0, the default active node drivers and any node driver, that has fields marked as password, are required to use cloud credentials. If you have upgraded to v2.2.0, existing node templates will continue to work with the previous account access  information, but when you edit the node template, you will be required to create a cloud credential and the node template will start using it.\n\n\nAfter cloud credentials are created, the user can start managing the cloud credentials that they created.\n\nNode Drivers\n\nIf you don’t find the node driver that you want to use, you can see if it is available in Rancher’s built-in node drivers and activate it, or you can add your own custom node driver.\n","postref":"690a18559c5c94f7365fad603f15ee30","objectID":"c7494f46b9db49e84804209d3302c9b5","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/node-pools/"},{"anchor":"#","title":"Creating an Amazon EC2 Cluster","content":"Use Rancher to create a Kubernetes cluster in Amazon EC2.\n\nPrerequisites\n\n\nAWS EC2 Access Key and Secret Key that will be used to create the instances. See Amazon Documentation: Creating Access Keys how to create an Access Key and Secret Key.\nIAM Policy created to add to the user of the Access Key And Secret Key. See Amazon Documentation: Creating IAM Policies (Console) how to create an IAM policy. See our three example JSON policies below:\n\n\nExample IAM Policy\nExample IAM Policy with PassRole (needed if you want to use Kubernetes Cloud Provider or want to pass an IAM Profile to an instance)\nExample IAM Policy to allow encrypted EBS volumes\n\nIAM Policy added as Permission to the user. See Amazon Documentation: Adding Permissions to a User (Console) how to attach it to an user.\n\n\nCreating an EC2 Cluster\n\nThe steps to create a cluster differ based on your Rancher version.\n\n\n  \n  \n  \nCreate your cloud credentials\nCreate a node template with your cloud credentials and information from EC2\nCreate a cluster with node pools using the node template\n\n\n1. Create your cloud credentials\n\n\nIn the Rancher UI, click the user profile button in the upper right corner, and click Cloud Credentials.\nClick Add Cloud Credential.\nEnter a name for the cloud credential.\nIn the Cloud Credential Type field, select Amazon.\nIn the Region field, select the AWS region where your cluster nodes will be located.\nEnter your AWS EC2 Access Key and Secret Key.\nClick Create.\n\n\nResult: You have created the cloud credentials that will be used to provision nodes in your cluster. You can reuse these credentials for other node templates, or in other clusters.\n\n2. Create a node template with your cloud credentials and information from EC2\n\nComplete each of the following forms using information available from the EC2 Management Console.\n\n\nIn the Rancher UI, click the user profile button in the upper right corner, and click Node Templates.\nClick Add Template.\nIn the Region field, select the same region that you used when creating your cloud credentials.\nIn the Cloud Credentials field, select your newly created cloud credentials.\nClick Next: Authenticate & configure nodes.\nChoose an availability zone and network settings for your cluster. Click Next: Select a Security Group.\nChoose the default security group or configure a security group. Please refer to Amazon EC2 security group when using Node Driver to see what rules are created in the rancher-nodes Security Group. Then click Next: Set Instance options.\nConfigure the instances that will be created. Make sure you configure the correct SSH User for the configured AMI.\n\n\n\nIf you need to pass an IAM Instance Profile Name (not ARN), for example, when you want to use a Kubernetes Cloud Provider, you will need an additional permission in your policy. See Example IAM policy with PassRole for an example policy.\n\n\nOptional: In the Engine Options section of the node template, you can configure the Docker daemon. You may want to specify the docker version or a Docker registry mirror.\n\n3. Create a cluster with node pools using the node template\n\nAdd one or more node pools to your cluster.A node pool is a collection of nodes based on a node template. A node template defines the configuration of a node, like what operating system to use, number of CPUs and amount of memory. Each node pool must have one or more nodes roles assigned.\n\n\n    Notes:\n    \n        Each node role (i.e. etcd, Control Plane, and Worker) should be assigned to a distinct node pool. Although it is possible to assign multiple node roles to a node pool, this should not be done for production clusters. \n        The recommended setup is to have a node pool with the etcd node role and a count of three, a node pool with the Control Plane node role and a count of at least two, and a node pool with the Worker node role and a count of at least two. Regarding the etcd node role, refer to the etcd Admin Guide.\n    \n\n\n\nFrom the Clusters page, click Add Cluster.\n\nChoose Amazon EC2.\n\nEnter a Cluster Name.\n\nCreate a node pool for each Kubernetes role. For each node pool, choose a node template that you created.\n\nClick Add Member to add users that can access the cluster.\n\nUse the Role drop-down to set permissions for each user.\n\nUse Cluster Options to choose the version of Kubernetes, what network provider will be used and if you want to enable project network isolation. Refer to Selecting Cloud Providers to configure the Kubernetes Cloud Provider.\n\nClick Create.\n\n\n\n    Result:\n    \n        Your cluster is created and assigned a state of Provisioning. Rancher is standing up your cluster.\n        You can access your cluster after its state is updated to Active.\n        Active clusters are assigned two Projects, Default (containing the namespace default) and System (containing the namespaces cattle-system,ingress-nginx,kube-public and kube-system, if present).\n    \n\n\n\n\n\n  \nFrom the Clusters page, click Add Cluster.\n\nChoose Amazon EC2.\n\nEnter a Cluster Name.\n\n\nUse Member Roles to configure user authorization for the cluster.\n\n    Click Add Member to add users that can access the cluster.\n    Use the Role drop-down to set permissions for each user.\n\n\n\n\nUse Cluster Options to choose the version of Kubernetes, what network provider will be used and if you want to enable project network isolation. To see more cluster options, click on Show advanced options.\nRefer to Selecting Cloud Providers to configure the Kubernetes Cloud Provider.\n\nAdd one or more node pools to your cluster.A node pool is a collection of nodes based on a node template. A node template defines the configuration of a node, like what operating system to use, number of CPUs and amount of memory. Each node pool must have one or more nodes roles assigned.\n\n\n\n    Notes:\n    \n        Each node role (i.e. etcd, Control Plane, and Worker) should be assigned to a distinct node pool. Although it is possible to assign multiple node roles to a node pool, this should not be done for production clusters. \n        The recommended setup is to have a node pool with the etcd node role and a count of three, a node pool with the Control Plane node role and a count of at least two, and a node pool with the Worker node role and a count of at least two. Regarding the etcd node role, refer to the etcd Admin Guide.\n    \n\n\n\nClick Add Node Template.\n\nComplete each of the following forms using information available from the EC2 Management Console.\n\n\nAccount Access is where you configure the region of the nodes, and the credentials (Access Key and Secret Key) used to create the machine. See Prerequisites how to create the Access Key and Secret Key and the needed permissions.\nZone and Network configures the availability zone and network settings for your cluster.\nSecurity Groups creates or configures the Security Groups applied to your nodes. Please refer to Amazon EC2 security group when using Node Driver to see what rules are created in the rancher-nodes Security Group.\nInstance configures the instances that will be created. Make sure you configure the correct SSH User for the configured AMI.\n\nIf you need to pass an IAM Instance Profile Name (not ARN), for example, when you want to use a Kubernetes Cloud Provider, you will need an additional permission in your policy. See Example IAM policy with PassRole for an example policy.\n\n\nThe Docker daemon configuration options include:\n\n    \n        \n            Labels: For information on labels, refer to the  Docker\n                object label documentation.\n        \n    \n    \n        \n            Docker Engine Install URL: Determines what Docker version will be installed on the instance. Note: If you are using RancherOS, please check what Docker versions are available using sudo ros engine list on the RancherOS version you want to use, as the default Docker version configured might not be available. If you experience issues installing Docker on other operating systems, please try to install Docker manually using the configured Docker Engine Install URL to troubleshoot.\n        \n    \n    \n        \n            Registry mirrors: Docker Registry mirror to be used by the Docker daemon\n        \n    \n    \n        Other advanced options: Refer to the Docker daemon option reference\n          \n        \n    \n\n\nClick Create.\n\nOptional: Add additional node pools.\n\nReview your cluster settings to confirm they are correct. Then click Create.\n\n\n\n    Result:\n    \n        Your cluster is created and assigned a state of Provisioning. Rancher is standing up your cluster.\n        You can access your cluster after its state is updated to Active.\n        Active clusters are assigned two Projects, Default (containing the namespace default) and System (containing the namespaces cattle-system,ingress-nginx,kube-public and kube-system, if present).\n    \n\n\n\n\n\n\n\nOptional Next Steps\n\nAfter creating your cluster, you can access it through the Rancher UI. As a best practice, we recommend setting up these alternate ways of accessing yo","postref":"3bdc0fc6625189da9d91d09ea30b2466","objectID":"51f2526a28e7509f9044ba3c35b00768","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/node-pools/ec2/"},{"anchor":"#","title":"Creating a DigitalOcean Cluster","content":"Use Rancher\n to create a Kubernetes cluster using DigitalOcean.\n\n\nFrom the Clusters page, click Add Cluster.\n\nChoose DigitalOcean.\n\nEnter a Cluster Name.\n\n\nUse Member Roles to configure user authorization for the cluster.\n\t\n\t\tClick Add Member to add users that can access the cluster.\n\t\tUse the Role drop-down to set permissions for each user.\n\t\n\t\n\n\n\nUse Cluster Options to choose the version of Kubernetes, what network provider will be used and if you want to enable project network isolation. To see more cluster options, click on Show advanced options.\n\n\nAdd one or more node pools to your cluster.A node pool is a collection of nodes based on a node template. A node template defines the configuration of a node, like what operating system to use, number of CPUs and amount of memory. Each node pool must have one or more nodes roles assigned.\n\n\n\tNotes:\n\t\n\t\tEach node role (i.e. etcd, Control Plane, and Worker) should be assigned to a distinct node pool. Although it is possible to assign multiple node roles to a node pool, this should not be done for production clusters. \n\t\tThe recommended setup is to have a node pool with the etcd node role and a count of three, a node pool with the Control Plane node role and a count of at least two, and a node pool with the Worker node role and a count of at least two. Regarding the etcd node role, refer to the etcd Admin Guide.\n\t\n\n\n\n\nClick Add Node Template. Note: As of v2.2.0, account access information is stored as a cloud credentials. Cloud credentials are stored as Kubernetes secrets. Multiple node templates can use the same cloud credential. You can use an existing cloud credential or create a new one. To create a new cloud credential, enter Name and Account Access data, then click Create.\n\nComplete the Digital Ocean Options form.\n\n\nAccess Token stores your DigitalOcean Personal Access Token. Refer to DigitalOcean Instructions: How To Generate a Personal Access Token.\n\nDroplet Options provision your cluster’s geographical region and specifications.\n\n\n\tThe Docker daemon configuration options include:\n\t\n\t\t\n\t\t\t\n\t\t\t\tLabels: For information on labels, refer to the  Docker\n\t\t\t\t\tobject label documentation.\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\tDocker Engine Install URL: Determines what Docker version will be installed on the instance. Note: If you are using RancherOS, please check what Docker versions are available using sudo ros engine list on the RancherOS version you want to use, as the default Docker version configured might not be available. If you experience issues installing Docker on other operating systems, please try to install Docker manually using the configured Docker Engine Install URL to troubleshoot.\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\tRegistry mirrors: Docker Registry mirror to be used by the Docker daemon\n\t\t\t\n\t\t\n\t\t\n\t\t\tOther advanced options: Refer to the Docker daemon option reference\n\t\t\t  \n\t\t    \n\t\t\n\t\n\n\nClick Create.\n\nOptional: Add additional node pools.\n\n\n\nReview your options to confirm they’re correct. Then click Create.\n\n\n\n\tResult:\n\t\n\t\tYour cluster is created and assigned a state of Provisioning. Rancher is standing up your cluster.\n\t\tYou can access your cluster after its state is updated to Active.\n\t\tActive clusters are assigned two Projects, Default (containing the namespace default) and System (containing the namespaces cattle-system,ingress-nginx,kube-public and kube-system, if present).\n\t\n\n\n\nOptional Next Steps\n\nAfter creating your cluster, you can access it through the Rancher UI. As a best practice, we recommend setting up these alternate ways of accessing your cluster:\n\n\nAccess your cluster with the kubectl CLI: Follow these steps to access clusters with kubectl on your workstation. In this case, you will be authenticated through the Rancher server’s authentication proxy, then Rancher will connect you to the downstream cluster. This method lets you manage the cluster without the Rancher UI.\nAccess your cluster with the kubectl CLI, using the authorized cluster endpoint: Follow these steps to access your cluster with kubectl directly, without authenticating through Rancher. We recommend setting up this alternative method to access your cluster so that in case you can’t connect to Rancher, you can still access the cluster.\n\n","postref":"84c0860c6c7fd42273feb642049b9d2c","objectID":"e76e79e029f1cbeb0d064c5f1d9612f0","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/node-pools/digital-ocean/"},{"anchor":"#","title":"Creating an Azure Cluster","content":"Use Rancher\n to create a Kubernetes cluster in Azure.\n\n\nFrom the Clusters page, click Add Cluster.\n\nChoose Azure.\n\nEnter a Cluster Name.\n\n\nUse Member Roles to configure user authorization for the cluster.\n\t\n\t\tClick Add Member to add users that can access the cluster.\n\t\tUse the Role drop-down to set permissions for each user.\n\t\n\t\n\n\n\nUse Cluster Options to choose the version of Kubernetes, what network provider will be used and if you want to enable project network isolation. To see more cluster options, click on Show advanced options.\n\n\nAdd one or more node pools to your cluster.A node pool is a collection of nodes based on a node template. A node template defines the configuration of a node, like what operating system to use, number of CPUs and amount of memory. Each node pool must have one or more nodes roles assigned.\n\n\n\tNotes:\n\t\n\t\tEach node role (i.e. etcd, Control Plane, and Worker) should be assigned to a distinct node pool. Although it is possible to assign multiple node roles to a node pool, this should not be done for production clusters. \n\t\tThe recommended setup is to have a node pool with the etcd node role and a count of three, a node pool with the Control Plane node role and a count of at least two, and a node pool with the Worker node role and a count of at least two. Regarding the etcd node role, refer to the etcd Admin Guide.\n\t\n\n\n\n\nClick Add Node Template.\n\nComplete the Azure Options form.\n\n\nAccount Access stores your account information for authenticating with Azure. Note: As of v2.2.0, account access information is stored as a cloud credentials. Cloud credentials are stored as Kubernetes secrets. Multiple node templates can use the same cloud credential. You can use an existing cloud credential or create a new one. To create a new cloud credential, enter Name and Account Access data, then click Create.\n\nPlacement sets the geographical region where your cluster is hosted and other location metadata.\n\nNetwork configures the networking used in your cluster.\n\nInstance customizes your VM configuration.\n\n\n\tThe Docker daemon configuration options include:\n\t\n\t\t\n\t\t\t\n\t\t\t\tLabels: For information on labels, refer to the  Docker\n\t\t\t\t\tobject label documentation.\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\tDocker Engine Install URL: Determines what Docker version will be installed on the instance. Note: If you are using RancherOS, please check what Docker versions are available using sudo ros engine list on the RancherOS version you want to use, as the default Docker version configured might not be available. If you experience issues installing Docker on other operating systems, please try to install Docker manually using the configured Docker Engine Install URL to troubleshoot.\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\tRegistry mirrors: Docker Registry mirror to be used by the Docker daemon\n\t\t\t\n\t\t\n\t\t\n\t\t\tOther advanced options: Refer to the Docker daemon option reference\n\t\t\t  \n\t\t    \n\t\t\n\t\n\n\nClick Create.\n\nOptional: Add additional node pools.\n\n\n\nReview your options to confirm they’re correct. Then click Create.\n\n\n\n\tResult:\n\t\n\t\tYour cluster is created and assigned a state of Provisioning. Rancher is standing up your cluster.\n\t\tYou can access your cluster after its state is updated to Active.\n\t\tActive clusters are assigned two Projects, Default (containing the namespace default) and System (containing the namespaces cattle-system,ingress-nginx,kube-public and kube-system, if present).\n\t\n\n\n\nOptional Next Steps\n\nAfter creating your cluster, you can access it through the Rancher UI. As a best practice, we recommend setting up these alternate ways of accessing your cluster:\n\n\nAccess your cluster with the kubectl CLI: Follow these steps to access clusters with kubectl on your workstation. In this case, you will be authenticated through the Rancher server’s authentication proxy, then Rancher will connect you to the downstream cluster. This method lets you manage the cluster without the Rancher UI.\nAccess your cluster with the kubectl CLI, using the authorized cluster endpoint: Follow these steps to access your cluster with kubectl directly, without authenticating through Rancher. We recommend setting up this alternative method to access your cluster so that in case you can’t connect to Rancher, you can still access the cluster.\n\n","postref":"0021ebc5270fe7831543fb74620bf94c","objectID":"a37cd1037bbb6a087cfd3320a6768bab","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/node-pools/azure/"},{"anchor":"#","title":"Creating a vSphere Cluster","content":"By using Rancher with vSphere, you can bring cloud operations on-premises.\n\nRancher can provision nodes in vSphere and install Kubernetes on them. When creating a Kubernetes cluster in vSphere, Rancher first provisions the specified number of virtual machines by communicating with the vCenter API. Then it installs Kubernetes on top of them.\n\nA vSphere cluster may consist of multiple groups of VMs with distinct properties, such as the amount of memory or the number of vCPUs. This grouping allows for fine-grained control over the sizing of nodes for each Kubernetes role.\n\nvSphere Enhancements\n\nThe vSphere node templates have been updated, allowing you to bring cloud operations on-premises with the following enhancements:\n\nSelf-healing Node Pools\n\nAvailable as of v2.3.0\n\nOne of the biggest advantages of provisioning vSphere nodes with Rancher is that it allows you to take advantage of Rancher’s self-healing node pools, also called the node auto-replace feature, in your on-premises clusters. Self-healing node pools are designed to help you replace worker nodes for stateless applications. When Rancher provisions nodes from a node template, Rancher can automatically replace unreachable nodes.\n\n\nImportant: It is not recommended to enable node auto-replace on a node pool of master nodes or nodes with persistent volumes attached, because VMs are treated ephemerally. When a node in a node pool loses connectivity with the cluster, its persistent volumes are destroyed, resulting in data loss for stateful applications.\n\n\nDynamically Populated Options for Instances and Scheduling\n\nAvailable as of v2.3.3\n\nNode templates for vSphere have been updated so that when you create a node template with your vSphere credentials, the template is automatically populated with the same options for provisioning VMs that you have access to in the vSphere console.\n\nFor the fields to be populated, your setup needs to fulfill the prerequisites.\n\nMore Supported Operating Systems\n\nIn Rancher v2.3.3+, you can provision VMs with any operating system that supports cloud-init. Only YAML format is supported for the cloud config.\n\nIn Rancher prior to v2.3.3, the vSphere node driver included in Rancher only supported the provisioning of VMs with RancherOS as the guest operating system.\n\nVideo Walkthrough of v2.3.3 Node Template Features\n\nIn this YouTube video, we demonstrate how to set up a node template with the new features designed to help you bring cloud operations to on-premises clusters.\n\n\n\n  \n\n\n","postref":"c17bbd0aa3f9a41f5f547cdf604009b5","objectID":"98dbde14ca28d6eb6ae9d3a892e41cc1","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/node-pools/vsphere/"},{"anchor":"#","title":"Launching Kubernetes on Existing Custom Nodes","content":"When you create a custom cluster, Rancher uses RKE (the Rancher Kubernetes Engine) to create a Kubernetes cluster in on-premise bare-metal servers, on-premise virtual machines, or in any node hosted by an infrastructure provider.\n\nTo use this option you’ll need access to servers you intend to use in your Kubernetes cluster. Provision each server according to the requirements, which includes some hardware specifications and Docker. After you install Docker on each server, run the command provided in the Rancher UI to turn each server into a Kubernetes node.\n\nThis section describes how to set up a custom cluster.\n\nCreating a Cluster with Custom Nodes\n\n\nWant to use Windows hosts as Kubernetes workers?\n\nSee Configuring Custom Clusters for Windows before you start.\n\n\n\n\n\n1. Provision a Linux Host\n2. Create the Custom Cluster\n3. Amazon Only: Tag Resources\n\n\n\n\n1. Provision a Linux Host\n\nBegin creation of a custom cluster by provisioning a Linux host. Your host can be:\n\n\nA cloud-host virtual machine (VM)\nAn on-premise VM\nA bare-metal server\n\n\nIf you want to reuse a node from a previous custom cluster, clean the node before using it in a cluster again. If you reuse a node that hasn’t been cleaned, cluster provisioning may fail.\n\nProvision the host according to the installation requirements and the checklist for production-ready clusters.\n\n2. Create the Custom Cluster\n\n\nFrom the Clusters page, click Add Cluster.\n\nChoose Custom.\n\nEnter a Cluster Name.\n\n\nUse Member Roles to configure user authorization for the cluster.\n\t\n\t\tClick Add Member to add users that can access the cluster.\n\t\tUse the Role drop-down to set permissions for each user.\n\t\n\t\n\n\n\nUse Cluster Options to choose the version of Kubernetes, what network provider will be used and if you want to enable project network isolation. To see more cluster options, click on Show advanced options.\n\n\n\nUsing Windows nodes as Kubernetes workers?\n\n\nSee Enable the Windows Support Option.\nThe only Network Provider available for clusters with Windows support is Flannel. See Networking Option.\n\n\n\nClick Next.\n\nFrom Node Role, choose the roles that you want filled by a cluster node.\n\n\nNotes:\n\n\nUsing Windows nodes as Kubernetes workers? See Node Configuration.\nBare-Metal Server Reminder: If you plan on dedicating bare-metal servers to each role, you must provision a bare-metal server for each role (i.e. provision multiple bare-metal servers).\n\n\n\nOptional: Click Show advanced options to specify IP address(es) to use when registering the node, override the hostname of the node, or to add labels or taints to the node.\n\nCopy the command displayed on screen to your clipboard.\n\nLog in to your Linux host using your preferred shell, such as PuTTy or a remote Terminal connection. Run the command copied to your clipboard.\n\n\nNote: Repeat steps 7-10 if you want to dedicate specific hosts to specific node roles. Repeat the steps as many times as needed.\n\n\nWhen you finish running the command(s) on your Linux host(s), click Done.\n\n\n\n\tResult:\n\t\n\t\tYour cluster is created and assigned a state of Provisioning. Rancher is standing up your cluster.\n\t\tYou can access your cluster after its state is updated to Active.\n\t\tActive clusters are assigned two Projects, Default (containing the namespace default) and System (containing the namespaces cattle-system,ingress-nginx,kube-public and kube-system, if present).\n\t\n\n\n\n3. Amazon Only: Tag Resources\n\nIf you have configured your cluster to use Amazon as Cloud Provider, tag your AWS resources with a cluster ID.\n\nAmazon Documentation: Tagging Your Amazon EC2 Resources\n\n\nNote: You can use Amazon EC2 instances without configuring a cloud provider in Kubernetes. You only have to configure the cloud provider if you want to use specific Kubernetes cloud provider functionality. For more information, see Kubernetes Cloud Providers\n\n\nThe following resources need to tagged with a ClusterID:\n\n\nNodes: All hosts added in Rancher.\nSubnet: The subnet used for your cluster\n\nSecurity Group: The security group used for your cluster.\n\n\nNote: Do not tag multiple security groups. Tagging multiple groups generates an error when creating Elastic Load Balancer.\n\n\n\nThe tag that should be used is:\n\nKey=kubernetes.io/cluster/<CLUSTERID>, Value=owned\n\n\n<CLUSTERID> can be any string you choose. However, the same string must be used on every resource you tag. Setting the tag value to owned informs the cluster that all resources tagged with the <CLUSTERID> are owned and managed by this cluster.\n\nIf you share resources between clusters, you can change the tag to:\n\nKey=kubernetes.io/cluster/CLUSTERID, Value=shared\n\n\nOptional Next Steps\n\nAfter creating your cluster, you can access it through the Rancher UI. As a best practice, we recommend setting up these alternate ways of accessing your cluster:\n\n\nAccess your cluster with the kubectl CLI: Follow these steps to access clusters with kubectl on your workstation. In this case, you will be authenticated through the Rancher server’s authentication proxy, then Rancher will connect you to the downstream cluster. This method lets you manage the cluster without the Rancher UI.\nAccess your cluster with the kubectl CLI, using the authorized cluster endpoint: Follow these steps to access your cluster with kubectl directly, without authenticating through Rancher. We recommend setting up this alternative method to access your cluster so that in case you can’t connect to Rancher, you can still access the cluster.\n\n","postref":"49db5f712676ab795416166283100d75","objectID":"9ca958199cc1d10e49fb51b2ccc2914e","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/custom-nodes/"},{"anchor":"#","title":"Launching Kubernetes on Windows Clusters","content":"Available as of v2.3.0\n\nWhen provisioning a custom cluster using Rancher, Rancher uses RKE (the Rancher Kubernetes Engine) to provision the Kubernetes custom cluster on your existing infrastructure.\n\nYou can use a mix of Linux and Windows hosts as your cluster nodes. Windows nodes can only be used for deploying workloads, while Linux nodes are required for cluster management.\n\nYou can only add Windows nodes to a cluster if Windows support is enabled. Windows support can be enabled for new custom clusters that use Kubernetes 1.15+ and the Flannel network provider. Windows support cannot be enabled for existing clusters.\n\n\nWindows clusters have more requirements than Linux clusters. For example, Windows nodes must have 50 GB of disk space. Make sure your Windows cluster fulfills all of the requirements.\n\n\nFor a summary of Kubernetes features supported in Windows, see the Kubernetes documentation on supported functionality and limitations for using Kubernetes with Windows or the guide for scheduling Windows containers in Kubernetes.\n\nThis guide covers the following topics:\n\n\n\n\nPrerequisites\nRequirements\n\n\nOS and Docker\nNodes\nNetworking\nArchitecture\nContainers\n\nTutorial: How to Create a Cluster with Windows Support\nConfiguration for Storage Classes in Azure\n\n\n\nPrerequisites\n\nBefore provisioning a new cluster, be sure that you have already installed Rancher on a device that accepts inbound network traffic. This is required in order for the cluster nodes to communicate with Rancher. If you have not already installed Rancher, please refer to the installation documentation before proceeding with this guide.\n\n\nNote on Cloud Providers: If you set a Kubernetes cloud provider in your cluster, some additional steps are required. You might want to set a cloud provider if you want to want to leverage a cloud provider’s capabilities, for example, to automatically provision storage, load balancers, or other infrastructure for your cluster. Refer to this page for details on how to configure a cloud provider cluster of nodes that meet the prerequisites.\n\n\nRequirements for Windows Clusters\n\nFor a custom cluster, the general node requirements for networking, operating systems, and Docker are the same as the node requirements for a Rancher installation.\n\nOS and Docker Requirements\n\nIn order to add Windows worker nodes to a cluster, the node must be running one of the following Windows Server versions and the corresponding version of Docker Engine - Enterprise Edition (EE):\n\n\nNodes with Windows Server core version 1809 should use Docker EE-basic 18.09 or Docker EE-basic 19.03.\nNodes with Windows Server core version 1903 should use Docker EE-basic 19.03.\n\n\n\nNotes:\n\n\nIf you are using AWS, Rancher recommends Microsoft Windows Server 2019 Base with Containers as the Amazon Machine Image (AMI).\nIf you are using GCE, Rancher recommends Windows Server 2019 Datacenter for Containers as the OS image.\n\n\n\nNode Requirements\n\nThe hosts in the cluster need to have at least:\n\n\n2 core CPUs\n5 GB memory\n50 GB disk space\n\n\nRancher will not provision the node if the node does not meet these requirements.\n\nNetworking Requirements\n\nRancher only supports Windows using Flannel as the network provider.\n\nThere are two network options: Host Gateway (L2bridge) and VXLAN (Overlay). The default option is VXLAN (Overlay) mode.\n\nFor Host Gateway (L2bridge) networking, it’s best to use the same Layer 2 network for all nodes. Otherwise, you need to configure the route rules for them. For details, refer to the documentation on configuring cloud-hosted VM routes. You will also need to disable private IP address checks if you are using Amazon EC2, Google GCE, or Azure VM.\n\nFor VXLAN (Overlay) networking, the KB4489899 hotfix must be installed. Most cloud-hosted VMs already have this hotfix.\n\nArchitecture Requirements\n\nThe Kubernetes cluster management nodes (etcd and controlplane) must be run on Linux nodes.\n\nThe worker nodes, which is where your workloads will be deployed on, will typically be Windows nodes, but there must be at least one worker node that is run on Linux in order to run the Rancher cluster agent, DNS, metrics server, and Ingress related containers.\n\nWe recommend the minimum three-node architecture listed in the table below, but you can always add additional Linux and Windows workers to scale up your cluster for redundancy:\n\n\n\n\n\n\nNode\nOperating System\nKubernetes Cluster Role(s)\nPurpose\n\n\n\n\n\nNode 1\nLinux (Ubuntu Server 18.04 recommended)\nControl Plane, etcd, Worker\nManage the Kubernetes cluster\n\n\n\nNode 2\nLinux (Ubuntu Server 18.04 recommended)\nWorker\nSupport the Rancher Cluster agent, Metrics server, DNS, and Ingress for the cluster\n\n\n\nNode 3\nWindows (Windows Server core version 1809 or above)\nWorker\nRun your Windows containers\n\n\n\n\nContainer Requirements\n\nWindows requires that containers must be built on the same Windows Server version that they are being deployed on. Therefore, containers must be built on Windows Server core version 1809 or above. If you have existing containers built for an earlier Windows Server core version, they must be re-built on Windows Server core version 1809 or above.\n\nTutorial: How to Create a Cluster with Windows Support\n\nThis tutorial describes how to create a Rancher-provisioned cluster with the three nodes in the recommended architecture.\n\nWhen you provision a custom cluster with Rancher, you will add nodes to the cluster by installing the Rancher agent on each one. When you create or edit your cluster from the Rancher UI, you will see a Customize Node Run Command that you can run on each server to add it to your custom cluster.\n\nTo set up a custom cluster with support for Windows nodes and containers, you will need to complete the tasks below.\n\n\n\n\nProvision Hosts\nCreate the Custom Cluster\nAdd Nodes to the Cluster\nOptional: Configuration for Azure Files\n\n\n\n1. Provision Hosts\n\nTo begin provisioning a custom cluster with Windows support, prepare your hosts.\n\nYour hosts can be:\n\n\nCloud-hosted VMs\nVMs from virtualization clusters\nBare-metal servers\n\n\nYou will provision three nodes:\n\n\nOne Linux node, which manages the Kubernetes control plane and stores your etcd\nA second Linux node, which will be another worker node\nThe Windows node, which will run your Windows containers as a worker node\n\n\n\n\n\nNode\nOperating System\n\n\n\n\n\nNode 1\nLinux (Ubuntu Server 18.04 recommended)\n\n\n\nNode 2\nLinux (Ubuntu Server 18.04 recommended)\n\n\n\nNode 3\nWindows (Windows Server core version 1809 or above required)\n\n\n\n\nIf your nodes are hosted by a Cloud Provider and you want automation support such as loadbalancers or persistent storage devices, your nodes have additional configuration requirements. For details, see Selecting Cloud Providers.\n\n2. Create the Custom Cluster\n\nThe instructions for creating a custom cluster that supports Windows nodes are very similar to the general instructions for creating a custom cluster with some Windows-specific requirements.\n\nWindows support only be enabled if the cluster uses Kubernetes v1.15+ and the Flannel network provider.\n\n\nFrom the Global view, click on the Clusters tab and click Add Cluster.\n\nClick From existing nodes (Custom).\n\nEnter a name for your cluster in the Cluster Name text box.\n\nIn the Kubernetes Version dropdown menu, select v1.15 or above.\n\nIn the Network Provider field, select Flannel.\n\nIn the Windows Support section, click Enable.\n\nOptional: After you enable Windows support, you will be able to choose the Flannel backend. There are two network options: Host Gateway (L2bridge) and VXLAN (Overlay). The default option is VXLAN (Overlay) mode.\n\nClick Next.\n\n\n\nImportant: For Host Gateway (L2bridge) networking, it’s best to use the same Layer 2 network for all nodes. Otherwise, you need to configure the route rules for them. For details, refer to the documentation on configuring cloud-hosted VM routes. You will also need to disable private IP address checks if you are using Amazon EC2, Google GCE, or Azure VM.\n\n\n3. Add Nodes to the Cluster\n\nThis section describes how to register your Linux and Worker nodes to your custom cluster.\n\nAdd Linux Master Node\n\nThe first node in your cluster should be a Linux host has both the Control Plane and etcd roles. At a minimum, both of these roles must be enabled for this node, and this node must be added to your cluster before you can add Windows hosts.\n\nIn this section, we fill out a form on the Rancher UI to get a custom command to install the Rancher agent on the Linux master node. Then we will copy the command and run it on our Linux master node to register the node in the cluster.\n\n\nIn the Node Operating System section, click Linux.\n\nIn the Node Role section, choose at least etcd and Control Plane. We recommend selecting all three.\n\nOptional: If you click Show advanced options, you can customize the settings for the Rancher agent and node labels.\n\nCopy the command displayed on the screen to your clipboard.\n\nSSH into your Linux host and run the command that you copied to your clipbo","postref":"b3d745a667545b43c68ef02d1e3b910f","objectID":"ce0cfcffe3d6702a0307af86262f58a6","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/windows-clusters/"},{"anchor":"#","title":"Cluster Configuration Reference","content":"As you configure a new cluster that’s provisioned using RKE, you can choose custom Kubernetes options.\n\nYou can configure Kubernetes options one of two ways:\n\n\nRancher UI: Use the Rancher UI to select options that are commonly customized when setting up a Kubernetes cluster.\nCluster Config File: Instead of using the Rancher UI to choose Kubernetes options for the cluster, advanced users can create an RKE config file. Using a config file allows you to set any of the options available in an RKE installation, except for system_images configuration, by specifying them in YAML.\n\n\nIn Rancher v2.0.0-v2.2.x, the config file is identical to the  cluster config file for the Rancher Kubernetes Engine, which is the tool Rancher uses to provision clusters. In Rancher v2.3.0, the RKE information is still included in the config file, but it is separated from other options, so that the RKE cluster config options are nested under the rancher_kubernetes_engine_config directive. For more information, see the section about the cluster config file.\n\nThis section is a cluster configuration reference, covering the following topics:\n\n\nRancher UI Options\n\n\nKubernetes version\nNetwork provider\nKubernetes cloud providers\nPrivate registries\nAuthorized cluster endpoint\n\nAdvanced Options\n\n\nNGINX Ingress\nNode port range\nMetrics server monitoring\nPod security policy support\nDocker version on nodes\nDocker root directory\nRecurring etcd snapshots\n\nCluster config file\n\n\nConfig file structure in Rancher v2.3.0+\nConfig file structure in Rancher v2.0.0-v2.2.x\nDefault DNS provider\n\nRancher specific parameters\n\n\nRancher UI Options\n\nWhen creating a cluster using one of the options described in Rancher Launched Kubernetes, you can configure basic Kubernetes options using the Cluster Options section.\n\nKubernetes Version\n\nThe version of Kubernetes installed on your cluster nodes. Rancher packages its own version of Kubernetes based on hyperkube.\n\nNetwork Provider\n\nThe Network Provider that the cluster uses. For more details on the different networking providers, please view our Networking FAQ.\n\n\nNote: After you launch the cluster, you cannot change your network provider. Therefore, choose which network provider you want to use carefully, as Kubernetes doesn’t allow switching between network providers. Once a cluster is created with a network provider, changing network providers would require you  tear down the entire cluster and all its applications.\n\n\nOut of the box, Rancher is compatible with the following network providers:\n\n\nCanal\nFlannel\nCalico\nWeave (Available as of v2.2.0)\n\n\nNotes on Canal:\n\nIn v2.0.0 - v2.0.4 and v2.0.6, this was the default option for these clusters was Canal with network isolation. With the network isolation automatically enabled, it prevented any pod communication between projects.\n\nAs of v2.0.7, if you use Canal, you also have the option of using Project Network Isolation, which will enable or disable communication between pods in different projects.\n\n\nAttention Rancher v2.0.0 - v2.0.6 Users\n\n\nIn previous Rancher releases, Canal isolates project network communications with no option to disable it. If you are using any of these Rancher releases, be aware that using Canal prevents all communication between pods in different projects.\nIf you have clusters using Canal and are upgrading to v2.0.7, those clusters enable Project Network Isolation by default. If you want to disable Project Network Isolation, edit the cluster and disable the option.\n\n\n\nNotes on Flannel:\n\nIn v2.0.5, this was the default option, which did not prevent any network isolation between projects.\n\nNotes on Weave:\n\nWhen Weave is selected as network provider, Rancher will automatically enable encryption by generating a random password. If you want to specify the password manually, please see how to configure your cluster using a Config File and the Weave Network Plug-in Options.\n\nKubernetes Cloud Providers\n\nYou can configure a Kubernetes cloud provider. If you want to use volumes and storage in Kubernetes, typically you must select the specific cloud provider in order to use it. For example, if you want to use Amazon EBS, you would need to select the aws cloud provider.\n\n\nNote: If the cloud provider you want to use is not listed as an option, you will need to use the config file option to configure the cloud provider. Please reference the RKE cloud provider documentation on how to configure the cloud provider.\n\n\nIf you want to see all the configuration options for a cluster, please click Show advanced options on the bottom right. The advanced options are described below:\n\nPrivate registries\n\nAvailable as of v2.2.0\n\nThe cluster-level private registry configuration is only used for provisioning clusters.\n\nThere are two main ways to set up private registries in Rancher: by setting up the global default registry through the Settings tab in the global view, and by setting up a private registry in the advanced options in the cluster-level settings. The global default registry is intended to be used for air-gapped setups, for registries that do not require credentials. The cluster-level private registry is intended to be used in all setups in which the private registry requires credentials.\n\nIf your private registry requires credentials, you need to pass the credentials to Rancher by editing the cluster options for each cluster that needs to pull images from the registry.\n\nThe private registry configuration option tells Rancher where to pull the system images or addon images that will be used in your cluster.\n\n\nSystem images are components needed to maintain the Kubernetes cluster.\nAdd-ons are used to deploy several cluster components, including network plug-ins, the ingress controller, the DNS provider, or the metrics server.\n\n\nSee the RKE documentation on private registries for more information on the private registry for components applied during the provisioning of the cluster.\n\nAuthorized Cluster Endpoint\n\nAvailable as of v2.2.0\n\nAuthorized Cluster Endpoint can be used to directly access the Kubernetes API server, without requiring communication through Rancher.\n\n\nThe authorized cluster endpoint only works on Rancher-launched Kubernetes clusters. In other words, it only works in clusters where Rancher used RKE to provision the cluster. It is not available for clusters in a hosted Kubernetes provider, such as Amazon’s EKS.\n\n\nThis is enabled by default in Rancher-launched Kubernetes clusters, using the IP of the node with the controlplane role and the default Kubernetes self signed certificates.\n\nFor more detail on how an authorized cluster endpoint works and why it is used, refer to the architecture section.\n\nWe recommend using a load balancer with the authorized cluster endpoint. For details, refer to the recommended architecture section.\n\nAdvanced Options\n\nThe following options are available when you create clusters in the Rancher UI. They are located under Advanced Options.\n\nNGINX Ingress\n\nOption to enable or disable the NGINX ingress controller.\n\nNode Port Range\n\nOption to change the range of ports that can be used for NodePort services. Default is 30000-32767.\n\nMetrics Server Monitoring\n\nOption to enable or disable Metrics Server.\n\nPod Security Policy Support\n\nOption to enable and select a default Pod Security Policy. You must have an existing Pod Security Policy configured before you can use this option.\n\nDocker Version on Nodes\n\nOption to require a supported Docker version installed on the cluster nodes that are added to the cluster, or to allow unsupported Docker versions installed on the cluster nodes.\n\nDocker Root Directory\n\nIf the nodes you are adding to the cluster have Docker configured with a non-default Docker Root Directory (default is /var/lib/docker), please specify the correct Docker Root Directory in this option.\n\nRecurring etcd Snapshots\n\nOption to enable or disable recurring etcd snapshots.\n\nCluster Config File\n\nInstead of using the Rancher UI to choose Kubernetes options for the cluster, advanced users can create an RKE config file. Using a config file allows you to set any of the options available in an RKE installation, except for system_images configuration. The system_images option is not supported when creating a cluster with the Rancher UI or API.\n\n\nNote: In Rancher v2.0.5 and v2.0.6, the names of services in the Config File (YAML) should contain underscores only: kube_api and kube_controller.\n\n\n\nTo edit an RKE config file directly from the Rancher UI, click Edit as YAML.\nTo read from an existing RKE file, click Read from a file.\n\n\n\n\nThe structure of the config file is different depending on your version of Rancher. Below are example config files for Rancher v2.0.0-v2.2.x and for Rancher v2.3.0+.\n\nConfig File Structure in Rancher v2.3.0+\n\nRKE (Rancher Kubernetes Engine) is the tool that Rancher uses to provision Kubernetes clusters. Rancher’s cluster config files used to have the same structure as RKE config files, but the structure changed so that in Ranc","postref":"e0a56d94ee917808a6d8662155683bb4","objectID":"1f8621af066cde03828ea3a11f85ee17","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/options/"},{"anchor":"#cloud-provider-options","title":"Cloud Provider Options","content":"When using the Azure cloud provider, you can leverage the following capabilities:\nLoad Balancers: Launches an Azure Load Balancer within a specific Network Security Group.\n\nPersistent Volumes: Supports using Azure Blob disks and Azure Managed Disks with standard and premium storage accounts.\n\nNetwork Storage: Support Azure Files via CIFS mounts.\nThe following account types are not supported for Azure Subscriptions:\nSingle tenant accounts (i.e. accounts with no subscriptions).\nMulti-subscription accounts.\nTo set up the Azure cloud provider following credentials need to be configured:\nSet up the Azure Tenant ID\nSet up the Azure Client ID and Azure Client Secret\nConfigure App Registration Permissions\nSet up Azure Network Security Group Name\n1. Set up the Azure Tenant IDVisit Azure portal, login and go to Azure Active Directory and select Properties. Your Directory ID is your Tenant ID (tenantID).If you want to use the Azure CLI, you can run the command az account show to get the information.2. Set up the Azure Client ID and Azure Client SecretVisit Azure portal, login and follow the steps below to create an App Registration and the corresponding Azure Client ID (aadClientId) and Azure Client Secret (aadClientSecret).\nSelect Azure Active Directory.\nSelect App registrations.\nSelect New application registration.\nChoose a Name, select Web app / API as Application Type and a Sign-on URL which can be anything in this case.\nSelect Create.\nIn the App registrations view, you should see your created App registration. The value shown in the column APPLICATION ID is what you need to use as Azure Client ID.The next step is to generate the Azure Client Secret:\nOpen your created App registration.\nIn the Settings view, open Keys.\nEnter a Key description, select an expiration time and select Save.\nThe generated value shown in the column Value is what you need to use as Azure Client Secret. This value will only be shown once.\n3. Configure App Registration PermissionsThe last thing you will need to do, is assign the appropriate permissions to your App registration.\nGo to More services, search for Subscriptions and open it.\nOpen Access control (IAM).\nSelect Add.\nFor Role, select Contributor.\nFor Select, select your created App registration name.\nSelect Save.\n4. Set up Azure Network Security Group NameA custom Azure Network Security Group (securityGroupName) is needed to allow Azure Load Balancers to work.If you provision hosts using Rancher Machine Azure driver, you will need to edit them manually to assign them to this Network Security Group.You should already assign custom hosts to this Network Security Group during provisioning.Only hosts expected to be load balancer back ends need to be in this group.When using the Amazon cloud provider, you can leverage the following capabilities:\nLoad Balancers: Launches an AWS Elastic Load Balancer (ELB) when choosing Layer-4 Load Balancer in Port Mapping or when launching a Service with type: LoadBalancer.\nPersistent Volumes: Allows you to use AWS Elastic Block Stores (EBS) for persistent volumes.\nSee cloud-provider-aws README for all information regarding the Amazon cloud provider.To set up the Amazon cloud provider,\nCreate an IAM role and attach to the instances\nConfigure the ClusterID\n1. Create an IAM Role and attach to the instancesAll nodes added to the cluster must be able to interact with EC2 so that they can create and remove resources. You can enable this interaction by using an IAM role attached to the instance. See Amazon documentation: Creating an IAM Role how to create an IAM role. There are two example policies:\nThe first policy is for the nodes with the controlplane role. These nodes have to be able to create/remove EC2 resources. The following IAM policy is an example, please remove any unneeded permissions for your use case.\nThe second policy is for the nodes with the etcd or worker role. These nodes only have to be able to retrieve information from EC2.\nWhile creating an Amazon EC2 cluster, you must fill in the IAM Instance Profile Name (not ARN) of the created IAM role when creating the Node Template.While creating a Custom cluster, you must manually attach the IAM role to the instance(s).IAM Policy for nodes with the controlplane role:{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n  {\n    \"Effect\": \"Allow\",\n    \"Action\": [\n      \"autoscaling:DescribeAutoScalingGroups\",\n      \"autoscaling:DescribeLaunchConfigurations\",\n      \"autoscaling:DescribeTags\",\n      \"ec2:DescribeInstances\",\n      \"ec2:DescribeRegions\",\n      \"ec2:DescribeRouteTables\",\n      \"ec2:DescribeSecurityGroups\",\n      \"ec2:DescribeSubnets\",\n      \"ec2:DescribeVolumes\",\n      \"ec2:CreateSecurityGroup\",\n      \"ec2:CreateTags\",\n      \"ec2:CreateVolume\",\n      \"ec2:ModifyInstanceAttribute\",\n      \"ec2:ModifyVolume\",\n      \"ec2:AttachVolume\",\n      \"ec2:AuthorizeSecurityGroupIngress\",\n      \"ec2:CreateRoute\",\n      \"ec2:DeleteRoute\",\n      \"ec2:DeleteSecurityGroup\",\n      \"ec2:DeleteVolume\",\n      \"ec2:DetachVolume\",\n      \"ec2:RevokeSecurityGroupIngress\",\n      \"ec2:DescribeVpcs\",\n      \"elasticloadbalancing:AddTags\",\n      \"elasticloadbalancing:AttachLoadBalancerToSubnets\",\n      \"elasticloadbalancing:ApplySecurityGroupsToLoadBalancer\",\n      \"elasticloadbalancing:CreateLoadBalancer\",\n      \"elasticloadbalancing:CreateLoadBalancerPolicy\",\n      \"elasticloadbalancing:CreateLoadBalancerListeners\",\n      \"elasticloadbalancing:ConfigureHealthCheck\",\n      \"elasticloadbalancing:DeleteLoadBalancer\",\n      \"elasticloadbalancing:DeleteLoadBalancerListeners\",\n      \"elasticloadbalancing:DescribeLoadBalancers\",\n      \"elasticloadbalancing:DescribeLoadBalancerAttributes\",\n      \"elasticloadbalancing:DetachLoadBalancerFromSubnets\",\n      \"elasticloadbalancing:DeregisterInstancesFromLoadBalancer\",\n      \"elasticloadbalancing:ModifyLoadBalancerAttributes\",\n      \"elasticloadbalancing:RegisterInstancesWithLoadBalancer\",\n      \"elasticloadbalancing:SetLoadBalancerPoliciesForBackendServer\",\n      \"elasticloadbalancing:AddTags\",\n      \"elasticloadbalancing:CreateListener\",\n      \"elasticloadbalancing:CreateTargetGroup\",\n      \"elasticloadbalancing:DeleteListener\",\n      \"elasticloadbalancing:DeleteTargetGroup\",\n      \"elasticloadbalancing:DescribeListeners\",\n      \"elasticloadbalancing:DescribeLoadBalancerPolicies\",\n      \"elasticloadbalancing:DescribeTargetGroups\",\n      \"elasticloadbalancing:DescribeTargetHealth\",\n      \"elasticloadbalancing:ModifyListener\",\n      \"elasticloadbalancing:ModifyTargetGroup\",\n      \"elasticloadbalancing:RegisterTargets\",\n      \"elasticloadbalancing:SetLoadBalancerPoliciesOfListener\",\n      \"iam:CreateServiceLinkedRole\",\n      \"kms:DescribeKey\"\n    ],\n    \"Resource\": [\n      \"*\"\n    ]\n  }\n]\n}IAM policy for nodes with the etcd or worker role:{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n    {\n        \"Effect\": \"Allow\",\n        \"Action\": [\n            \"ec2:DescribeInstances\",\n            \"ec2:DescribeRegions\",\n            \"ecr:GetAuthorizationToken\",\n            \"ecr:BatchCheckLayerAvailability\",\n            \"ecr:GetDownloadUrlForLayer\",\n            \"ecr:GetRepositoryPolicy\",\n            \"ecr:DescribeRepositories\",\n            \"ecr:ListImages\",\n            \"ecr:BatchGetImage\"\n        ],\n        \"Resource\": \"*\"\n    }\n]\n}2. Configure the ClusterIDThe following resources need to tagged with a ClusterID:\nNodes: All hosts added in Rancher.\nSubnet: The subnet used for your cluster.\nSecurity Group: The security group used for your cluster.\n\nNote: Do not tag multiple security groups. Tagging multiple groups generates an error when creating an Elastic Load Balancer (ELB).\nWhen you create an Amazon EC2 Cluster, the ClusterID is automatically configured for the created nodes. Other resources still need to be tagged manually.Use the following tag:Key = kubernetes.io/cluster/CLUSTERID Value = ownedCLUSTERID can be any string you like, as long as it is equal across all tags set.Setting the value of the tag to owned tells the cluster that all resources with this tag are owned and managed by this cluster. If you share resources between clusters, you can change the tag to:Key = kubernetes.io/cluster/CLUSTERID Value = shared.Using Amazon Elastic Container Registry (ECR)The kubelet component has the ability to automatically obtain ECR credentials, when the IAM profile mentioned in Create an IAM Role and attach to the instances is attached to the instance(s). When using a Kubernetes version older than v1.15.0, the Amazon cloud provider needs be configured in the cluster. Starting with Kubernetes version v1.15.0, the kubelet can obtain ECR credentials without having the Amazon cloud provider configured in the cluster.By default, the Cloud Provider option is set to None. Supported cloud providers are:\nAmazon\nAzure\nThe Custom cloud provider is available if you want to configure any Kubernetes cloud provider.For the custom cloud provider option, you can refer to the RKE docs on how to edit the yaml file for your specific cloud provider. There are sp","postref":"51c078cad85d0a923adecc8b5559c5f7","objectID":"78557b147240194724596f2021b8b074","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/options/cloud-providers/"},{"anchor":"#adding-a-default-pod-security-policy","title":"Adding a Default Pod Security Policy","content":"When you create a new cluster with RKE, you can configure it to apply a PSP immediately. As you create the cluster, use the Cluster Options to enable a PSP. The PSP assigned to the cluster will be the default PSP for projects within the cluster.\nPrerequisite:\nCreate a Pod Security Policy within Rancher. Before you can assign a default PSP to a new cluster, you must have a PSP available for assignment. For instruction, see Creating Pod Security Policies.\nNote:\nFor security purposes, we recommend assigning a PSP as you create your clusters.\nTo enable a default Pod Security Policy, set the Pod Security Policy Support option to  Enabled, and then make a selection from the Default Pod Security Policy drop-down.When the cluster finishes provisioning, the PSP you selected is applied to all projects within the cluster.","postref":"38aedb377defe72ff1f54f81c1235226","objectID":"6f2ff6495f3222a1a279cc755ce01fca","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/options/pod-security-policies/"},{"anchor":"#what-is-cni","title":"What is CNI?","content":"CNI (Container Network Interface), a Cloud Native Computing Foundation project, consists of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of  plugins. CNI concerns itself only with network connectivity of containers and removing allocated resources when the container is deleted.Kubernetes uses CNI as an interface between network providers and Kubernetes pod networking.For more information visit CNI GitHub project.What Network Models are Used in CNI?CNI network providers implement their network fabric using either an encapsulated network model such as Virtual Extensible Lan (VXLAN) or an unencapsulated network model such as Border Gateway Protocol (BGP).What is an Encapsulated Network?This network model provides a logical Layer 2 (L2) network encapsulated over the existing Layer 3 (L3) network topology that spans the Kubernetes cluster nodes. With this model you have an isolated L2 network for containers without needing routing distribution, all at the cost of minimal overhead in terms of processing and increased IP package size, which comes from an IP header generated by overlay encapsulation. Encapsulation information is distributed by UDP ports between Kubernetes workers, interchanging network control plane information about how MAC addresses can be reached. Common encapsulation used in this kind of network model is VXLAN, Internet Protocol Security (IPSec), and IP-in-IP.In simple terms, this network model generates a kind of network bridge extended between Kubernetes workers, where pods are connected.This network model is used when an extended L2 bridge is preferred. This network model is sensitive to L3 network latencies of the Kubernetes workers. If datacenters are in distinct geolocations, be sure to have low latencies between them to avoid eventual network segmentation.CNI network providers using this network model include Flannel, Canal, and Weave.What is an Unencapsulated Network?This network model provides an L3 network to route packets between containers. This model doesn’t generate an isolated l2 network, nor generates overhead. These benefits come at the cost of Kubernetes workers having to manage any route distribution that’s needed. Instead of using IP headers for encapsulation, this network model uses a network protocol between Kubernetes workers to distribute routing information to reach pods, such as BGP.In simple terms, this network model generates a kind of network router extended between Kubernetes workers, which provides information about how to reach pods.This network model is used when a routed L3 network is preferred. This mode dynamically updates routes at the OS level for Kubernetes workers. It’s less sensitive to latency.CNI network providers using this network model include Calico and Romana.What CNI Providers are Provided by Rancher?Out-of-the-box, Rancher provides the following CNI network providers for Kubernetes clusters: Canal, Flannel, Calico and Weave (Weave is available as of v2.2.0). You can choose your CNI network provider when you create new Kubernetes clusters from Rancher.CanalCanal is a CNI network provider that gives you the best of Flannel and Calico. It allows users to easily deploy Calico and Flannel networking together as a unified networking solution, combining Calico’s network policy enforcement with the rich superset of Calico (unencapsulated) and/or Flannel (encapsulated) network connectivity options.In Rancher, Canal is the default CNI network provider combined with Flannel and VXLAN encapsulation.Kubernetes workers should open UDP port 8472 (VXLAN) and TCP port 9099 (healthcheck). For details, refer to the port requirements for user clusters.For more information, see the Canal GitHub Page.FlannelFlannel is a simple and easy way to configure L3 network fabric designed for Kubernetes. Flannel runs a single binary agent named flanneld on each host, which is responsible for allocating a subnet lease to each host out of a larger, preconfigured address space. Flannel uses either the Kubernetes API or etcd directly to store the network configuration, the allocated subnets, and any auxiliary data (such as the host’s public IP). Packets are forwarded using one of several backend mechanisms, with the default encapsulation being VXLAN.Encapsulated traffic is unencrypted by default. Therefore, flannel provides an experimental backend for encryption, IPSec, which makes use of strongSwan to establish encrypted IPSec tunnels between Kubernetes workers.Kubernetes workers should open UDP port 8472 (VXLAN) and TCP port 9099 (healthcheck). See the port requirements for user clusters for more details.For more information, see the Flannel GitHub Page.CalicoCalico enables networking and network policy in Kubernetes clusters across the cloud. Calico uses a pure, unencapsulated IP network fabric and policy engine to provide networking for your Kubernetes workloads. Workloads are able to communicate over both cloud infrastructure and on-premise using BGP.Calico also provides a stateless IP-in-IP encapsulation mode that can be used, if necessary. Calico also offers policy isolation, allowing you to secure and govern your Kubernetes workloads using advanced ingress and egress policies.Kubernetes workers should open TCP port 179 (BGP). See the port requirements for user clusters for more details.For more information, see the following pages:\nProject Calico Official Site\nProject Calico GitHub Page\nWeaveAvailable as of v2.2.0Weave enables networking and network policy in Kubernetes clusters across the cloud. Additionally, it support encrypting traffic between the peers.Kubernetes workers should open TCP port 6783 (control port), UDP port 6783 and UDP port 6784 (data ports). See the port requirements for user clusters for more details.For more information, see the following pages:\nWeave Net Official Site\nCNI Features by ProviderThe following table summarizes the different features available for each CNI network provider provided by Rancher.\n\n\nProvider\nNetwork Model\nRoute Distribution\nNetwork Policies\nMesh\nExternal Datastore\nEncryption\nIngress/Egress Policies\n\n\n\n\n\nCanal\nEncapsulated (VXLAN)\nNo\nYes\nNo\nK8S API\nNo\nYes\n\n\n\nFlannel\nEncapsulated (VXLAN)\nNo\nNo\nNo\nK8S API\nNo\nNo\n\n\n\nCalico\nUnencapsulated\nYes\nYes\nYes\nEtcd\nNo\nYes\n\n\n\nWeave\nEncapsulated\nYes\nYes\nYes\nNo\nYes\nYes\n\n\n\nNetwork Model: Encapsulated or unencapsulated. For more information, see What Network Models are Used in CNI?\n\nRoute Distribution: An exterior gateway protocol designed to exchange routing and reachability information on the Internet. BGP can assist with pod-to-pod networking between clusters. This feature is a must on unencapsulated CNI network providers, and it is typically done by BGP. If you plan to build clusters split across network segments, route distribution is a feature that’s nice-to-have.\n\nNetwork Policies: Kubernetes offers functionality to enforce rules about which services can communicate with each other using network policies. This feature is stable as of Kubernetes v1.7 and is ready to use with certain networking plugins.\n\nMesh: This feature allows service-to-service networking communication between distinct Kubernetes clusters.\n\nExternal Datastore: CNI network providers with this feature need an external datastore for its data.\n\nEncryption: This feature allows cyphered and secure network control and data planes.\n\nIngress/Egress Policies: This feature allows you to manage routing control for both Kubernetes and non-Kubernetes communications.\nCNI Community PopularityThe following table summarizes different GitHub metrics to give you an idea of each project’s popularity and activity. This data was collected in January 2020.\n\n\nProvider\nProject\nStars\nForks\nContributors\n\n\n\n\n\nCanal\nhttps://github.com/projectcalico/canal\n614\n89\n19\n\n\n\nflannel\nhttps://github.com/coreos/flannel\n4977\n1.4k\n140\n\n\n\nCalico\nhttps://github.com/projectcalico/calico\n1534\n429\n135\n\n\n\nWeave\nhttps://github.com/weaveworks/weave/\n5737\n559\n73\n\n\nWhich CNI Provider Should I Use?It depends on your project needs. There are many different providers, which each have various features and options. There isn’t one provider that meets everyone’s needs.As of Rancher v2.0.7, Canal is the default CNI network provider. We recommend it for most use cases. It provides encapsulated networking for containers with Flannel, while adding Calico network policies that can provide project/namespace isolation in terms of networking.How can I configure a CNI network provider?Please see Cluster Options on how to configure a network provider for your cluster. For more advanced configuration options, please see how to configure your cluster using a Config File and the options for Network Plug-ins.","postref":"4639cdbb1ea40a8b3725dc72219c9356","objectID":"e4032d9521c777a5bc418554a966818e","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/faq/networking/cni-providers/"},{"anchor":"#","title":"Importing Existing Clusters into Rancher","content":"When managing an imported cluster, Rancher connects to a Kubernetes cluster that has already been set up. Therefore, Rancher does not provision Kubernetes, but only sets up the Rancher agents to communicate with the cluster.\n\nKeep in mind that editing your Kubernetes cluster still has to be done outside of Rancher. Some examples of editing the cluster include adding and removing nodes, upgrading the Kubernetes version, and changing Kubernetes component parameters.\n\nPrerequisites\n\nIf your existing Kubernetes cluster already has a cluster-admin role defined, you must have this cluster-admin privilege to import the cluster into Rancher.\n\nIn order to apply the privilege, you need to run:\nkubectl create clusterrolebinding cluster-admin-binding \\\n  --clusterrole cluster-admin \\\n  --user [USER_ACCOUNT]\nbefore running the kubectl command to import the cluster.\n\nBy default, GKE users are not given this privilege, so you will need to run the command before importing GKE clusters. To learn more about role-based access control for GKE, please click here.\n\nImporting a Cluster\n\n\nFrom the Clusters page, click Add Cluster.\nChoose Import.\nEnter a Cluster Name.\n\nUse Member Roles to configure user authorization for the cluster.\n\t\n\t\tClick Add Member to add users that can access the cluster.\n\t\tUse the Role drop-down to set permissions for each user.\n\t\n\t\n\n\nClick Create.\nThe prerequisite for cluster-admin privileges is shown (see Prerequisites above), including an example command to fulfil the prerequisite.\nCopy the kubectl command to your clipboard and run it on a node where kubeconfig is configured to point to the cluster you want to import. If you are unsure it is configured correctly, run kubectl get nodes to verify before running the command shown in Rancher\n.\nIf you are using self signed certificates, you will receive the message certificate signed by unknown authority. To work around this validation, copy the command starting with curl displayed in Rancher\n to your clipboard. Then run the command on a node where kubeconfig is configured to point to the cluster you want to import.\nWhen you finish running the command(s) on your node, click Done.\n\n\tResult:\n\t\n\t\tYour cluster is created and assigned a state of Pending. Rancher is deploying resources to manage your cluster.\n\t\tYou can access your cluster after its state is updated to Active.\n\t\tActive clusters are assigned two Projects Default (containing the namespace default) and System (containing the namespaces cattle-system,ingress-nginx,kube-public and kube-system, if present).\n\t\n\n\n\n\n\nNote:\nYou can not re-import a cluster that is currently active in a Rancher setup.\n\n","postref":"a0d239114f7784346a6327801016daf4","objectID":"6c4e2355d1c23263b2a6695cd6991d14","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/imported-clusters/"},{"anchor":"#","title":"Rancher Agents","content":"There are two different agent resources deployed on Rancher managed clusters:\n\n\ncattle-cluster-agent\ncattle-node-agent\n\n\nFor a conceptual overview of how the Rancher server provisions clusters and communicates with them, refer to the architecture\n\ncattle-cluster-agent\n\nThe cattle-cluster-agent is used to connect to the Kubernetes API of Rancher Launched Kubernetes clusters. The cattle-cluster-agent is deployed using a Deployment resource.\n\ncattle-node-agent\n\nThe cattle-node-agent is used to interact with nodes in a Rancher Launched Kubernetes cluster when performing cluster operations. Examples of cluster operations are upgrading Kubernetes version and creating/restoring etcd snapshots. The cattle-node-agent is deployed using a DaemonSet resource to make sure it runs on every node. The cattle-node-agent is used as fallback option to connect to the Kubernetes API of Rancher Launched Kubernetes clusters when cattle-cluster-agent is unavailable.\n\n\nNote: In Rancher v2.2.4 and lower, the cattle-node-agent pods did not tolerate all taints, causing Kubernetes upgrades to fail on these nodes. The fix for this has been included in Rancher v2.2.5 and higher.\n\n\nScheduling rules\n\nApplies to v2.3.0 and higher\n\n\n\n\nComponent\nnodeAffinity nodeSelectorTerms\nnodeSelector\nTolerations\n\n\n\n\n\ncattle-cluster-agent\nbeta.kubernetes.io/os:NotIn:windows\nnone\noperator:Exists\n\n\n\ncattle-node-agent\nbeta.kubernetes.io/os:NotIn:windows\nnone\noperator:Exists\n\n\n\n\nThe cattle-cluster-agent Deployment has preferred scheduling rules using requiredDuringSchedulingIgnoredDuringExecution, favoring to be scheduled on nodes with the controlplane node. See Kubernetes: Assigning Pods to Nodes to find more information about scheduling rules.\n\nThe requiredDuringSchedulingIgnoredDuringExecution configuration is shown in the table below:\n\n\n\n\nWeight\nExpression\n\n\n\n\n\n100\nnode-role.kubernetes.io/controlplane:In:\"true\"\n\n\n\n1\nnode-role.kubernetes.io/etcd:In:\"true\"\n\n\n\n","postref":"1994b73f001878e5de400101cd895c4e","objectID":"5fbefeaaef23a531148a8fc03e560c82","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/rancher-agents/"},{"anchor":"#switching-between-projects","title":"Switching between Projects","content":"To switch between projects, use the drop-down available in the navigation bar. Alternatively, you can switch between projects directly in the navigation bar.\nFrom the Global view, navigate to the project that you want to configure.\n\nSelect Projects/Namespaces from the navigation bar.\n\nSelect the link for the project that you want to open.\n","postref":"cc891f40ff93c2e6be3aafd4bea10863","objectID":"0d104aecd0cebd119c8154800038ec6d","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/project-admin/"},{"anchor":"#general-options","title":"General options","content":"\n\n\nParameter\nEnvironment variable\nDescription\n\n\n\n\n\n--address\nCATTLE_ADDRESS\nThe IP address the node will be registered with (defaults to the IP used to reach 8.8.8.8)\n\n\n\n--internal-address\nCATTLE_INTERNAL_ADDRESS\nThe IP address used for inter-host communication on a private network\n\n\nDynamic IP address optionsFor automation purposes, you can’t have a specific IP address in a command as it has to be generic to be used for every node. For this, we have dynamic IP address options. They are used as a value to the existing IP address options. This is supported for --address and --internal-address.\n\n\nValue\nExample\nDescription\n\n\n\n\n\nInterface name\n--address eth0\nThe first configured IP address will be retrieved from the given interface\n\n\n\nipify\n--address ipify\nValue retrieved from https://api.ipify.org will be used\n\n\n\nawslocal\n--address awslocal\nValue retrieved from http://169.254.169.254/latest/meta-data/local-ipv4 will be used\n\n\n\nawspublic\n--address awspublic\nValue retrieved from http://169.254.169.254/latest/meta-data/public-ipv4 will be used\n\n\n\ndoprivate\n--address doprivate\nValue retrieved from http://169.254.169.254/metadata/v1/interfaces/private/0/ipv4/address will be used\n\n\n\ndopublic\n--address dopublic\nValue retrieved from http://169.254.169.254/metadata/v1/interfaces/public/0/ipv4/address will be used\n\n\n\nazprivate\n--address azprivate\nValue retrieved from http://169.254.169.254/metadata/instance/network/interface/0/ipv4/ipAddress/0/privateIpAddress?api-version=2017-08-01&format=text will be used\n\n\n\nazpublic\n--address azpublic\nValue retrieved from http://169.254.169.254/metadata/instance/network/interface/0/ipv4/ipAddress/0/publicIpAddress?api-version=2017-08-01&format=text will be used\n\n\n\ngceinternal\n--address gceinternal\nValue retrieved from http://metadata.google.internal/computeMetadata/v1/instance/network-interfaces/0/ip will be used\n\n\n\ngceexternal\n--address gceexternal\nValue retrieved from http://metadata.google.internal/computeMetadata/v1/instance/network-interfaces/0/access-configs/0/external-ip will be used\n\n\n\npacketlocal\n--address packetlocal\nValue retrieved from https://metadata.packet.net/2009-04-04/meta-data/local-ipv4 will be used\n\n\n\npacketpublic\n--address packetlocal\nValue retrieved from https://metadata.packet.net/2009-04-04/meta-data/public-ipv4 will be used\n\n\n\n\n\nParameter\nEnvironment variable\nDescription\n\n\n\n\n\n--all-roles\nALL=true\nApply all roles (etcd,controlplane,worker) to the node\n\n\n\n--etcd\nETCD=true\nApply the role etcd to the node\n\n\n\n--controlplane\nCONTROL=true\nApply the role controlplane to the node\n\n\n\n--worker\nWORKER=true\nApply the role worker to the node\n\n\n\n\n\nParameter\nEnvironment variable\nDescription\n\n\n\n\n\n--server\nCATTLE_SERVER\nThe configured Rancher server-url setting which the agent connects to\n\n\n\n--token\nCATTLE_TOKEN\nToken that is needed to register the node in Rancher\n\n\n\n--ca-checksum\nCATTLE_CA_CHECKSUM\nThe SHA256 checksum of the configured Rancher cacerts setting to validate\n\n\n\n--node-name\nCATTLE_NODE_NAME\nOverride the hostname that is used to register the node (defaults to hostname -s)\n\n\n\n--label\nCATTLE_NODE_LABEL\nAdd node labels to the node. For multiple labels, pass additional --label options. (--label key=value)\n\n\n\n--taints\nCATTLE_NODE_TAINTS\nAdd node taints to the node. For multiple taints, pass additional --taints options.  (--taints key=value:effect)\n\n\n","postref":"d6b345637af1edf641a0b311c8be0bc2","objectID":"3e27c9d576b23386d864c89fd8be6c61","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/custom-nodes/agent-options/"},{"anchor":"#","title":"Adding Users to Projects","content":"If you want to provide a user with access and permissions to specific projects and resources within a cluster, assign the user a project membership.\n\nYou can add members to a project as it is created, or add them to an existing project.\n\n\nTip: Want to provide a user with access to all projects within a cluster? See Adding Cluster Members instead.\n\n\nAdding Members to a New Project\n\nYou can add members to a project as you create it (recommended if possible). For details on creating a new project, refer to the cluster administration section.\n\nAdding Members to an Existing Project\n\nFollowing project creation, you can add users as project members so that they can access its resources.\n\n\nFrom the Global view, open the project that you want to add members to.\n\nFrom the main menu, select Members. Then click Add Member.\n\nSearch for the user or group that you want to add to the project.\n\nIf external authentication is configured:\n\n\nRancher returns users from your external authentication source as you type.\n\nA drop-down allows you to add groups instead of individual users. The dropdown only lists groups that you, the logged in user, are included in.\n\n\n\nNote: If you are logged in as a local user, external users do not display in your search results.\n\n\nAssign the user or group Project roles.\n\nWhat are Project Roles?\n\n\nNotes:\n\n\nUsers assigned the Owner or Member role for a project automatically inherit the namespace creation role. However, this role is a Kubernetes ClusterRole, meaning its scope extends to all projects in the cluster. Therefore, users explicitly assigned the Owner or Member role for a project can create namespaces in other projects they’re assigned to, even with only the Read Only role assigned.\n\nFor Custom roles, you can modify the list of individual roles available for assignment.\n\n\nTo add roles to the list, Add a Custom Role.\nTo remove roles from the list, Lock/Unlock Roles.\n\n\n\n\n\nResult: The chosen users are added to the project.\n\n\nTo revoke project membership, select the user and click Delete. This action deletes membership, not the user.\nTo modify a user’s roles in the project, delete them from the project, and then re-add them with modified roles.\n\n","postref":"659d241a55ced371e820ddb60567fb9d","objectID":"c121ea3f466dd332cbe8c5e8d3c8ce10","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/project-admin/project-members/"},{"anchor":"#","title":"Project Resource Quotas","content":"Available as of v2.1.0\n\nIn situations where several teams share a cluster, one team may overconsume the resources available: CPU, memory, storage, services, Kubernetes objects like pods or secrets, and so on.  To prevent this overconsumption, you can apply a resource quota, which is a Rancher feature that limits the resources available to a project or namespace.\n\nThis page is a how-to guide for creating resource quotas in existing projects.\n\nResource quotas can also be set when a new project is created. For details, refer to the section on creating new projects.\n\n\nResource quotas in Rancher include the same functionality as the native version of Kubernetes. However, in Rancher, resource quotas have been extended so that you can apply them to projects. For details on how resource quotas work with projects in Rancher, refer to this page.\n\n\nApplying Resource Quotas to Existing Projects\n\nAvailable as of v2.0.1\n\nEdit resource quotas when:\n\n\nYou want to limit the resources that a project and its namespaces can use.\nYou want to scale the resources available to a project up or down when a research quota is already in effect.\n\n\n\nFrom the Global view, open the cluster containing the project to which you want to apply a resource quota.\n\nFrom the main menu, select Projects/Namespaces.\n\nFind the project that you want to add a resource quota to. From that project, select Ellipsis (…) > Edit.\n\nExpand Resource Quotas and click Add Quota. Alternatively, you can edit existing quotas.\n\nSelect a Resource Type.\n\nEnter values for the Project Limit and the Namespace Default Limit.\n\n\n\n\nField\nDescription\n\n\n\n\n\nProject Limit\nThe overall resource limit for the project.\n\n\n\nNamespace Default Limit\nThe default resource limit available for each namespace. This limit is propagated to each namespace in the project. The combined limit of all project namespaces shouldn’t exceed the project limit.\n\n\n\n\nOptional: Add more quotas.\n\nClick Create.\n\n\nResult: The resource quota is applied to your project and namespaces. When you add more namespaces in the future, Rancher validates that the project can accommodate the namespace. If the project can’t allocate the resources, Rancher won’t let you save your changes.\n","postref":"44cf6e294b630b6165277a8c0e875d99","objectID":"8e7964422319eceee1eb3655e376cd47","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/project-admin/resource-quotas/"},{"anchor":"#","title":"Namespaces","content":"Within Rancher, you can further divide projects into different namespaces, which are virtual clusters within a project backed by a physical cluster. Should you require another level of organization beyond projects and the default namespace, you can use multiple namespaces to isolate applications and resources.\n\nAlthough you assign resources at the project level so that each namespace in the project can use them, you can override this inheritance by assigning resources explicitly to a namespace.\n\nResources that you can assign directly to namespaces include:\n\n\nWorkloads\nLoad Balancers/Ingress\nService Discovery Records\nPersistent Volume Claims\nCertificates\nConfigMaps\nRegistries\nSecrets\n\n\nTo manage permissions in a vanilla Kubernetes cluster, cluster admins configure role-based access policies for each namespace. With Rancher, user permissions are assigned on the project level instead, and permissions are automatically inherited by any namespace owned by the particular project.\n\n\nNote: If you create a namespace with kubectl, it may be unusable because kubectl doesn’t require your new namespace to be scoped within a project that you have access to. If your permissions are restricted to the project level, it is better to create a namespace through Rancher to ensure that you will have permission to access the namespace.\n\n\nCreating Namespaces\n\nCreate a new namespace to isolate apps and resources in a project.\n\nWhen working with project resources that you can assign to a namespace (i.e., workloads, certificates, ConfigMaps, etc.) you can create a namespace on the fly.\n\n\nFrom the Global view, open the project where you want to create a namespace.\n\n\nTip: As a best practice, we recommend creating namespaces from the project level. However, cluster owners and members can create them from the cluster level as well.\n\n\nFrom the main menu, select Namespace. The click Add Namespace.\n\nOptional: If your project has Resource Quotas in effect, you can override the default resource Limits (which places a cap on the resources that the namespace can consume).\n\nEnter a Name and then click Create.\n\n\nResult: Your namespace is added to the project. You can begin assigning cluster resources to the namespace.\n\nMoving Namespaces to Another Project\n\nCluster admins and members may occasionally need to move a namespace to another project, such as when you want a different team to start using the application.\n\n\nFrom the Global view, open the cluster that contains the namespace you want to move.\n\nFrom the main menu, select Projects/Namespaces.\n\nSelect the namespace(s) that you want to move to a different project. Then click Move. You can move multiple namespaces at one.\n\n\nNotes:\n\n\nDon’t move the namespaces in the System project. Moving these namespaces can adversely affect cluster networking.\nYou cannot move a namespace into a project that already has a resource quota configured.\nIf you move a namespace from a project that has a quota set to a project with no quota set, the quota is removed from the namespace.\n\n\n\nChoose a new project for the new namespace and then click Move. Alternatively, you can remove the namespace from all projects by selecting None.\n\n\nResult: Your namespace is moved to a different project (or is unattached from all projects). If any project resources are attached to the namespace, the namespace releases them and then attached resources from the new project.\n\nEditing Namespace Resource Quotas\n\nYou can always override the namespace default limit to provide a specific namespace with access to more (or less) project resources.\n\nFor more information, see how to edit namespace resource quotas.\n","postref":"03cdb208e43b88ede35eecb73435fdbe","objectID":"baa0eb9ceea24b4a65a5fcbd92b79c83","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/project-admin/namespaces/"},{"anchor":"#notifiers-and-alerts","title":"Notifiers and Alerts","content":"Available as of v2.2.0Using Rancher, you can monitor the state and processes of your cluster nodes, Kubernetes components, and software deployments through integration with Prometheus, a leading open-source monitoring solution. For details, refer to the monitoring section.Logging is helpful because it allows you to:\nCapture and analyze the state of your cluster\nLook for trends in your environment\nSave your logs to a safe location outside of your cluster\nStay informed of events like a container crashing, a pod eviction, or a node dying\nMore easily debugg and troubleshoot problems\nRancher can integrate with Elasticsearch, splunk, kafka, syslog, and fluentd.For details, refer to the logging section.Notifiers and alerts are two features that work together to inform you of events in the Rancher system.Notifiers are services that inform you of alert events. You can configure notifiers to send alert notifications to staff best suited to take corrective action. Notifications can be sent with Slack, email, PagerDuty, WeChat, and webhooks.Alerts are rules that trigger those notifications. Before you can receive alerts, you must configure one or more notifier in Rancher. The scope for alerts can be set at either the cluster or project level.","postref":"16947df0429561bace3d45e1bb53ebd7","objectID":"b0610c1fca3d729e7f7d5a9360df7a37","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/project-admin/tools/"},{"anchor":"#alerts-scope","title":"Alerts Scope","content":"To manage project alerts, browse to the project that alerts you want to manage. Then select Tools > Alerts. In versions prior to v2.2.0, you can choose Resources > Alerts. You can:\nDeactivate/Reactive alerts\nEdit alert settings\nDelete unnecessary alerts\nMute firing alerts\nUnmute muted alerts\n\nPrerequisite: Before you can receive project alerts, you must add a notifier.\n\nFrom the Global view, navigate to the project that you want to configure project alerts for. Select Tools > Alerts. In versions prior to v2.2.0, you can choose Resources > Alerts.\n\nClick Add Alert Group.\n\nEnter a Name for the alert that describes its purpose, you could group alert rules for the different purpose.\n\nBased on the type of alert you want to create, complete one of the instruction subsets below.\n\n  \n  Pod Alerts\n  \n    This alert type monitors for the status of a specific pod.\n\n\nSelect the Pod option, and then select a pod from the drop-down.\n\nSelect a pod status that triggers an alert:\n\n\nNot Running\nNot Scheduled\nRestarted <x> times with the last <x> Minutes\n\n\nSelect the urgency level of the alert. The options are:\n\n\nCritical: Most urgent\nWarning: Normal urgency\nInfo: Least urgent\n\n\nSelect the urgency level of the alert based on pod state. For example, select Info for Job pod which stop running after job finished. However, if an important pod isn’t scheduled, it may affect operations, so choose Critical.\n\nConfigure advanced options. By default, the below options will apply to all alert rules within the group. You can disable these advanced options when configuring a specific rule.\n\n\nGroup Wait Time: How long to wait to buffer alerts of the same group before sending initially, default to 30 seconds.\nGroup Interval Time: How long to wait before sending an alert that has been added to a group which contains already fired alerts, default to 30 seconds.\nRepeat Wait Time: How long to wait before sending an alert that has been added to a group which contains already fired alerts, default to 1 hour.\n\n\n\n  \n\n  \n  Workload Alerts\n  \n    This alert type monitors for the availability of a workload.\n\n\nChoose the Workload option. Then choose a workload from the drop-down.\n\nChoose an availability percentage using the slider. The alert is triggered when the workload’s availability on your cluster nodes drops below the set percentage.\n\nSelect the urgency level of the alert.\n\n\nCritical: Most urgent\nWarning: Normal urgency\nInfo: Least urgent\n\n\nSelect the urgency level of the alert based on the percentage you choose and the importance of the workload.\n\nConfigure advanced options. By default, the below options will apply to all alert rules within the group. You can disable these advanced options when configuring a specific rule.\n\n\nGroup Wait Time: How long to wait to buffer alerts of the same group before sending initially, default to 30 seconds.\nGroup Interval Time: How long to wait before sending an alert that has been added to a group which contains already fired alerts, default to 30 seconds.\nRepeat Wait Time: How long to wait before sending an alert that has been added to a group which contains already fired alerts, default to 1 hour.\n\n\n\n  \n\n  \n  Workload Selector Alerts\n  \n    This alert type monitors for the availability of all workloads marked with tags that you’ve specified.\n\n\nSelect the Workload Selector option, and then click Add Selector to enter the key value pair for a label. If one of the workloads drops below your specifications, an alert is triggered. This label should be applied to one or more of your workloads.\n\nSelect the urgency level of the alert.\n\n\nCritical: Most urgent\nWarning: Normal urgency\nInfo: Least urgent\n\n\nSelect the urgency level of the alert based on the percentage you choose and the importance of the workload.\n\nConfigure advanced options. By default, the below options will apply to all alert rules within the group. You can disable these advanced options when configuring a specific rule.\n\n\nGroup Wait Time: How long to wait to buffer alerts of the same group before sending initially, default to 30 seconds.\nGroup Interval Time: How long to wait before sending an alert that has been added to a group which contains already fired alerts, default to 30 seconds.\nRepeat Wait Time: How long to wait before sending an alert that has been added to a group which contains already fired alerts, default to 1 hour.\n\n\n\n  \n\n  \n  Metric Expression Alerts\n  \n    \nAvailable as of v2.2.4\n\nIf you enable project monitoring, this alert type monitors for the overload from Prometheus expression querying.\n\n\nInput or select an Expression, the drop down shows the original metrics from Prometheus, including:\n\n\nContainer\nKubernetes Resources\nCustomize\nProject Level Grafana\nProject Level Prometheus\n\n\nChoose a comparison.\n\n\nEqual: Trigger alert when expression value equal to the threshold.\nNot Equal: Trigger alert when expression value not equal to the threshold.\nGreater Than: Trigger alert when expression value greater than to threshold.\nLess Than: Trigger alert when expression value equal or less than the threshold.\nGreater or Equal: Trigger alert when expression value greater to equal to the threshold.\nLess or Equal: Trigger alert when expression value less or equal to the threshold.\n\n\nInput a Threshold, for trigger alert when the value of expression cross the threshold.\n\nChoose a Comparison.\n\nSelect a Duration, for trigger alert when expression value crosses the threshold longer than the configured duration.\n\nSelect the urgency level of the alert.\n\n\nCritical: Most urgent\nWarning: Normal urgency\nInfo: Least urgent\n\n\nSelect the urgency level of the alert based on its impact on operations. For example, an alert triggered when a expression for container memory close to the limit raises above 60% deems an urgency of Info, but raised about 95% deems an urgency of Critical.\n\n\nConfigure advanced options. By default, the below options will apply to all alert rules within the group. You can disable these advanced options when configuring a specific rule.\n\n\nGroup Wait Time: How long to wait to buffer alerts of the same group before sending initially, default to 30 seconds.\nGroup Interval Time: How long to wait before sending an alert that has been added to a group which contains already fired alerts, default to 30 seconds.\nRepeat Wait Time: How long to wait before sending an alert that has been added to a group which contains already fired alerts, default to 1 hour.\n\n\n\n\n  \n\nContinue adding more Alert Rule to the group.\n\nFinally, choose the notifiers that send you alerts.\n\n\nYou can set up multiple notifiers.\nYou can change notifier recipients on the fly.\n\nResult: Your alert is configured. A notification is sent when the alert is triggered.When you enable monitoring for the project, some project-level alerts are provided. You can receive these alerts if a notifier for them is configured at the cluster level.\n\n\nAlert\nExplanation\n\n\n\n\n\nLess than half workload available\nA critical alert is triggered if less than half of a workload is available, based on workloads where the key is app and the value is workload.\n\n\n\nMemory usage close to the quota\nA warning alert is triggered if the workload’s memory usage exceeds the memory resource quota that is set for the workload. You can see the memory limit in the Rancher UI if you go to the workload under the Security & Host Config tab.\n\n\nFor information on other default alerts, refer to the section on cluster-level alerts.The scope for alerts can be set at either the cluster level or project level.At the project level, Rancher monitors specific deployments and sends alerts for:\nDeployment availability\nWorkloads status\nPod status\nThe Prometheus expression cross the thresholds\n","postref":"ce4492a177877452c7ac5f8c60ed91ad","objectID":"b3672ffcd44666e923e08f34df8b0928","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/project-admin/tools/alerts/"},{"anchor":"#requirements","title":"Requirements","content":"Logging Architecture\nFrom the Global view, navigate to the project that you want to configure project logging.\n\nSelect Tools > Logging in the navigation bar. In versions prior to v2.2.0, you can choose Resources > Logging.\n\nSelect a logging service and enter the configuration. Refer to the specific service for detailed configuration. Rancher supports the following services:\n\n\nElasticsearch\nSplunk\nKafka\nSyslog\nFluentd\n\n\n(Optional) Instead of using the UI to configure the logging services, you can enter custom advanced configurations by clicking on Edit as File, which is located above the logging targets. This link is only visible after you select a logging service.\n\n\nWith the file editor, enter raw fluentd configuration for any logging service. Refer to the documentation for each logging service on how to setup the output configuration.\n\n\nElasticsearch Documentation\nSplunk Documentation\nKafka Documentation\nSyslog Documentation\nFluentd Documentation\n\n\nIf the logging service is using TLS, you also need to complete the SSL Configuration form.\n\n\nProvide the Client Private Key and Client Certificate. You can either copy and paste them or upload them by using the Read from a file button.\n\n\nYou can use either a self-signed certificate or one provided by a certificate authority.\n\nYou can generate a self-signed certificate using an openssl command. For example:\n\nopenssl req -x509 -newkey rsa:2048 -keyout myservice.key -out myservice.cert -days 365 -nodes -subj \"/CN=myservice.example.com\"\n\n\n\nIf you are using a self-signed certificate, provide the CA Certificate PEM.\n\n\n\n(Optional) Complete the Additional Logging Configuration form.\n\n\nOptional: Use the Add Field button to add custom log fields to your logging configuration. These fields are key value pairs (such as foo=bar) that you can use to filter the logs from another system.\n\nEnter a Flush Interval. This value determines how often Fluentd flushes data to the logging server. Intervals are measured in seconds.\n\nInclude System Log. The logs from pods in system project and RKE components will be sent to the target. Uncheck it to exclude the system logs.\n\n\nClick Test. Rancher sends a test log to the service.\n\n\nNote: This button is replaced with Dry Run if you are using the custom configuration editor. In this case, Rancher calls the fluentd dry run command to validate the configuration.\n\n\nClick Save.\nResult: Rancher is now configured to send logs to the selected service. Log into the logging service so that you can start viewing the logs.You can configure logging at either cluster level or project level.\nCluster logging writes logs for every pod in the cluster, i.e. in all the projects. For RKE clusters, it also writes logs for all the Kubernetes system components.\n\nProject logging writes logs for every pod in that particular project.\nLogs that are sent to your logging service are from the following locations:\nPod logs stored at /var/log/containers.\n\nKubernetes system components logs stored at /var/lib/rancher/rke/logs/.\nSetting up a logging service to collect logs from your cluster/project has several advantages:\nLogs errors and warnings in your Kubernetes infrastructure to a stream. The stream informs you of events like a container crashing, a pod eviction, or a node dying.\nAllows you to capture and analyze the state of your cluster and look for trends in your environment using the log stream.\nHelps you when troubleshooting or debugging.\nSaves your logs to a safe location outside of your cluster, so that you can still access them even if your cluster encounters issues.\nThe Docker daemon on each node in the cluster should be configured with the (default) log-driver: json-file. You can check the log-driver by running the following command:$ docker info | grep 'Logging Driver'\nLogging Driver: json-file\n","postref":"76abeba68655940a6214b72fa7b50906","objectID":"fd98ad0365a988c88606bcbe8db21149","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/project-admin/tools/logging/"},{"anchor":"#","title":"Monitoring","content":"Available as of v2.2.4\n\nUsing Rancher, you can monitor the state and processes of your cluster nodes, Kubernetes components, and software deployments through integration with Prometheus, a leading open-source monitoring solution.\n\n\nFor more information about how Prometheus works, refer to the cluster administration section.\n\n\nThis section covers the following topics:\n\n\nMonitoring scope\nPermissions to configure project monitoring\nEnabling project monitoring\nProject-level monitoring resource requirements\nProject metrics\n\n\nMonitoring Scope\n\nUsing Prometheus, you can monitor Rancher at both the cluster level and project level. For each cluster and project that is enabled for monitoring, Rancher deploys a Prometheus server.\n\n\nCluster monitoring allows you to view the health of your Kubernetes cluster. Prometheus collects metrics from the cluster components below, which you can view in graphs and charts.\n\n\nKubernetes control plane\netcd database\nAll nodes (including workers)\n\n\nProject monitoring allows you to view the state of pods running in a given project. Prometheus collects metrics from the project’s deployed HTTP and TCP/UDP workloads.\n\n\nPermissions to Configure Project Monitoring\n\nOnly administrators, cluster owners or members, or project owners can configure project level monitoring. Project members can only view monitoring metrics.\n\nEnabling Project Monitoring\n\n\nPrerequisite: Cluster monitoring must be enabled.\n\n\n\nGo to the project where monitoring should be enabled. Note: When cluster monitoring is enabled, monitoring is also enabled by default in the System project.\n\nSelect Tools > Monitoring in the navigation bar.\n\nSelect Enable to show the Prometheus configuration options. Enter in your desired configuration options.\n\nClick Save.\n\n\nProject-Level Monitoring Resource Requirements\n\n\n\n\nContainer\nCPU - Request\nMem - Request\nCPU - Limit\nMem - Limit\nConfigurable\n\n\n\n\n\nPrometheus\n750m\n750Mi\n1000m\n1000Mi\nYes\n\n\n\nGrafana\n100m\n100Mi\n200m\n200Mi\nNo\n\n\n\n\nResult: A single application,project-monitoring, is added as an application to the project.  After the application is active, you can start viewing project metrics through the Rancher dashboard or directly from Grafana.\n\nProject Metrics\n\nWorkload metrics are available for the project if monitoring is enabled at the cluster level and at the project level.\n\nYou can monitor custom metrics from any exporters. You can also expose some custom endpoints on deployments without needing to configure Prometheus for your project.\n\n\nExample:\nA Redis application is deployed in the namespace redis-app in the project Datacenter. It is monitored via Redis exporter. After enabling project monitoring, you can edit the application to configure the Advanced Options -> Custom Metrics section. Enter the Container Port and Path and select the Protocol.\n\n\nTo access a project-level Grafana instance,\n\n\nFrom the Global view, navigate to a cluster that has monitoring enabled.\n\nGo to a project that has monitoring enabled.\n\nFrom the project view, click Apps. In versions prior to v2.2.0, choose Catalog Apps on the main navigation bar.\n\nGo to the project-monitoring application.\n\nIn the project-monitoring application, there are two /index.html links: one that leads to a Grafana instance and one that leads to a Prometheus instance. When you click the Grafana link, it will redirect you to a new webpage for Grafana, which shows metrics for the cluster.\n\nYou will be signed in to the Grafana instance automatically. The default username is admin and the default password is admin. For security, we recommend that you log out of Grafana, log back in with the admin password, and change your password.\n\n\nResults: You will be logged into Grafana from the Grafana instance. After logging in, you can view the preset Grafana dashboards, which are imported via the Grafana provisioning mechanism, so you cannot modify them directly. For now, if you want to configure your own dashboards, clone the original and modify the new copy.\n","postref":"e70c03d00a832d6042d5e9cf822a44c8","objectID":"2f21da29ba25a2d241f37b84e59aba57","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/project-admin/tools/monitoring/"},{"anchor":"#workloads","title":"Workloads","content":"Within the context of a Rancher project or namespace, resources are files and data that support operation of your pods. Within Rancher, certificates, registries, and secrets are all considered resources. However, Kubernetes classifies resources as different types of secrets. Therefore, within a single project or namespace, individual resources must have unique names to avoid conflicts. Although resources are primarily used to carry sensitive information, they have other uses as well.Resources include:\nCertificates: Files used to encrypt/decrypt data entering or leaving the cluster.\nConfigMaps: Files that store general configuration information, such as a group of config files.\nSecrets: Files that store sensitive data like passwords, tokens, or keys.\nRegistries: Files that carry credentials used to authenticate with private registries.\nBesides launching individual components of an application, you can use the Rancher catalog to start launching applications, which are Helm charts.For more information, see Applications in a Project.After your project has been configured to a version control provider, you can add the repositories and start configuring a pipeline for each repository.For more information, see Pipelines.After you expose your cluster to external requests using a load balancer and/or ingress, it’s only available by IP address. To create a resolveable hostname, you must create a service record, which is a record that maps an IP address, external hostname, DNS record alias, workload(s), or labelled pods to a specific hostname.For more information, see Service Discovery.Load BalancersAfter you launch an application, it’s only available within the cluster. It can’t be reached externally.If you want your applications to be externally accessible, you must add a load balancer to your cluster. Load balancers create a gateway for external connections to access your cluster, provided that the user knows the load balancer’s IP address and the application’s port number.Rancher supports two types of load balancers:\nLayer-4 Load Balancers\nLayer-7 Load Balancers\nFor more information, see load balancers.IngressLoad Balancers can only handle one IP address per service, which means if you run multiple services in your cluster, you must have a load balancer for each service. Running multiples load balancers can be expensive. You can get around this issue by using an ingress.Ingress is a set or rules that act as a load balancer. Ingress works in conjunction with one or more ingress controllers to dynamically route service requests. When the ingress receives a request, the ingress controller(s) in your cluster program the load balancer to direct the request to the correct service based on service subdomains or path rules that you’ve configured.For more information, see Ingress.When using ingresses in a project, you can program the ingress hostname to an external DNS by setting up a Global DNS entry.For more information, see Global DNS.Deploy applications to your cluster nodes using workloads, which are objects that contain pods that run your apps, along with metadata that set rules for the deployment’s behavior. Workloads can be deployed within the scope of the entire clusters or within a namespace.When deploying a workload, you can deploy from any image. There are a variety of workload types to choose from which determine how your application should run.Following a workload deployment, you can continue working with it. You can:\nUpgrade the workload to a newer version of the application it’s running.\nRoll back a workload to a previous version, if an issue occurs during upgrade.\nAdd a sidecar, which is a workload that supports a primary workload.\n","postref":"417b029dbbc114173f179f6bb58d8355","objectID":"d10f74a74464edb9f84b7b363f0d494e","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/k8s-in-rancher/"},{"anchor":"#workload-options","title":"Workload Options","content":"External Links\nServices\nThis section of the documentation contains instructions for deploying workloads and using workload options.\nDeploy Workloads\nUpgrade Workloads\nRollback Workloads\n","postref":"26a1ceb40cbb6b6381a2e7a32a9a44cc","objectID":"b9144509df0895f5b9f947d320f2d7e4","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/k8s-in-rancher/workloads/"},{"anchor":"#","title":"Deploying Workloads","content":"Deploy a workload to run an application in one or more containers.\n\n\nFrom the Global view, open the project that you want to deploy a workload to.\n\n\nClick Resources > Workloads. (In versions prior to v2.3.0, click the Workloads tab.) From the Workloads view, click Deploy.\n\n\nEnter a Name for the workload.\n\nSelect a workload type. The workload defaults to a scalable deployment, by can change the workload type by clicking More options.\n\nFrom the Docker Image field, enter the name of the Docker image that you want to deploy to the project, optionally prefacing it with the registry host (e.g. quay.io, registry.gitlab.com, etc.). During deployment, Rancher pulls this image from the specified public or private registry. If no registry host is provided, Rancher will pull the image from Docker Hub. Enter the name exactly as it appears in the registry server, including any required path, and optionally including the desired tag (e.g. registry.gitlab.com/user/path/image:tag). If no tag is provided, the latest tag will be automatically used.\n\nEither select an existing namespace, or click Add to a new namespace and enter a new namespace.\n\nClick Add Port to enter a port mapping, which enables access to the application inside and outside of the cluster . For more information, see Services.\n\nConfigure the remaining options:\n\n\nEnvironment Variables\n\nUse this section to either specify environment variables for your workload to consume on the fly, or to pull them from another source, such as a secret or ConfigMap.\n\nNode Scheduling\n\nHealth Check\n\nVolumes\n\nUse this section to add storage for your workload. You can manually specify the volume that you want to add, use a persistent volume claim to dynamically create a volume for the workload, or read data for a volume to use from a file such as a ConfigMap.\n\nWhen you are deploying a Stateful Set, you should use a Volume Claim Template when using Persistent Volumes. This will ensure that Persistent Volumes are created dynamically when you scale your Stateful Set. This option is available in the UI as of Rancher v2.2.0.\n\nScaling/Upgrade Policy\n\n\nAmazon Note for Volumes:\n\nTo mount an Amazon EBS volume:\n\n\nIn Amazon AWS, the nodes must be in the same Availability Zone and possess IAM permissions to attach/unattach volumes.\n\nThe cluster must be using the AWS cloud provider option. For more information on enabling this option see Creating an Amazon EC2 Cluster or Creating a Custom Cluster.\n\n\n\n\nClick Show Advanced Options and configure:\n\n\nCommand\nNetworking\nLabels & Annotations\nSecurity and Host Config\n\n\nClick Launch.\n\n\nResult: The workload is deployed to the chosen namespace. You can view the workload’s status from the project’s Workloads view.\n","postref":"ddd3938332175936b65cca22c0804337","objectID":"1283f6c3636cc032d6762ef943c892c5","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/k8s-in-rancher/workloads/deploy-workloads/"},{"anchor":"#managing-hpas","title":"Managing HPAs","content":"In Rancher v2.3.x+, you can see your HPA’s current number of replicas by going to your project and clicking Resources > HPA. For more information, refer to Get HPA Metrics and Status.You can also use kubectl to get the status of HPAs that you test with your load testing tool. For more information, refer to Testing HPAs with kubectl.The way that you manage HPAs is different based on your version of the Kubernetes API:\nFor Kubernetes API version autoscaling/V2beta1: This version of the Kubernetes API lets you autoscale your pods based on the CPU and memory utilization of your application.\nFor Kubernetes API Version autoscaling/V2beta2: This version of the Kubernetes API lets you autoscale your pods based on CPU and memory utilization, in addition to custom metrics.\nHPAs are also managed differently based on your version of Rancher:\nFor Rancher v2.3.0+: You can create, manage, and delete HPAs using the Rancher UI. From the Rancher UI you can configure the HPA to scale based on CPU and memory utilization. For more information, refer to Managing HPAs with the Rancher UI. To scale the HPA based on custom metrics, you still need to use kubectl. For more information, refer to Configuring HPA to Scale Using Custom Metrics with Prometheus.\nFor Rancher Prior to v2.3.0: To manage and configure HPAs, you need to use kubectl. For instructions on how to create, manage, and scale HPAs, refer to Managing HPAs with kubectl.\nYou might have additional HPA installation steps if you are using an older version of Rancher:\nFor Rancher v2.0.7+: Clusters created in Rancher v2.0.7 and higher automatically have all the requirements needed (metrics-server and Kubernetes cluster configuration) to use HPA.\nFor Rancher Prior to v2.0.7: Clusters created in Rancher prior to v2.0.7 don’t automatically have the requirements needed to use HPA. For instructions on installing HPA for these clusters, refer to Manual HPA Installation for Clusters Created Before Rancher v2.0.7.\n","postref":"e3d1ec1b6c76ab60a53b51e936025197","objectID":"69083b803596026f7e1857fee308cd44","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/k8s-in-rancher/horitzontal-pod-autoscaler/"},{"anchor":"#why-use-horizontal-pod-autoscaler","title":"Why Use Horizontal Pod Autoscaler?","content":"HPA is an API resource in the Kubernetes autoscaling API group. The current stable version is autoscaling/v1, which only includes support for CPU autoscaling. To get additional support for scaling based on memory and custom metrics, use the beta version instead: autoscaling/v2beta1.For more information about the HPA API object, see the HPA GitHub Readme.HPA is implemented as a control loop, with a period controlled by the kube-controller-manager flags below:\n\n\nFlag\nDefault\nDescription\n\n\n\n\n\n--horizontal-pod-autoscaler-sync-period\n30s\nHow often HPA audits resource/custom metrics in a deployment.\n\n\n\n--horizontal-pod-autoscaler-downscale-delay\n5m0s\nFollowing completion of a downscale operation, how long HPA must wait before launching another downscale operations.\n\n\n\n--horizontal-pod-autoscaler-upscale-delay\n3m0s\nFollowing completion of an upscale operation, how long HPA must wait before launching another upscale operation.\n\n\nFor full documentation on HPA, refer to the Kubernetes Documentation.Using HPA, you can automatically scale the number of pods within a replication controller, deployment, or replica set up or down. HPA automatically scales the number of pods that are running for maximum efficiency. Factors that affect the number of pods include:\nA minimum and maximum number of pods allowed to run, as defined by the user.\nObserved CPU/memory use, as reported in resource metrics.\nCustom metrics provided by third-party metrics application like Prometheus, Datadog, etc.\nHPA improves your services by:\nReleasing hardware resources that would otherwise be wasted by an excessive number of pods.\nIncrease/decrease performance as needed to accomplish service level agreements.\n","postref":"2bceda28489952ebb59fb3f0c8c863ce","objectID":"d8783d5c2e623d00047fc7dbadc68246","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/k8s-in-rancher/horitzontal-pod-autoscaler/hpa-background/"},{"anchor":"#","title":"Rolling Back Workloads","content":"Sometimes there is a need to rollback to the previous version of the application, either for debugging purposes or because an upgrade did not go as planned.\n\n\nFrom the Global view, open the project running the workload you want to rollback.\n\nFind the workload that you want to rollback and select Vertical Ellipsis (… ) > Rollback.\n\nChoose the revision that you want to roll back to. Click Rollback.\n\n\nResult: Your workload reverts to the previous version that you chose. Wait a few minutes for the action to complete.\n","postref":"132a534810cf94271256afde9bd751f3","objectID":"87dc1f902b03b6c1e1a14fc37e62372f","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/k8s-in-rancher/workloads/rollback-workloads/"},{"anchor":"#creating-an-hpa","title":"Creating an HPA","content":"\nFrom the Global view, open the project that you want to delete an HPA from.\n\nClick Resources > HPA.\n\nFind the HPA which you would like to delete.\n\nClick Ellipsis (…) > Delete.\n\nClick Delete to confirm.\n\nResult: The HPA is deleted from the current cluster.\n\nFrom the Global view, open the project with the HPAs you want to look at.\n\nClick Resources > HPA. The HPA tab shows the number of current replicas.\n\nFor more detailed metrics and status of a specific HPA, click the name of the HPA. This leads to the HPA detail page.\n\nFrom the Global view, open the project that you want to deploy a HPA to.\n\nClick Resources > HPA.\n\nClick Add HPA.\n\nEnter a Name for the HPA.\n\nSelect a Namespace for the HPA.\n\nSelect a Deployment as scale target for the HPA.\n\nSpecify the Minimum Scale and Maximum Scale for the HPA.\n\nConfigure the metrics for the HPA. You can choose memory or CPU usage as the metric that will cause the HPA to scale the service up or down. In the Quantity field, enter the percentage of the workload’s memory or CPU usage that will cause the HPA to scale the service. To configure other HPA metrics, including metrics available from Prometheus, you need to manage HPAs using kubectl.\n\nClick Create to create the HPA.\n\nResult: The HPA is deployed to the chosen namespace. You can view the HPA’s status from the project’s Resources > HPA view.\n","postref":"1b7109c17a4781bb1a48e876947a32ca","objectID":"76b84821a8b6899b43132673e7dc04a7","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/k8s-in-rancher/horitzontal-pod-autoscaler/manage-hpa-with-rancher-ui/"},{"anchor":"#","title":"Upgrading Workloads","content":"When a new version of an application image is released on Docker Hub, you can upgrade any workloads running a previous version of the application to the new one.\n\n\nFrom the Global view, open the project running the workload you want to upgrade.\n\nFind the workload that you want to upgrade and select Vertical Ellipsis (… ) > Edit.\n\nUpdate the Docker Image to the updated version of the application image on Docker Hub.\n\nUpdate any other options that you want to change.\n\nReview and edit the workload’s Scaling/Upgrade policy.\n\nThese options control how the upgrade rolls out to containers that are currently running. For example, for scalable deployments, you can chose whether you want to stop old pods before deploying new ones, or vice versa, as well as the upgrade batch size.\n\nClick Upgrade.\n\n\nResult: The workload begins upgrading its containers, per your specifications. Note that scaling up the deployment or updating the upgrade/scaling policy won’t result in the pods recreation.\n","postref":"da6f3d42488ab2c374ee507e84fc5a5c","objectID":"656c313f04b4037ed3abc5f3d3615fea","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/k8s-in-rancher/workloads/upgrade-workloads/"},{"anchor":"#related-links","title":"Related Links","content":"\nThe Distributed System ToolKit: Patterns for Composite Containers\n","postref":"68d0d0692d0397b5991a632b1ad15e82","objectID":"693b6a0824c8034e39ff108d92fd34fe","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/k8s-in-rancher/workloads/add-a-sidecar/"},{"anchor":"#","title":"Managing HPAs with kubectl","content":"This section describes HPA management with kubectl. This document has instructions for how to:\n\n\nCreate an HPA\nGet information on HPAs\nDelete an HPA\nConfigure your HPAs to scale with CPU or memory utilization\nConfigure your HPAs to scale using custom metrics, if you use a third-party tool such as Prometheus for metrics\n\n\nNote For Rancher v2.3.x\n\nIn Rancher v2.3.x, you can create, view, and delete HPAs from the Rancher UI. You can also configure them to scale based on CPU or memory usage from the Rancher UI. For more information, refer to Managing HPAs with the Rancher UI. For scaling HPAs based on other metrics than CPU or memory, you still need kubectl.\n\nNote For Rancher Prior to v2.0.7\n\nClusters created with older versions of Rancher don’t automatically have all the requirements to create an HPA. To install an HPA on these clusters, refer to Manual HPA Installation for Clusters Created Before Rancher v2.0.7.\n\nBasic kubectl Command for Managing HPAs\n\nIf you have an HPA manifest file, you can create, manage, and delete HPAs using kubectl:\n\n\nCreating HPA\n\n\nWith manifest: kubectl create -f <HPA_MANIFEST>\n\nWithout manifest (Just support CPU): kubectl autoscale deployment hello-world --min=2 --max=5 --cpu-percent=50\n\n\nGetting HPA info\n\n\nBasic: kubectl get hpa hello-world\n\nDetailed description: kubectl describe hpa hello-world\n\n\nDeleting HPA\n\n\nkubectl delete hpa hello-world\n\n\n\nHPA Manifest Definition Example\n\nThe HPA manifest is the config file used for managing an HPA with kubectl.\n\nThe following snippet demonstrates use of different directives in an HPA manifest. See the list below the sample to understand the purpose of each directive.\napiVersion: autoscaling/v2beta1\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: hello-world\nspec:\n  scaleTargetRef:\n    apiVersion: extensions/v1beta1\n    kind: Deployment\n    name: hello-world\n  minReplicas: 1\n  maxReplicas: 10\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      targetAverageUtilization: 50\n  - type: Resource\n    resource:\n      name: memory\n      targetAverageValue: 100Mi\n\n\n\nDirective\nDescription\n\n\n\n\n\napiVersion: autoscaling/v2beta1\nThe version of the Kubernetes autoscaling API group in use. This example manifest uses the beta version, so scaling by CPU and memory is enabled.\n\n\n\nname: hello-world\nIndicates that HPA is performing autoscaling for the hello-word deployment.\n\n\n\nminReplicas: 1\nIndicates that the minimum number of replicas running can’t go below 1.\n\n\n\nmaxReplicas: 10\nIndicates the maximum number of replicas in the deployment can’t go above 10.\n\n\n\ntargetAverageUtilization: 50\nIndicates the deployment will scale pods up when the average running pod uses more than 50% of its requested CPU.\n\n\n\ntargetAverageValue: 100Mi\nIndicates the deployment will scale pods up when the average running pod uses more that 100Mi of memory.\n\n\n\n\n\n\nConfiguring HPA to Scale Using Resource Metrics (CPU and Memory)\n\nClusters created in Rancher v2.0.7 and higher have all the requirements needed (metrics-server and Kubernetes cluster configuration) to use Horizontal Pod Autoscaler. Run the following commands to check if metrics are available in your installation:\n\n$ kubectl top nodes\nNAME                              CPU(cores)   CPU%      MEMORY(bytes)   MEMORY%\nnode-controlplane   196m         9%        1623Mi          42%\nnode-etcd           80m          4%        1090Mi          28%\nnode-worker         64m          3%        1146Mi          29%\n$ kubectl -n kube-system top pods\nNAME                                   CPU(cores)   MEMORY(bytes)\ncanal-pgldr                            18m          46Mi\ncanal-vhkgr                            20m          45Mi\ncanal-x5q5v                            17m          37Mi\ncanal-xknnz                            20m          37Mi\nkube-dns-7588d5b5f5-298j2              0m           22Mi\nkube-dns-autoscaler-5db9bbb766-t24hw   0m           5Mi\nmetrics-server-97bc649d5-jxrlt         0m           12Mi\n$ kubectl -n kube-system logs -l k8s-app=metrics-server\nI1002 12:55:32.172841       1 heapster.go:71] /metrics-server --source=kubernetes.summary_api:https://kubernetes.default.svc?kubeletHttps=true&kubeletPort=10250&useServiceAccount=true&insecure=true\nI1002 12:55:32.172994       1 heapster.go:72] Metrics Server version v0.2.1\nI1002 12:55:32.173378       1 configs.go:61] Using Kubernetes client with master \"https://kubernetes.default.svc\" and version\nI1002 12:55:32.173401       1 configs.go:62] Using kubelet port 10250\nI1002 12:55:32.173946       1 heapster.go:128] Starting with Metric Sink\nI1002 12:55:32.592703       1 serving.go:308] Generated self-signed cert (apiserver.local.config/certificates/apiserver.crt, apiserver.local.config/certificates/apiserver.key)\nI1002 12:55:32.925630       1 heapster.go:101] Starting Heapster API server...\n[restful] 2018/10/02 12:55:32 log.go:33: [restful/swagger] listing is available at https:///swaggerapi\n[restful] 2018/10/02 12:55:32 log.go:33: [restful/swagger] https:///swaggerui/ is mapped to folder /swagger-ui/\nI1002 12:55:32.928597       1 serve.go:85] Serving securely on 0.0.0.0:443\n\n\nIf you have created your cluster in Rancher v2.0.6 or before, please refer to Manual installation\n\nConfiguring HPA to Scale Using Custom Metrics with Prometheus\n\nYou can configure HPA to autoscale based on custom metrics provided by third-party software. The most common use case for autoscaling using third-party software is based on application-level metrics (i.e., HTTP requests per second). HPA uses the custom.metrics.k8s.io API to consume these metrics. This API is enabled by deploying a custom metrics adapter for the metrics collection solution.\n\nFor this example, we are going to use Prometheus. We are beginning with the following assumptions:\n\n\nPrometheus is deployed in the cluster.\nPrometheus is configured correctly and collecting proper metrics from pods, nodes, namespaces, etc.\nPrometheus is exposed at the following URL and port: http://prometheus.mycompany.io:80\n\n\nPrometheus is available for deployment in the Rancher v2.0 catalog. Deploy it from Rancher catalog if it isn’t already running in your cluster.\n\nFor HPA to use custom metrics from Prometheus, package k8s-prometheus-adapter is required in the kube-system namespace of your cluster. To install k8s-prometheus-adapter, we are using the Helm chart available at banzai-charts.\n\n\nInitialize Helm in your cluster.\n\n# kubectl -n kube-system create serviceaccount tiller\nkubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller\nhelm init --service-account tiller\n\n\nClone the banzai-charts repo from GitHub:\n\n# git clone https://github.com/banzaicloud/banzai-charts\n\n\nInstall the prometheus-adapter chart, specifying the Prometheus URL and port number.\n\n# helm install --name prometheus-adapter banzai-charts/prometheus-adapter --set prometheus.url=\"http://prometheus.mycompany.io\",prometheus.port=\"80\" --namespace kube-system\n\n\nCheck that prometheus-adapter is running properly. Check the service pod and logs in the kube-system namespace.\n\n\nCheck that the service pod is Running. Enter the following command.\n\n# kubectl get pods -n kube-system\n\n\nFrom the resulting output, look for a status of Running.\n\nNAME                                  READY     STATUS    RESTARTS   AGE\n...\nprometheus-adapter-prometheus-adapter-568674d97f-hbzfx   1/1       Running   0          7h\n...\n\n\nCheck the service logs to make sure the service is running correctly by entering the command that follows.\n\n# kubectl logs prometheus-adapter-prometheus-adapter-568674d97f-hbzfx -n kube-system\n\n\nThen review the log output to confirm the service is running.\n\n  \n  Prometheus Adaptor Logs\n  \n    ...\nI0724 10:18:45.696679       1 round_trippers.go:436] GET https://10.43.0.1:443/api/v1/namespaces/default/pods?labelSelector=app%3Dhello-world 200 OK in 2 milliseconds\nI0724 10:18:45.696695       1 round_trippers.go:442] Response Headers:\nI0724 10:18:45.696699       1 round_trippers.go:445]     Date: Tue, 24 Jul 2018 10:18:45 GMT\nI0724 10:18:45.696703       1 round_trippers.go:445]     Content-Type: application/json\nI0724 10:18:45.696706       1 round_trippers.go:445]     Content-Length: 2581\nI0724 10:18:45.696766       1 request.go:836] Response Body: {\"kind\":\"PodList\",\"apiVersion\":\"v1\",\"metadata\":{\"selfLink\":\"/api/v1/namespaces/default/pods\",\"resourceVersion\":\"6237\"},\"items\":[{\"metadata\":{\"name\":\"hello-world-54764dfbf8-q6l82\",\"generateName\":\"hello-world-54764dfbf8-\",\"namespace\":\"default\",\"selfLink\":\"/api/v1/namespaces/default/pods/hello-world-54764dfbf8-q6l82\",\"uid\":\"484cb929-8f29-11e8-99d2-067cac34e79c\",\"resourceVersion\":\"4066\",\"creationTimestamp\":\"2018-07-24T10:06:50Z\",\"labels\":{\"app\":\"hello-world\",\"pod-template-hash\":\"1032089694\"},\"annotations\":{\"cni.projectcalico.org/podIP\":\"10.42.0.7/32\"},\"ownerReferences\":[{\"apiVersion\":\"extensions/v1beta1\",\"kind\":\"ReplicaSet\",\"name\":\"hello-world-54764dfbf8\",\"uid\":\"4849b9b1-8f29-11e8-99d2-067cac34e79c\",\"controller\":true,\"blockOwnerDeletion","postref":"0588b11e2de99b957d2df1392987f86f","objectID":"2dfaf607443e5e23eb3e25eae9c8392f","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/k8s-in-rancher/horitzontal-pod-autoscaler/manage-hpa-with-kubectl/"},{"anchor":"#","title":"Testing HPAs with kubectl","content":"This document describes how to check the status of your HPAs after scaling them up or down with your load testing tool. For information on how to check the status from the Rancher UI (at least version 2.3.x), refer to Managing HPAs with the Rancher UI.\n\nFor HPA to work correctly, service deployments should have resources request definitions for containers. Follow this hello-world example to test if HPA is working correctly.\n\n\nConfigure kubectl to connect to your Kubernetes cluster.\n\nCopy the hello-world deployment manifest below.\n\n  \n  Hello World Manifest\n  \n    apiVersion: apps/v1beta2\nkind: Deployment\nmetadata:\n  labels:\n    app: hello-world\n  name: hello-world\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hello-world\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: hello-world\n    spec:\n      containers:\n      - image: rancher/hello-world\n        imagePullPolicy: Always\n        name: hello-world\n        resources:\n          requests:\n            cpu: 500m\n            memory: 64Mi\n        ports:\n        - containerPort: 80\n          protocol: TCP\n      restartPolicy: Always\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: hello-world\n  namespace: default\nspec:\n  ports:\n  - port: 80\n    protocol: TCP\n    targetPort: 80\n  selector:\n    app: hello-world\n\n\n  \n\n\n\nDeploy it to your cluster.\n\n# kubectl create -f <HELLO_WORLD_MANIFEST>\n\n\nCopy one of the HPAs below based on the metric type you’re using:\n\n  \n  Hello World HPA: Resource Metrics\n  \n    apiVersion: autoscaling/v2beta1\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: hello-world\n  namespace: default\nspec:\n  scaleTargetRef:\n    apiVersion: extensions/v1beta1\n    kind: Deployment\n    name: hello-world\n  minReplicas: 1\n  maxReplicas: 10\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      targetAverageUtilization: 50\n  - type: Resource\n    resource:\n      name: memory\n      targetAverageValue: 1000Mi\n\n\n  \n\n\n\n  \n  Hello World HPA: Custom Metrics\n  \n    apiVersion: autoscaling/v2beta1\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: hello-world\n  namespace: default\nspec:\n  scaleTargetRef:\n    apiVersion: extensions/v1beta1\n    kind: Deployment\n    name: hello-world\n  minReplicas: 1\n  maxReplicas: 10\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      targetAverageUtilization: 50\n  - type: Resource\n    resource:\n      name: memory\n      targetAverageValue: 100Mi\n  - type: Pods\n    pods:\n      metricName: cpu_system\n      targetAverageValue: 20m\n\n\n  \n\n\n\nView the HPA info and description. Confirm that metric data is shown.\n\n  \n  Resource Metrics\n  \n    \nEnter the following commands.\n\n# kubectl get hpa\nNAME          REFERENCE                TARGETS                     MINPODS   MAXPODS   REPLICAS   AGE\nhello-world   Deployment/hello-world   1253376 / 100Mi, 0% / 50%   1         10        1          6m\n# kubectl describe hpa\nName:                                                  hello-world\nNamespace:                                             default\nLabels:                                                <none>\nAnnotations:                                           <none>\nCreationTimestamp:                                     Mon, 23 Jul 2018 20:21:16 +0200\nReference:                                             Deployment/hello-world\nMetrics:                                               ( current / target )\n  resource memory on pods:                             1253376 / 100Mi\n  resource cpu on pods  (as a percentage of request):  0% (0) / 50%\nMin replicas:                                          1\nMax replicas:                                          10\nConditions:\n  Type            Status  Reason              Message\n  ----            ------  ------              -------\n  AbleToScale     True    ReadyForNewScale    the last scale time was sufficiently old as to warrant a new scale\n  ScalingActive   True    ValidMetricFound    the HPA was able to successfully calculate a replica count from memory resource\n  ScalingLimited  False   DesiredWithinRange  the desired count is within the acceptable range\nEvents:           <none>\n\n\n\n  \n\n\n\n  \n  Custom Metrics\n  \n    \nEnter the following command.\n\n# kubectl describe hpa\n\n\nYou should receive the output that follows.\n\nName:                                                  hello-world\nNamespace:                                             default\nLabels:                                                <none>\nAnnotations:                                           <none>\nCreationTimestamp:                                     Tue, 24 Jul 2018 18:36:28 +0200\nReference:                                             Deployment/hello-world\nMetrics:                                               ( current / target )\n  resource memory on pods:                             3514368 / 100Mi\n  \"cpu_system\" on pods:                                0 / 20m\n  resource cpu on pods  (as a percentage of request):  0% (0) / 50%\nMin replicas:                                          1\nMax replicas:                                          10\nConditions:\n  Type            Status  Reason              Message\n  ----            ------  ------              -------\n  AbleToScale     True    ReadyForNewScale    the last scale time was sufficiently old as to warrant a new scale\n  ScalingActive   True    ValidMetricFound    the HPA was able to successfully calculate a replica count from memory resource\n  ScalingLimited  False   DesiredWithinRange  the desired count is within the acceptable range\nEvents:           <none>\n\n\n\n  \n\n\n\nGenerate a load for the service to test that your pods autoscale as intended. You can use any load-testing tool (Hey, Gatling, etc.), but we’re using Hey.\n\nTest that pod autoscaling works as intended.\nTo Test Autoscaling Using Resource Metrics:\n\n  \n  Upscale to 2 Pods: CPU Usage Up to Target\n  \n    Use your load testing tool to scale up to two pods based on CPU Usage.\n\n\nView your HPA.\n\n# kubectl describe hpa\n\n\nYou should receive output similar to what follows.\n\nName:                                                  hello-world\nNamespace:                                             default\nLabels:                                                <none>\nAnnotations:                                           <none>\nCreationTimestamp:                                     Mon, 23 Jul 2018 22:22:04 +0200\nReference:                                             Deployment/hello-world\nMetrics:                                               ( current / target )\n  resource memory on pods:                             10928128 / 100Mi\n  resource cpu on pods  (as a percentage of request):  56% (280m) / 50%\nMin replicas:                                          1\nMax replicas:                                          10\nConditions:\n  Type            Status  Reason              Message\n  ----            ------  ------              -------\n  AbleToScale     True    SucceededRescale    the HPA controller was able to update the target scale to 2\n  ScalingActive   True    ValidMetricFound    the HPA was able to successfully calculate a replica count from cpu resource utilization (percentage of request)\n  ScalingLimited  False   DesiredWithinRange  the desired count is within the acceptable range\nEvents:\n  Type    Reason             Age   From                       Message\n  ----    ------             ----  ----                       -------\n  Normal  SuccessfulRescale  13s   horizontal-pod-autoscaler  New size: 2; reason: cpu resource utilization (percentage of request) above target\n\n\nEnter the following command to confirm you’ve scaled to two pods.\n\n  # kubectl get pods\n\n\nYou should receive output similar to what follows:\n\n  NAME                                                     READY     STATUS    RESTARTS   AGE\n  hello-world-54764dfbf8-k8ph2                             1/1       Running   0          1m\n  hello-world-54764dfbf8-q6l4v                             1/1       Running   0          3h\n\n\n\n  \n\n\n\n  \n  Upscale to 3 pods: CPU Usage Up to Target\n  \n    Use your load testing tool to upscale to 3 pods based on CPU usage with horizontal-pod-autoscaler-upscale-delay set to 3 minutes.\n\n\nEnter the following command.\n\n# kubectl describe hpa\n\n\nYou should receive output similar to what follows\n\n  Name:                                                  hello-world\n  Namespace:                                             default\n  Labels:                                                <none>\n  Annotations:                                           <none>\n  CreationTimestamp:                                     Mon, 23 Jul 2018 22:22:04 +0200\n  Reference:                                             Deployment/hello-world\n  Metrics:                                               ( current / target )\n    resource memory on pods:                             9424896 / 100Mi\n    resource cpu on pods  (as a percentage of requ","postref":"c328f68ee7053ae9b22612c1ee05395d","objectID":"304f60d4c6992252b5f8c6ee3ebf1385","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/k8s-in-rancher/horitzontal-pod-autoscaler/testing-hpa/"},{"anchor":"#load-balancers","title":"Load Balancers","content":"As mentioned in the limitations above, the disadvantages of using a load balancer are:\nLoad Balancers can only handle one IP address per service.\nIf you run multiple services in your cluster, you must have a load balancer for each service.\nIt can be expensive to have a load balancer for every service.\nIn contrast, when an ingress is used as the entrypoint into a cluster, the ingress can route traffic to multiple services with greater flexibility. It can map multiple HTTP requests to services without individual IP addresses for each service.Therefore, it is useful to have an ingress if you want multiple services to be exposed with the same IP address, the same Layer 7 protocol, or the same privileged node-ports: 80 and 443.Ingress works in conjunction with one or more ingress controllers to dynamically route service requests. When the ingress receives a request, the ingress controller(s) in your cluster direct the request to the correct service based on service subdomains or path rules that you’ve configured.Each Kubernetes Ingress resource corresponds roughly to a file in /etc/nginx/sites-available/ containing a server{} configuration block, where requests for specific files and folders are configured.Your ingress, which creates a port of entry to your cluster similar to a load balancer, can reside within your cluster or externally. Ingress and ingress controllers residing in RKE-launcher clusters are powered by Nginx.Ingress can provide other functionality as well, such as SSL termination, name-based virtual hosting, and more.\nUsing Rancher in a High Availability Configuration?\n\nRefrain from adding an Ingress to the local cluster. The Nginx Ingress Controller that Rancher uses acts as a global entry point for all clusters managed by Rancher, including the local cluster.  Therefore, when users try to access an application, your Rancher connection may drop due to the Nginx configuration being reloaded. We recommend working around this issue by deploying applications only in clusters that you launch using Rancher.\n\nFor more information on how to set up ingress in Rancher, see Ingress.\nFor complete information about ingress and ingress controllers, see the Kubernetes Ingress Documentation\nWhen using ingresses in a project, you can program the ingress hostname to an external DNS by setting up a Global DNS entry, see Global DNS.\nAfter you launch an application, the app is only available within the cluster. It can’t be reached from outside the cluster.If you want your applications to be externally accessible, you must add a load balancer or ingress to your cluster. Load balancers create a gateway for external connections to access your cluster, provided that the user knows the load balancer’s IP address and the application’s port number.Rancher supports two types of load balancers:\nLayer-4 Load Balancers\nLayer-7 Load Balancers\nFor more information, see load balancers.Load Balancer LimitationsLoad Balancers have a couple of limitations you should be aware of:\nLoad Balancers can only handle one IP address per service, which means if you run multiple services in your cluster, you must have a load balancer for each service. Running multiples load balancers can be expensive.\n\nIf you want to use a load balancer with a Hosted Kubernetes cluster (i.e., clusters hosted in GKE, EKS, or AKS), the load balancer must be running within that cloud provider’s infrastructure. Please review the compatibility tables regarding support for load balancers based on how you’ve provisioned your clusters:\n\n\nSupport for Layer-4 Load Balancing\n\nSupport for Layer-7 Load Balancing\n\n","postref":"d6f02e2b1d65a2e3e5b431700f0e44bd","objectID":"630954264a26b99ac0096d9121892eb5","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/k8s-in-rancher/load-balancers-and-ingress/"},{"anchor":"#layer-4-load-balancer","title":"Layer-4 Load Balancer","content":"\nCreate an External Load Balancer\nTutorials\nKubernetes installation with External Load Balancer (HTTPS/Layer 7)\nKubernetes installation with External Load Balancer (TCP/Layer 4)\nDocker Installation with External Load Balancer\nLayer-7 load balancer (or the ingress controller) supports host and path-based load balancing and SSL termination. Layer-7 load balancer only forwards HTTP and HTTPS traffic and therefore they listen on ports 80 and 443 only. Cloud providers such as Amazon and Google support layer-7 load balancer. In addition, RKE clusters deploys the Nginx Ingress Controller.Support for Layer-7 Load BalancingSupport for layer-7 load balancer varies based on the underlying cloud provider.\n\n\nCluster Deployment\nLayer-7 Load Balancer Support\n\n\n\n\n\nAmazon EKS\nSupported by AWS cloud provider\n\n\n\nGoogle GKE\nSupported by GKE cloud provider\n\n\n\nAzure AKS\nNot Supported\n\n\n\nRKE on EC2\nNginx Ingress Controller\n\n\n\nRKE on DigitalOcean\nNginx Ingress Controller\n\n\n\nRKE on vSphere\nNginx Ingress Controller\n\n\n\nRKE on Custom Hosts(e.g. bare-metal servers)\nNginx Ingress Controller\n\n\nHost Names in Layer-7 Load BalancerSome cloud-managed layer-7 load balancers (such as the ALB ingress controller on AWS) expose DNS addresses for ingress rules. You need to map (via CNAME) your domain name to the DNS address generated by the layer-7 load balancer.Other layer-7 load balancers, such as the Google Load Balancer or Nginx Ingress Controller, directly expose one or more IP addresses. Google Load Balancer provides a single routable IP address. Nginx Ingress Controller exposes the external IP of all nodes that run the Nginx Ingress Controller. You can do either of the following:\nConfigure your own DNS to map (via A records) your domain name to the IP addresses exposes by the Layer-7 load balancer.\nAsk Rancher to generate an xip.io host name for your ingress rule. Rancher will take one of your exposed IPs, say a.b.c.d, and generate a host name ..a.b.c.d.xip.io.\nThe benefit of using xip.io is that you obtain a working entrypoint URL immediately after you create the ingress rule. Setting up your own domain name, on the other hand, requires you to configure DNS servers and wait for DNS to propagate.Layer-4 load balancer (or the external load balancer) forwards traffic to Nodeports. Layer-4 load balancer allows you to forward both HTTP and TCP traffic.Often, the Layer-4 load balancer is supported by the underlying cloud provider, so when you deploy RKE clusters on bare-metal servers and vSphere clusters, Layer-4 load balancer is not supported. However, a single globally managed config-map can be used to expose services on NGINX or third-party ingress.\nNote: It is possible to deploy a cluster with a non-cloud load balancer, such as MetalLB. However, that use case is more advanced than the Layer-4 load balancer supported by a cloud provider, and it is not configurable in Rancher or RKE.\nSupport for Layer-4 Load BalancingSupport for layer-4 load balancer varies based on the underlying cloud provider.\n\n\nCluster Deployment\nLayer-4 Load Balancer Support\n\n\n\n\n\nAmazon EKS\nSupported by AWS cloud provider\n\n\n\nGoogle GKE\nSupported by GCE cloud provider\n\n\n\nAzure AKS\nSupported by Azure cloud provider\n\n\n\nRKE on EC2\nSupported by AWS cloud provider\n\n\n\nRKE on DigitalOcean\nLimited NGINX or third-party Ingress*\n\n\n\nRKE on vSphere\nLimited NGINX or third party-Ingress*\n\n\n\nRKE on Custom Hosts(e.g. bare-metal servers)\nLimited NGINX or third-party Ingress*\n\n\n\nThird-party MetalLB\nLimited NGINX or third-party Ingress*\n\n\n* Services can be exposed through a single globally managed config-map.","postref":"d035bd70f3f860902720e484c10610cd","objectID":"c7748827761377e85215a0f00e227a12","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/k8s-in-rancher/load-balancers-and-ingress/load-balancers/"},{"anchor":"#","title":"Adding Ingresses to Your Project","content":"Ingress can be added for workloads to provide load balancing, SSL termination and host/path based routing. When using ingresses in a project, you can program the ingress hostname to an external DNS by setting up a Global DNS entry.\n\n\nFrom the Global view, open the project that you want to add ingress to.\n\nClick Resources in the main navigation bar. Click the Load Balancing tab. (In versions prior to v2.3.0, just click the Load Balancing tab.) Then click Add Ingress.\n\nEnter a Name for the ingress.\n\nSelect an existing Namespace from the drop-down list. Alternatively, you can create a new namespace on the fly by clicking Add to a new namespace.\n\nCreate ingress forwarding Rules.\n\n\nAutomatically generate a xip.io hostname\n\nIf you choose this option, ingress routes requests to hostname to a DNS name that’s automatically generated. Rancher uses xip.io to automatically generates the DNS name. This option is best used for testing, not production environments.\n\n\nNote: To use this option, you must be able to resolve to xip.io addresses.\n\n\n\nAdd a Target Backend. By default, a workload is added to the ingress, but you can add more targets by clicking either Service or Workload.\n\nOptional: If you want specify a workload or service when a request is sent to a particular hostname path, add a Path for the target. For example, if you want requests for www.mysite.com/contact-us to be sent to a different service than www.mysite.com, enter /contact-us in the Path field.\n\nTypically, the first rule that you create does not include a path.\n\nSelect a workload or service from the Target drop-down list for each target you’ve added.\n\nEnter the Port number that each target operates on.\n\n\nSpecify a hostname to use\n\nIf you use this option, ingress routes requests for a hostname to the service or workload that you specify.\n\n\nEnter the hostname that your ingress will handle request forwarding for. For example, www.mysite.com.\n\nAdd a Target Backend. By default, a workload is added to the ingress, but you can add more targets by clicking either Service or Workload.\n\nOptional: If you want specify a workload or service when a request is sent to a particular hostname path, add a Path for the target. For example, if you want requests for www.mysite.com/contact-us to be sent to a different service than www.mysite.com, enter /contact-us in the Path field.\n\nTypically, the first rule that you create does not include a path.\n\nSelect a workload or service from the Target drop-down list for each target you’ve added.\n\nEnter the Port number that each target operates on.\n\n\nUse as the default backend\n\nUse this option to set an ingress rule for handling requests that don’t match any other ingress rules. For example, use this option to route requests that can’t be found to a 404 page.\n\n\nNote: If you deployed Rancher using RKE, a default backend for 404s and 202s is already configured.\n\n\n\nAdd a Target Backend. Click either Service or Workload to add the target.\n\nSelect a service or workload from the Target drop-down list.\n\n\n\nOptional: click Add Rule to create additional ingress rules. For example, after you create ingress rules to direct requests for your hostname, you’ll likely want to create a default backend to handle 404s.\n\nIf any of your ingress rules handle requests for encrypted ports, add a certificate to encrypt/decrypt communications.\n\n\nNote: You must have an SSL certificate that the ingress can use to encrypt/decrypt communications. For more information see Adding SSL Certificates.\n\n\n\nClick Add Certificate.\n\nSelect a Certificate from the drop-down list.\n\nEnter the Host using encrypted communication.\n\nTo add additional hosts that use the certificate, click Add Hosts.\n\n\nOptional: Add Labels and/or Annotations to provide metadata for your ingress.\n\nFor a list of annotations available for use, see the Nginx Ingress Controller Documentation.\n\n\nResult: Your ingress is added to the project. The ingress begins enforcing your ingress rules.\n","postref":"6a9a4f3a2b4ba5336d61fbc0ba8e2085","objectID":"5c39d002a6a1c5e4f17ae6e22a5db87d","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/k8s-in-rancher/load-balancers-and-ingress/ingress/"},{"anchor":"#related-links","title":"Related Links","content":"\nAdding entries to Pod /etc/hosts with HostAliases\n","postref":"3dfae7f0f1d57bb813bf43a5e4f0e0e4","objectID":"8a1bcfdea40984f3065666b41f8a88d5","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/k8s-in-rancher/service-discovery/"},{"anchor":"#concepts","title":"Concepts","content":"For your convenience, the following variables are available for your pipeline configuration scripts. During pipeline executions, these variables are replaced by metadata. You can reference them in the form of ${VAR_NAME}.\n\n\nVariable Name\nDescription\n\n\n\n\n\nCICD_GIT_REPO_NAME\nRepository name (Github organization omitted).\n\n\n\nCICD_GIT_URL\nURL of the Git repository.\n\n\n\nCICD_GIT_COMMIT\nGit commit ID being executed.\n\n\n\nCICD_GIT_BRANCH\nGit branch of this event.\n\n\n\nCICD_GIT_REF\nGit reference specification of this event.\n\n\n\nCICD_GIT_TAG\nGit tag name, set on tag event.\n\n\n\nCICD_EVENT\nEvent that triggered the build (push, pull_request or tag).\n\n\n\nCICD_PIPELINE_ID\nRancher ID for the pipeline.\n\n\n\nCICD_EXECUTION_SEQUENCE\nBuild number of the pipeline.\n\n\n\nCICD_EXECUTION_ID\nCombination of {CICD_PIPELINE_ID}-{CICD_EXECUTION_SEQUENCE}.\n\n\n\nCICD_REGISTRY\nAddress for the Docker registry for the previous publish image step, available in the Kubernetes manifest file of a Deploy YAML step.\n\n\n\nCICD_IMAGE\nName of the image built from the previous publish image step, available in the Kubernetes manifest file of a Deploy YAML step. It does not contain the image tag. Example\n\n\nWithin a pipeline, there are multiple advanced options for different parts of the pipeline.\nTrigger Rules\nEnvironment Variables\nSecrets\nTrigger RulesTrigger rules can be created to have fine-grained control of pipeline executions in your pipeline configuration. Trigger rules come in two types:\nRun this when:\n\nThis type of rule starts the pipeline, stage, or step when a trigger explicitly occurs.\n\nDo Not Run this when:\n\nThis type of rule skips the pipeline, stage, or step when a trigger explicitly occurs.\nIf all conditions evaluate to true, then the pipeline/stage/step is executed. Otherwise it is skipped. When a pipeline is skipped, none of the pipeline is executed. When a stage/step is skipped, it is considered successful and follow-up stages/steps continue to run.Wildcard character (*) expansion is supported in branch conditions.\n  \n  \n  \nFrom the Global view, navigate to the project that you want to configure a pipeline trigger rule.\n\nClick Resources > Pipelines. In versions prior to v2.3.0, click Workloads > Pipelines.\n\nFrom the repository for which you want to manage trigger rules, select the vertical Ellipsis (…) > Edit Config.\n\nClick on Show Advanced Options.\n\nIn the Trigger Rules section, configure rules to run or skip the pipeline.\n\n\nClick Add Rule. In the Value field, enter the name of the branch that triggers the pipeline.\n\nOptional: Add more branches that trigger a build.\n\n\nClick Done.\n\n\n\n\n\n  \nFrom the Global view, navigate to the project that you want to configure a stage trigger rule.\n\nClick Resources > Pipelines. In versions prior to v2.3.0, click Workloads > Pipelines.\n\nFrom the repository for which you want to manage trigger rules, select the vertical Ellipsis (…) > Edit Config.\n\nFind the stage that you want to manage trigger rules, click the Edit icon for that stage.\n\nClick Show advanced options.\n\nIn the Trigger Rules section, configure rules to run or skip the stage.\n\n\nClick Add Rule.\n\nChoose the Type that triggers the stage and enter a value.\n\n\n\n\nType\nValue\n\n\n\n\n\nBranch\nThe name of the branch that triggers the stage.\n\n\n\nEvent\nThe type of event that triggers the stage. Values are: Push, Pull Request, Tag\n\n\n\n\n\nClick Save.\n\n\n\n\n\n  \nFrom the Global view, navigate to the project that you want to configure a stage trigger rule.\n\nClick Resources > Pipelines. In versions prior to v2.3.0, click Workloads > Pipelines.\n\nFrom the repository for which you want to manage trigger rules, select the vertical Ellipsis (…) > Edit Config.\n\nFind the step that you want to manage trigger rules, click the Edit icon for that step.\n\nClick Show advanced options.\n\nIn the Trigger Rules section, configure rules to run or skip the step.\n\n\nClick Add Rule.\n\nChoose the Type that triggers the step and enter a value.\n\n\n\n\nType\nValue\n\n\n\n\n\nBranch\nThe name of the branch that triggers the step.\n\n\n\nEvent\nThe type of event that triggers the step. Values are: Push, Pull Request, Tag\n\n\n\n\n\nClick Save.\n\n\n\n\n\n  \n# example\nstages:\n  - name: Build something\n    # Conditions for stages\n    when:\n      branch: master\n      event: [ push, pull_request ]\n    # Multiple steps run concurrently\n    steps:\n    - runScriptConfig:\n        image: busybox\n        shellScript: date -R\n      # Conditions for steps\n      when:\n        branch: [ master, dev ]\n        event: push\n# branch conditions for the pipeline\nbranch:\n  include: [ master, feature/*]\n  exclude: [ dev ]\n\n\n\n\nEnvironment VariablesWhen configuring a pipeline, certain step types allow you to use environment variables to configure the step’s script.\n  \n  \n  \nFrom the Global view, navigate to the project that you want to configure pipelines.\n\nClick Resources > Pipelines. In versions prior to v2.3.0, click Workloads > Pipelines.\n\nFrom the pipeline for which you want to edit build triggers, select Ellipsis (…) > Edit Config.\n\nWithin one of the stages, find the step that you want to add an environment variable for, click the Edit icon.\n\nClick Show advanced options.\n\nClick Add Variable, and then enter a key and value in the fields that appear. Add more variables if needed.\n\nAdd your environment variable(s) into either the script or file.\n\nClick Save.\n\n\n\n\n\n  \n# example\nstages:\n  - name: Build something\n    steps:\n    - runScriptConfig:\n        image: busybox\n        shellScript: echo ${FIRST_KEY} && echo ${SECOND_KEY}\n      env:\n        FIRST_KEY: VALUE\n        SECOND_KEY: VALUE2\n\n\n\n\nSecretsIf you need to use security-sensitive information in your pipeline scripts (like a password), you can pass them in using Kubernetes secrets.PrerequisiteCreate a secret in the same project as your pipeline, or explicitly in the namespace where pipeline build pods run.\n\nNote: Secret injection is disabled on pull request events.\n\n  \n  \n  \nFrom the Global view, navigate to the project that you want to configure pipelines.\n\nClick Resources > Pipelines. In versions prior to v2.3.0, click Workloads > Pipelines.\n\nFrom the pipeline for which you want to edit build triggers, select Ellipsis (…) > Edit Config.\n\nWithin one of the stages, find the step that you want to use a secret for, click the Edit icon.\n\nClick Show advanced options.\n\nClick Add From Secret. Select the secret file that you want to use. Then choose a key. Optionally, you can enter an alias for the key.\n\nClick Save.\n\n\n\n\n\n  \n# example\nstages:\n  - name: Build something\n    steps:\n    - runScriptConfig:\n        image: busybox\n        shellScript: echo ${ALIAS_ENV}\n      # environment variables from project secrets\n      envFrom:\n      - sourceName: my-secret\n        sourceKey: secret-key\n        targetKey: ALIAS_ENV\n\n\n\n\nWithin each stage, you can add as many steps as you’d like. When there are multiple steps in one stage, they run concurrently.\nRun Script\nBuild and Publish Images\nPublish Catalog Template\nDeploy YAML\nDeploy Catalog App\nRun ScriptThe Run Script step executes arbitrary commands in the workspace inside a specified container. You can use it to build, test and do more, given whatever utilities the base image provides. For your convenience, you can use variables to refer to metadata of a pipeline execution. Please refer to the pipeline variable substitution reference for the list of available variables.\n  \n  \n  \nFrom the Step Type drop-down, choose Run Script and fill in the form.\n\nClick Add.\n\n\n\n\n\n  \n# example\nstages:\n- name: Build something\n  steps:\n  - runScriptConfig:\n      image: golang\n      shellScript: go build\n\n\n\n\nBuild and Publish ImagesThe Build and Publish Image step builds and publishes a Docker image. This process requires a Dockerfile in your source code’s repository to complete successfully.Available as of Rancher v2.1.0The option to publish an image to an insecure registry is not exposed in the UI, but you can specify an environment variable in the YAML that allows you to publish an image insecurely.\n  \n  \n  \nFrom the Step Type drop-down, choose Build and Publish.\n\nFill in the rest of the form. Descriptions for each field are listed below. When you’re done, click Add.\n\n\n\n\nField\nDescription\n\n\n\n\n\nDockerfile Path\nThe relative path to the Dockerfile in the source code repo. By default, this path is ./Dockerfile, which assumes the Dockerfile is in the root directory. You can set it to other paths in different use cases (./path/to/myDockerfile for example).\n\n\n\nImage Name\nThe image name in name:tag format. The registry address is not required. For example, to build  example.com/repo/my-image:dev, enter repo/my-image:dev.\n\n\n\nPush image to remote repository\nAn option to set the registry that publishes the image that’s built.  To use this option, enable it and choose a registry from the drop-down. If this option is disabled, the image is pushed to the internal registry.\n\n\n\nBuild Context  (Show advanced options)\nBy default, the root directory of the source code (.). For more details, see the Docker build command documentatio","postref":"f93f7f63067f68b2e728b9b3efd61c8a","objectID":"330b47379a17d79a2b61552a6a524469","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/k8s-in-rancher/pipelines/"},{"anchor":"#","title":"Manual HPA Installation for Clusters Created Before Rancher v2.0.7","content":"This section describes how to manually install HPAs for clusters created with Rancher prior to v2.0.7. This section also describes how to configure your HPA to scale up or down, and how to assign roles to your HPA.\n\nBefore you can use HPA in your Kubernetes cluster, you must fulfill some requirements.\n\nRequirements\n\nBe sure that your Kubernetes cluster services are running with these flags at minimum:\n\n\nkube-api: requestheader-client-ca-file\nkubelet: read-only-port at 10255\n\nkube-controller: Optional, just needed if distinct values than default are required.\n\n\nhorizontal-pod-autoscaler-downscale-delay: \"5m0s\"\nhorizontal-pod-autoscaler-upscale-delay: \"3m0s\"\nhorizontal-pod-autoscaler-sync-period: \"30s\"\n\n\n\nFor an RKE Kubernetes cluster definition, add this snippet in the services section. To add this snippet using the Rancher v2.0 UI, open the Clusters view and select Ellipsis (…) > Edit for the cluster in which you want to use HPA. Then, from Cluster Options, click Edit as YAML. Add the following snippet to the services section:\n\nservices:\n...\n  kube-api:\n    extra_args:\n      requestheader-client-ca-file: \"/etc/kubernetes/ssl/kube-ca.pem\"\n  kube-controller:\n    extra_args:\n      horizontal-pod-autoscaler-downscale-delay: \"5m0s\"\n      horizontal-pod-autoscaler-upscale-delay: \"1m0s\"\n      horizontal-pod-autoscaler-sync-period: \"30s\"\n  kubelet:\n    extra_args:\n      read-only-port: 10255\n\n\nOnce the Kubernetes cluster is configured and deployed, you can deploy metrics services.\n\n\nNote: kubectl command samples in the sections that follow were tested in a cluster running Rancher v2.0.6 and Kubernetes v1.10.1.\n\n\nConfiguring HPA to Scale Using Resource Metrics\n\nTo create HPA resources based on resource metrics such as CPU and memory use, you need to deploy the metrics-server package in the kube-system namespace of your Kubernetes cluster. This deployment allows HPA to consume the metrics.k8s.io API.\n\n\nPrerequisite: You must be running kubectl 1.8 or later.\n\n\n\nConnect to your Kubernetes cluster using kubectl.\n\nClone the GitHub metrics-server repo:\n\n# git clone https://github.com/kubernetes-incubator/metrics-server\n\n\nInstall the metrics-server package.\n\n# kubectl create -f metrics-server/deploy/1.8+/\n\n\nCheck that metrics-server is running properly. Check the service pod and logs in the kube-system namespace.\n\n\nCheck the service pod for a status of running. Enter the following command:\n\n# kubectl get pods -n kube-system\n\n\nThen check for the status of running.\n\nNAME                                  READY     STATUS    RESTARTS   AGE\n...\nmetrics-server-6fbfb84cdd-t2fk9       1/1       Running   0          8h\n...\n\n\nCheck the service logs for service availability. Enter the following command:\n\n# kubectl -n kube-system logs metrics-server-6fbfb84cdd-t2fk9\n\n\nThen review the log to confirm that the metrics-server package is running.\n\n  \n  Metrics Server Log Output\n  \n    I0723 08:09:56.193136       1 heapster.go:71] /metrics-server --source=kubernetes.summary_api:''\nI0723 08:09:56.193574       1 heapster.go:72] Metrics Server version v0.2.1\nI0723 08:09:56.194480       1 configs.go:61] Using Kubernetes client with master \"https://10.43.0.1:443\" and version\nI0723 08:09:56.194501       1 configs.go:62] Using kubelet port 10255\nI0723 08:09:56.198612       1 heapster.go:128] Starting with Metric Sink\nI0723 08:09:56.780114       1 serving.go:308] Generated self-signed cert (apiserver.local.config/certificates/apiserver.crt, apiserver.local.config/certificates/apiserver.key)\nI0723 08:09:57.391518       1 heapster.go:101] Starting Heapster API server...\n[restful] 2018/07/23 08:09:57 log.go:33: [restful/swagger] listing is available at https:///swaggerapi\n[restful] 2018/07/23 08:09:57 log.go:33: [restful/swagger] https:///swaggerui/ is mapped to folder /swagger-ui/\nI0723 08:09:57.394080       1 serve.go:85] Serving securely on 0.0.0.0:443\n\n\n  \n\n\n\n\nCheck that the metrics api is accessible from kubectl.\n\n\nIf you are accessing the cluster through Rancher, enter your Server URL in the kubectl config in the following format: https://<RANCHER_URL>/k8s/clusters/<CLUSTER_ID>. Add the suffix /k8s/clusters/<CLUSTER_ID> to API path.\n\n# kubectl get --raw /k8s/clusters/<CLUSTER_ID>/apis/metrics.k8s.io/v1beta1\n\n\nIf the API is working correctly, you should receive output similar to the output below.\n\n{\"kind\":\"APIResourceList\",\"apiVersion\":\"v1\",\"groupVersion\":\"metrics.k8s.io/v1beta1\",\"resources\":[{\"name\":\"nodes\",\"singularName\":\"\",\"namespaced\":false,\"kind\":\"NodeMetrics\",\"verbs\":[\"get\",\"list\"]},{\"name\":\"pods\",\"singularName\":\"\",\"namespaced\":true,\"kind\":\"PodMetrics\",\"verbs\":[\"get\",\"list\"]}]}\n\n\nIf you are accessing the cluster directly, enter your Server URL in the kubectl config in the following format: https://<K8s_URL>:6443.\n\n# kubectl get --raw /apis/metrics.k8s.io/v1beta1\n\n\nIf the API is working correctly, you should receive output similar to the output below.\n\n{\"kind\":\"APIResourceList\",\"apiVersion\":\"v1\",\"groupVersion\":\"metrics.k8s.io/v1beta1\",\"resources\":[{\"name\":\"nodes\",\"singularName\":\"\",\"namespaced\":false,\"kind\":\"NodeMetrics\",\"verbs\":[\"get\",\"list\"]},{\"name\":\"pods\",\"singularName\":\"\",\"namespaced\":true,\"kind\":\"PodMetrics\",\"verbs\":[\"get\",\"list\"]}]}\n\n\n\n\nAssigning Additional Required Roles to Your HPA\n\nBy default, HPA reads resource and custom metrics with the user system:anonymous. Assign system:anonymous to view-resource-metrics and view-custom-metrics in the ClusterRole and ClusterRoleBindings manifests. These roles are used to access metrics.\n\nTo do it, follow these steps:\n\n\nConfigure kubectl to connect to your cluster.\n\nCopy the ClusterRole and ClusterRoleBinding manifest for the type of metrics you’re using for your HPA.\n\n  \n  Resource Metrics: ApiGroups resource.metrics.k8s.io\n  \n        apiVersion: rbac.authorization.k8s.io/v1\n    kind: ClusterRole\n    metadata:\n      name: view-resource-metrics\n    rules:\n    - apiGroups:\n        - metrics.k8s.io\n      resources:\n        - pods\n        - nodes\n      verbs:\n        - get\n        - list\n        - watch\n    ---\n    apiVersion: rbac.authorization.k8s.io/v1\n    kind: ClusterRoleBinding\n    metadata:\n      name: view-resource-metrics\n    roleRef:\n      apiGroup: rbac.authorization.k8s.io\n      kind: ClusterRole\n      name: view-resource-metrics\n    subjects:\n      - apiGroup: rbac.authorization.k8s.io\n        kind: User\n        name: system:anonymous\n\n\n  \n\n\n\n  \n  Custom Metrics: ApiGroups custom.metrics.k8s.io\n  \n      apiVersion: rbac.authorization.k8s.io/v1\n  kind: ClusterRole\n  metadata:\n    name: view-custom-metrics\n  rules:\n  - apiGroups:\n      - custom.metrics.k8s.io\n    resources:\n      - \"*\"\n    verbs:\n      - get\n      - list\n      - watch\n  ---\n  apiVersion: rbac.authorization.k8s.io/v1\n  kind: ClusterRoleBinding\n  metadata:\n    name: view-custom-metrics\n  roleRef:\n    apiGroup: rbac.authorization.k8s.io\n    kind: ClusterRole\n    name: view-custom-metrics\n  subjects:\n    - apiGroup: rbac.authorization.k8s.io\n      kind: User\n      name: system:anonymous\n\n\n  \n\n\n\nCreate them in your cluster using one of the follow commands, depending on the metrics you’re using.\n\n# kubectl create -f <RESOURCE_METRICS_MANIFEST>\n# kubectl create -f <CUSTOM_METRICS_MANIFEST>\n\n\n","postref":"2286f7dc4b2216f12badbe94a4ad20ea","objectID":"5900d88e7946381d2ef1d9a8b64a9aeb","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/k8s-in-rancher/horitzontal-pod-autoscaler/hpa-for-rancher-before-2_0_7/"},{"anchor":"#","title":"Creating Persistent Storage in Amazon's EBS","content":"This section describes how to set up Amazon’s Elastic Block Store in EC2.\n\n\nFrom the EC2 console, go to the ELASTIC BLOCK STORE section in the left panel and click Volumes.\nClick Create Volume.\nOptional: Configure the size of the volume or other options. The volume should be created in the same availability zone as the instance it will be attached to.\nClick Create Volume.\nClick Close.\n\n\nResult: Persistent storage has been created.\n\nFor details on how to set up the newly created storage in Rancher, refer to the section on setting up existing storage.\n","postref":"9772e6d82ea9733a85465ff26002ee21","objectID":"5d4832d8e775666f5470508436981029","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/volumes-and-storage/examples/ebs/"},{"anchor":"#","title":"Provisioning Storage Examples","content":"Rancher supports persistent storage with a variety of volume plugins. However, before you use any of these plugins to bind persistent storage to your workloads, you have to configure the storage itself, whether its a cloud-based solution from a service-provider or an on-prem solution that you manage yourself.\n\nFor your convenience, Rancher offers documentation on how to configure some of the popular storage methods:\n\n\nNFS\nvSphere\n\n","postref":"67ee1d819e17c952faca9470a099b7fb","objectID":"9eedd86b6dc42a988279e1168a40d614","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/volumes-and-storage/examples/"},{"anchor":"#what-s-next","title":"What’s Next?","content":"Within Rancher, add the NFS server as a storage volume and/or storage class. After adding the server, you can use it for storage for your deployments.","postref":"b8cf628825465deb75496585c4604e6f","objectID":"db428e2830b9ddb01923212a7bd37635","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/volumes-and-storage/examples/nfs/"},{"anchor":"#why-to-use-statefulsets-instead-of-deployments","title":"Why to Use StatefulSets Instead of Deployments","content":"\nvSphere Storage for Kubernetes\nKubernetes Persistent Volumes\nYou should always use StatefulSets for workloads consuming vSphere storage, as this resource type is designed to address a VMDK block storage caveat.Since vSphere volumes are backed by VMDK block storage, they only support an access mode of ReadWriteOnce. This setting restricts the volume so that it can only be mounted to a single pod at a time, unless all pods consuming that volume are co-located on the same node. This behavior makes a deployment resource unusable for scaling beyond a single replica if it consumes vSphere volumes.Even using a deployment resource with just a single replica may result in a deadlock situation while updating the deployment. If the updated pod is scheduled to a node different from where the existing pod lives, it will fail to start because the VMDK is still attached to the other node.","postref":"78922f3997ce405bec5de33a918c2628","objectID":"569e0883d26512ce46d711e0e9d66d5c","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/volumes-and-storage/examples/vsphere/"},{"anchor":"#what-s-next","title":"What’s Next?","content":"Now you can add the certificate when launching an ingress within the current project or namespace. For more information, see Adding Ingress.","postref":"71f4475714b3f369ca7cea1bee02cb62","objectID":"b86abbadc7cb5152ab759734ebe0f208","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/k8s-in-rancher/certificates/"},{"anchor":"#what-s-next","title":"What’s Next?","content":"Now that you have a ConfigMap added to a namespace, you can add it to a workload that you deploy from the namespace of origin. You can use the ConfigMap to specify information for you application to consume, such as:\nApplication environment variables.\nSpecifying parameters for a Volume mounted to the workload.\nFor more information on adding ConfigMaps to a workload, see Deploying Workloads.","postref":"da0df7b22b75b48e60570ac69f6334fe","objectID":"117490332abea0e95f66ede8df5fb2af","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/k8s-in-rancher/configmaps/"},{"anchor":"#","title":"Secrets","content":"Secrets store sensitive data like passwords, tokens, or keys. They may contain one or more key value pairs.\n\n\nThis page is about secrets in general. For details on setting up a private registry, refer to the section on registries.\n\n\nWhen configuring a workload, you’ll be able to choose which secrets to include. Like config maps, secrets can be referenced by workloads as either an environment variable or a volume mount.\n\nAny update to an active secrets won’t automatically update the pods that are using it. Restart those pods to have them use the new secret.\n\nCreating Secrets\n\nWhen creating a secret, you can make it available for any deployment within a project, or you can limit it to a single namespace.\n\n\nFrom the Global view, select the project containing the namespace(s) where you want to add a secret.\n\nFrom the main menu, select Resources > Secrets. Click Add Secret.\n\nEnter a Name for the secret.\n\n\nNote: Kubernetes classifies secrets, certificates, ConfigMaps, and registries all as secrets, and no two secrets in a project or namespace can have duplicate names. Therefore, to prevent conflicts, your secret must have a unique name among all secrets within your workspace.\n\n\nSelect a Scope for the secret. You can either make the registry available for the entire project or a single namespace.\n\nFrom Secret Values, click Add Secret Value to add a key value pair. Add as many values as you need.\n\n\nTip: You can add multiple key value pairs to the secret by copying and pasting.\n\n\n\n\n    \n    \n    \n    \n    \n    \n\n\n\n\nClick Save.\n\n\nResult: Your secret is added to the project or namespace, depending on the scope you chose. You can view the secret in the Rancher UI from the Resources > Secrets view.\n\nAny update to an active secrets won’t automatically update the pods that are using it. Restart those pods to have them use the new secret.\n\nWhat’s Next?\n\nNow that you have a secret added to the project or namespace, you can add it to a workload that you deploy.\n\nFor more information on adding secret to a workload, see Deploying Workloads.\n","postref":"b6d1bbafa14970d98df1ce5656fb614c","objectID":"cf21ef282cb8319c0321fd1e1c9daaf0","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/k8s-in-rancher/secrets/"},{"anchor":"#","title":"Kubernetes Registry and Docker Registry","content":"Registries are Kubernetes secrets containing credentials used to authenticate with private Docker registries.\n\nThe word “registry” can mean two things, depending on whether it is used to refer to a Docker or Kubernetes registry:\n\n\nA Docker registry contains Docker images that you can pull in order to use them in your deployment. The registry is a stateless, scalable server side application that stores and lets you distribute Docker images.\nThe Kubernetes registry is an image pull secret that your deployment uses to authenticate with a Docker registry.\n\n\nDeployments use the Kubernetes registry secret to authenticate with a private Docker registry and then pull a Docker image hosted on it.\n\nCurrently, deployments pull the private registry credentials automatically only if the workload is created in the Rancher UI and not when it is created via kubectl.\n\nCreating a Registry\n\n\nPrerequisites: You must have a private registry available to use.\n\n\n\nFrom the Global view, select the project containing the namespace(s) where you want to add a registry.\n\nFrom the main menu, click Resources > Secrets > Registry Credentials. (For Rancher prior to v2.3, click Resources > Registries.)\n\nClick Add Registry.\n\nEnter a Name for the registry.\n\n\nNote: Kubernetes classifies secrets, certificates, ConfigMaps, and registries all as secrets, and no two secrets in a project or namespace can have duplicate names. Therefore, to prevent conflicts, your registry must have a unique name among all secrets within your workspace.\n\n\nSelect a Scope for the registry. You can either make the registry available for the entire project or a single namespace.\n\nSelect the website that hosts your private registry. Then enter credentials that authenticate with the registry. For example, if you use DockerHub, provide your DockerHub username and password.\n\nClick Save.\n\n\nResult:\n\n\nYour secret is added to the project or namespace, depending on the scope you chose.\nYou can view the secret in the Rancher UI from the Resources > Registries view.\nAny workload that you create in the Rancher UI will have the credentials to access the registry if the workload is within the registry’s scope.\n\n\nUsing a Private Registry\n\nYou can deploy a workload with an image from a private registry through the Rancher UI, or with kubectl.\n\nUsing the Private Registry with the Rancher UI\n\nTo deploy a workload with an image from your private registry,\n\n\nGo to the project view,\nClick Resources > Workloads. In versions prior to v2.3.0, go to the Workloads tab.\nClick Deploy.\nEnter a unique name for the workload and choose a namespace.\nIn the Docker Image field, enter the URL of the path to the Docker image in your private registry. For example, if your private registry is on Quay.io, you could use quay.io/<Quay profile name>/<Image name>.\nClick Launch.\n\n\nResult: Your deployment should launch, authenticate using the private registry credentials you added in the Rancher UI, and pull the Docker image that you specified.\n\nUsing the Private Registry with kubectl\n\nWhen you create the workload using kubectl, you need to configure the pod so that its YAML has the path to the image in the private registry. You also have to create and reference the registry secret because the pod only automatically gets access to the private registry credentials if it is created in the Rancher UI.\n\nThe secret has to be created in the same namespace where the workload gets deployed.\n\nBelow is an example pod.yml for a workload that uses an image from a private registry. In this example, the pod uses an image from Quay.io, and the .yml specifies the path to the image. The pod authenticates with the registry using credentials stored in a Kubernetes secret called testquay, which is specified in spec.imagePullSecrets in the name field:\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: private-reg\nspec:\n  containers:\n  - name: private-reg-container\n    image: quay.io/<Quay profile name>/<image name>\n  imagePullSecrets:\n  - name: testquay\n\n\nIn this example, the secret named testquay is in the default namespace.\n\nYou can use kubectl to create the secret with the private registry credentials. This command creates the secret named testquay:\n\nkubectl create secret docker-registry testquay \\\n    --docker-server=quay.io \\\n    --docker-username=<Profile name> \\\n    --docker-password=<password>\n\n\nTo see how the secret is stored in Kubernetes, you can use this command:\n\nkubectl get secret testquay --output=\"jsonpath={.data.\\.dockerconfigjson}\" | base64 --decode\n\n\nThe result looks like this:\n\n{\"auths\":{\"quay.io\":{\"username\":\"<Profile name>\",\"password\":\"<password>\",\"auth\":\"c291bXlhbGo6dGVzdGFiYzEyMw==\"}}}\n\n\nAfter the workload is deployed, you can check if the image was pulled successfully:\n\nkubectl get events\n\n\nThe result should look like this:\n\n14s         Normal    Scheduled          Pod    Successfully assigned default/private-reg2 to minikube\n11s         Normal    Pulling            Pod    pulling image \"quay.io/<Profile name>/<image name>\"\n10s         Normal    Pulled             Pod    Successfully pulled image \"quay.io/<Profile name>/<image name>\"\n\n\nFor more information, refer to the Kubernetes documentation on creating a pod that uses your secret.\n","postref":"73ee3371b3a27bc5f7c79acaddaaa94e","objectID":"a22b5d85a1197da69926b09425374487","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/k8s-in-rancher/registries/"},{"anchor":"#","title":"Istio","content":"Available as of v2.3.0\n\nUsing Rancher, you can connect, secure, control, and observe services through integration with Istio, a leading open-source service mesh solution. Istio provides behavioral insights and operational control over the service mesh as a whole, offering a complete solution to satisfy the diverse requirements of microservice applications.\n\nThis service mesh provides features that include but are not limited to the following:\n\n\nTraffic management features\nEnhanced monitoring and tracing\nService discovery and routing\nSecure connections and service-to-service authentication with mutual TLS\nLoad balancing\nAutomatic retries, backoff, and circuit breaking\n\n\nIstio needs to be set up by a Rancher administrator or cluster administrator before it can be used in a project for comprehensive data visualizations, traffic management, or any of its other features.\n\nFor information on how Istio is integrated with Rancher and how to set it up, refer to the section about Istio.\n","postref":"a4c38d4f9648ed268cb54426888a3347","objectID":"e82474dd1ab1cabb0496d6438494cad4","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/project-admin/istio/"},{"anchor":"#managing-built-in-global-catalogs","title":"Managing Built-in Global Catalogs","content":"\nPrerequisites: In order to manage the built-in catalogs or manage global catalogs, you need one of the following permissions:\n\n\nAdministrator Global Permissions\nCustom Global Permissions with the Manage Catalogs role assigned.\n\n\nFrom the Global view, choose Tools > Catalogs in the navigation bar. In versions prior to v2.2.0, you can select Catalogs directly in the navigation bar.\n\nToggle the default catalogs that you want use to a setting of Enabled.\n\n\nLibrary\n\nThe Library Catalog includes charts curated by Rancher. Rancher stores charts in a Git repository to expedite the fetch and update of charts. In Rancher 2.x, only global catalogs are supported. Support for cluster-level and project-level charts will be added in the future.\n\nThis catalog features Rancher Charts, which include some notable advantages over native Helm charts.\n\nHelm Stable\n\nThis catalog, , which is maintained by the Kubernetes community, includes native Helm charts. This catalog features the largest pool of apps.\n\nHelm Incubator\n\nSimilar in user experience to Helm Stable, but this catalog is filled with applications in beta.\n\nResult: The chosen catalogs are enabled. Wait a few minutes for Rancher to replicate the catalog charts. When replication completes, you’ll be able to see them in any of your projects by selecting Apps from the main navigation bar. In versions prior to v2.2.0, within a project, you can select Catalog Apps from the main navigation bar.","postref":"b71218c79399eb8923d5df91d742bdf2","objectID":"d49e063e050abf6d1be84944c65aee54","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/catalog/built-in/"},{"anchor":"#","title":"Catalogs, Helm Charts and Apps","content":"Rancher provides the ability to use a catalog of Helm charts that make it easy to repeatedly deploy applications.\n\n\nCatalogs are GitHub repositories or Helm Chart repositories filled with applications that are ready-made for deployment. Applications are bundled in objects called Helm charts.\nHelm charts are a collection of files that describe a related set of Kubernetes resources. A single chart might be used to deploy something simple, like a memcached pod, or something complex, like a full web app stack with HTTP servers, databases, caches, and so on.\n\n\nRancher improves on Helm catalogs and charts. All native Helm charts can work within Rancher, but Rancher adds several enhancements to improve their user experience.\n\nThis section covers the following topics:\n\n\nPrerequisites\nCatalog scopes\nEnabling built-in global catalogs\nAdding custom global catalogs\n\n\nAdd custom Git repositories\nAdd custom Helm chart repositories\nAdd private Git/Helm chart repositories\n\nLaunching catalog applications\nWorking with catalogs\n\n\nApps\nGlobal DNS\nChart compatibility with Rancher\n\n\n\nPrerequisites\n\nWhen Rancher deploys a catalog app, it launches an ephemeral instance of a Helm service account that has the permissions of the user deploying the catalog app. Therefore, a user cannot gain more access to the cluster through Helm or a catalog application than they otherwise would have.\n\nTo launch a catalog app or a multi-cluster app, you should have at least one of the following permissions:\n\n\nA project-member role in the target cluster, which gives you the ability to create, read, update, and delete the workloads\nA cluster owner role for the cluster that include the target project\n\n\nCatalog Scopes\n\nWithin Rancher, you can manage catalogs at three different scopes. Global catalogs are shared across all clusters and project. There are some use cases where you might not want to share catalogs across between different clusters or even projects in the same cluster. By leveraging cluster and project scoped catalogs, you will be able to provide applications for specific teams without needing to share them with all clusters and/or projects.\n\n\n\n\nScope\nDescription\nAvailable As of\n\n\n\n\n\nGlobal\nAll clusters and all projects can access the Helm charts in this catalog\nv2.0.0\n\n\n\nCluster\nAll projects in the specific cluster can access the Helm charts in this catalog\nv2.2.0\n\n\n\nProject\nThis specific cluster can access the Helm charts in this catalog\nv2.2.0\n\n\n\n\nEnabling Built-in Global Catalogs\n\nWithin Rancher, there are default catalogs packaged as part of Rancher. These can be enabled or disabled by an administrator.\n\n\nFrom the Global view, choose Tools > Catalogs in the navigation bar. In versions prior to v2.2.0, you can select Catalogs directly in the navigation bar.\n\nToggle the default catalogs that you want use to a setting of Enabled.\n\n\nLibrary\n\nThe Library Catalog includes charts curated by Rancher. Rancher stores charts in a Git repository to expedite the fetch and update of charts. In Rancher 2.x, only global catalogs are supported. Support for cluster-level and project-level charts will be added in the future.\n\nThis catalog features Rancher Charts, which include some notable advantages over native Helm charts.\n\nHelm Stable\n\nThis catalog, , which is maintained by the Kubernetes community, includes native Helm charts. This catalog features the largest pool of apps.\n\nHelm Incubator\n\nSimilar in user experience to Helm Stable, but this catalog is filled with applications in beta.\n\n\n\nResult: The chosen catalogs are enabled. Wait a few minutes for Rancher to replicate the catalog charts. When replication completes, you’ll be able to see them in any of your projects by selecting Apps from the main navigation bar. In versions prior to v2.2.0, you can select Catalog Apps from the main navigation bar.\n\nAdding Custom Global Catalogs\n\nAdding a catalog is as simple as adding a catalog name, a URL and a branch name.\n\nAdd Custom Git Repositories\n\nThe Git URL needs to be one that git clone can handle and must end in .git. The branch name must be a branch that is in your catalog URL. If no branch name is provided, it will use the master branch by default. Whenever you add a catalog to Rancher, it will be available immediately.\n\nAdd Custom Helm Chart Repositories\n\nA Helm chart repository is an HTTP server that houses one or more packaged charts. Any HTTP server that can serve YAML files and tar files and can answer GET requests can be used as a repository server.\n\nHelm comes with built-in package server for developer testing (helm serve). The Helm team has tested other servers, including Google Cloud Storage with website mode enabled, S3 with website mode enabled or hosting custom chart repository server using open-source projects like ChartMuseum.\n\nIn Rancher, you can add the custom Helm chart repository with only a catalog name and the URL address of the chart repository.\n\nAdd Private Git/Helm Chart Repositories\n\nAvailable as of v2.2.0\n\nIn Rancher v2.2.0, you can add private catalog repositories using credentials like Username and Password. You may also want to use the\nOAuth token if your Git or Helm repository server support that.\n\nRead More About Adding Private Git/Helm Catalogs\n\n\n\n\nFrom the Global view, choose Tools > Catalogs in the navigation bar. In versions prior to v2.2.0, you can select Catalogs directly in the navigation bar.\nClick Add Catalog.\nComplete the form and click Create.\n\n\nResult: Your catalog is added to Rancher.\n\nLaunching Catalog Applications\n\nAfter you’ve either enabled the built-in catalogs or added your own custom catalog, you can start launching any catalog application.>\n\n\nFrom the Global view, open the project that you want to deploy to.\n\nFrom the main navigation bar, choose Apps. In versions prior to v2.2.0, choose Catalog Apps on the main navigation bar. Click Launch.\n\nFind the app that you want to launch, and then click View Now.\n\nUnder Configuration Options enter a Name. By default, this name is also used to create a Kubernetes namespace for the application.\n\n\nIf you would like to change the Namespace, click Customize and enter a new name.\nIf you want to use a different namespace that already exists, click Customize, and then click Use an existing namespace. Choose a namespace from the list.\n\n\nSelect a Template Version.\n\nComplete the rest of the Configuration Options.\n\n\nFor native Helm charts (i.e., charts from the Helm Stable or Helm Incubator catalogs), answers are provided as key value pairs in the Answers section.\nKeys and values are available within Detailed Descriptions.\n\nWhen entering answers, you must format them using the syntax rules found in Using Helm: The format and limitations of –set, as Rancher passes them as --set flags to Helm.\n\nFor example, when entering an answer that includes two values separated by a comma (i.e., abc, bcd), wrap the values with double quotes (i.e., \"abc, bcd\").\n\n\nReview the files in Preview. When you’re satisfied, click Launch.\n\n\nResult: Your application is deployed to your chosen namespace. You can view the application status from the project’s:\n\nBy creating a customized repository with added files, Rancher improves on Helm repositories and charts. All native Helm charts can work within Rancher, but Rancher adds several enhancements to improve their user experience.\n\nWorking with Catalogs\n\nThere are two types of catalogs in Rancher. Learn more about each type:\n\n\nBuilt-in Global Catalogs\nCustom Catalogs\n\n\nApps\n\nIn Rancher, applications are deployed from the templates in a catalog. Rancher supports two types of applications:\n\n\nMulti-cluster applications\nApplications deployed in a specific Project\n\n\nGlobal DNS\n\nAvailable as v2.2.0\n\nWhen creating applications that span multiple Kubernetes clusters, a Global DNS entry can be created to route traffic to the endpoints in all of the different clusters. An external DNS server will need be programmed to assign a fully qualified domain name (a.k.a FQDN) to your application. Rancher will use the FQDN you provide and the IP addresses where your application is running to program the DNS. Rancher will gather endpoints from all the Kubernetes clusters running your application and program the DNS.\n\nFor more information on how to use this feature, see Global DNS.\n\nChart Compatibility with Rancher\n\nCharts now support the fields rancher_min_version and rancher_max_version in the questions.yml file to specify the versions of Rancher that the chart is compatible with. When using the UI, only app versions that are valid for the version of Rancher running will be shown. API validation is done to ensure apps that don’t meet the Rancher requirements cannot be launched. An app that is already running will not be affected on a Rancher upgrade if the newer Rancher version does not meet the app’s requirements.\n","postref":"a28cf81f6f8bcfd4d4960c063a6fea98","objectID":"00e1aa604023c3aa7612f4a19e48a1f0","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/catalog/"},{"anchor":"#chart-types","title":"Chart Types","content":"You can fill your custom catalogs with either Helm Charts or Rancher Charts, although we recommend Rancher Charts due to their enhanced user experience.\nNote: For a complete walkthrough of developing charts, see the upstream Helm chart developer reference.\n\nWithin the GitHub repo that you’re using as your custom catalog, create a directory structure that mirrors the structure listed in Chart Directory Structure.\n\nRancher requires this directory structure, although app-readme.md and questions.yml are optional.\n\n\nTip:\n\n\nTo begin customizing a chart, copy one from either the Rancher Library or the Helm Stable.\nFor a complete walk through of developing charts, see the upstream Helm chart developer reference.\n\n\n\nRecommended: Create an app-readme.md file.\n\nUse this file to create custom text for your chart’s header in the Rancher UI. You can use this text to notify users that the chart is customized for your environment or provide special instruction on how to use it.\n\n\nExample:\n\n$ cat ./app-readme.md\n\n# Wordpress ROCKS!\n\n\nRecommended: Create a questions.yml file.\n\nThis file creates a form for users to specify deployment parameters when they deploy the custom chart. Without this file, users must specify the parameters manually using key value pairs, which isn’t user-friendly.\n\n\nThe example below creates a form that prompts users for persistent volume size and a storage class.\n\n\nFor a list of variables you can use when creating a questions.yml file, see Question Variable Reference.\n\n    categories:\n    - Blog\n    - CMS\n    questions:\n    - variable: persistence.enabled\n    default: \"false\"\n    description: \"Enable persistent volume for WordPress\"\n    type: boolean\n    required: true\n    label: WordPress Persistent Volume Enabled\n    show_subquestion_if: true\n    group: \"WordPress Settings\"\n    subquestions:\n    - variable: persistence.size\n        default: \"10Gi\"\n        description: \"WordPress Persistent Volume Size\"\n        type: string\n        label: WordPress Volume Size\n    - variable: persistence.storageClass\n        default: \"\"\n        description: \"If undefined or null, uses the default StorageClass. Default to null\"\n        type: storageclass\n        label: Default StorageClass for WordPress\n\n\nCheck the customized chart into your GitHub repo.\nResult: Your custom chart is added to the repo. Your Rancher Server will replicate the chart within a few minutes.Before you create your own custom catalog, you should have a basic understanding about how a Rancher chart differs from a native Helm chart. Rancher charts differ slightly from Helm charts in their directory structures. Rancher charts include two files that Helm charts do not.\napp-readme.md\n\nA file that provides descriptive text in the chart’s UI header. The following image displays the difference between a Rancher chart (which includes app-readme.md) and a native Helm chart (which does not).\n\nRancher Chart with app-readme.md (left) vs. Helm Chart without (right)\n\n\n\nquestions.yml\n\nA file that contains questions for a form. These form questions simplify deployment of a chart. Without it, you must configure the deployment using key value pairs, which is more difficult. The following image displays the difference between a Rancher chart (which includes questions.yml) and a native Helm chart (which does not).\n\nRancher Chart with questions.yml (left) vs. Helm Chart without (right)\n\n\nQuestions.ymlInside the questions.yml, most of the content will be around the questions to ask the end user, but there are some additional fields that can be set in this file.Min/Max Rancher versionsAvailable as of v2.3.0For each chart, you can add the minimum and/or maximum Rancher version, which determines whether or not this chart is available to be deployed from Rancher.\nNote: Even though Rancher release versions are prefixed with a v, there is no prefix for the release version when using this option.\nrancher_min_version: 2.3.0\nrancher_max_version: 2.3.99\nQuestion Variable ReferenceThis reference contains variables that you can use in questions.yml nested under questions:.\n\n\nVariable\nType\nRequired\nDescription\n\n\n\n\n\nvariable\nstring\ntrue\nDefine the variable name specified in the values.yml file, using foo.bar for nested objects.\n\n\n\nlabel\nstring\ntrue\nDefine the UI label.\n\n\n\ndescription\nstring\nfalse\nSpecify the description of the variable.\n\n\n\ntype\nstring\nfalse\nDefault to string if not specified (current supported types are string, multiline, boolean, int, enum, password, storageclass, hostname, pvc, and secret).\n\n\n\nrequired\nbool\nfalse\nDefine if the variable is required or not (true | false)\n\n\n\ndefault\nstring\nfalse\nSpecify the default value.\n\n\n\ngroup\nstring\nfalse\nGroup questions by input value.\n\n\n\nmin_length\nint\nfalse\nMin character length.\n\n\n\nmax_length\nint\nfalse\nMax character length.\n\n\n\nmin\nint\nfalse\nMin integer length.\n\n\n\nmax\nint\nfalse\nMax integer length.\n\n\n\noptions\n[]string\nfalse\nSpecify the options when the variable type is enum, for example: options: - “ClusterIP”  - “NodePort”  - “LoadBalancer”\n\n\n\nvalid_chars\nstring\nfalse\nRegular expression for input chars validation.\n\n\n\ninvalid_chars\nstring\nfalse\nRegular expression for invalid input chars validation.\n\n\n\nsubquestions\n[]subquestion\nfalse\nAdd an array of subquestions.\n\n\n\nshow_if\nstring\nfalse\nShow current variable if conditional variable is true. For example show_if: \"serviceType=Nodeport\"\n\n\n\nshow_subquestion_if\nstring\nfalse\nShow subquestions if is true or equal to one of the options. for example show_subquestion_if: \"true\"\n\n\n\nNote: subquestions[] cannot contain subquestions or show_subquestions_if keys, but all other keys in the above table are supported.\nThe following table demonstrates the directory structure for a chart, which can be found in a chart directory: charts/<APPLICATION>/<APP_VERSION>/. This information is helpful when customizing charts for a custom catalog. Files denoted with Rancher Specific are specific to Rancher charts, but are optional for chart customization.charts/<APPLICATION>/<APP_VERSION>/\n|--charts/           # Directory containing dependency charts.\n|--templates/        # Directory containing templates that, when combined with values.yml, generates Kubernetes YAML.\n|--app-readme.md     # Text displayed in the charts header within the Rancher UI.*\n|--Chart.yml         # Required Helm chart information file.\n|--questions.yml     # Form questions displayed within the Rancher UI. Questions display in Configuration Options.*\n|--README.md         # Optional: Helm Readme file displayed within Rancher UI. This text displays in Detailed Descriptions.\n|--requirements.yml  # Optional: YAML file listing dependencies for the chart.\n|--values.yml        # Default configuration values for the chart.\nRancher supports two different types of charts:\nHelm Charts\n\nNative Helm charts include an application along with other software required to run it. When deploying native Helm charts, you’ll learn the chart’s parameters and then configure them using Answers, which are sets of key value pairs.\n\nThe Helm Stable and Helm Incubators are populated with native Helm charts. However, you can also use native Helm charts in Custom catalogs (although we recommend Rancher Charts).\n\nRancher Charts\n\nRancher charts mirror native helm charts, although they add two files that enhance user experience: app-readme.md and questions.yaml. Read more about them in Rancher Chart Additional Files.\n\nAdvantages of Rancher charts include:\n\n\nEnhanced Revision Tracking\n\nWhile Helm supports versioned deployments, Rancher adds tracking and revision history to display changes between different versions of the chart.\n\nStreamlined Application Launch\n\nRancher charts add simplified chart descriptions and configuration forms to make catalog application deployment easy. Rancher users need not read through the entire list of Helm variables to understand how to launch an application.\n\nApplication Resource Management\n\nRancher tracks all the resources created by a specific application. Users can easily navigate to and troubleshoot on a page listing all the workload objects used to power an application.\n\n","postref":"71256de4b7790163f5709fcb661e7182","objectID":"5ead5206a5df974d923b493e753958c7","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/catalog/custom/creating/"},{"anchor":"#overview","title":"Overview","content":"The internal Docker registry and the Minio workloads use ephemeral volumes by default. This default storage works out-of-the-box and makes testing easy, but you lose the build images and build logs if the node running the Docker Registry or Minio fails. In most cases this is fine. If you want build images and logs to survive node failures, you can configure the Docker Registry and Minio to use persistent volumes.\nPrerequisites (for both parts A and B):\n\nPersistent volumes must be available for the cluster.\nA. Configuring Persistent Data for Docker Registry\nFrom the project that you’re configuring a pipeline for, and click Resources > Workloads. In versions prior to v2.3.0, select the Workloads tab.\n\nFind the docker-registry workload and select Ellipsis (…) > Edit.\n\nScroll to the Volumes section and expand it. Make one of the following selections from the Add Volume menu, which is near the bottom of the section:\n\n\nAdd Volume > Add a new persistent volume (claim)\nAdd Volume > Use an existing persistent volume (claim)\n\n\nComplete the form that displays to choose a persistent volume for the internal Docker registry.\n\n  \n  \n  \n1. Enter a Name for the volume claim.\n\n\nSelect a volume claim Source:\n\n\nIf you select Use a Storage Class to provision a new persistent volume, select a Storage Class and enter a Capacity.\n\nIf you select Use an existing persistent volume, choose a Persistent Volume from the drop-down.\n\n\nFrom the Customize section, choose the read/write access for the volume.\n\nClick Define.\n\n\n\n\n\n  \n1. Enter a Name for the volume claim.\n\n\nChoose a Persistent Volume Claim from the drop-down.\n\nFrom the Customize section, choose the read/write access for the volume.\n\nClick Define.\n\n\n\n\n\n\n\nFrom the Mount Point field, enter /var/lib/registry, which is the data storage path inside the Docker registry container.\n\nClick Upgrade.\nB. Configuring Persistent Data for Minio\nFrom the project view, click Resources > Workloads. (In versions prior to v2.3.0, click the Workloads tab.) Find the minio workload and select Ellipsis (…) > Edit.\n\nScroll to the Volumes section and expand it. Make one of the following selections from the Add Volume menu, which is near the bottom of the section:\n\n\nAdd Volume > Add a new persistent volume (claim)\nAdd Volume > Use an existing persistent volume (claim)\n\n\nComplete the form that displays to choose a persistent volume for the internal Docker registry.\n\n  \n  \n  \n1. Enter a Name for the volume claim.\n\n\nSelect a volume claim Source:\n\n\nIf you select Use a Storage Class to provision a new persistent volume, select a Storage Class and enter a Capacity.\n\nIf you select Use an existing persistent volume, choose a Persistent Volume from the drop-down.\n\n\nFrom the Customize section, choose the read/write access for the volume.\n\nClick Define.\n\n\n\n\n\n  \n1. Enter a Name for the volume claim.\n\n\nChoose a Persistent Volume Claim from the drop-down.\n\nFrom the Customize section, choose the read/write access for the volume.\n\nClick Define.\n\n\n\n\n\n\n\nFrom the Mount Point field, enter /data, which is the data storage path inside the Minio container.\n\nClick Upgrade.\nResult: Persistent storage is configured for your pipeline components.After configuring a version control provider, there are several options that can be configured globally on how pipelines are executed in Rancher.\nFrom the Global view, navigate to the project that you want to configure pipelines.\n\nSelect Tools > Pipelines in the navigation bar. In versions prior to v2.2.0, you can select Resources > Pipelines.\n\nEdit the different settings:\n\n\n  \n  Executor Quota\n  \n    Select the maximum number of pipeline executors. The executor quota decides how many builds can run simultaneously in the project. If the number of triggered builds exceeds the quota, subsequent builds will queue until a vacancy opens. By default, the quota is 2. A value of 0 or less removes the quota limit.\n\n  \n\n\n\n\n  \n  Resource Quota for Executors\n  \n    Available as of v2.2.0\n\nConfigure compute resources for Jenkins agent containers. When a pipeline execution is triggered, a build pod is dynamically provisioned to run your CI tasks. Under the hood, A build pod consists of one Jenkins agent container and one container for each pipeline step. You can manage compute resources for every containers in the pod.\n\nEdit the Memory Reservation, Memory Limit, CPU Reservation or CPU Limit, then click Update Limit and Reservation.\n\nTo configure compute resources for pipeline-step containers:\n\n  \n  \n  You can configure compute resources for pipeline-step containers in the .rancher-pipeline.yml file.\n\nIn a step type, you will provide the following information:\n\n\nCPU Reservation (CpuRequest): CPU request for the container of a pipeline step.\nCPU Limit (CpuLimit): CPU limit for the container of a pipeline step.\nMemory Reservation (MemoryRequest): Memory request for the container of a pipeline step.\nMemory Limit (MemoryLimit): Memory limit for the container of a pipeline step.\n\n# example\nstages:\n  - name: Build something\n    steps:\n    - runScriptConfig:\n        image: busybox\n        shellScript: ls\n      cpuRequest: 100m\n      cpuLimit: 1\n      memoryRequest:100Mi\n      memoryLimit: 1Gi\n    - publishImageConfig:\n        dockerfilePath: ./Dockerfile\n        buildContext: .\n        tag: repo/app:v1\n      cpuRequest: 100m\n      cpuLimit: 1\n      memoryRequest:100Mi\n      memoryLimit: 1Gi\n\nNote: Rancher sets default compute resources for pipeline steps except for Build and Publish Images and Run Script steps. You can override the default value by specifying compute resources in the same way.\n\n\n\n\n\n\n  \n\n\n\n  \n  Custom CA\n  \n    Available as of v2.2.0\n\nIf you want to use a version control provider with a certificate from a custom/internal CA root, the CA root certificates need to be added as part of the version control provider configuration in order for the pipeline build pods to succeed.\n\n\nClick Edit cacerts.\n\nPaste in the CA root certificates and click Save cacerts.\n\n\nResult: Pipelines can be used and new pods will be able to work with the self-signed-certificate.\n\n  \n\n\nBefore you can start configuring a pipeline for your repository, you must configure and authorize a version control provider.\n\n\nProvider\nAvailable as of\n\n\n\n\n\nGitHub\nv2.0.0\n\n\n\nGitLab\nv2.1.0\n\n\n\nBitbucket\nv2.2.0\n\n\nSelect your provider’s tab below and follow the directions.\n  \n  \n  \nFrom the Global view, navigate to the project that you want to configure pipelines.\n\nSelect Tools > Pipelines in the navigation bar. In versions prior to v2.2.0, you can select Resources > Pipelines.\n\nFollow the directions displayed to Setup a Github application. Rancher redirects you to Github to setup an OAuth App in Github.\n\nFrom GitHub, copy the Client ID and Client Secret. Paste them into Rancher.\n\nIf you’re using GitHub for enterprise, select Use a private github enterprise installation. Enter the host address of your GitHub installation.\n\nClick Authenticate.\n\n\n\n\n\n  Available as of v2.1.0\n\n\nFrom the Global view, navigate to the project that you want to configure pipelines.\n\nSelect Tools > Pipelines in the navigation bar. In versions prior to v2.2.0, you can select Resources > Pipelines.\n\nFollow the directions displayed to Setup a GitLab application. Rancher redirects you to GitLab.\n\nFrom GitLab, copy the Application ID and Secret. Paste them into Rancher.\n\nIf you’re using GitLab for enterprise setup, select Use a private gitlab enterprise installation. Enter the host address of your GitLab installation.\n\nClick Authenticate.\n\n\n\nNote:\n1. Pipeline uses Gitlab v4 API and the supported Gitlab version is 9.0+.\n2. If you use GitLab 10.7+ and your Rancher setup is in a local network, enable the Allow requests to the local network from hooks and services option in GitLab admin settings.\n\n\n\n\n\n  Available as of v2.2.0\n\n\nFrom the Global view, navigate to the project that you want to configure pipelines.\n\nSelect Tools > Pipelines in the navigation bar.\n\nChoose the Use public Bitbucket Cloud option.\n\nFollow the directions displayed to Setup a Bitbucket Cloud application. Rancher redirects you to Bitbucket to setup an OAuth consumer in Bitbucket.\n\nFrom Bitbucket, copy the consumer Key and Secret. Paste them into Rancher.\n\nClick Authenticate.\n\n\n\n\n\n  Available as of v2.2.0\n\n\nFrom the Global view, navigate to the project that you want to configure pipelines.\n\nSelect Tools > Pipelines in the navigation bar.\n\nChoose the Use private Bitbucket Server setup option.\n\nFollow the directions displayed to Setup a Bitbucket Server application.\n\nEnter the host address of your Bitbucket server installation.\n\nClick Authenticate.\n\n\n\nNote:\nBitbucket server needs to do SSL verification when sending webhooks to Rancher. Please ensure that Rancher server’s certificate is trusted by the Bitbucket server. There are two options:\n\n\nSetup Rancher server with a certificate from a trusted CA.\nIf you’re using self-signed certificates, import Rancher server’s certificate to the Bitbucket server. For instructions, see the Bitbucket s","postref":"e19e082a642566a02a148a3929ff4b9c","objectID":"993fde8aab6566990ba062977eb20a7d","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/project-admin/pipelines/"},{"anchor":"#adding-global-catalogs","title":"Adding Global Catalogs","content":"Available as of v2.2.0\nPrerequisites: In order to manage project scoped catalogs, you need one of the following permissions:\n\n\nAdministrator Global Permissions\nCluster Owner Permissions\nProject Owner Permissions\nCustom Project Permissions with the Manage Project Catalogs role assigned.\n\n\nFrom the Global view, navigate to your project that you want to start adding custom catalogs.\nChoose the Tools > Catalogs in the navigation bar.\nClick Add Catalog.\nComplete the form. By default, the form will provide the ability to select Scope of the catalog. When you have added a catalog from the Project scope, it is defaulted to Cluster.\nClick Create.\nResult: Your custom project catalog is added to Rancher. Once it is in Active state, it has completed synchronization and you will be able to start deploying  applications in that project from this catalog.Available as of v2.2.0\nPrerequisites: In order to manage cluster scoped catalogs, you need one of the following permissions:\n\n\nAdministrator Global Permissions\nCluster Owner Permissions\nCustom Cluster Permissions with the Manage Cluster Catalogs role assigned.\n\n\nFrom the Global view, navigate to your cluster that you want to start adding custom catalogs.\nChoose the Tools > Catalogs in the navigation bar.\nClick Add Catalog.\nComplete the form. By default, the form will provide the ability to select Scope of the catalog. When you have added a catalog from the Cluster scope, it is defaulted to Cluster.\nClick Create.\nResult: Your custom cluster catalog is added to Rancher. Once it is in Active state, it has completed synchronization and you will be able to start deploying  applications in any project in that cluster from this catalog.\nPrerequisites: In order to manage the built-in catalogs or manage global catalogs, you need one of the following permissions:\n\n\nAdministrator Global Permissions\nCustom Global Permissions with the Manage Catalogs role assigned.\n\n\nFrom the Global view, choose Tools > Catalogs in the navigation bar. In versions prior to v2.2.0, you can select Catalogs directly in the navigation bar.\nClick Add Catalog.\nComplete the form and click Create.\nResult: Your custom global catalog is added to Rancher. Once it is in Active state, it has completed synchronization and you will be able to start deploying multi-cluster apps or applications in any project from this catalog.","postref":"2d2f3b7f823cc71f4424ecc237fbd58c","objectID":"cafcacf1240563f3d3124ff585aa9ee6","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/catalog/custom/adding/"},{"anchor":"#types-of-repositories","title":"Types of Repositories","content":"Available as of v2.2.0Private Git or Helm chart repositories can be added into Rancher using either credentials, i.e. Username and Password. Private Git repositories also support authentication using OAuth tokens.Using Username and Password\nWhen adding the catalog, select the Use private catalog checkbox.\n\nProvide the Username and Password for your Git or Helm repository.\nUsing an OAuth tokenRead using Git over HTTPS and OAuth for more details on how OAuth authentication works.\nCreate an OAuth token\nwith repo permission selected, and click Generate token.\n\nWhen adding the catalog, select the Use private catalog checkbox.\n\nFor Username, provide the Git generated OAuth token. For Password, enter x-oauth-basic.\nWhen adding your catalog to Rancher, you’ll provide the following information:\n\n\nVariable\nDescription\n\n\n\n\n\nName\nName for your custom catalog to distinguish the repositories in Rancher\n\n\n\nCatalog URL\nURL of your custom chart repository\n\n\n\nUse Private Catalog\nSelected if you are using a private repository that requires authentication\n\n\n\nUsername (Optional)\nUsername or OAuth Token\n\n\n\nPassword (Optional)\nIf you are authenticating using username, the associated password. If you are using an OAuth Token, use x-oauth-basic.\n\n\n\nBranch\nFor a Git repository, the branch name. Default: master. For a Helm Chart repository, this field is ignored.\n\n\nRancher supports adding in different types of repositories as a catalog:\nCustom Git Repository\nCustom Helm Chart Repository\nCustom Git RepositoryThe Git URL needs to be one that git clone can handle and must end in .git. The branch name must be a branch that is in your catalog URL. If no branch name is provided, it will default to use the master branch. Whenever you add a catalog to Rancher, it will be available almost immediately.Custom Helm Chart RepositoryA Helm chart repository is an HTTP server that contains one or more packaged charts. Any HTTP server that can serve YAML files and tar files and can answer GET requests can be used as a repository server.Helm comes with a built-in package server for developer testing (helm serve). The Helm team has tested other servers, including Google Cloud Storage with website mode enabled, S3 with website mode enabled or hosting custom chart repository server using open-source projects like ChartMuseum.In Rancher, you can add the custom Helm chart repository with only a catalog name and the URL address of the chart repository.","postref":"2486d4c95976c3276219cb777571c5a9","objectID":"0a52978a24485a434cc102c483d6d24f","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/catalog/custom/"},{"anchor":"#","title":"GlusterFS Volumes","content":"\nThis section only applies to RKE clusters.\n\n\nIn clusters that store data on GlusterFS volumes, you may experience an issue where pods fail to mount volumes after restarting the kubelet. The logging of the kubelet will show: transport endpoint is not connected. To prevent this from happening, you can configure your cluster to mount the systemd-run binary in the kubelet container. There are two requirements before you can change the cluster configuration:\n\n\nThe node needs to have the systemd-run binary installed (this can be checked by using the command which systemd-run on each cluster node)\nThe systemd-run binary needs to be compatible with Debian OS on which the hyperkube image is based (this can be checked using the following command on each cluster node, replacing the image tag with the Kubernetes version you want to use)\n\n\ndocker run -v /usr/bin/systemd-run:/usr/bin/systemd-run --entrypoint /usr/bin/systemd-run rancher/hyperkube:v1.16.2-rancher1 --version\n\n\n\nNote:\n\nBefore updating your Kubernetes YAML to mount the systemd-run binary, make sure the systemd package is installed on your cluster nodes. If this package isn’t installed before the bind mounts are created in your Kubernetes YAML, Docker will automatically create the directories and files on each node and will not allow the package install to succeed.\n\n\nservices:\n  kubelet:\n    extra_binds:\n      - \"/usr/bin/systemd-run:/usr/bin/systemd-run\"\n\n\nAfter the cluster has finished provisioning, you can check the kubelet container logging to see if the functionality is activated by looking for the following logline:\n\nDetected OS with systemd\n\n","postref":"e45eeb88585e63b682445136dcadc4ce","objectID":"2564d69ce173d3c040f1be8ab993d898","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/volumes-and-storage/glusterfs-volumes/"},{"anchor":"#launching-a-multi-cluster-app","title":"Launching a Multi-Cluster App","content":"\nFrom the Global view, choose Apps in the navigation bar.\n\nChoose the multi-cluster application you want to delete and click the Vertical Ellipsis (…) > Delete. When deleting the multi-cluster application, all applications and namespaces are deleted in all of the target projects.\n\nNote: The applications in the target projects, that are created for a multi-cluster application, cannot be deleted individually. The applications can only be deleted when the multi-cluster application is deleted.\nOne of the benefits of using a multi-cluster application as opposed to multiple individual applications of the same type, is the ease of management. Multi-cluster applications can be cloned, upgraded or rolled back.\nFrom the Global view, choose Apps in the navigation bar.\n\nChoose the multi-cluster application you want to take one of these actions on and click the Vertical Ellipsis (…). Select one of the following options:\n\n\nClone: Creates another multi-cluster application with the same configuration. By using this option, you can easily duplicate a multi-cluster application.\nUpgrade: Upgrade your multi-cluster application to change some part of the configuration. When performing an upgrade for multi-cluster application, the upgrade strategy can be modified if you have the correct access type.\nRollback: Rollback your application to a specific version. If after an upgrade, there are issues for your multi-cluster application for one or more of your targets, Rancher has stored up to 10 versions of the multi-cluster application. Rolling back a multi-cluster application reverts the application for all target clusters and projects, not just the targets(s) affected by the upgrade issue.\n\n\nChanging Roles on an existing Multi-Cluster app\nThe creator and any users added with the access-type “owner” to a multi-cluster app, can upgrade its Roles. When adding a new Role, we check if the user has that exact role in all current target projects. These checks allow the same relaxations for global admins, cluster owners and project-owners as described in the installation section for the field Roles.\n\nAdding/Removing target projects\n\nThe creator and any users added with access-type “owner” to a multi-cluster app, can add or remove its target projects. When adding a new project, we check if the caller of this request has all Roles defined on multi-cluster app, in the new projects they want to add. The roles checks are again relaxed for global admins, cluster-owners and project-owners.\n\nWe do not do these membership checks when removing target projects. This is because the caller’s permissions could have with respect to the target project, or the project could have been deleted and hence the caller wants to remove it from targets list.\n\nFrom the Global view, choose Apps in the navigation bar. Click Launch.\n\nFind the application that you want to launch, and then click View Details.\n\n(Optional) Review the detailed descriptions, which are derived from the Helm chart’s README.\n\nUnder Configuration Options enter a Name for the multi-cluster application. By default, this name is also used to create a Kubernetes namespace in each target project for the multi-cluster application. The namespace is named as <MULTI-CLUSTER_APPLICATION_NAME>-<PROJECT_ID>.\n\nSelect a Template Version.\n\nComplete the multi-cluster applications specific configuration options as well as the application configuration options.\n\nSelect the Members who can interact with the multi-cluster application.\n\nAdd any custom application configuration answers that would change the configuration for specific project(s) from the default application configuration answers.\n\nReview the files in the Preview section. When you’re satisfied, click Launch.\nResult: Your application is deployed to your chosen namespace. You can view the application status from the project’s:Configuration Options to Make a Multi-Cluster AppRancher has divided the configuration option for the multi-cluster application into several sections.TargetsIn the Targets section, select the projects that you want the application to be deployed in. The list of projects is based on what projects you have access to. For each project that you select, it will be added to the list, which shows the cluster name and project name that were selected. To remove a target project, click on -.UpgradesIn the Upgrades section, select the upgrade strategy to use, when you decide to upgrade your application.\nRolling Update (batched): When selecting this upgrade strategy, the number of applications upgraded at a time is based on the selected Batch size and the Interval specifies how many seconds to wait before starting the next batch of updates.\n\nUpgrade all apps simultaneously: When selecting this upgrade strategy, all applications across all projects will be upgraded at the same time.\nRolesIn the Roles section, you define the role of the multi-cluster application. Typically, when a user launches catalog applications, that specific user’s permissions are used for creation of all workloads/resources that is required by the app.For multi-cluster applications, the application is deployed by a system user and is assigned as the creator of all underlying resources. A system user is used instead of the actual user due to the fact that the actual user could be removed from one of the target projects. If the actual user was removed from one of the projects, then that user would no longer be able to manage the application for the other projects.Rancher will let you select from two options for Roles, Project and Cluster. Rancher will allow creation using any of these roles based on the user’s permissions.\nProject - This is the equivalent of a project member. If you select this role, Rancher will check that in all the target projects, the user has minimally the project member role. While the user might not be explicitly granted the project member role, if the user is an administrator, a cluster owner, or a project owner, then the user is considered to have the appropriate level of permissions.\n\nCluster - This is the equivalent of a cluster owner. If you select this role, Rancher will check that in all the target projects, the user has minimally the cluster owner role. While the user might not be explicitly granted the cluster owner role, if the user is an administrator, then the user is considered to have the appropriate level of permissions.\nWhen launching the application, Rancher will confirm if you have these permissions in the target projects before launching the application.\nNote: There are some applications like Grafana or Datadog that require access to specific cluster-scoped resources. These applications will require the Cluster role. If you find out later that the application requires cluster roles, the multi-cluster application can be upgraded to update the roles.\nApplication Configuration OptionsFor each Helm chart, there are a list of desired answers that must be entered in order to successfully deploy the chart. When entering answers, you must format them using the syntax rules found in Using Helm: The format and limitations of –set, as Rancher passes them as --set flags to Helm.\nFor example, when entering an answer that includes two values separated by a comma (i.e. abc, bcd), it is required to wrap the values with double quotes (i.e., \"abc, bcd\").\nUsing a questions.yml fileIf the Helm chart that you are deploying contains a questions.yml file, Rancher’s UI will translate this file to display an easy to use UI to collect the answers for the questions.Key Value Pairs for Native Helm ChartsFor native Helm charts (i.e., charts from the Helm Stable or Helm Incubator catalogs or a custom Helm chart repository), answers are provided as key value pairs in the Answers section. These answers are used to override the default values.MembersBy default, multi-cluster applications can only be managed by the user who created it. In the Members section, other users can be added so that they can also help manage or view the multi-cluster application.\nFind the user that you want to add by typing in the member’s name in the Member search box.\n\nSelect the Access Type for that member. There are three access types for a multi-cluster project, but due to how the permissions of a multi-cluster application are launched, please read carefully to understand what these access types mean.\n\n\nOwner: This access type can manage any configuration part of the multi-cluster application including the template version, the multi-cluster applications specific configuration options, the application specific configuration options, the members who can interact with the multi-cluster application and the custom application configuration answers. Since a multi-cluster application is created with a different set of permissions from the user, any owner of the multi-cluster application can manage/remove applications in target projects without explicitly having access to these project(s). Only trusted users should ","postref":"c8653ee7945301ea0a3fed89d729be8f","objectID":"d93340ae810c36972592c6fa5a66e89f","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/catalog/multi-cluster-apps/"},{"anchor":"#prerequisites","title":"Prerequisites","content":"After deploying an application, one of the benefits of using an application versus individual workloads/resources is the ease of being able to manage many workloads/resources applications. Apps can be cloned, upgraded or rolled back.Cloning Catalog ApplicationsAfter an application is deployed, you can easily clone it to use create another application with almost the same configuration. It saves you the work of manually filling in duplicate information.Upgrading Catalog ApplicationsAfter an application is deployed, you can easily upgrade to a different template version.\nFrom the Global view, navigate to the project that contains the catalog application that you want to upgrade.\n\nFrom the main navigation bar, choose Apps. In versions prior to v2.2.0, choose Catalog Apps on the main navigation bar. Click Launch.\n\nFind the application that you want to upgrade, and then click the Ellipsis to find Upgrade.\n\nSelect the Template Version that you want to deploy.\n\n(Optional) Update your Configuration Options.\n\n(Optional) Select whether or not you want to force the catalog application to be upgraded by checking the box for Delete and recreate resources if needed during the upgrade.\n\n\nIn Kubernetes, some fields are designed to be immutable or cannot be updated directly. As of v2.2.0, you can now force your catalog application to be updated regardless of these fields. This will cause the catalog apps to be deleted and resources to be re-created if needed during the upgrade.\n\n\nReview the files in the Preview section. When you’re satisfied, click Launch.\nResult: Your application is updated. You can view the application status from the project’s:\nWorkloads view\nApps view. In versions prior to v2.2.0, this is the Catalog Apps view.\nRolling Back Catalog ApplicationsAfter an application has been upgraded, you can easily rollback to a different template version.\nFrom the Global view, navigate to the project that contains the catalog application that you want to upgrade.\n\nFrom the main navigation bar, choose Apps. In versions prior to v2.2.0, choose Catalog Apps on the main navigation bar. Click Launch.\n\nFind the application that you want to rollback, and then click the Ellipsis to find Rollback.\n\nSelect the Revision that you want to roll back to. By default, Rancher saves up to the last 10 revisions.\n\n(Optional) Select whether or not you want to force the catalog application to be upgraded by checking the box for Delete and recreate resources if needed during the upgrade.\n\n\nIn Kubernetes, some fields are designed to be immutable or cannot be updated directly. As of v2.2.0, you can now force your catalog application to be updated regardless of these fields. This will cause the catalog apps to be deleted and resources to be re-created if needed during the rollback.\n\n\nClick Rollback.\nResult: Your application is updated. You can view the application status from the project’s:\nWorkloads view\nApps view. In versions prior to v2.2.0, this is the Catalog Apps view.\nDeleting Catalog Application DeploymentsAs a safeguard to prevent you from unintentionally deleting other catalog applications that share a namespace, deleting catalog applications themselves does not delete the namespace they’re assigned to.Therefore, if you want to delete both an app and the namespace that contains the app, you should remove the app and the namespace separately:\nUninstall the app using the app’s uninstall function.\n\nFrom the Global view, navigate to the project that contains the catalog application that you want to delete.\n\nFrom the main menu, choose Namespaces.\n\nFind the namespace running your catalog app. Select it and click Delete.\nResult: The catalog application deployment and its namespace are deleted.After you’ve either enabled the built-in global catalogs or added your own custom catalog, you can start launching catalog applications.\nFrom the Global view, navigate to your project that you want to start deploying applications.\n\nFrom the main navigation bar, choose Apps. In versions prior to v2.2.0, choose Catalog Apps on the main navigation bar. Click Launch.\n\nFind the application that you want to launch, and then click View Details.\n\n(Optional) Review the detailed descriptions, which comes from the Helm chart’s README.\n\nUnder Configuration Options enter a Name. By default, this name is also used to create a Kubernetes namespace for the application.\n\n\nIf you would like to change the Namespace, click Customize and change the name of the namespace.\nIf you want to use a different namespace that already exists, click Customize, and then click Use an existing namespace. Choose a namespace from the list.\n\n\nSelect a Template Version.\n\nComplete the rest of the Configuration Options. Rancher handles how to customize your configuration options depending on whether or not the custom catalog includes the questions.yml file.\n\nReview the files in the Preview section. When you’re satisfied, click Launch.\nResult: Your application is deployed to your chosen namespace. You can view the application status from the project’s:\nWorkloads view\nApps view. In versions prior to v2.2.0, this is the Catalog Apps view.\nConfiguration OptionsFor each Helm chart, there are a list of desired answers that must be entered in order to successfully deploy the chart. When entering answers, you must format them using the syntax rules found in Using Helm: The format and limitations of –set, as Rancher passes them as --set flags to Helm.\nFor example, when entering an answer that includes two values separated by a comma (i.e. abc, bcd), it is required to wrap the values with double quotes (i.e., \"abc, bcd\").\n\n  \n  \n  Using a questions.yml file\n\nIf the Helm chart that you are deploying contains a questions.yml file, Rancher’s UI will translate this file to display an easy to use UI to collect the answers for the questions.\n\nKey Value Pairs for Native Helm Charts\n\nFor native Helm charts (i.e., charts from the Helm Stable or Helm Incubator catalogs or a custom Helm chart repository), answers are provided as key value pairs in the Answers section. These answers are used to override the default values.\n\n\n\n\n  Available as of v2.1.0\n\nIf you do not want to input answers using the UI, you can choose the Edit as YAML option.\n\nWith this example YAML:\nouter:\n  inner: value\nservers:\n- port: 80\n  host: example\nKev Value Pairs\n\nYou can have a YAML file that translates these fields to match how to format custom values so that it can be used with --set.\n\nThese values would be translated to:\n\nouter.inner=value\nservers[0].port=80\nservers[0].host=example\n\n\nYAML files\n\nAvailable as of v2.2.0\n\nYou can directly paste that YAML formatted structure into the YAML editor. By allowing custom values to be set using a YAML formatted structure, Rancher has the ability to easily customize for more complicated input values (e.g. multi-lines, array and JSON objects).\n\n\n\nTo create a multi-cluster app in Rancher, you must have at least one of the following permissions:\nA project-member role in the target cluster, which gives you the ability to create, read, update, and delete the workloads\nA cluster owner role for the cluster that include the target project\n","postref":"f6053699d5fc33a05e89528d013aa7b0","objectID":"49ba94da7ef061e32b530b3bdd8a81a4","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/catalog/apps/"},{"anchor":"#global-dns-providers","title":"Global DNS Providers","content":"The global administrators, creator of the Global DNS entry and any users added as members to a Global DNS entry, have owner access to that DNS entry. Any members can edit the following fields:\nFQDN\nGlobal DNS Provider\nTarget Projects or Multi-Cluster App\nDNS TTL\nMembers\nAny users who can access the Global DNS entry can only add target projects that they have access to. However, users can remove any target project as there is no check to confirm if that user has access to the target project.Permission checks are relaxed for removing target projects in order to support situations where the user’s permissions might have changed before they were able to delete the target project. Another use case could be that the target project was removed from the cluster before being removed from a target project of the Global DNS entry.\nFrom the Global View, select Tools > Global DNS Entries.\n\nFor the Global DNS entry that you want to edit, click the Vertical Ellipsis (…) > Edit.\nThe global administrators, creator of the Global DNS provider and any users added as members to a Global DNS provider, have owner access to that provider. Any members can edit the following fields:\nRoot Domain\nAccess Key & Secret Key\nMembers\n\nFrom the Global View, select Tools > Global DNS Providers.\n\nFor the Global DNS provider that you want to edit, click the Vertical Ellipsis (…) > Edit.\nIn order for Global DNS entries to be programmed, you will need to add a specific annotation on an ingress in your application or target project and this ingress needs to use a specific hostname and an annotation that should match the FQDN of the Global DNS entry.\nFor any application that you want targeted for your Global DNS entry, find an ingress associated with the application.\nIn order for the DNS to be programmed, the following requirements must be met:\n\n\nThe ingress routing rule must be set to use a hostname that matches the FQDN of the Global DNS entry.\nThe ingress must have an annotation (rancher.io/globalDNS.hostname) and the value of this annotation should match the FQDN of the Global DNS entry.\n\nOnce the ingress in your multi-cluster application or in your target projects are in active state, the FQDN will be programmed on the external DNS against the Ingress IP addresses.\nAdd a Global DNS Provider\nFrom the Global View, select Tools > Global DNS Providers.\nTo add a provider, choose from the available provider options and configure the Global DNS Provider with necessary credentials and an optional domain.\n(Optional) Add additional users so they could  use the provider when creating Global DNS entries as well as manage the Global DNS provider.\n\n  \n  Route53\n  \n    \nEnter a Name for the provider.\n(Optional) Enter the Root Domain of the hosted zone on AWS Route53. If this is not provided, Rancher’s Global DNS Provider will work with all hosted zones that the AWS keys can access.\nEnter the AWS Access Key.\nEnter the AWS Secret Key.\nUnder Member Access, search for any users that you want to have the ability to use this provider. By adding this user, they will also be able to manage the Global DNS Provider entry.\nClick Create.\n\n\n  \n\n  \n  CloudFlare\n  \n    \nEnter a Name for the provider.\nEnter the Root Domain, this field is optional, in case this is not provided, Rancher’s Global DNS Provider will work with all domains that the keys can access.\nEnter the CloudFlare API Email.\nEnter the CloudFlare API Key.\nUnder Member Access, search for any users that you want to have the ability to use this provider. By adding this user, they will also be able to manage the Global DNS Provider entry.\nClick Create.\n\n\n  \n\n  \n  AliDNS\n  \n    \nEnter a Name for the provider.\nEnter the Root Domain, this field is optional, in case this is not provided, Rancher’s Global DNS Provider will work with all domains that the keys can access.\nEnter the Access Key.\nEnter the Secret Key.\nUnder Member Access, search for any users that you want to have the ability to use this provider. By adding this user, they will also be able to manage the Global DNS Provider entry.\nClick Create.\n\n\n\nNotes:\n\n\nAlibaba Cloud SDK uses TZ data. It needs to be present on /usr/share/zoneinfo path of the nodes running local cluster, and it is mounted to the external DNS pods. If it is not available on the nodes, please follow the instruction to prepare it.\nDifferent versions of AliDNS have different allowable TTL range, where the default TTL for a global DNS entry may not be valid. Please see the reference before adding an AliDNS entry.\n\n\n\n  \nAdd a Global DNS Entry\nFrom the Global View, select Tools > Global DNS Entries.\nClick on Add DNS Entry.\nEnter the FQDN you wish to program on the external DNS.\nSelect a Global DNS Provider from the list.\nSelect if this DNS entry will be for a multi-cluster application or for workloads in different projects.  You will need to ensure that annotations are added to any ingresses for the applications that you want to target.\nConfigure the DNS TTL value in seconds. By default, it will be 300 seconds.\nUnder Member Access, search for any users that you want to have the ability to manage this Global DNS entry.\nBy default, only global administrators and the creator of the Global DNS provider or Global DNS entry have access to use, edit and delete them. When creating the provider or entry, the creator can add additional users in order for those users to access and manage them. By default, these members will get Owner role to manage them.For each application that you want to route traffic to, you will need to create a Global DNS Entry. This entry will use a fully qualified domain name (a.k.a FQDN) from a global DNS provider to target applications. The applications can either resolve to a single multi-cluster application or to specific projects. You must add specific annotation labels to the ingresses in order for traffic to be routed correctly to the applications. Without this annotation, the programming for the DNS entry will not work.Prior to adding in Global DNS entries, you will need to configure access to an external provider.The following table lists the first version of Rancher each provider debuted.\n\n\nDNS Provider\nAvailable as of\n\n\n\n\n\nAWS Route53\nv2.2.0\n\n\n\nCloudFlare\nv2.2.0\n\n\n\nAliDNS\nv2.2.0\n\n\n","postref":"a6fb259c31d24b0bce4a61345fb0519a","objectID":"5306eac962f1cb265739e65e257777e2","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/catalog/globaldns/"},{"anchor":"#","title":"Pod Security Policies","content":"\nThese cluster options are only available for clusters in which Rancher has launched Kubernetes.\n\n\nYou can always assign a pod security policy (PSP) to an existing project if you didn’t assign one during creation.\n\nPrerequisites\n\n\nCreate a Pod Security Policy within Rancher. Before you can assign a default PSP to an existing project, you must have a PSP available for assignment. For instruction, see Creating Pod Security Policies.\nAssign a default Pod Security Policy to the project’s cluster. You can’t assign a PSP to a project until one is already applied to the cluster. For more information, see Existing Cluster: Adding a Pod Security Policy.\n\n\nApplying a Pod Security Policy\n\n\nFrom the Global view, find the cluster containing the project you want to apply a PSP to.\nFrom the main menu, select Projects/Namespaces.\nFind the project that you want to add a PSP to. From that project, select Vertical Ellipsis (…) > Edit.\n\nFrom the Pod Security Policy drop-down, select the PSP you want to apply to the project.\nAssigning a PSP to a project will:\n\n\nOverride the cluster’s default PSP.\nApply the PSP to the project.\nApply the PSP to any namespaces you add to the project later.\n\n\nClick Save.\n\n\nResult: The PSP is applied to the project and any namespaces added to the project.\n\n\nNote: Any workloads that are already running in a cluster or project before a PSP is assigned will not be checked to determine if they comply with the PSP. Workloads would need to be cloned or upgraded to see if they pass the PSP.\n\n","postref":"9d1ff3c2b091a51168519363a6127c44","objectID":"3db7f9ac38515903cb6f742a38b38f90","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/project-admin/pod-security-policies/"},{"anchor":"#","title":"Using the Rancher Command Line Interface","content":"The Rancher CLI (Command Line Interface) is a unified tool that you can use to interact with Rancher. With this tool, you can operate Rancher using a command line rather than the GUI.\n\nDownload Rancher CLI\n\nThe binary can be downloaded directly from the UI. The link can be found in the right hand side of the footer in the UI. We have binaries for Windows, Mac, and Linux. You can also check the releases page for our CLI for direct downloads of the binary.\n\nRequirements\n\nAfter you download the Rancher CLI, you need to make a few configurations. Rancher CLI requires:\n\n\nYour Rancher Server URL, which is used to connect to Rancher Server.\nAn API Bearer Token, which is used to authenticate with Rancher. For more information about obtaining a Bearer Token, see Creating an API Key.\n\n\nCLI Authentication\n\nBefore you can use Rancher CLI to control your Rancher Server, you must authenticate using an API Bearer Token. Log in using the following command (replace <BEARER_TOKEN> and <SERVER_URL> with your information):\n$ ./rancher login https://<SERVER_URL> --token <BEARER_TOKEN>\nIf Rancher Server uses a self-signed certificate, Rancher CLI prompts you to continue with the connection.\n\nProject Selection\n\nBefore you can perform any commands, you must select a Rancher project to perform those commands against. To select a project to work on, use the command ./rancher context switch. When you enter this command, a list of available projects displays. Enter a number to choose your project.\n\nExample: ./rancher context switch Output\n\nUser:rancher-cli-directory user$ ./rancher context switch\nNUMBER    CLUSTER NAME   PROJECT ID              PROJECT NAME   \n1         cluster-2      c-7q96s:p-h4tmb         project-2      \n2         cluster-2      c-7q96s:project-j6z6d   Default        \n3         cluster-1      c-lchzv:p-xbpdt         project-1      \n4         cluster-1      c-lchzv:project-s2mch   Default       \nSelect a Project:\n\n\nAfter you enter a number, the console displays a message that you’ve changed projects.\n\nINFO[0005] Setting new context to project project-1\nINFO[0005] Saving config to /Users/markbishop/.rancher/cli2.json\n\n\nCommands\n\nThe following commands are available for use in Rancher CLI.\n\n\n\n\nCommand\nResult\n\n\n\n\n\napps, [app]\nPerforms operations on catalog applications (i.e. individual Helm charts or Rancher charts).\n\n\n\ncatalog\nPerforms operations on catalogs.\n\n\n\nclusters, [cluster]\nPerforms operations on your clusters.\n\n\n\ncontext\nSwitches between Rancher projects. For an example, see Project Selection.\n\n\n\ninspect [OPTIONS] [RESOURCEID RESOURCENAME]\nDisplays details about Kubernetes resources or Rancher resources (i.e.: projects and workloads). Specify resources by name or ID.\n\n\n\nkubectl\nRuns kubectl commands.\n\n\n\nlogin, [l]\nLogs into a Rancher Server. For an example, see CLI Authentication.\n\n\n\nnamespaces, [namespace]\nPerforms operations on namespaces.\n\n\n\nnodes, [node]\nPerforms operations on nodes.\n\n\n\nprojects, [project]\nPerforms operations on projects.\n\n\n\nps\nDisplays workloads in a project.\n\n\n\nsettings, [setting]\nShows the current settings for your Rancher Server.\n\n\n\nssh\nConnects to one of your cluster nodes using the SSH protocol.\n\n\n\nhelp, [h]\nShows a list of commands or help for one command.\n\n\n\n\nRancher CLI Help\n\nOnce logged into Rancher Server using the CLI, enter ./rancher --help for a list of commands.\n\nAll commands accept the --help flag, which documents each command’s usage.\n","postref":"99358bc407a164ba491e9e71658e365d","objectID":"4ec960be97b8853609a866e0fe409699","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cli/"},{"anchor":"#","title":"iSCSI Volumes","content":"In Rancher Launched Kubernetes clusters that store data on iSCSI volumes, you may experience an issue where kubelets fail to automatically connect with iSCSI volumes. This failure is likely due to an incompatibility issue involving the iSCSI initiator tool. You can resolve this issue by installing the iSCSI initiator tool on each of your cluster nodes.\n\nRancher Launched Kubernetes clusters storing data on iSCSI volumes leverage the iSCSI initiator tool, which is embedded in the kubelet’s rancher/hyperkube Docker image. From each kubelet (i.e., the initiator), the tool discovers and launches sessions with an iSCSI volume (i.e., the target). However, in some instances, the versions of the iSCSI initiator tool installed on the initiator and the target may not match, resulting in a connection failure.\n\nIf you encounter this issue, you can work around it by installing the initiator tool on each node in your cluster. You can install the iSCSI initiator tool by logging into your cluster nodes and entering one of the following commands:\n\n\n\n\nPlatform\nPackage Name\nInstall Command\n\n\n\n\n\nUbuntu/Debian\nopen-iscsi\nsudo apt install open-iscsi\n\n\n\nRHEL\niscsi-initiator-utils\nyum install iscsi-initiator-utils -y\n\n\n\n\nAfter installing the initiator tool on your nodes, edit the YAML for your cluster, editing the kubelet configuration to mount the iSCSI binary and configuration, as shown in the sample below.\n\n\nNote:\n\nBefore updating your Kubernetes YAML to mount the iSCSI binary and configuration, make sure either the open-iscsi (deb) or iscsi-initiator-utils (yum) package is installed on your cluster nodes. If this package isn’t installed before the bind mounts are created in your Kubernetes YAML, Docker will automatically create the directories and files on each node and will not allow the package install to succeed.\n\n\nservices:\n  kubelet:\n    extra_binds:\n      - \"/etc/iscsi:/etc/iscsi\"\n      - \"/sbin/iscsiadm:/sbin/iscsiadm\"\n\n","postref":"6cf91c9304b1240271dcc6011429fb08","objectID":"73366fd963bebbd1b89f17e07d0d5b31","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/volumes-and-storage/iscsi-volumes/"},{"anchor":"#","title":"System Tools","content":"System Tools is a tool to perform operational tasks on Rancher Launched Kubernetes clusters or RKE cluster as used for installing Rancher on Kubernetes. The tasks include:\n\n\nCollect logging and system metrics from nodes.\nRemove Kubernetes resources created by Rancher.\n\n\nThe following commands are available:\n\n\n\n\nCommand\nDescription\n\n\n\n\n\nlogs\nCollect Kubernetes cluster component logs from nodes.\n\n\n\nstats\nStream system metrics from nodes.\n\n\n\nremove\nRemove Kubernetes resources created by Rancher.\n\n\n\n\nDownload System Tools\n\nYou can download the latest version of System Tools from the GitHub releases page. Download the version of system-tools for the OS that you are using to interact with the cluster.\n\n\n\n\nOperating System\nFilename\n\n\n\n\n\nMacOS\nsystem-tools_darwin-amd64\n\n\n\nLinux\nsystem-tools_linux-amd64\n\n\n\nWindows\nsystem-tools_windows-amd64.exe\n\n\n\n\nAfter you download the tools, complete the following actions:\n\n\nRename the file to system-tools.\n\nGive the file executable permissions by running the following command:\n\n\nUsing Windows?\nThe file is already an executable, you can skip this step.\n\n\nchmod +x system-tools\n\n\n\nLogs\n\nThe logs subcommand will collect log files of core Kubernetes cluster components from nodes in Rancher-launched Kubernetes clusters or nodes on an RKE Kubernetes cluster that Rancher is installed on.. See Troubleshooting for a list of core Kubernetes cluster components.\n\nSystem Tools will use the provided kubeconfig file to deploy a DaemonSet, that will copy all the logfiles from the core Kubernetes cluster components and add them to a single tar file (cluster-logs.tar by default). If you only want to collect logging from a single node, you can specify the node by using --node NODENAME or -n NODENAME.\n\nUsage\n\n./system-tools_darwin-amd64 logs --kubeconfig <KUBECONFIG>\n\n\nThe following are the options for the logs command:\n\n\n\n\nOption\nDescription\n\n\n\n\n\n--kubeconfig <KUBECONFIG_PATH>, -c <KUBECONFIG_PATH>\nThe cluster’s kubeconfig file.\n\n\n\n--output <FILENAME>, -o cluster-logs.tar\nName of the created tarball containing the logs. If no output filename is defined, the options defaults to cluster-logs.tar.\n\n\n\n--node <NODENAME>, -n node1\nSpecify the nodes to collect the logs from. If no node is specified, logs from all nodes in the cluster will be collected.\n\n\n\n\nStats\n\nThe stats subcommand will display system metrics from nodes in Rancher-launched Kubernetes clusters or nodes in an RKE Kubernetes cluster that Rancher is installed on..\n\nSystem Tools will deploy a DaemonSet, and run a predefined command based on sar (System Activity Report) to show system metrics.\n\nUsage\n\n./system-tools_darwin-amd64 stats --kubeconfig <KUBECONFIG>\n\n\nThe following are the options for the stats command:\n\n\n\n\nOption\nDescription\n\n\n\n\n\n--kubeconfig <KUBECONFIG_PATH>, -c <KUBECONFIG_PATH>\nThe cluster’s kubeconfig file.\n\n\n\n--node <NODENAME>, -n node1\nSpecify the nodes to display the system metrics from. If no node is specified, logs from all nodes in the cluster will be displayed.\n\n\n\n--stats-command value, -s value\nThe command to run to display the system metrics. If no command is defined, the options defaults to /usr/bin/sar -u -r -F 1 1.\n\n\n\n\nRemove\n\n\nWarning: This command will remove data from your etcd nodes. Make sure you have created a backup of etcd before executing the command.\n\n\nWhen you install Rancher on a Kubernetes cluster, it will create Kubernetes resources to run and to store configuration data. If you want to remove Rancher from your cluster, you can use the remove subcommand to remove the Kubernetes resources. When you use the remove subcommand, the following resources will be removed:\n\n\nThe Rancher deployment namespace (cattle-system by default).\nAny serviceAccount, clusterRoles, and clusterRoleBindings that Rancher applied the cattle.io/creator:norman label to. Rancher applies this label to any resource that it creates as of v2.1.0.\nLabels, annotations, and finalizers.\nRancher Deployment.\nMachines, clusters, projects, and user custom resource deployments (CRDs).\nAll resources create under the management.cattle.io API Group.\nAll CRDs created by Rancher v2.x.\n\n\n\nUsing 2.0.8 or Earlier?\n\nThese versions of Rancher do not automatically delete the serviceAccount, clusterRole, and clusterRoleBindings resources after the job runs. You’ll have to delete them yourself.\n\n\nUsage\n\nWhen you run the command below, all the resources listed above will be removed from the cluster.\n\n\nWarning: This command will remove data from your etcd nodes. Make sure you have created a backup of etcd before executing the command.\n\n\n./system-tools remove --kubeconfig <KUBECONFIG> --namespace <NAMESPACE>\n\n\nThe following are the options for the remove command:\n\n\n\n\nOption\nDescription\n\n\n\n\n\n--kubeconfig <KUBECONFIG_PATH>, -c <KUBECONFIG_PATH>\nThe cluster’s kubeconfig file\n\n\n\n--namespace <NAMESPACE>, -n cattle-system\nRancher 2.x deployment namespace (<NAMESPACE>). If no namespace is defined, the options defaults to cattle-system.\n\n\n\n--force\nSkips the interactive removal confirmation and removes the Rancher deployment without prompt.\n\n\n\n","postref":"00639ea12cbe7ea64ed11a3911090a3f","objectID":"2b2c1573184dd9713625b2fbcbbb3e42","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/system-tools/"},{"anchor":"#","title":"User Settings","content":"Within Rancher, each user has a number of settings associated with their login: personal preferences, API keys, etc. You can configure these settings by choosing from the User Settings menu. You can open this menu by clicking your avatar, located within the main menu.\n\n\n\nThe available user settings are:\n\n\nAPI & Keys: If you want to interact with Rancher programmatically, you need an API key. Follow the directions in this section to obtain a key.gferfgre\nCloud Credentials: Manage cloud credentials used by node templates to provision nodes for clusters. Note: Available as of v2.2.0.\nNode Templates: Manage templates used by Rancher to provision nodes for clusters.\nPreferences: Sets superficial preferences for the Rancher UI.\nLog Out: Ends your user session.\n\n","postref":"286403e822b0e2f919269f3891c766a9","objectID":"bd40a889b36ecfef75cd241e0ca4edd1","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/user-settings/"},{"anchor":"#api-keys-and-user-authentication","title":"API Keys and User Authentication","content":"If you need to revoke an API key, delete it. You should delete API keys:\nThat may have been compromised.\nThat have expired.\nTo delete an API, select the stale key and click Delete.\nEnter your API key information into the application that will send requests to the Rancher API.\nLearn more about the Rancher endpoints and parameters by selecting View in API for an object in the Rancher UI.\nAPI keys are used for API calls and Rancher CLI.\n\nSelect User Avatar > API & Keys from the User Settings menu in the upper-right.\n\nClick Add Key.\n\nOptional: Enter a description for the API key and select an expiration period or a scope. We recommend setting an expiration date.\n\nThe API key won’t be valid after expiration. Shorter expiration periods are more secure.\n\nA scope will limit the API key so that it will only work against the Kubernetes API of the specified cluster. If the cluster is configured with an Authorized Cluster Endpoint, you will be able to use a scoped token directly against the cluster’s API without proxying through the Rancher server. See Authorized Cluster Endpoints for more information.\n\nClick Create.\n\nStep Result: Your API Key is created. Your API Endpoint, Access Key, Secret Key, and Bearer Token are displayed.\n\nUse the Bearer Token to authenticate with Rancher CLI.\n\nCopy the information displayed to a secure location. This information is only displayed once, so if you lose your key, you’ll have to make a new one.\nIf you want to access your Rancher clusters, projects, or other objects using external applications, you can do so using the Rancher API. However, before your application can access the API, you must provide the app with a key used to authenticate with Rancher. You can obtain a key using the Rancher UI.An API key is also required for using Rancher CLI.API Keys are composed of four components:\nEndpoint: This is the IP address and path that other applications use to send requests to the Rancher API.\nAccess Key: The token’s username.\nSecret Key: The token’s password. For applications that prompt you for two different strings for API authentication, you usually enter the two keys together.\nBearer Token: The token username and password concatenated together. Use this string for applications that prompt you for one authentication string.\n","postref":"12ca310a5b8c1e5578d322dacaf80d1c","objectID":"e3e0d9523c7939c9b3969c40f2b31f4b","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/user-settings/api-keys/"},{"anchor":"#creating-a-node-template-from-user-settings","title":"Creating a Node Template from User Settings","content":"When you no longer use a node template, you can delete it from your user settings.\nFrom your user settings, select User Avatar > Node Templates.\nSelect one or more template from the list. Then click Delete. Confirm the delete when prompted.\nWhen creating new node templates from your user settings, you can clone an existing template and quickly update its settings rather than creating a new one from scratch. Cloning templates saves you the hassle of re-entering access keys for the cloud provider.\nFrom your user settings, select User Avatar > Node Templates.\nFind the template you want to clone. Then select Ellipsis > Clone.\nComplete the rest of the form.\nResult: The template is cloned and configured. You can use the template later when you provision a node pool cluster.\nFrom your user settings, select User Avatar > Node Templates.\n\nChoose the node template that you want to edit and click the Vertical Ellipsis (…) > Edit.\n\n\nNote: As of v2.2.0, the default active node drivers and any node driver, that has fields marked as password, are required to use cloud credentials. If you have upgraded to v2.2.0, existing node templates will continue to work with the previous account access  information, but when you edit the node template, you will be required to create a cloud credential and the node template will start using it.\n\n\nEdit the required information and click Save.\nResult: The node template is updated. All node pools using this node template will automatically use the updated information when new nodes are added.\nFrom your user settings, select User Avatar > Node Templates.\nClick Add Template.\nSelect one of the cloud providers available. Then follow the instructions on screen to configure the template.\nResult: The template is configured. You can use the template later when you provision a node pool cluster.","postref":"e3b1f2f21622038fe550ae06791c18dc","objectID":"f975522e44924421b0205f7b25eae758","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/user-settings/node-templates/"},{"anchor":"#","title":"RKE Templates","content":"Available as of Rancher v2.3.0\n\nRKE templates are designed to allow DevOps and security teams to standardize and simplify the creation of Kubernetes clusters.\n\nRKE is the Rancher Kubernetes Engine, which is the tool that Rancher uses to provision Kubernetes clusters.\n\nWith Kubernetes increasing in popularity, there is a trend toward managing a larger number of smaller clusters. When you want to create many clusters, it’s more important to manage them consistently. Multi-cluster management comes with challenges to enforcing security and add-on configurations that need to be standardized before turning clusters over to end users.\n\nRKE templates help standardize these configurations. Regardless of whether clusters are created with the Rancher UI, the Rancher API, or an automated process, Rancher will guarantee that every cluster it provisions from an RKE template is uniform and consistent in the way it is produced.\n\nAdmins control which cluster options can be changed by end users. RKE templates can also be shared with specific users and groups, so that admins can create different RKE templates for different sets of users.\n\nIf a cluster was created with an RKE template, you can’t change it to a different RKE template. You can only update the cluster to a new revision of the same template.\n\nAs of Rancher v2.3.3, you can save the configuration of an existing cluster as an RKE template. Then the cluster’s settings can only be changed if the template is updated. The new template can also be used to launch new clusters.\n\nThe core features of RKE templates allow DevOps and security teams to:\n\n\nStandardize cluster configuration and ensure that Rancher-provisioned clusters are created following best practices\nPrevent less technical users from making uninformed choices when provisioning clusters\nShare different templates with different sets of users and groups\nDelegate ownership of templates to users who are trusted to make changes to them\nControl which users can create templates\nRequire users to create clusters from a template\n\n\nConfigurable Settings\n\nRKE templates can be created in the Rancher UI or defined in YAML format. They can define all the same parameters that can be specified when you use Rancher to provision custom nodes or nodes from an infrastructure provider:\n\n\nCloud provider options\nPod security options\nNetwork providers\nIngress controllers\nNetwork security configuration\nNetwork plugins\nPrivate registry URL and credentials\nAdd-ons\nKubernetes options, including configurations for Kubernetes components such as kube-api, kube-controller, kubelet, and services\n\n\nThe add-on section of an RKE template is especially powerful because it allows a wide range of customization options.\n\nScope of RKE Templates\n\nRKE templates are supported for Rancher-provisioned clusters. The templates can be used to provision custom clusters or clusters that are launched by an infrastructure provider.\n\nRKE templates are for defining Kubernetes and Rancher settings. Node templates are responsible for configuring nodes. For tips on how to use RKE templates in conjunction with hardware, refer to RKE Templates and Hardware.\n\nRKE templates can be created from scratch to pre-define cluster configuration. They can be applied to launch new clusters, or templates can also be exported from existing running clusters.\n\nAs of v2.3.3, the settings of an existing cluster can be saved as an RKE template. This creates a new template and binds the cluster settings to the template, so that the cluster can only be upgraded if the template is updated, and the cluster is upgraded to use a newer version of the template. The new template can also be used to create new clusters.\n\nExample Scenarios\n\nWhen an organization has both basic and advanced Rancher users, administrators might want to give the advanced users more options for cluster creation, while restricting the options for basic users.\n\nThese example scenarios describe how an organization could use templates to standardize cluster creation.\n\nSome of the example scenarios include the following:\n\n\nEnforcing templates: Administrators might want to enforce one or more template settings for everyone if they want all new Rancher-provisioned clusters to have those settings.\nSharing different templates with different users: Administrators might give different templates to basic and advanced users, so that basic users can have more restricted options and advanced users can use more discretion when creating clusters.\nUpdating template settings: If an organization’s security and DevOps teams decide to embed best practices into the required settings for new clusters, those best practices could change over time. If the best practices change, a template can be updated to a new revision and clusters created from the template can upgrade to the new version of the template.\nSharing ownership of a template: When a template owner no longer wants to maintain a template, or wants to share ownership of the template, this scenario describes how template ownership can be shared.\n\n\nTemplate Management\n\nWhen you create an RKE template, it is available in the Rancher UI from the Global view under Tools > RKE Templates. When you create a template, you become the template owner, which gives you permission to revise and share the template. You can share the RKE templates with specific users or groups, and you can also make it public.\n\nAdministrators can turn on template enforcement to require users to always use RKE templates when creating a cluster. This allows administrators to guarantee that Rancher always provisions clusters with specific settings.\n\nRKE template updates are handled through a revision system. If you want to change or update a template, you create a new revision of the template. Then a cluster that was created with the older version of the template can be upgraded to the new template revision.\n\nIn an RKE template, settings can be restricted to what the template owner chooses, or they can be open for the end user to select the value. The difference is indicated by the Allow User Override toggle over each setting in the Rancher UI when the template is created.\n\nFor the settings that cannot be overridden, the end user will not be able to directly edit them. In order for a user to get different options of these settings, an RKE template owner would need to create a new revision of the RKE template, which would allow the user to upgrade and change that option.\n\nThe documents in this section explain the details of RKE template management:\n\n\nGetting permission to create templates\nCreating and revising templates\nEnforcing template settings\nOverriding template settings\nSharing templates with cluster creators\nSharing ownership of a template\n\n\nAn example YAML configuration file for a template is provided for reference.\n\nApplying Templates\n\nYou can create a cluster from a template that you created, or from a template that has been shared with you.\n\nIf the RKE template owner creates a new revision of the template, you can upgrade your cluster to that revision.\n\nRKE templates can be created from scratch to pre-define cluster configuration. They can be applied to launch new clusters, or templates can also be exported from existing running clusters.\n\nAs of Rancher v2.3.3, you can save the configuration of an existing cluster as an RKE template. Then the cluster’s settings can only be changed if the template is updated.\n\nStandardizing Hardware\n\nRKE templates are designed to standardize Kubernetes and Rancher settings. If you want to standardize your infrastructure as well, you use RKE templates in conjunction with other tools.\n\nYAML Customization\n\nIf you define an RKE template as a YAML file, you can modify this example RKE template YAML. The YAML in the RKE template uses the same customization that Rancher uses when creating an RKE cluster, but since the YAML is located within the context of a Rancher provisioned cluster, you will need to nest the RKE template customization under the rancher_kubernetes_engine_config directive in the YAML.\n\nThe RKE documentation also has annotated cluster.yml files that you can use for reference.\n\nFor guidance on available options, refer to the RKE documentation on cluster configuration.\n\nAdd-ons\n\nThe add-on section of the RKE template configuration file works the same way as the add-on section of a cluster configuration file.\n\nThe user-defined add-ons directive allows you to either call out and pull down Kubernetes manifests or put them inline directly. If you include these manifests as part of your RKE template, Rancher will provision those in the cluster.\n\nSome things you could do with add-ons include:\n\n\nInstall applications on the Kubernetes cluster after it starts\nInstall plugins on nodes that are deployed with a Kubernetes daemonset\nAutomatically set up namespaces, service accounts, or role binding\n\n\nThe RKE template configuration must be nested within the rancher_kubernetes_engine_config directive. To set add","postref":"75c5cf4987b52542d5883bd1501e6cf0","objectID":"74f0c27b80722c872f1781c5310138bd","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/rke-templates/"},{"anchor":"#creating-a-cloud-credential-from-user-settings","title":"Creating a Cloud Credential from User Settings","content":"In order to delete cloud credentials, there must not be any node template associated with it. If you are unable to delete the cloud credential, delete any node templates that are still associated to that cloud credential.\nFrom your user settings, select User Avatar > Cloud Credentials.\n\nYou can either individually delete a cloud credential or bulk delete.\n\n\nTo individually delete one, choose the cloud credential you want to edit and click the Vertical Ellipsis (…) > Delete.\nTo bulk delete cloud credentials, select one or more cloud credentials from the list. Click Delete.\n\n\nConfirm that you want to delete these cloud credentials.\nWhen access credentials are changed or compromised, updating a cloud credential allows you to rotate those credentials while keeping the same node template.\nFrom your user settings, select User Avatar > Cloud Credentials.\nChoose the cloud credential you want to edit and click the Vertical Ellipsis (…) > Edit.\nUpdate the credential information and click Save.\nResult: The cloud credential is updated with the new access credentials. All existing node templates using this cloud credential will automatically use the updated information whenever new nodes are added.\nFrom your user settings, select User Avatar > Cloud Credentials.\nClick Add Cloud Credential.\nEnter a name for the cloud credential.\nSelect a Cloud Credential Type from the drop down. The values of this dropdown is based on the active node drivers in Rancher.\nBased on the selected cloud credential type, enter the required values to authenticate with the infrastructure provider.\nClick Create.\nResult: The cloud credential is created and can immediately be used to create node templates.","postref":"2728a2c9fa7ece7d6338ba70bd89b0dc","objectID":"8786264cb5ee70ebb50b7e2fc6bd36c7","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/user-settings/cloud-credentials/"},{"anchor":"#theme","title":"Theme","content":"On pages that display system objects like clusters or deployments in a table, you can set the number of objects that display on the page before you must paginate. The default setting is 50.This section displays the Name (your display name) and Username (your login) used for your session. To change your login’s current password, click the Change Password button.Choose your background color for the Rancher UI. If you choose Auto, the background color changes from light to dark at 6 PM, and then changes back at 6 AM.","postref":"dbb07fc5f136873d94d0fb772c51fca5","objectID":"1e2ee5d4d4fd4766c339518c268b66bd","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/user-settings/preferences/"},{"anchor":"#how-to-use-the-api","title":"How to use the API","content":"API responses are paginated with a limit of 100 resources per page by default.  This can be changed with the limit query parameter, up to a maximum of 1000, e.g. /v3/pods?limit=1000.  The pagination map in collection responses tells you whether or not you have the full result set and has a link to the next page if you do not.Most collections can be sorted on the server-side by common fields using HTTP query parameters.  The sortLinks map shows you what sorts are available, along with the URL to get the collection sorted by that.  It also includes info about what the current response was sorted by, if specified.Most collections can be filtered on the server-side by common fields using HTTP query parameters.  The filters map shows you what fields can be filtered on and what the filtered values were for the request you made.  The API UI has controls to setup filtering and show you the appropriate request.  For simple “equals” matches it’s just field=value.  Modifiers can be added to the field name, e.g. field_gt=42 for “field is greater than 42”.  See the API spec for full details.The API is generally RESTful but has several features to make the definition of everything discoverable by a client so that generic clients can be written instead of having to write specific code for every type of resource.  For detailed info about the generic API spec, see here.\nEvery type has a Schema which describes:\n\n\nThe URL to get to the collection of this type of resources\nEvery field the resource can have, along with their type, basic validation rules, whether they are required or optional, etc.\nEvery action that is possible on this type of resource, with their inputs and outputs (also as schemas).\nEvery field that filtering is allowed on\nWhat HTTP verb methods are available for the collection itself, or for individual resources in the collection.\n\n\nSo the theory is that you can load just the list of schemas and know everything about the API.  This is in fact how the UI for the API works, it contains no code specific to Rancher itself.  The URL to get Schemas is sent in every HTTP response as a X-Api-Schemas header.  From there you can follow the collection link on each schema to know where to list resources, and other links inside of the returned resources to get any other information.\n\nIn practice, you will probably just want to construct URL strings.  We highly suggest limiting this to the top-level to list a collection (/v3/<type>) or get a specific resource (/v3/<type>/<id>).  Anything deeper than that is subject to change in future releases.\n\nResources have relationships between each other called links.  Each resource includes a map of links with the name of the link and the URL to retrieve that information.  Again you should GET the resource and then follow the URL in the links map, not construct these strings yourself.\n\nMost resources have actions, which do something or change the state of the resource.  To use these, send a HTTP POST to the URL in the actions map for the action you want.  Some actions require input or produce output, see the individual documentation for each type or the schemas for specific information.\n\nTo edit a resource, send a HTTP PUT to the links.update link on the resource with the fields that you want to change.  If the link is missing then you don’t have permission to update the resource. Unknown fields and ones that are not editable are ignored.\n\nTo delete a resource, send a HTTP DELETE to the links.remove link on the resource.  If the link is missing then you don’t have permission to update the resource.\n\nTo create a new resource, HTTP POST to the collection URL in the schema (which is /v3/<type>).\nAPI requests must include authentication information.  Authentication is done with HTTP basic authentication using API Keys. API keys can create new clusters and have access to multiple clusters via /v3/clusters/. Cluster and project roles apply to these keys and restrict what clusters and projects the account can see and what actions they can take.By default, some cluster-level API tokens are generated with infinite time-to-live (ttl=0). In other words, API tokens with ttl=0 never expire unless you invalidate them. For details on how to invalidate them, refer to the API tokens page.The API has its own user interface accessible from a web browser.  This is an easy way to see resources, perform actions, and see the equivalent cURL or HTTP request & response.  To access it, click on your user avatar in the upper right corner. Under API & Keys, you can find the URL endpoint as well as create API keys.","postref":"e9a97f51121835d55a12f5f82b50512e","objectID":"3b83bf4859539ce0294d35e1057cf13d","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/api/"},{"anchor":"#","title":"Security","content":"\n\n\nSecurity policy\nRancher Labs supports responsible disclosure, and endeavours to resolve all issues in a reasonable time frame. \n\n\nReporting process\nPlease submit possible security issues by emailing security@rancher.com\n\n\nAnnouncements\nSubscribe to the Rancher announcements forum for release updates.\n\n\n\n\nSecurity is at the heart of all Rancher features. From integrating with all the popular authentication tools and services, to an enterprise grade RBAC capability, Rancher makes your Kubernetes clusters even more secure.\n\nOn this page, we provide security-related documentation along with resources to help you secure your Rancher installation and your downstream Kubernetes clusters:\n\n\nRunning a CIS security scan on a Kubernetes cluster\nGuide to hardening Rancher installations\nThe CIS Benchmark and self-assessment\nThird-party penetration test reports\nRancher CVEs and resolutions\nSecurity Tips and Best Practices\n\n\nRunning a CIS Security Scan on a Kubernetes Cluster\n\nAvailable as of v2.4.0-alpha1\n\nRancher leverages kube-bench to run a security scan to check whether Kubernetes is deployed according to security best practices as defined in the CIS (Center for Internet Security) Kubernetes Benchmark.\n\nThe CIS Kubernetes Benchmark is a reference document that can be used to establish a secure configuration baseline for Kubernetes.\n\nThe Center for Internet Security (CIS) is a 501©(3) nonprofit organization, formed in October 2000, with a mission is to “identify, develop, validate, promote, and sustain best practice solutions for cyber defense and build and lead communities to enable an environment of trust in cyberspace.”\n\nCIS Benchmarks are best practices for the secure configuration of a target system. CIS Benchmarks are developed through the generous volunteer efforts of subject matter experts, technology vendors, public and private community members, and the CIS Benchmark Development team.\n\nThe Benchmark provides recommendations of two types: Scored and Not Scored. We run tests related to only Scored recommendations.\n\nWhen Rancher runs a CIS security scan on a cluster, it generates a report showing the results of each test, including a summary with the number of passed, skipped and failed tests. The report also includes remediation steps for any failed tests.\n\nFor details, refer to the section on security scans.\n\nRancher Hardening Guide\n\nThe Rancher Hardening Guide is based off of controls and best practices found in the CIS Kubernetes Benchmark from the Center for Internet Security.\n\nThe hardening guide provides prescriptive guidance for hardening a production installation of Rancher v2.1.x, v2.2.x and v.2.3.x. See Rancher’s Self Assessment of the CIS Kubernetes Benchmark for the full list of security controls.\n\n\nThe hardening guides describe how to secure the nodes in your cluster, and it is recommended to follow a hardening guide before installing Kubernetes.\n\n\nEach version of the hardening guide is intended to be used with specific versions of the CIS Kubernetes Benchmark, Kubernetes, and Rancher:\n\n\n\n\nHardening Guide Version\nRancher Version\nCIS Benchmark Version\nKubernetes Version\n\n\n\n\n\nHardening Guide v2.3.5\nRancher v2.3.5\nBenchmark v1.5\nKubernetes v1.15\n\n\n\nHardening Guide v2.3.3\nRancher v2.3.3\nBenchmark v1.4.1\nKubernetes v1.14, v1.15, and v1.16\n\n\n\nHardening Guide v2.3\nRancher v2.3.0-v2.3.2\nBenchmark v1.4.1\nKubernetes v1.15\n\n\n\nHardening Guide v2.2\nRancher v2.2.x\nBenchmark v1.4.1 and 1.4.0\nKubernetes v1.13\n\n\n\nHardening Guide v2.1\nRancher v2.1.x\nBenchmark v1.3.0\nKubernetes v1.11\n\n\n\n\nThe CIS Benchmark and Self-Assessment\n\nThe benchmark self-assessment is a companion to the Rancher security hardening guide. While the hardening guide shows you how to harden the cluster, the benchmark guide is meant to help you evaluate the level of security of the hardened cluster.\n\nBecause Rancher and RKE install Kubernetes services as Docker containers, many of the control verification checks in the CIS Kubernetes Benchmark don’t apply. This guide will walk through the various controls and provide updated example commands to audit compliance in Rancher created clusters.  The original benchmark documents can be downloaded from the CIS website.\n\nEach version of Rancher’s self assessment guide corresponds to specific versions of the hardening guide, Rancher, Kubernetes, and the CIS Benchmark:\n\n\n\n\nSelf Assessment Guide Version\nRancher Version\nHardening Guide Version\nKubernetes Version\nCIS Benchmark Version\n\n\n\n\n\nSelf Assessment Guide v2.3.5\nRancher v2.3.5\nHardening Guide v2.3.3\nKubernetes v1.15\nBenchmark v1.5.0\n\n\n\nSelf Assessment Guide v2.3.3\nRancher v2.3.3\nHardening Guide v2.3.3\nKubernetes v1.16\nBenchmark v1.4.1\n\n\n\nSelf Assessment Guide v2.3\nRancher v2.3.0-2.3.2\nHardening Guide v2.3\nKubernetes v1.15\nBenchmark v1.4.1\n\n\n\nSelf Assessment Guide v2.2\nRancher v2.2.x\nHardening Guide v2.2\nKubernetes v1.13\nBenchmark v1.4.0 and v1.4.1\n\n\n\nSelf Assessment Guide v2.1\nRancher v2.1.x\nHardening Guide v2.1\nKubernetes v1.11\nBenchmark 1.3.0\n\n\n\n\nThird-party Penetration Test Reports\n\nRancher periodically hires third parties to perform security audits and penetration tests of the Rancher 2.x software stack. The environments under test follow the Rancher provided hardening guides at the time of the testing. Results are posted when the third party has also verified fixes classified MEDIUM or above.\n\nResults:\n\n\nCure53 Pen Test - 7⁄2019\nUntamed Theory Pen Test- 3⁄2019\n\n\nRancher CVEs and Resolutions\n\nRancher is committed to informing the community of security issues in our products. Rancher will publish CVEs (Common Vulnerabilities and Exposures) for issues we have resolved.\n\n\n\n\nID\nDescription\nDate\nResolution\n\n\n\n\n\nCVE-2018-20321\nAny project member with access to the default namespace can mount the netes-default service account in a pod and then use that pod to execute administrative privileged commands against the Kubernetes cluster.\n29 Jan 2019\nRancher v2.1.6 and Rancher v2.0.11 - Rolling back from these versions or greater have specific instructions.\n\n\n\nCVE-2019-6287\nProject members continue to get access to namespaces from projects that they were removed from if they were added to more than one project.\n29 Jan 2019\nRancher v2.1.6 and Rancher v2.0.11\n\n\n\nCVE-2019-11202\nThe default admin, that is shipped with Rancher, will be re-created upon restart of Rancher despite being explicitly deleted.\n16 Apr 2019\nRancher v2.2.2, Rancher v2.1.9 and Rancher v2.0.14\n\n\n\nCVE-2019-12274\nNodes using the built-in node drivers using a file path option allows the machine to read arbitrary files including sensitive ones from inside the Rancher server container.\n5 Jun 2019\nRancher v2.2.4, Rancher v2.1.10 and Rancher v2.0.15\n\n\n\nCVE-2019-12303\nProject owners can inject extra fluentd logging configurations that makes it possible to read files or execute arbitrary commands inside the fluentd container. Reported by Tyler Welton from Untamed Theory.\n5 Jun 2019\nRancher v2.2.4, Rancher v2.1.10 and Rancher v2.0.15\n\n\n\nCVE-2019-13209\nThe vulnerability is known as a Cross-Site Websocket Hijacking attack. This attack allows an exploiter to gain access to clusters managed by Rancher with the roles/permissions of a victim. It requires that a victim to be logged into a Rancher server and then access a third-party site hosted by the exploiter. Once that is accomplished, the exploiter is able to execute commands against the Kubernetes API with the permissions and identity of the victim. Reported by Matt Belisle and Alex Stevenson from Workiva.\n15 Jul 2019\nRancher v2.2.5, Rancher v2.1.11 and Rancher v2.0.16\n\n\n\nCVE-2019-14436\nThe vulnerability allows a member of a project that has access to edit role bindings to be able to assign themselves or others a cluster level role granting them administrator access to that cluster. The issue was found and reported by Michal Lipinski at Nokia.\n5 Aug 2019\nRancher v2.2.7 and Rancher v2.1.12\n\n\n\nCVE-2019-14435\nThis vulnerability allows authenticated users to potentially extract otherwise private data out of IPs reachable from system service containers used by Rancher. This can include but not only limited to services such as cloud provider metadata services. Although Rancher allow users to configure whitelisted domains for system service access, this flaw can still be exploited by a carefully crafted HTTP request. The issue was found and reported by Matt Belisle and Alex Stevenson at Workiva.\n5 Aug 2019\nRancher v2.2.7 and Rancher v2.1.12\n\n\n\n\nSecurity Tips and Best Practices\n\nOur best practices guide includes basic tips for increasing security in Rancher.\n","postref":"5ef338ea395ff63a67a928aacc540b69","objectID":"4956fbe1530917c5c0b295e2760f1fee","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/security/"},{"anchor":"#","title":"Running on ARM64 (Experimental)","content":"Available as of v2.2.0\n\n\nImportant:\n\nRunning on an ARM64 platform is currently an experimental feature and is not yet officially supported in Rancher. Therefore, we do not recommend using ARM64 based nodes in a production environment.\n\n\nThe following options are available when using an ARM64 platform:\n\n\nRunning Rancher on ARM64 based node(s)\n\n\nOnly Docker Install\n\nCreate custom cluster and adding ARM64 based node(s)\n\n\nKubernetes cluster version must be 1.12 or higher\nCNI Network Provider must be Flannel\n\nImporting clusters that contain ARM64 based nodes\n\n\nKubernetes cluster version must be 1.12 or higher\n\n\n\nPlease see Cluster Options how to configure the cluster options.\n\nThe following features are not tested:\n\n\nMonitoring, alerts, notifiers, pipelines and logging\nLaunching apps from the catalog\n\n","postref":"3cbdbf780765eba2e5eeabcd384359a3","objectID":"482a6d5d20d04c5ad405b6c963af0f21","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/arm64-platform/"},{"anchor":"#","title":"Enabling Experimental Features","content":"Available as of v2.3.0\n\nRancher includes some features that are experimental and disabled by default. You might want to enable these features, for example, if you decide that the benefits of using an unsupported storage type outweighs the risk of using an untested feature. Feature flags were introduced to allow you to try these features that are not enabled by default.\n\nThe features can be enabled in three ways:\n\n\nEnable features when starting Rancher. When installing Rancher with a CLI, you can use a feature flag to enable a feature by default.\nEnable features from the Rancher UI in Rancher v2.3.3+ by going to the Settings page.\nEnable features with the Rancher API after installing Rancher.\n\n\nEach feature has two values:\n\n\nA default value, which can be configured with a flag or environment variable from the command line\nA set value, which can be configured with the Rancher API or UI\n\n\nIf no value has been set, Rancher uses the default value.\n\nBecause the API sets the actual value and the command line sets the default value, that means that if you enable or disable a feature with the API or UI, it will override any value set with the command line.\n\nFor example, if you install Rancher, then set a feature flag to true with the Rancher API, then upgrade Rancher with a command that sets the feature flag to false, the default value will still be false, but the feature will still be enabled because it was set with the Rancher API. If you then deleted the set value (true) with the Rancher API, setting it to NULL, the default value (false) would take effect.\n\nThe following is a list of the feature flags available in Rancher:\n\n\nunsupported-storage-drivers: This feature allows unsupported storage drivers. In other words, it enables types for storage providers and provisioners that are not enabled by default.\nistio-virtual-service-ui: This feature enables a UI to create, read, update, and delete Istio virtual services and destination rules, which are traffic management features of Istio.\n\n\nThe below table shows the availability and default value for feature flags in Rancher:\n\n\n\n\nFeature Flag Name\nDefault Value\nStatus\nAvailable as of\n\n\n\n\n\nunsupported-storage-drivers\nfalse\nExperimental\nv2.3.0\n\n\n\nistio-virtual-service-ui\nfalse\nExperimental\nv2.3.0\n\n\n\nistio-virtual-service-ui\ntrue\nGA\nv2.3.2\n\n\n\n\nEnabling Features when Starting Rancher\n\nWhen you install Rancher, enable the feature you want with a feature flag. The command is different depending on whether you are installing Rancher on a single node or if you are doing a Kubernetes Installation of Rancher.\n\n\nNote: Values set from the Rancher API will override the value passed in through the command line.\n\n\n\n  \n  \n  When installing Rancher with a Helm chart, use the --features option. In the below example, two features are enabled by passing the feature flag names names in a comma separated list:\n\nhelm install rancher-latest/rancher \\\n  --name rancher \\\n  --namespace cattle-system \\\n  --set hostname=rancher.my.org \\\n  --set 'extraEnv[0].name=CATTLE_FEATURES' # Available as of v2.3.0\n  --set 'extraEnv[0].value=<FEATURE-FLAG-NAME-1>=true,<FEATURE-FLAG-NAME-2>=true' # Available as of v2.3.0\n\n\nNote: If you are installing an alpha version, Helm requires adding the --devel option to the command.\n\nRendering the Helm Chart for Air Gap Installations\n\nFor an air gap installation of Rancher, you need to add a Helm chart repository and render a Helm template before installing Rancher with Helm. For details, refer to the air gap installation documentation.\n\nHere is an example of a command for passing in the feature flag names when rendering the Helm template. In the below example, two features are enabled by passing the feature flag names in a comma separated list.\n\nThe Helm 3 command is as follows:\n\nhelm template rancher ./rancher-<VERSION>.tgz --output-dir . \\\n  --namespace cattle-system \\\n  --set hostname=<RANCHER.YOURDOMAIN.COM> \\\n  --set rancherImage=<REGISTRY.YOURDOMAIN.COM:PORT>/rancher/rancher \\\n  --set ingress.tls.source=secret \\\n  --set systemDefaultRegistry=<REGISTRY.YOURDOMAIN.COM:PORT> \\ # Available as of v2.2.0, set a default private registry to be used in Rancher\n  --set useBundledSystemChart=true # Available as of v2.3.0, use the packaged Rancher system charts\n  --set 'extraEnv[0].name=CATTLE_FEATURES' # Available as of v2.3.0\n  --set 'extraEnv[0].value=<FEATURE-FLAG-NAME-1>=true,<FEATURE-FLAG-NAME-2>=true' # Available as of v2.3.0\n\n\nThe Helm 2 command is as follows:\n\nhelm template ./rancher-<VERSION>.tgz --output-dir . \\\n  --name rancher \\\n  --namespace cattle-system \\\n  --set hostname=<RANCHER.YOURDOMAIN.COM> \\\n  --set rancherImage=<REGISTRY.YOURDOMAIN.COM:PORT>/rancher/rancher \\\n  --set ingress.tls.source=secret \\\n  --set systemDefaultRegistry=<REGISTRY.YOURDOMAIN.COM:PORT> \\ # Available as of v2.2.0, set a default private registry to be used in Rancher\n  --set useBundledSystemChart=true # Available as of v2.3.0, use the packaged Rancher system charts\n  --set 'extraEnv[0].name=CATTLE_FEATURES' # Available as of v2.3.0\n  --set 'extraEnv[0].value=<FEATURE-FLAG-NAME-1>=true,<FEATURE-FLAG-NAME-2>=true' # Available as of v2.3.0\n\n\n\n\n\n  When installing Rancher with Docker, use the --features option. In the below example, two features are enabled by passing the feature flag names in a comma separated list:\n\ndocker run -d -p 80:80 -p 443:443 \\\n  --restart=unless-stopped \\\n  rancher/rancher:rancher-latest \\\n  --features=<FEATURE-FLAG-NAME-1>=true,<FEATURE-NAME-2>=true # Available as of v2.3.0\n\n\n\n\n\n\n\nEnabling Features with the Rancher UI\n\nAvailable as of Rancher v2.3.3\n\n\nGo to the Global view and click Settings.\nClick the Feature Flags tab. You will see a list of experimental features.\nTo enable a feature, go to the disabled feature you want to enable and click Ellipsis (…) > Activate.\n\n\nResult: The feature is enabled.\n\nDisabling Features with the Rancher UI\n\n\nGo to the Global view and click Settings.\nClick the Feature Flags tab. You will see a list of experimental features.\nTo disable a feature, go to the enabled feature you want to disable and click Ellipsis (…) > Deactivate.\n\n\nResult: The feature is disabled.\n\nEnabling Features with the Rancher API\n\n\nGo to <RANCHER-SERVER-URL>/v3/features.\nIn the data section, you will see an array containing all of the features that can be turned on with feature flags. The name of the feature is in the id field. Click the name of the feature you want to enable.\nIn the upper left corner of the screen, under Operations, click Edit.\nIn the Value drop-down menu, click True.\nClick Show Request.\nClick Send Request.\nClick Close.\n\n\nResult: The feature is enabled.\n\nDisabling Features with the Rancher API\n\n\nGo to <RANCHER-SERVER-URL>/v3/features.\nIn the data section, you will see an array containing all of the features that can be turned on with feature flags. The name of the feature is in the id field. Click the name of the feature you want to enable.\nIn the upper left corner of the screen, under Operations, click Edit.\nIn the Value drop-down menu, click False.\nClick Show Request.\nClick Send Request.\nClick Close.\n\n\nResult: The feature is disabled.\n","postref":"e75ee4f3ca6570362f698d2f3380b0fe","objectID":"a6b3d028477d91abf7c2c3dd47ffffe5","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/feature-flags/"},{"anchor":"#","title":"FAQ","content":"This FAQ is a work in progress designed to answers the questions our users most frequently ask about Rancher v2.x.\n\nSee Technical FAQ, for frequently asked technical questions.\n\n\n\nDoes Rancher v2.x support Docker Swarm and Mesos as environment types?\n\nWhen creating an environment in Rancher v2.x, Swarm and Mesos will no longer be standard options you can select. However, both Swarm and Mesos will continue to be available as Catalog applications you can deploy. It was a tough decision to make but, in the end, it came down to adoption. For example, out of more than 15,000 clusters, only about 200 or so are running Swarm.\n\n\n\nIs it possible to manage Azure Kubernetes Services with Rancher v2.x?\n\nYes.\n\n\n\nDoes Rancher support Windows?\n\nAs of Rancher 2.3.0, we support Windows Server 1809 containers. For details on how to set up a cluster with Windows worker nodes, refer to the section on configuring custom clusters for Windows.\n\n\n\nDoes Rancher support Istio?\n\nAs of Rancher 2.3.0, we support Istio.\n\nFurthermore, Istio is implemented in our micro-PaaS “Rio”, which works on Rancher 2.x along with any CNCF compliant Kubernetes cluster. You can read more about it here\n\n\n\nWill Rancher v2.x support Hashicorp’s Vault for storing secrets?\n\nSecrets management is on our roadmap but we haven’t assigned it to a specific release yet.\n\n\n\nDoes Rancher v2.x support RKT containers as well?\n\nAt this time, we only support Docker.\n\n\n\nDoes Rancher v2.x support Calico, Contiv, Contrail, Flannel, Weave net, etc., for embedded and imported Kubernetes?\n\nOut-of-the-box, Rancher provides the following CNI network providers for Kubernetes clusters: Canal, Flannel, Calico and Weave (Weave is available as of v2.2.0).  Always refer to the Rancher Support Matrix for details about what is officially supported.\n\n\n\nAre you planning on supporting Traefik for existing setups?\n\nWe don’t currently plan on providing embedded Traefik support, but we’re still exploring load-balancing approaches.\n\n\n\nCan I import OpenShift Kubernetes clusters into v2.x?\n\nOur goal is to run any upstream Kubernetes clusters. Therefore, Rancher v2.x should work with OpenShift, but we haven’t tested it yet.\n\n\n\nAre you going to integrate Longhorn?\n\nYes. Longhorn was on a bit of a hiatus while we were working on v2.0. We plan to re-engage on the project.\n","postref":"2fba5d52c1b2174cbce764fbf97beb17","objectID":"2d0aab3a286759909cda15d9e28de5c5","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/faq/"},{"anchor":"#","title":"Networking","content":"Networking FAQ’s\n\n\nCNI Providers\n\n","postref":"e31568ca4240c15ec45196c3ab667da3","objectID":"f0eb4eaf1631b648fbd6dc2b71e4cbed","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/faq/networking/"},{"anchor":"#","title":"Technical","content":"How can I reset the administrator password?\n\nDocker Install:\n\n$ docker exec -ti <container_id> reset-password\nNew password for default administrator (user-xxxxx):\n<new_password>\n\n\nKubernetes install (Helm):\n\n$ KUBECONFIG=./kube_config_rancher-cluster.yml\n$ kubectl --kubeconfig $KUBECONFIG -n cattle-system exec $(kubectl --kubeconfig $KUBECONFIG -n cattle-system get pods -l app=rancher | grep '1/1' | head -1 | awk '{ print $1 }') -- reset-password\nNew password for default administrator (user-xxxxx):\n<new_password>\n\n\nKubernetes install (RKE add-on):\n\n$ KUBECONFIG=./kube_config_rancher-cluster.yml\n$ kubectl --kubeconfig $KUBECONFIG exec -n cattle-system $(kubectl --kubeconfig $KUBECONFIG get pods -n cattle-system -o json | jq -r '.items[] | select(.spec.containers[].name==\"cattle-server\") | .metadata.name') -- reset-password\nNew password for default administrator (user-xxxxx):\n<new_password>\n\n\nI deleted/deactivated the last admin, how can I fix it?\n\nDocker Install:\n\n$ docker exec -ti <container_id> ensure-default-admin\nNew default administrator (user-xxxxx)\nNew password for default administrator (user-xxxxx):\n<new_password>\n\n\nKubernetes install (Helm):\n\n$ KUBECONFIG=./kube_config_rancher-cluster.yml\n$ kubectl --kubeconfig $KUBECONFIG -n cattle-system exec $(kubectl --kubeconfig $KUBECONFIG -n cattle-system get pods -l app=rancher | grep '1/1' | head -1 | awk '{ print $1 }') -- ensure-default-admin\nNew password for default administrator (user-xxxxx):\n<new_password>\n\n\nKubernetes install (RKE add-on):\n\n$ KUBECONFIG=./kube_config_rancher-cluster.yml\n$ kubectl --kubeconfig $KUBECONFIG exec -n cattle-system $(kubectl --kubeconfig $KUBECONFIG get pods -n cattle-system -o json | jq -r '.items[] | select(.spec.containers[].name==\"cattle-server\") | .metadata.name') -- ensure-default-admin\nNew password for default admin user (user-xxxxx):\n<new_password>\n\n\nHow can I enable debug logging?\n\n\nDocker Install\n\n\nEnable\n\n$ docker exec -ti <container_id> loglevel --set debug\nOK\n$ docker logs -f <container_id>\n\n\nDisable\n\n$ docker exec -ti <container_id> loglevel --set info\nOK\n\n\n\nKubernetes install (Helm)\n\n\nEnable\n\n$ KUBECONFIG=./kube_config_rancher-cluster.yml\n$ kubectl --kubeconfig $KUBECONFIG -n cattle-system get pods -l app=rancher | grep '1/1' | awk '{ print $1 }' | xargs -I{} kubectl --kubeconfig $KUBECONFIG -n cattle-system exec {} -- loglevel --set debug\nOK\nOK\nOK\n$ kubectl --kubeconfig $KUBECONFIG -n cattle-system logs -l app=rancher\n\n\nDisable\n\n$ KUBECONFIG=./kube_config_rancher-cluster.yml\n$ kubectl --kubeconfig $KUBECONFIG -n cattle-system get pods -l app=rancher | grep '1/1' | awk '{ print $1 }' | xargs -I{} kubectl --kubeconfig $KUBECONFIG -n cattle-system exec {} -- loglevel --set info\nOK\nOK\nOK\n\n\n\nKubernetes install (RKE add-on)\n\n\nEnable\n\n$ KUBECONFIG=./kube_config_rancher-cluster.yml\n$ kubectl --kubeconfig $KUBECONFIG exec -n cattle-system $(kubectl --kubeconfig $KUBECONFIG get pods -n cattle-system -o json | jq -r '.items[] | select(.spec.containers[].name==\"cattle-server\") | .metadata.name') -- loglevel --set debug\nOK\n$ kubectl --kubeconfig $KUBECONFIG logs -n cattle-system -f $(kubectl --kubeconfig $KUBECONFIG get pods -n cattle-system -o json | jq -r '.items[] | select(.spec.containers[].name=\"cattle-server\") | .metadata.name')\n\n\nDisable\n\n$ KUBECONFIG=./kube_config_rancher-cluster.yml\n$ kubectl --kubeconfig $KUBECONFIG exec -n cattle-system $(kubectl --kubeconfig $KUBECONFIG get pods -n cattle-system -o json | jq -r '.items[] | select(.spec.containers[].name==\"cattle-server\") | .metadata.name') -- loglevel --set info\nOK\n\n\n\n\nMy ClusterIP does not respond to ping\n\nClusterIP is a virtual IP, which will not respond to ping. Best way to test if the ClusterIP is configured correctly, is by using curl to access the IP and port to see if it responds.\n\nWhere can I manage Node Templates?\n\nNode Templates can be accessed by opening your account menu (top right) and selecting Node Templates.\n\nWhy is my Layer-4 Load Balancer in Pending state?\n\nThe Layer-4 Load Balancer is created as type: LoadBalancer. In Kubernetes, this needs a cloud provider or controller that can satisfy these requests, otherwise these will be in Pending state forever. More information can be found on Cloud Providers or Create External Load Balancer\n\nWhere is the state of Rancher stored?\n\n\nDocker Install: in the embedded etcd of the rancher/rancher container, located at /var/lib/rancher.\nKubernetes install: in the etcd of the RKE cluster created to run Rancher.\n\n\nHow are the supported Docker versions determined?\n\nWe follow the validated Docker versions for upstream Kubernetes releases. The validated versions can be found under External Dependencies in the Kubernetes release CHANGELOG.md.\n\nHow can I access nodes created by Rancher?\n\nSSH keys to access the nodes created by Rancher can be downloaded via the Nodes view. Choose the node which you want to access and click on the vertical ellipsis button at the end of the row, and choose Download Keys as shown in the picture below.\n\n\n\nUnzip the downloaded zip file, and use the file id_rsa to connect to you host. Be sure to use the correct username (rancher or docker for RancherOS, ubuntu for Ubuntu, ec2-user for Amazon Linux)\n\n$ ssh -i id_rsa user@ip_of_node\n\n\nHow can I automate task X in Rancher?\n\nThe UI consists of static files, and works based on responses of the API. That means every action/task that you can execute in the UI, can be automated via the API. There are 2 ways to do this:\n\n\nVisit https://your_rancher_ip/v3 and browse the API options.\nCapture the API calls when using the UI (Most commonly used for this is Chrome Developer Tools but you can use anything you like)\n\n\nThe IP address of a node changed, how can I recover?\n\nA node is required to have a static IP configured (or a reserved IP via DHCP). If the IP of a node has changed, you will have to remove it from the cluster and readd it. After it is removed, Rancher will update the cluster to the correct state. If the cluster is no longer in Provisioning state, the node is removed from the cluster.\n\nWhen the IP address of the node changed, Rancher lost connection to the node, so it will be unable to clean the node properly. See Cleaning cluster nodes to clean the node.\n\nWhen the node is removed from the cluster, and the node is cleaned, you can readd the node to the cluster.\n\nHow can I add additional arguments/binds/environment variables to Kubernetes components in a Rancher Launched Kubernetes cluster?\n\nYou can add additional arguments/binds/environment variables via the Config File option in Cluster Options. For more information, see the Extra Args, Extra Binds, and Extra Environment Variables in the RKE documentation or browse the Example Cluster.ymls.\n\nHow do I check if my certificate chain is valid?\n\nUse the openssl verify command to validate your certificate chain:\n\n\nNote: Configure SSL_CERT_DIR and SSL_CERT_FILE to a dummy location to make sure the OS installed certificates are not used when verifying manually.\n\n\nSSL_CERT_DIR=/dummy SSL_CERT_FILE=/dummy openssl verify -CAfile ca.pem rancher.yourdomain.com.pem\nrancher.yourdomain.com.pem: OK\n\n\nIf you receive the error unable to get local issuer certificate, the chain is incomplete. This usually means that there is an intermediate CA certificate that issued your server certificate. If you already have this certificate, you can use it in the verification of the certificate like shown below:\n\nSSL_CERT_DIR=/dummy SSL_CERT_FILE=/dummy openssl verify -CAfile ca.pem -untrusted intermediate.pem rancher.yourdomain.com.pem\nrancher.yourdomain.com.pem: OK\n\n\nIf you have successfully verified your certificate chain, you should include needed intermediate CA certificates in the server certificate to complete the certificate chain for any connection made to Rancher (for example, by the Rancher agent). The order of the certificates in the server certificate file should be first the server certificate itself (contents of rancher.yourdomain.com.pem), followed by intermediate CA certificate(s) (contents of intermediate.pem).\n\n-----BEGIN CERTIFICATE-----\n%YOUR_CERTIFICATE%\n-----END CERTIFICATE-----\n-----BEGIN CERTIFICATE-----\n%YOUR_INTERMEDIATE_CERTIFICATE%\n-----END CERTIFICATE-----\n\n\nIf you still get errors during verification, you can retrieve the subject and the issuer of the server certificate using the following command:\n\nopenssl x509 -noout -subject -issuer -in rancher.yourdomain.com.pem\nsubject= /C=GB/ST=England/O=Alice Ltd/CN=rancher.yourdomain.com\nissuer= /C=GB/ST=England/O=Alice Ltd/CN=Alice Intermediate CA\n\n\nHow do I check Common Name and Subject Alternative Names in my server certificate?\n\nAlthough technically an entry in Subject Alternative Names is required, having the hostname in both Common Name and as entry in Subject Alternative Names gives you maximum compatibility with older browser/applications.\n\nCheck Common Name:\n\nopenssl x509 -noout -subject -in cert.pem\nsubject= /CN=rancher.my.org\n\n\nCheck Subject Alternative Names:","postref":"44bbe8a7346dc54eaf6fce71e172dd3e","objectID":"c8a2b52bc3fc3b7f81bcc60ccf5332a6","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/faq/technical/"},{"anchor":"#","title":"Security","content":"Is there a Hardening Guide?\n\nThe Hardening Guide is now located in the main Security section.\n\n\n\nWhat are the results of Rancher’s Kubernetes cluster when it is CIS benchmarked?\n\nWe have run the CIS Kubernetes benchmark against a hardened Rancher Kubernetes cluster.  The results of that assessment can be found in the main Security section.\n","postref":"efe479318ff293478a1c6fb0e9c5a67b","objectID":"6958ca9053f366d8c4f16022cef655c9","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/faq/security/"},{"anchor":"#","title":"Telemetry","content":"What is Telemetry?\n\nTelemetry collects aggregate information about the size of Rancher installations, versions of components used, and which features are used.  This information is used by Rancher Labs to help make the product better and is not shared with third-parties.\n\nWhat information is collected?\n\nNo specific identifying information like usernames, passwords, or the names or addresses of user resources will ever be collected.\n\nThe primary things collected include:\n\n\nAggregate counts (smallest, average, largest, total) of nodes per-cluster and their size (e.g. CPU cores & RAM).\nAggregate counts of logical resources like Clusters, Projects, Namespaces, and Pods.\nCounts of what driver was used to deploy clusters and nodes (e.g. GKE vs EC2 vs Imported vs Custom).\nVersions of Kubernetes components, Operating Systems and Docker that are deployed on nodes.\nWhether some optional components are enabled or not (e.g. which auth providers are used).\nThe image name & version of Rancher that is running.\nA unique randomly-generated identifier for this installation.\n\n\nCan I see the information that is being sent?\n\nIf Telemetry is enabled, you can go to https://<your rancher server>/v1-telemetry in your installation to see the current data.\n\nIf Telemetry is not enabled, the process that collects the data is not running, so there is nothing being collected to look at.\n\nHow do I turn it on or off?\n\nAfter initial setup, an administrator can go to the Settings page in the Global section of the UI and click Edit to change the telemetry-opt setting to either in or out.\n","postref":"677d923ca2d682ecf50a3d0408c29bd8","objectID":"8fed0d7706c718590686047a71e797eb","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/faq/telemetry/"},{"anchor":"#","title":"Rancher is No Longer Needed","content":"This page is intended to answer questions about what happens if you don’t want Rancher anymore, if you don’t want a cluster to be managed by Rancher anymore, or if the Rancher server is deleted.\n\n\nIf the Rancher server is deleted, what happens to the workloads in my downstream clusters?\nIf the Rancher server is deleted, how do I access my downstream clusters?\nWhat if I don’t want Rancher anymore?\nWhat if I don’t want my imported cluster managed by Rancher?\nWhat if I don’t want my RKE cluster or hosted Kubernetes cluster managed by Rancher?\n\n\nIf the Rancher server is deleted, what happens to the workloads in my downstream clusters?\n\nIf Rancher is ever deleted or unrecoverable, all workloads in the downstream Kubernetes clusters managed by Rancher will continue to function as normal.\n\nIf the Rancher server is deleted, how do I access my downstream clusters?\n\nThe capability to access a downstream cluster without Rancher depends on the type of cluster and the way that the cluster was created. To summarize:\n\n\nImported clusters: The cluster will be unaffected and you can access the cluster using the same methods that you did before the cluster was imported into Rancher.\nHosted Kubernetes clusters: If you created the cluster in a cloud-hosted Kubernetes provider such as EKS, GKE, or AKS, you can continue to manage the cluster using your provider’s cloud credentials.\nRKE clusters: To access an RKE cluster, the cluster must have the authorized cluster endpoint enabled, and you must have already downloaded the cluster’s kubeconfig file from the Rancher UI. (The authorized cluster endpoint is enabled by default for RKE clusters.) With this endpoint, you can access your cluster with kubectl directly instead of communicating through the Rancher server’s authentication proxy. For instructions on how to configure kubectl to use the authorized cluster endpoint, refer to the section about directly accessing clusters with kubectl and the kubeconfig file. These clusters will use a snapshot of the authentication as it was configured when Rancher was removed.\n\n\nWhat if I don’t want Rancher anymore?\n\nIf you installed Rancher on a Kubernetes cluster, remove Rancher by using the System Tools with the remove subcommand.\n\nIf you installed Rancher with Docker, you can uninstall Rancher by removing the single Docker container that it runs in.\n\nImported clusters will not be affected by Rancher being removed. For other types of clusters, refer to the section on accessing downstream clusters when Rancher is removed.\n\nWhat if I don’t want my imported cluster managed by Rancher?\n\nIf an imported cluster is deleted from the Rancher UI, the cluster is detached from Rancher, leaving it intact and accessible by the same methods that were used to access it before it was imported into Rancher.\n\nTo detach the cluster,\n\n\nFrom the Global view in Rancher, go to the Clusters tab.\nGo to the imported cluster that should be detached from Rancher and click Ellipsis (…) > Delete.\nClick Delete.\n\n\nResult: The imported cluster is detached from Rancher and functions normally outside of Rancher.\n\nWhat if I don’t want my RKE cluster or hosted Kubernetes cluster managed by Rancher?\n\nAt this time, there is no functionality to detach these clusters from Rancher. In this context, “detach” is defined as the ability to remove Rancher components from the cluster and manage access to the cluster independently of Rancher.\n\nThe capability to manage these clusters without Rancher is being tracked in this issue.\n\nFor information about how to access clusters if the Rancher server is deleted, refer to this section.\n","postref":"633cc875a8eb9d067eaef7229009d2d3","objectID":"8cc72dd4c694eb63e2215d45a03b4cf4","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/faq/removing-rancher/"},{"anchor":"#","title":"Troubleshooting","content":"This section contains information to help you troubleshoot issues when using Rancher.\n\n\nKubernetes components\n\nIf you need help troubleshooting core Kubernetes cluster components like:\n\n\netcd\nkube-apiserver\nkube-controller-manager\nkube-scheduler\nkubelet\nkube-proxy\nnginx-proxy\n\n\nKubernetes resources\n\nOptions for troubleshooting Kubernetes resources like Nodes, Ingress Controller and Rancher Agents are described in this section.\n\nNetworking\n\nSteps to troubleshoot networking issues can be found here.\n\nDNS\n\nWhen you experience name resolution issues in your cluster.\n\nTroubleshooting Rancher installed on Kubernetes\n\nIf you experience issues with your Rancher server installed on Kubernetes\n\nImported clusters\n\nIf you experience issues when Importing Kubernetes Clusters\n\n","postref":"0fcb0a97182297d11b3d8d5e1220ffd1","objectID":"d85a1d0921a0df45236d29246f08040c","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/troubleshooting/"},{"anchor":"#","title":"Contributing to Rancher","content":"This section explains the repositories used for Rancher, how to build the repositories, and what information to include when you file an issue.\n\nFor more detailed information on how to contribute to the development of Rancher projects, refer to the Rancher Developer Wiki. The wiki has resources on many topics, including the following:\n\n\nHow to set up the Rancher development environment and run tests\nThe typical flow of an issue through the development lifecycle\nCoding guidelines and development best practices\nDebugging and troubleshooting\nDeveloping the Rancher API\n\n\nOn the Rancher Users Slack, the channel for developers is #developer.\n\nRepositories\n\nAll of repositories are located within our main GitHub organization. There are many repositories used for Rancher, but we’ll provide descriptions of some of the main ones used in Rancher.\n\n\n\n\nRepository\nURL\nDescription\n\n\n\n\n\nRancher\nhttps://github.com/rancher/rancher\nThis repository is the main source code for Rancher 2.x.\n\n\n\nTypes\nhttps://github.com/rancher/types\nThis repository is the repository that has all the API types for Rancher 2.x.\n\n\n\nAPI Framework\nhttps://github.com/rancher/norman\nThis repository is an API framework for building Rancher style APIs backed by Kubernetes Custom Resources.\n\n\n\nUser Interface\nhttps://github.com/rancher/ui\nThis repository is the source of the UI.\n\n\n\n(Rancher) Docker Machine\nhttps://github.com/rancher/machine\nThis repository is the source of the Docker Machine binary used when using Node Drivers. This is a fork of the docker/machine repository.\n\n\n\nmachine-package\nhttps://github.com/rancher/machine-package\nThis repository is used to build the Rancher Docker Machine binary.\n\n\n\nkontainer-engine\nhttps://github.com/rancher/kontainer-engine\nThis repository is the source of kontainer-engine, the tool to provision hosted Kubernetes clusters.\n\n\n\nRKE repository\nhttps://github.com/rancher/rke\nThis repository is the source of Rancher Kubernetes Engine, the tool to provision Kubernetes clusters on any machine.\n\n\n\nCLI\nhttps://github.com/rancher/cli\nThis repository is the source code for the Rancher CLI used in Rancher 2.x.\n\n\n\n(Rancher) Helm repository\nhttps://github.com/rancher/helm\nThis repository is the source of the packaged Helm binary. This is a fork of the helm/helm repository.\n\n\n\nTelemetry repository\nhttps://github.com/rancher/telemetry\nThis repository is the source for the Telemetry binary.\n\n\n\nloglevel repository\nhttps://github.com/rancher/loglevel\nThis repository is the source of the loglevel binary, used to dynamically change log levels.\n\n\n\n\nTo see all libraries/projects used in Rancher, see the go.mod file in the rancher/rancher repository.\n\n\nRancher components used for provisioning/managing Kubernetes clusters.\n\nBuilding\n\nEvery repository should have a Makefile and can be built using the make command. The make targets are based on the scripts in the /scripts directory in the repository, and each target will use Dapper to run the target in an isolated environment. The Dockerfile.dapper will be used for this process, and includes all the necessary build tooling needed.\n\nThe default target is ci, and will run ./scripts/validate, ./scripts/build, ./scripts/test and ./scripts/package. The resulting binaries of the build will be in ./build/bin and are usually also packaged in a Docker image.\n\nBugs, Issues or Questions\n\nIf you find any bugs or are having any trouble, please search the reported issue as someone may have experienced the same issue or we are actively working on a solution.\n\nIf you can’t find anything related to your issue, contact us by filing an issue. Though we have many repositories related to Rancher, we want the bugs filed in the Rancher repository so we won’t miss them! If you want to ask a question or ask fellow users about an use case, we suggest creating a post on the Rancher Forums.\n\nChecklist for Filing Issues\n\nPlease follow this checklist when filing an issue which will helps us investigate and fix the issue. More info means more data we can use to determine what is causing the issue or what might be related to the issue.\n\n\nNote: For large amounts of data, please use GitHub Gist or similar and link the created resource in the issue.\nImportant: Please remove any sensitive data as it will be publicly viewable.\n\n\n\nResources: Provide as much as detail as possible on the used resources. As the source of the issue can be many things, including as much of detail as possible helps to determine the root cause. See some examples below:\n\n\nHosts: What specifications does the host have, like CPU/memory/disk, what cloud does it happen on, what Amazon Machine Image are you using, what DigitalOcean droplet are you using, what image are you provisioning that we can rebuild or use when we try to reproduce\nOperating System: What operating system are you using? Providing specifics helps here like the output of cat /etc/os-release for exact OS release and uname -r for exact kernel used\nDocker: What Docker version are you using, how did you install it? Most of the details of Docker can be found by supplying output of docker version and docker info\nEnvironment: Are you in a proxy environment, are you using recognized CA/self signed certificates, are you using an external loadbalancer\nRancher: What version of Rancher are you using, this can be found on the bottom left of the UI or be retrieved from the image tag you are running on the host\nClusters: What kind of cluster did you create, how did you create it, what did you specify when you were creating it\n\nSteps to reproduce the issue: Provide as much detail on how you got into the reported situation. This helps the person to reproduce the situation you are in.\n\n\nProvide manual steps or automation scripts used to get from a newly created setup to the situation you reported.\n\n\nLogs: Provide data/logs from the used resources.\n\n\nRancher\n\n\nDocker install\n\n\ndocker logs \\\n--timestamps \\\n$(docker ps | grep -E \"rancher/rancher:|rancher/rancher \" | awk '{ print $1 }')\n\n\n\nKubernetes install using kubectl\n\n\n\nNote: Make sure you configured the correct kubeconfig (for example, export KUBECONFIG=$PWD/kube_config_rancher-cluster.yml if Rancher is installed on a Kubernetes cluster) or are using the embedded kubectl via the UI.\n\n\nkubectl -n cattle-system \\\nlogs \\\n-l app=rancher \\\n--timestamps=true\n\n\n\nDocker install using docker on each of the nodes in the RKE cluster\n\n\ndocker logs \\\n--timestamps \\\n$(docker ps | grep -E \"rancher/rancher@|rancher_rancher\" | awk '{ print $1 }')\n\n\n\nKubernetes Install with RKE Add-On\n\n\n\nNote: Make sure you configured the correct kubeconfig (for example, export KUBECONFIG=$PWD/kube_config_rancher-cluster.yml if the Rancher server is installed on a Kubernetes cluster) or are using the embedded kubectl via the UI.\n\n\nkubectl -n cattle-system \\\nlogs \\\n--timestamps=true \\\n-f $(kubectl --kubeconfig $KUBECONFIG get pods -n cattle-system -o json | jq -r '.items[] | select(.spec.containers[].name=\"cattle-server\") | .metadata.name')\n\n\nSystem logging (these might not all exist, depending on operating system)\n\n\n/var/log/messages\n/var/log/syslog\n/var/log/kern.log\n\n\nDocker daemon logging (these might not all exist, depending on operating system)\n\n\n/var/log/docker.log\n\n\n\nMetrics: If you are experiencing performance issues, please provide as much of data (files or screenshots) of metrics which can help determining what is going on. If you have an issue related to a machine, it helps to supply output of top, free -m, df which shows processes/memory/disk usage.\n\n\nDocs\n\nIf you have any updates to our documentation, please make any pull request to our docs repo.\n\n\nRancher 2.x Docs repository: This repo is where all the docs for Rancher 2.x are located. They are located in the content folder in the repo.\n\nRancher 1.x Docs repository: This repo is where all the docs for Rancher 1.x are located. They are located in the rancher folder in the repo.\n\n","postref":"1fda90268d6b647a6db191c0cee74e0f","objectID":"ecc9a50df81b89801b5bf8568a1cf1e9","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/contributing/"},{"anchor":"#enabling-ci-pipelines","title":"Enabling CI Pipelines","content":"For your convenience the following environment variables are available in your build steps:\n\n\nVariable Name\nDescription\n\n\n\n\n\nCICD_GIT_REPO_NAME\nRepository Name (Stripped of Github Organization)\n\n\n\nCICD_PIPELINE_NAME\nName of the pipeline\n\n\n\nCICD_GIT_BRANCH\nGit branch of this event\n\n\n\nCICD_TRIGGER_TYPE\nEvent that triggered the build\n\n\n\nCICD_PIPELINE_ID\nRancher ID for the pipeline\n\n\n\nCICD_GIT_URL\nURL of the Git repository\n\n\n\nCICD_EXECUTION_SEQUENCE\nBuild number of the pipeline\n\n\n\nCICD_EXECUTION_ID\nCombination of {CICD_PIPELINE_ID}-{CICD_EXECUTION_SEQUENCE}\n\n\n\nCICD_GIT_COMMIT\nGit commit ID being executed.\n\n\n\nGo to create / edit mode of the pipeline.\n\nClick “Add Step” button in the stage that you would like to add a step in.\n\nFill out the form as detailed above\n\nTo add a new stage the user must click the ‘add a new stage’ link in either create or edit mode of the pipeline view.\n\nProvide a name for the stage.\n\nClick save.\n\nGo to the project you want this pipeline to run in.\n\nClick Resources > Pipelines. In versions prior to v2.3.0,click Workloads > Pipelines.\n\nClick Add pipeline button.\n\nEnter in your repository name (Autocomplete should help zero in on it quickly).\n\nSelect Branch options.\n\n\nOnly the branch {BRANCH NAME}: Only events triggered by changes to this branch will be built.\n\nEverything but {BRANCH NAME}: Build any branch that triggered an event EXCEPT events from this branch.\n\nAll branches: Regardless of the branch that triggered the event always build.\n\n\n\nNote: If you want one path for master, but another for PRs or development/test/feature branches, create two separate pipelines.\n\n\nSelect the build trigger events. By default, builds will only happen by manually clicking build now in Rancher UI.\n\n\nAutomatically build this pipeline whenever there is a git commit. (This respects the branch selection above)\n\nAutomatically build this pipeline whenever there is a new PR.\n\nAutomatically build the pipeline. (Allows you to configure scheduled builds similar to Cron)\n\n\nClick Add button.\n\nBy default, Rancher provides a three stage pipeline for you. It consists of a build stage where you would compile, unit test, and scan code. The publish stage has a single step to publish a docker image.\n\nAdd a name to the pipeline in order to complete adding a pipeline.\n\nClick on the ‘run a script’ box under the ‘Build’ stage.\n\nHere you can set the image, or select from pre-packaged envs.\n\nConfigure a shell script to run inside the container when building.\n\nClick Save to persist the changes.\n\nClick the “publish an image’ box under the “Publish” stage.\n\nSet the location of the Dockerfile. By default it looks in the root of the workspace. Instead, set the build context for building the image relative to the root of the workspace.\n\nSet the image information.\n\nThe registry is the remote registry URL. It is defaulted to Docker hub.\nRepository is the <org>/<repo> in the repository.\n\nSelect the Tag. You can hard code a tag like ‘latest’ or select from a list of available variables.\n\nIf this is the first time using this registry, you can add the username/password for pushing the image. You must click save for the registry credentials AND also save for the modal.\n\nSelect cluster from drop down.\n\nUnder tools menu select pipelines.\n\nFollow instructions for setting up github auth on page.\n","postref":"1181d99604204df23d299ccfe25d26f1","objectID":"eca8d2330e509c2004ac40908506df03","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/project-admin/pipelines/docs-for-v2.0.x/"},{"anchor":"#os-and-container-requirements","title":"OS and Container Requirements","content":"In Windows clusters, containers communicate with each other using the host-gw mode of Flannel. In host-gw mode, all containers on the same node belong to a private subnet, and traffic routes from a subnet on one node to a subnet on another node through the host network.\nWhen worker nodes are provisioned on AWS, virtualization clusters, or bare metal servers, make sure they belong to the same layer 2 subnet. If the nodes don’t belong to the same layer 2 subnet, host-gw networking will not work.\n\nWhen worker nodes are provisioned on GCE or Azure, they are not on the same layer 2 subnet. Nodes on GCE and Azure belong to a routable layer 3 network. Follow the instructions below to configure GCE and Azure so that the cloud network knows how to route the host subnets on each node.\nTo configure host subnet routing on GCE or Azure, first run the following command to find out the host subnets on each worker node:kubectl get nodes -o custom-columns=nodeName:.metadata.name,nodeIP:status.addresses[0].address,routeDestination:.spec.podCIDRThen follow the instructions for each cloud provider to configure routing rules for each node:\n\n\nService\nInstructions\n\n\n\n\n\nGoogle GCE\nFor GCE, add a static route for each node: Adding a Static Route.\n\n\n\nAzure VM\nFor Azure, create a routing table: Custom Routes: User-defined.\n\n\nYou can add Windows hosts to a custom cluster by editing the cluster and choosing the Windows option.\nFrom the main menu, select Nodes.\n\nClick Edit Cluster.\n\nScroll down to Node Operating System. Choose Windows.\n\nSelect the Worker role.\n\nCopy the command displayed on screen to your clipboard.\n\nLog in to your Windows host using your preferred tool, such as Microsoft Remote Desktop. Run the command copied to your clipboard in the Command Prompt (CMD).\n\nFrom Rancher, click Save.\n\nOptional: Repeat these instruction if you want to add more Windows nodes to your cluster.\nResult: The worker role is installed on your Windows host, and the node registers with Rancher.After the initial provisioning of your custom cluster, your cluster only has a single Linux host. Add another Linux host, which will be used to support Ingress for your cluster.\nUsing the content menu, open the custom cluster your created in 2. Create the Custom Cluster.\n\nFrom the main menu, select Nodes.\n\nClick Edit Cluster.\n\nScroll down to Node Operating System. Choose Linux.\n\nSelect the Worker role.\n\nCopy the command displayed on screen to your clipboard.\n\nLog in to your Linux host using a remote Terminal connection. Run the command copied to your clipboard.\n\nFrom Rancher, click Save.\nResult: The worker role is installed on your Linux host, and the node registers with Rancher.To create a custom cluster that supports Windows nodes, follow the instructions in Creating a Cluster with Custom Nodes, starting from 2. Create the Custom Cluster. While completing the linked instructions, look for steps that requires special actions for Windows nodes, which are flagged with a note. These notes will link back here, to the special Windows instructions listed in the subheadings below.Enable the Windows Support OptionWhile choosing Cluster Options, set Windows Support (Experimental) to Enabled.After you select this option, resume Creating a Cluster with Custom Nodes from step 6.Networking OptionWhen choosing a network provider for a cluster that supports Windows, the only option available is Flannel, as host-gw is needed for IP routing.If your nodes are hosted by a cloud provider and you want automation support such as load balancers or persistent storage devices, see Selecting Cloud Providers for configuration info.Node ConfigurationThe first node in your cluster should be a Linux host that fills the Control Plane role. This role must be fulfilled before you can add Windows hosts to your cluster. At minimum, the node must have this role enabled, but we recommend enabling all three. The following table lists our recommended settings (we’ll provide the recommended settings for nodes 2 and 3 later).\n\n\nOption\nSetting\n\n\n\n\n\nNode Operating System\nLinux\n\n\n\nNode Roles\netcd  Control Plane  Worker\n\n\nWhen you’re done with these configurations, resume Creating a Cluster with Custom Nodes from step 8.\nNote: This step only applies to nodes hosted on cloud-hosted virtual machines. If you’re using virtualization clusters or bare-metal servers, skip ahead to Create the Custom Cluster.\nIf you’re hosting your nodes on any of the cloud services listed below, you must disable the private IP address checks for both your Linux or Windows hosts on startup. To disable this check for each node, follow the directions provided by each service below.\n\n\nService\nDirections to disable private IP address checks\n\n\n\n\n\nAmazon EC2\nDisabling Source/Destination Checks\n\n\n\nGoogle GCE\nEnabling IP Forwarding for Instances\n\n\n\nAzure VM\nEnable or Disable IP Forwarding\n\n\nTo begin provisioning a custom cluster with Windows support, prepare your host servers. Provision three nodes according to our requirements—two Linux, one Windows. Your hosts can be:\nCloud-hosted VMs\nVMs from virtualization clusters\nBare-metal servers\nThe table below lists the Kubernetes roles you’ll assign to each host, although you won’t enable these roles until further along in the configuration process—we’re just informing you of each node’s purpose. The first node, a Linux host, is primarily responsible for managing the Kubernetes control plane, although, in this use case, we’re installing all three roles on this node. Node 2 is also a Linux worker, which is responsible for Ingress support. Finally, the third node is your Windows worker, which will run your Windows applications.\n\n\nNode\nOperating System\nFuture Cluster Role(s)\n\n\n\n\n\nNode 1\nLinux (Ubuntu Server 16.04 recommended)\nControl Plane, etcd, Worker\n\n\n\nNode 2\nLinux (Ubuntu Server 16.04 recommended)\nWorker (This node is used for Ingress support)\n\n\n\nNode 3\nWindows (Windows Server core version 1809 or above)\nWorker\n\n\nRequirements\nYou can view node requirements for Linux and Windows nodes in the installation section.\nAll nodes in a virtualization cluster or a bare metal cluster must be connected using a layer 2 network.\nTo support Ingress, your cluster must include at least one Linux node dedicated to the worker role.\nAlthough we recommend the three node architecture listed in the table above, you can add additional Linux and Windows workers to scale up your cluster for redundancy.\nWhen setting up a custom cluster with support for Windows nodes and containers, complete the series of tasks below.\n1. Provision Hosts\n2. Cloud-host VM Networking Configuration\n3. Create the Custom Cluster\n4. Add Linux Host for Ingress Support\n5. Adding Windows Workers\n6. Cloud-host VM Routes Configuration\n\nFor clusters provisioned with Rancher v2.1.x and v2.2.x, containers must run on Windows Server 1809 or above.\nYou must build containers on a Windows Server core version 1809 or above to run these containers on the same server version.\n","postref":"3903fa9dc119786a55c837434d065aec","objectID":"9f7404d311a6c05acc62f84f8f6af7cf","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/windows-clusters/docs-for-2.1-and-2.2/"},{"anchor":"#enabling-api-audit-log","title":"Enabling API Audit Log","content":"After you enable auditing, each API request or response is logged by Rancher in the form of JSON. Each of the following code samples provide examples of how to identify each API transaction.Metadata LevelIf you set your AUDIT_LEVEL to 1, Rancher logs the metadata header for every API request, but not the body. The header provides basic information about the API transaction, such as the transaction’s ID, who initiated the transaction, the time it occurred, etc.{\n  \"auditID\": \"30022177-9e2e-43d1-b0d0-06ef9d3db183\",\n  \"requestURI\": \"/v3/schemas\",\n  \"sourceIPs\": [\"::1\"],\n  \"user\": {\n    \"name\": \"user-f4tt2\",\n    \"group\": [\"system:authenticated\"]\n  },\n  \"verb\": \"GET\",\n  \"stage\": \"RequestReceived\",\n  \"stageTimestamp\": \"2018-07-20 10:22:43 +0800\"\n}Metadata and Request Body LevelIf you set your AUDIT_LEVEL to 2, Rancher logs the metadata header and body for every API request.The code sample below depicts an API request, with both its metadata header and body.{\n  \"auditID\": \"ef1d249e-bfac-4fd0-a61f-cbdcad53b9bb\",\n  \"requestURI\": \"/v3/project/c-bcz5t:p-fdr4s/workloads/deployment:default:nginx\",\n  \"sourceIPs\": [\"::1\"],\n  \"user\": {\n    \"name\": \"user-f4tt2\",\n    \"group\": [\"system:authenticated\"]\n  },\n  \"verb\": \"PUT\",\n  \"stage\": \"RequestReceived\",\n  \"stageTimestamp\": \"2018-07-20 10:28:08 +0800\",\n  \"requestBody\": {\n    \"hostIPC\": false,\n    \"hostNetwork\": false,\n    \"hostPID\": false,\n    \"paused\": false,\n    \"annotations\": {},\n    \"baseType\": \"workload\",\n    \"containers\": [\n      {\n        \"allowPrivilegeEscalation\": false,\n        \"image\": \"nginx\",\n        \"imagePullPolicy\": \"Always\",\n        \"initContainer\": false,\n        \"name\": \"nginx\",\n        \"ports\": [\n          {\n            \"containerPort\": 80,\n            \"dnsName\": \"nginx-nodeport\",\n            \"kind\": \"NodePort\",\n            \"name\": \"80tcp01\",\n            \"protocol\": \"TCP\",\n            \"sourcePort\": 0,\n            \"type\": \"/v3/project/schemas/containerPort\"\n          }\n        ],\n        \"privileged\": false,\n        \"readOnly\": false,\n        \"resources\": {\n          \"type\": \"/v3/project/schemas/resourceRequirements\",\n          \"requests\": {},\n          \"limits\": {}\n        },\n        \"restartCount\": 0,\n        \"runAsNonRoot\": false,\n        \"stdin\": true,\n        \"stdinOnce\": false,\n        \"terminationMessagePath\": \"/dev/termination-log\",\n        \"terminationMessagePolicy\": \"File\",\n        \"tty\": true,\n        \"type\": \"/v3/project/schemas/container\",\n        \"environmentFrom\": [],\n        \"capAdd\": [],\n        \"capDrop\": [],\n        \"livenessProbe\": null,\n        \"volumeMounts\": []\n      }\n    ],\n    \"created\": \"2018-07-18T07:34:16Z\",\n    \"createdTS\": 1531899256000,\n    \"creatorId\": null,\n    \"deploymentConfig\": {\n      \"maxSurge\": 1,\n      \"maxUnavailable\": 0,\n      \"minReadySeconds\": 0,\n      \"progressDeadlineSeconds\": 600,\n      \"revisionHistoryLimit\": 10,\n      \"strategy\": \"RollingUpdate\"\n    },\n    \"deploymentStatus\": {\n      \"availableReplicas\": 1,\n      \"conditions\": [\n        {\n          \"lastTransitionTime\": \"2018-07-18T07:34:38Z\",\n          \"lastTransitionTimeTS\": 1531899278000,\n          \"lastUpdateTime\": \"2018-07-18T07:34:38Z\",\n          \"lastUpdateTimeTS\": 1531899278000,\n          \"message\": \"Deployment has minimum availability.\",\n          \"reason\": \"MinimumReplicasAvailable\",\n          \"status\": \"True\",\n          \"type\": \"Available\"\n        },\n        {\n          \"lastTransitionTime\": \"2018-07-18T07:34:16Z\",\n          \"lastTransitionTimeTS\": 1531899256000,\n          \"lastUpdateTime\": \"2018-07-18T07:34:38Z\",\n          \"lastUpdateTimeTS\": 1531899278000,\n          \"message\": \"ReplicaSet \\\"nginx-64d85666f9\\\" has successfully progressed.\",\n          \"reason\": \"NewReplicaSetAvailable\",\n          \"status\": \"True\",\n          \"type\": \"Progressing\"\n        }\n      ],\n      \"observedGeneration\": 2,\n      \"readyReplicas\": 1,\n      \"replicas\": 1,\n      \"type\": \"/v3/project/schemas/deploymentStatus\",\n      \"unavailableReplicas\": 0,\n      \"updatedReplicas\": 1\n    },\n    \"dnsPolicy\": \"ClusterFirst\",\n    \"id\": \"deployment:default:nginx\",\n    \"labels\": {\n      \"workload.user.cattle.io/workloadselector\": \"deployment-default-nginx\"\n    },\n    \"name\": \"nginx\",\n    \"namespaceId\": \"default\",\n    \"projectId\": \"c-bcz5t:p-fdr4s\",\n    \"publicEndpoints\": [\n      {\n        \"addresses\": [\"10.64.3.58\"],\n        \"allNodes\": true,\n        \"ingressId\": null,\n        \"nodeId\": null,\n        \"podId\": null,\n        \"port\": 30917,\n        \"protocol\": \"TCP\",\n        \"serviceId\": \"default:nginx-nodeport\",\n        \"type\": \"publicEndpoint\"\n      }\n    ],\n    \"restartPolicy\": \"Always\",\n    \"scale\": 1,\n    \"schedulerName\": \"default-scheduler\",\n    \"selector\": {\n      \"matchLabels\": {\n        \"workload.user.cattle.io/workloadselector\": \"deployment-default-nginx\"\n      },\n      \"type\": \"/v3/project/schemas/labelSelector\"\n    },\n    \"state\": \"active\",\n    \"terminationGracePeriodSeconds\": 30,\n    \"transitioning\": \"no\",\n    \"transitioningMessage\": \"\",\n    \"type\": \"deployment\",\n    \"uuid\": \"f998037d-8a5c-11e8-a4cf-0245a7ebb0fd\",\n    \"workloadAnnotations\": {\n      \"deployment.kubernetes.io/revision\": \"1\",\n      \"field.cattle.io/creatorId\": \"user-f4tt2\"\n    },\n    \"workloadLabels\": {\n      \"workload.user.cattle.io/workloadselector\": \"deployment-default-nginx\"\n    },\n    \"scheduling\": {\n      \"node\": {}\n    },\n    \"description\": \"my description\",\n    \"volumes\": []\n  }\n}Metadata, Request Body, and Response Body LevelIf you set your AUDIT_LEVEL to 3, Rancher logs:\nThe metadata header and body for every API request.\nThe metadata header and body for every API response.\nRequestThe code sample below depicts an API request, with both its metadata header and body.{\n  \"auditID\": \"a886fd9f-5d6b-4ae3-9a10-5bff8f3d68af\",\n  \"requestURI\": \"/v3/project/c-bcz5t:p-fdr4s/workloads/deployment:default:nginx\",\n  \"sourceIPs\": [\"::1\"],\n  \"user\": {\n    \"name\": \"user-f4tt2\",\n    \"group\": [\"system:authenticated\"]\n  },\n  \"verb\": \"PUT\",\n  \"stage\": \"RequestReceived\",\n  \"stageTimestamp\": \"2018-07-20 10:33:06 +0800\",\n  \"requestBody\": {\n    \"hostIPC\": false,\n    \"hostNetwork\": false,\n    \"hostPID\": false,\n    \"paused\": false,\n    \"annotations\": {},\n    \"baseType\": \"workload\",\n    \"containers\": [\n      {\n        \"allowPrivilegeEscalation\": false,\n        \"image\": \"nginx\",\n        \"imagePullPolicy\": \"Always\",\n        \"initContainer\": false,\n        \"name\": \"nginx\",\n        \"ports\": [\n          {\n            \"containerPort\": 80,\n            \"dnsName\": \"nginx-nodeport\",\n            \"kind\": \"NodePort\",\n            \"name\": \"80tcp01\",\n            \"protocol\": \"TCP\",\n            \"sourcePort\": 0,\n            \"type\": \"/v3/project/schemas/containerPort\"\n          }\n        ],\n        \"privileged\": false,\n        \"readOnly\": false,\n        \"resources\": {\n          \"type\": \"/v3/project/schemas/resourceRequirements\",\n          \"requests\": {},\n          \"limits\": {}\n        },\n        \"restartCount\": 0,\n        \"runAsNonRoot\": false,\n        \"stdin\": true,\n        \"stdinOnce\": false,\n        \"terminationMessagePath\": \"/dev/termination-log\",\n        \"terminationMessagePolicy\": \"File\",\n        \"tty\": true,\n        \"type\": \"/v3/project/schemas/container\",\n        \"environmentFrom\": [],\n        \"capAdd\": [],\n        \"capDrop\": [],\n        \"livenessProbe\": null,\n        \"volumeMounts\": []\n      }\n    ],\n    \"created\": \"2018-07-18T07:34:16Z\",\n    \"createdTS\": 1531899256000,\n    \"creatorId\": null,\n    \"deploymentConfig\": {\n      \"maxSurge\": 1,\n      \"maxUnavailable\": 0,\n      \"minReadySeconds\": 0,\n      \"progressDeadlineSeconds\": 600,\n      \"revisionHistoryLimit\": 10,\n      \"strategy\": \"RollingUpdate\"\n    },\n    \"deploymentStatus\": {\n      \"availableReplicas\": 1,\n      \"conditions\": [\n        {\n          \"lastTransitionTime\": \"2018-07-18T07:34:38Z\",\n          \"lastTransitionTimeTS\": 1531899278000,\n          \"lastUpdateTime\": \"2018-07-18T07:34:38Z\",\n          \"lastUpdateTimeTS\": 1531899278000,\n          \"message\": \"Deployment has minimum availability.\",\n          \"reason\": \"MinimumReplicasAvailable\",\n          \"status\": \"True\",\n          \"type\": \"Available\"\n        },\n        {\n          \"lastTransitionTime\": \"2018-07-18T07:34:16Z\",\n          \"lastTransitionTimeTS\": 1531899256000,\n          \"lastUpdateTime\": \"2018-07-18T07:34:38Z\",\n          \"lastUpdateTimeTS\": 1531899278000,\n          \"message\": \"ReplicaSet \\\"nginx-64d85666f9\\\" has successfully progressed.\",\n          \"reason\": \"NewReplicaSetAvailable\",\n          \"status\": \"True\",\n          \"type\": \"Progressing\"\n        }\n      ],\n      \"observedGeneration\": 2,\n      \"readyReplicas\": 1,\n      \"replicas\": 1,\n      \"type\": \"/v3/project/schemas/deploymentStatus\",\n      \"unavailableReplicas\": 0,\n      \"updatedReplicas\": 1\n    },\n    \"dnsPolicy\": \"ClusterFirst\",\n    \"id\": \"deployment:default:nginx\",\n    \"labels\": {\n      \"workload.user.cattle.io/workloadselector\": \"deployment-default-nginx\"\n    },\n    \"name\": \"nginx\",\n    \"namespaceId\": \"default\",\n    \"projectId\": \"c-bcz5t:p-fdr4s\",\n    \"publicEndpoints\": [\n      {\n        \"","postref":"0fb5723df5ebe790dee7cf0cbb1e2700","objectID":"a0780f21dd005779910b46cbf0594c87","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/api-audit-log/"},{"anchor":"#video","title":"Video","content":"Throughout this migration guide, we will reference several example services from Rancher v1.6 that we’re migrating to v2.x. These services are:\nA service named web, which runs Let’s Chat, a self-hosted chat for small teams.\nA service named database, which runs Mongo DB, an open source document database.\nA service named webLB, which runs HAProxy, an open source load balancer used in Rancher v1.6.\nDuring migration, we’ll export these services from Rancher v1.6.  The export generates a unique directory for each Rancher v1.6 environment and stack, and two files are output into each stack’s directory:\ndocker-compose.yml\n\nA file that contains standard Docker directives for each service in your stack. We’ll be converting these files to Kubernetes manifests that can be read by Rancher v2.x.\n\nrancher-compose.yml\n\nA file for Rancher-specific functionality such as health checks and load balancers. These files cannot be read by Rancher v2.x, so don’t worry about their contents—we’re discarding them and recreating them using the v2.x UI.\nNext: Get Started\nWant to more about Kubernetes before getting started? Read our Kubernetes Introduction.\n\n1. Get Started\n\n\nAlready a Kubernetes user in v1.6?\n\nGet Started is the only section you need to review for migration to v2.x. You can skip everything else.\n\n\n2. Migrate Your Services\n\n3. Expose Your Services\n\n4. Configure Health Checks\n\n5. Schedule Your Services\n\n6. Service Discovery\n\n7. Load Balancing\nThis video demonstrates a complete walk through of migration from Rancher v1.6 to v2.x.\n  \n","postref":"9df68b8bf5441b195821f7f057747d4d","objectID":"895ad6e5f7e642d1fb5f10e627b8b676","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/v1.6-migration/"},{"anchor":"#configuring-tls-settings","title":"Configuring TLS settings","content":"If you need to configure TLS the same way as it was before Rancher v2.1.7, please use the following settings:\n\n\nParameter\nLegacy value\n\n\n\n\n\nCATTLE_TLS_MIN_VERSION\n1.0\n\n\n\nCATTLE_TLS_CIPHERS\nTLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA,TLS_RSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA,TLS_RSA_WITH_3DES_EDE_CBC_SHA\n\n\n\n\n\nParameter\nDescription\nDefault\nAvailable options\n\n\n\n\n\nCATTLE_TLS_MIN_VERSION\nMinimum TLS version\n1.2\n1.0, 1.1, 1.2\n\n\n\nCATTLE_TLS_CIPHERS\nAllowed TLS cipher suites\nTLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305\nSee Golang tls constants\n\n\nThe Audit Log is enabled and configured by passing environment variables to the Rancher server container. See the following to enable on your installation.\nInstalling Rancher on a single node with Docker\n\nInstalling Rancher on Kubernetes\n","postref":"0a63dfbc1b14f8b43f6373e8ae67ead9","objectID":"c95a2cbf94f57d1ab0d568f51e60c3d1","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/tls-settings/"},{"anchor":"#","title":"Opening Ports with firewalld","content":"Some distributions of Linux derived from RHEL, including Oracle Linux, may have default firewall rules that block communication with Helm.\n\nFor example, one Oracle Linux image in AWS has REJECT rules that stop Helm from communicating with Tiller:\n\nChain INPUT (policy ACCEPT)\ntarget     prot opt source               destination\nACCEPT     all  --  anywhere             anywhere             state RELATED,ESTABLISHED\nACCEPT     icmp --  anywhere             anywhere\nACCEPT     all  --  anywhere             anywhere\nACCEPT     tcp  --  anywhere             anywhere             state NEW tcp dpt:ssh\nREJECT     all  --  anywhere             anywhere             reject-with icmp-host-prohibited\n\nChain FORWARD (policy ACCEPT)\ntarget     prot opt source               destination\nREJECT     all  --  anywhere             anywhere             reject-with icmp-host-prohibited\n\nChain OUTPUT (policy ACCEPT)\ntarget     prot opt source               destination\n\n\nYou can check the default firewall rules with this command:\n\nsudo iptables --list\n\n\nThis section describes how to use firewalld to apply the firewall port rules for nodes in a high-availability Rancher server cluster.\n\nPrerequisite\n\nInstall v7.x or later ofvfirewalld:\n\nyum install firewalld\nsystemctl start firewalld\nsystemctl enable firewalld\n\n\nApplying Firewall Port Rules\n\nIn the Rancher high-availability installation instructions, the Rancher server is set up on three nodes that have all three Kubernetes roles: etcd, controlplane, and worker. If your Rancher server nodes have all three roles, run the following commands on each node:\n\nfirewall-cmd --permanent --add-port=22/tcp\nfirewall-cmd --permanent --add-port=80/tcp\nfirewall-cmd --permanent --add-port=443/tcp\nfirewall-cmd --permanent --add-port=2376/tcp\nfirewall-cmd --permanent --add-port=2379/tcp\nfirewall-cmd --permanent --add-port=2380/tcp\nfirewall-cmd --permanent --add-port=6443/tcp\nfirewall-cmd --permanent --add-port=8472/udp\nfirewall-cmd --permanent --add-port=9099/tcp\nfirewall-cmd --permanent --add-port=10250/tcp\nfirewall-cmd --permanent --add-port=10254/tcp\nfirewall-cmd --permanent --add-port=30000-32767/tcp\nfirewall-cmd --permanent --add-port=30000-32767/udp\n\n\nIf your Rancher server nodes have separate roles, use the following commands based on the role of the node:\n\n# For etcd nodes, run the following commands:\nfirewall-cmd --permanent --add-port=2376/tcp\nfirewall-cmd --permanent --add-port=2379/tcp\nfirewall-cmd --permanent --add-port=2380/tcp\nfirewall-cmd --permanent --add-port=8472/udp\nfirewall-cmd --permanent --add-port=9099/tcp\nfirewall-cmd --permanent --add-port=10250/tcp\n\n# For control plane nodes, run the following commands:\nfirewall-cmd --permanent --add-port=80/tcp\nfirewall-cmd --permanent --add-port=443/tcp\nfirewall-cmd --permanent --add-port=2376/tcp\nfirewall-cmd --permanent --add-port=6443/tcp\nfirewall-cmd --permanent --add-port=8472/udp\nfirewall-cmd --permanent --add-port=9099/tcp\nfirewall-cmd --permanent --add-port=10250/tcp\nfirewall-cmd --permanent --add-port=10254/tcp\nfirewall-cmd --permanent --add-port=30000-32767/tcp\nfirewall-cmd --permanent --add-port=30000-32767/udp\n\n# For worker nodes, run the following commands:\nfirewall-cmd --permanent --add-port=22/tcp\nfirewall-cmd --permanent --add-port=80/tcp\nfirewall-cmd --permanent --add-port=443/tcp\nfirewall-cmd --permanent --add-port=2376/tcp\nfirewall-cmd --permanent --add-port=8472/udp\nfirewall-cmd --permanent --add-port=9099/tcp\nfirewall-cmd --permanent --add-port=10250/tcp\nfirewall-cmd --permanent --add-port=10254/tcp\nfirewall-cmd --permanent --add-port=30000-32767/tcp\nfirewall-cmd --permanent --add-port=30000-32767/udp\n\n\nAfter the firewall-cmd commands have been run on a node, use the following command to enable the firewall rules:\n\nfirewall-cmd --reload\n\n\nResult: The firewall is updated so that Helm can communicate with the Rancher server nodes.\n","postref":"d0a6c797c46d01f832a0243889363519","objectID":"543dddfbf4037076bd43261a3f03dbe7","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/firewall/"},{"anchor":"#","title":"Configuring Google OAuth","content":"Available as of v2.3.0\n\nIf your organization uses G Suite for user authentication, you can configure Rancher to allow your users to log in using their G Suite credentials.\n\nOnly admins of the G Suite domain have access to the Admin SDK. Therefore, only G Suite admins can configure Google OAuth for Rancher.\n\nWithin Rancher, only administrators or users with the Manage Authentication global role can configure authentication.\n\nPrerequisites\n\n\nYou must have a G Suite admin account configured.\nG Suite requires a top private domain FQDN as an authorized domain. One way to get an FQDN is by creating an A-record in Route53 for your Rancher server. You do not need to update your Rancher Server URL setting with that record, because there could be clusters using that URL.\nYou must have the Admin SDK API enabled for your G Suite domain. You can enable it using the steps on this page.\n\n\nAfter the Admin SDK API is enabled, your G Suite domain’s API screen should look like this:\n\n\nSetting up G Suite for OAuth with Rancher\n\nBefore you can set up Google OAuth in Rancher, you need to log in to your G Suite account and do the following:\n\n\nAdd Rancher as an authorized domain in G Suite\nGenerate OAuth2 credentials for the Rancher server\nCreate service account credentials for the Rancher server\nRegister the service account key as an OAuth Client\n\n\n1. Adding Rancher as an Authorized Domain\n\n\nClick here to go to credentials page of your Google domain.\nSelect your project and click OAuth consent screen.\n\nGo to Authorized Domains and enter the top private domain of your Rancher server URL in the list. The top private domain is the rightmost superdomain. So for example, www.foo.co.uk a top private domain of foo.co.uk. For more information on top-level domains, refer to this article.\nGo to Scopes for Google APIs and make sure email, profile and openid are enabled.\n\n\nResult: Rancher has been added as an authorized domain for the Admin SDK API.\n\n2. Creating OAuth2 Credentials for the Rancher Server\n\n\nGo to the Google API console, select your project, and go to the credentials page.\n\nOn the Create Credentials dropdown, select OAuth client ID.\nClick Web application.\nProvide a name.\nFill out the Authorized JavaScript origins and Authorized redirect URIs. Note: The Rancher UI page for setting up Google OAuth (available from the Global view under Security > Authentication > Google) provides you the exact links to enter for this step.\n\n\nUnder Authorized JavaScript origins, enter your Rancher server URL.\nUnder Authorized redirect URIs, enter your Rancher server URL appended with the path verify-auth. For example, if your URI is https://rancherServer, you will enter https://rancherServer/verify-auth.\n\nClick on Create.\nAfter the credential is created, you will see a screen with a list of your credentials. Choose the credential you just created, and in that row on rightmost side, click Download JSON. Save the file so that you can provide these credentials to Rancher.\n\n\nResult: Your OAuth credentials have been successfully created.\n\n3. Creating Service Account Credentials\n\nSince the Google Admin SDK is available only to admins, regular users cannot use it to retrieve profiles of other users or their groups. Regular users cannot even retrieve their own groups.\n\nSince Rancher provides group-based membership access, we require the users to be able to get their own groups, and look up other users and groups when needed.\n\nAs a workaround to get this capability, G Suite recommends creating a service account and delegating authority of your G Suite domain to that service account.\n\nThis section describes how to:\n\n\nCreate a service account\nCreate a key for the service account and download the credentials as JSON\n\n\n\nClick here and select your project for which you generated OAuth credentials.\nClick on Create Service Account.\nEnter a name and click Create.\n\nDon’t provide any roles on the Service account permissions page and click Continue\n\nClick on Create Key and select the JSON option. Download the JSON file and save it so that you can provide it as the service account credentials to Rancher.\n\n\n\nResult: Your service account is created.\n\n4. Register the Service Account Key as an OAuth Client\n\nYou will need to grant some permissions to the service account you created in the last step. Rancher requires you to grant only read-only permissions for users and groups.\n\nUsing the Unique ID of the service account key, register it as an Oauth Client using the following steps:\n\n\nGet the Unique ID of the key you just created. If it’s not displayed in the list of keys right next to the one you created, you will have to enable it. To enable it, click Unique ID and click OK. This will add a Unique ID column to the list of service account keys. Save the one listed for the service account you created. NOTE: This is a numeric key, not to be confused with the alphanumeric field Key ID.\n\n\n\nGo to the Manage OAuth Client Access page.\n\nAdd the Unique ID obtained in the previous step in the Client Name field.\n\nIn the One or More API Scopes field, add the following scopes:\n\nopenid,profile,email,https://www.googleapis.com/auth/admin.directory.user.readonly,https://www.googleapis.com/auth/admin.directory.group.readonly\n\n\nClick Authorize.\n\n\nResult: The service account is registered as an OAuth client in your G Suite account.\n\nConfiguring Google OAuth in Rancher\n\n\nSign into Rancher using a local user assigned the administrator role. This user is also called the local principal.\nFrom the Global view, click Security > Authentication from the main menu.\nClick Google. The instructions in the UI cover the steps to set up authentication with Google OAuth.\n\n\nAdmin Email: Provide the email of an administrator account from your GSuite setup. In order to perform user and group lookups, google apis require an administrator’s email in conjunction with the service account key.\nDomain: Provide the domain on which you have configured GSuite. Provide the exact domain and not any aliases.\nNested Group Membership: Check this box to enable nested group memberships. Rancher admins can disable this at any time after configuring auth.\nStep One is about adding Rancher as an authorized domain, which we already covered in this section.\nFor Step Two, provide the OAuth credentials JSON that you downloaded after completing this section. You can upload the file or paste the contents into the OAuth Credentials field.\nFor Step Three, provide the service account credentials JSON that downloaded at the end of this section. The credentials will only work if you successfully registered the service account key as an OAuth client in your G Suite account.\n\nClick Authenticate with Google.\nClick Save.\n\n\nResult: Google authentication is successfully configured.\n","postref":"41399c655ec749ab1ec9ada1e6a76880","objectID":"44e8a40630752e98302f3e21154fdb12","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/authentication/google/"},{"anchor":"#","title":"Documentation | Rancher Labs","content":"\n    \n        \n            \n                \n                    Rancher docs\n\n                    Rancher is open source software that combines everything an organization needs to adopt and run containers in production. Built on Kubernetes, Rancher makes it easy for DevOps teams to test, deploy and manage their applications. \n                \n            \n        \n\n        \n            \n                \n                    \n                    \n                \n            \n        \n    \n\n\n\n  \n    \n      \n        \n      \n\n      \n      \n        \n          \n        \n        \n          Featured resource\n          Read about how to migrate from Rancher v1.6 Cattle to v2.x\n        \n\n        \n          \n            \n                Read More\n            \n          \n        \n      \n    \n  \n\n\n\n    \n        \n            \n            \n            \n        \n\n        \n            \n                \n                    \n                        \n                            \n                                2.x\n                                Rancher 2.x\n                            \n\n                            \n\n                            Rancher manages all of your Kubernetes clusters everywhere, unifies them under centralized RBAC, monitors them and lets you easily deploy and manage workloads through an intuitive user interface.\n\n                            \n                              \n                                \n                                    Read the docs\n                                \n                              \n                            \n                        \n                    \n                    \n                        \n                            \n                                1.6\n                                Rancher 1.6\n                            \n\n                            \n\n                            If you haven't yet migrated to Rancher 2.x, you can still find documentation for 1.6 here. This is only for legacy users of the 1.6 product.\n\n                            \n                              \n                                \n                                    Read the docs\n                                \n                              \n                            \n                        \n                    \n                    \n                        \n                            \n                                OS\n                                RancherOS\n                            \n\n                            \n\n                            RancherOS is the lightest, easiest way to run Docker in production. Engineered from the ground up for security and speed, it runs all system services and user workloads within Docker containers.\n\n                            \n                                \n                                \n                                    Read the docs\n                                \n                              \n                            \n                        \n                    \n                    \n                        \n                            \n                                RKE\n                                Rancher Kubernetes Engine\n                            \n\n                            \n\n                            Rancher Kubernetes Engine (RKE) is an extremely simple, lightning fast Kubernetes installer that \u0003works everywhere.\n\n                            \n                              \n                                \n                                    Read the docs\n                                \n                              \n                            \n                        \n                    \n                    \n                        \n                            \n                                K3S\n                                K3S\n                            \n\n                            \n\n                            Lightweight Kubernetes.  Easy to install, half the memory, all in a binary less than 40mb.\n\n                            \n                              \n                                \n                                    Read the docs\n                                \n                              \n                            \n                        \n                    \n                \n            \n        \n    \n\n\n","postref":"3971dee9690c45ec29595f6ae97154f9","objectID":"01d6d1fb6122420593240d779959c530","permalink":"http://jijeesh.github.io/docs/"},{"anchor":"#","title":"Enable Istio with Pod Security Policies","content":"\nNote: The following guide is only for RKE provisioned clusters.\n\n\nIf you have restrictive Pod Security Policies enabled, then Istio may not be able to function correctly, because it needs certain permissions in order to install itself and manage pod infrastructure. In this section, we will configure a cluster with PSPs enabled for an Istio install, and also set up the Istio CNI plugin.\n\nThe Istio CNI plugin removes the need for each application pod to have a privileged NET_ADMIN container. For further information, see the Istio CNI Plugin docs. Please note that the Istio CNI Plugin is in alpha.\n\n\n1. Configure the System Project Policy to allow Istio install.\n2. Install the CNI plugin in the System project.\n3. Install Istio.\n\n\n1. Configure the System Project Policy to allow Istio install\n\n\nFrom the main menu of the Dashboard, select Projects/Namespaces.\nFind the Project: System project and select the Ellipsis (…) > Edit.\nChange the Pod Security Policy option to be unrestricted, then click Save.\n\n\n2. Install the CNI Plugin in the System Project\n\n\nFrom the main menu of the Dashboard, select Projects/Namespaces.\nSelect the Project: System project.\nChoose Tools > Catalogs in the navigation bar.\nAdd a catalog with the following:\n\n\nName: istio-cni\nCatalog URL: https://github.com/istio/cni\nBranch: The branch that matches your current release, for example: release-1.4.\n\nFrom the main menu select Apps\nClick Launch and select istio-cni\nUpdate the namespace to be “kube-system”\nIn the answers section, click “Edit as YAML” and paste in the following, then click launch:\n\n\n---\n  logLevel: \"info\"\n  excludeNamespaces:\n    - \"istio-system\"\n    - \"kube-system\"\n\n\n3. Install Istio\n\nFollow the primary instructions, adding a custom answer: istio_cni.enabled: true.\n\nAfter Istio has finished installing, the Apps page in System Projects should show both istio and istio-cni applications deployed successfully. Sidecar injection will now be functional.\n","postref":"e73500f30ab0bf7d548794fd2090b330","objectID":"bfe4a6552b0a092a8ede020570efdddf","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/tools/istio/setup/enable-istio-in-cluster/enable-istio-with-psp/"},{"anchor":"#","title":"K3s - 5 less than K8s","content":"Lightweight Kubernetes.  Easy to install, half the memory, all in a binary less than 50mb.\n\nGreat for:\n\n\nEdge\nIoT\nCI\nARM\nSituations where a PhD in k8s clusterology is infeasible\n\n\nWhat is K3s?\n\nK3s is a fully compliant Kubernetes distribution with the following enhancements:\n\n\nAn embedded SQLite database has replaced etcd as the default datastore. External datastores such as PostgreSQL, MySQL, and etcd are also supported.\nSimple but powerful “batteries-included” features have been added, such as: a local storage provider, a service load balancer, a Helm controller, and the Traefik ingress controller.\nOperation of all Kubernetes control plane components is encapsulated in a single binary and process. This allows K3s to automate and manage complex cluster operations like distributing certificates.\nIn-tree cloud providers and storage plugins have been removed.\nExternal dependencies have been minimized (just a modern kernel and cgroup mounts needed). K3s packages required dependencies, including:\n\n\ncontainerd\nFlannel\nCoreDNS\nHost utilities (iptables, socat, etc)\n\n\n","postref":"e0e39269981c95ac058def185a9d13ac","objectID":"ddb2b795d9143d74d44d3ea85be7ce5b","permalink":"http://jijeesh.github.io/docs/k3s/latest/en/"},{"authors":null,"categories":null,"content":"This cluster uses the default Nginx controller to allow traffic into the cluster.\nA Rancher administrator or cluster owner can configure Rancher to deploy Istio in a Kubernetes cluster.\n If the cluster has a Pod Security Policy enabled there are prerequisites steps\n  From the Global view, navigate to the cluster where you want to enable Istio. Click Tools &gt; Istio. Optional: Configure member access and resource limits for the Istio components.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-admin/tools/istio/setup/enable-istio-in-cluster/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"b74fb438ced642236354564596d8abb8","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/tools/istio/setup/enable-istio-in-cluster/","postref":"b74fb438ced642236354564596d8abb8","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rancher/v2.x/en/cluster-admin/tools/istio/setup/enable-istio-in-cluster/","section":"rancher","tags":null,"title":"1. Enable Istio in the Cluster","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-admin/tools/istio/setup/enable-istio-in-cluster/","weight":1,"wordcount":168},{"authors":null,"categories":null,"content":"By default, some cluster-level API tokens are generated with infinite time-to-live (ttl=0). In other words, API tokens with ttl=0 never expire unless you invalidate them. Tokens are not invalidated by changing a password.\nYou can deactivate API tokens by deleting them or by deactivating the user account.\nTo delete a token,\n Go to the list of all tokens in the Rancher API view at https://&lt;Rancher-Server-IP&gt;/v3/tokens.\n Access the token you want to delete by its ID.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/api/api-tokens/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"fb204bf6f5a3d3134b5786322632de9b","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/api/api-tokens/","postref":"fb204bf6f5a3d3134b5786322632de9b","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rancher/v2.x/en/api/api-tokens/","section":"rancher","tags":null,"title":"API Tokens","type":"rancher","url":"/docs/rancher/v2.x/en/api/api-tokens/","weight":1,"wordcount":144},{"authors":null,"categories":null,"content":"Available as of v2.3.0\nThis feature allows you to use types for storage providers and provisioners that are not enabled by default.\nTo enable or disable this feature, refer to the instructions on the main page about enabling experimental features.\n   Environment Variable Key Default Value Description     unsupported-storage-drivers false This feature enables types for storage providers and provisioners that are not enabled by default.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/options/feature-flags/enable-not-default-storage-drivers/","expirydate":-62135596800,"fuzzywordcount":300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"c05963a78c0960be947e88fa3e0420d5","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/feature-flags/enable-not-default-storage-drivers/","postref":"c05963a78c0960be947e88fa3e0420d5","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rancher/v2.x/en/installation/options/feature-flags/enable-not-default-storage-drivers/","section":"rancher","tags":null,"title":"Allow Unsupported Storage Drivers","type":"rancher","url":"/docs/rancher/v2.x/en/installation/options/feature-flags/enable-not-default-storage-drivers/","weight":1,"wordcount":202},{"authors":null,"categories":null,"content":"This page describes the architecture of a high-availability K3s server cluster and how it differs from a single-node server cluster.\nIt also describes how agent nodes are registered with K3s servers.\nA server node is defined as a machine (bare-metal or virtual) running the k3s server command. A worker node is defined as a machine running the k3s agent command.\nThis page covers the following topics:\n Single-server setup with an embedded database High-availability K3s server with an external database  Fixed registration address for agent nodes  How agent node registration works  Single-server Setup with an Embedded DB The following diagram shows an example of a cluster that has a single-node K3s server with an embedded SQLite database.","date":-62135596800,"description":"","dir":"k3s/latest/en/architecture/","expirydate":-62135596800,"fuzzywordcount":500,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"2e772cba3eb53ed72b5cc22935090564","permalink":"http://jijeesh.github.io/docs/k3s/latest/en/architecture/","postref":"2e772cba3eb53ed72b5cc22935090564","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/k3s/latest/en/architecture/","section":"k3s","tags":null,"title":"Architecture","type":"k3s","url":"/docs/k3s/latest/en/architecture/","weight":1,"wordcount":403},{"authors":null,"categories":null,"content":"This section focuses on the Rancher server, its components, and how Rancher communicates with downstream Kubernetes clusters.\nFor information on the different ways that Rancher can be installed, refer to the overview of installation options.\nFor a list of main features of the Rancher API server, refer to the overview section.\nFor guidance about setting up the underlying infrastructure for the Rancher server, refer to the architecture recommendations.\n This section assumes a basic familiarity with Docker and Kubernetes.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/overview/architecture/","expirydate":-62135596800,"fuzzywordcount":1700,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"65c227de4504b149f50d28d89a234079","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/overview/architecture/","postref":"65c227de4504b149f50d28d89a234079","publishdate":"0001-01-01T00:00:00Z","readingtime":8,"relpermalink":"/docs/rancher/v2.x/en/overview/architecture/","section":"rancher","tags":null,"title":"Architecture","type":"rancher","url":"/docs/rancher/v2.x/en/overview/architecture/","weight":1,"wordcount":1614},{"authors":null,"categories":null,"content":"Available as of v2.3.0\nThis section describes the minimum recommended computing resources for the Istio components in a cluster.\nThe CPU and memory allocations for each component are configurable.\nBefore enabling Istio, we recommend that you confirm that your Rancher worker nodes have enough CPU and memory to run all of the components of Istio.\n Tip: In larger deployments, it is strongly advised that the infrastructure be placed on dedicated nodes in the cluster by adding a node selector for each Istio component.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-admin/tools/istio/resources/","expirydate":-62135596800,"fuzzywordcount":1400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"e945314c6f33683a878544516a911488","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/tools/istio/resources/","postref":"e945314c6f33683a878544516a911488","publishdate":"0001-01-01T00:00:00Z","readingtime":7,"relpermalink":"/docs/rancher/v2.x/en/cluster-admin/tools/istio/resources/","section":"rancher","tags":null,"title":"CPU and Memory Allocations","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-admin/tools/istio/resources/","weight":1,"wordcount":1352},{"authors":null,"categories":null,"content":"There are many ways you can interact with Kubernetes clusters that are managed by Rancher:\n Rancher UI\nRancher provides an intuitive user interface for interacting with your clusters. All options available in the UI use the Rancher API. Therefore any action possible in the UI is also possible in the Rancher CLI or Rancher API.\n kubectl\nYou can use the Kubernetes command-line tool, kubectl, to manage your clusters. You have two options for using kubectl:","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-admin/cluster-access/","expirydate":-62135596800,"fuzzywordcount":300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"ce26920a09ab4990cb2e79acce57e9b0","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/cluster-access/","postref":"ce26920a09ab4990cb2e79acce57e9b0","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/cluster-admin/cluster-access/","section":"rancher","tags":null,"title":"Cluster Access","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-admin/cluster-access/","weight":1,"wordcount":237},{"authors":null,"categories":null,"content":"Available as of v2.2.0\nCluster drivers are used to create clusters in a hosted Kubernetes provider, such as Google GKE. The availability of which cluster driver to display when creating clusters is defined by the cluster driver&rsquo;s status. Only active cluster drivers will be displayed as an option for creating clusters. By default, Rancher is packaged with several existing cloud provider cluster drivers, but you can also add custom cluster drivers to Rancher.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/admin-settings/drivers/cluster-drivers/","expirydate":-62135596800,"fuzzywordcount":300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"be2403f60f2c9111216f4d36f9e46c3d","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/drivers/cluster-drivers/","postref":"be2403f60f2c9111216f4d36f9e46c3d","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/admin-settings/drivers/cluster-drivers/","section":"rancher","tags":null,"title":"Cluster Drivers","type":"rancher","url":"/docs/rancher/v2.x/en/admin-settings/drivers/cluster-drivers/","weight":1,"wordcount":299},{"authors":null,"categories":null,"content":"This section describes how to create a vSphere username and password. You will need to provide these vSphere credentials to Rancher, which allows Rancher to provision resources in vSphere.\nThe following table lists the permissions required for the vSphere user account:\n   Privilege Group Operations     Datastore AllocateSpace  Browse  FileManagement (Low level file operations)  UpdateVirtualMachineFiles  UpdateVirtualMachineMetadata   Network Assign   Resource AssignVMToPool   Virtual Machine Config (All)  GuestOperations (All)  Interact (All)  Inventory (All)  Provisioning (All)    The following steps create a role with the required privileges and then assign it to a new user in the vSphere console:","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-provisioning/rke-clusters/node-pools/vsphere/provisioning-vsphere-clusters/creating-credentials/","expirydate":-62135596800,"fuzzywordcount":300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"c6512a71b5ce93989e51c9ba5f0a2b48","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/node-pools/vsphere/provisioning-vsphere-clusters/creating-credentials/","postref":"c6512a71b5ce93989e51c9ba5f0a2b48","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/node-pools/vsphere/provisioning-vsphere-clusters/creating-credentials/","section":"rancher","tags":null,"title":"Creating Credentials in the vSphere Console","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/node-pools/vsphere/provisioning-vsphere-clusters/creating-credentials/","weight":1,"wordcount":202},{"authors":null,"categories":null,"content":"The below example shows how to configure a custom network plug-in with an in-line add-on to the cluster.yml.\nFirst, to edit the network plug-ins, change the network section of the YAML from:\nnetwork: options: flannel_backend_type: &quot;vxlan&quot; plugin: &quot;canal&quot;  to:\nnetwork: plugin: none  Then, in the addons section of the cluster.yml, you can add the add-on manifest of a cluster that has the network plugin-that you want. In the below example, we are replacing the Canal plugin with a Flannel plugin by adding the add-on manifest for the cluster through the addons field:","date":-62135596800,"description":"","dir":"rke/latest/en/config-options/add-ons/network-plugins/custom-network-plugin-example/","expirydate":-62135596800,"fuzzywordcount":500,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"c05ee7b8207b2581dff07b1374776a8a","permalink":"http://jijeesh.github.io/docs/rke/latest/en/config-options/add-ons/network-plugins/custom-network-plugin-example/","postref":"c05ee7b8207b2581dff07b1374776a8a","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rke/latest/en/config-options/add-ons/network-plugins/custom-network-plugin-example/","section":"rke","tags":null,"title":"Custom Network Plug-in Example","type":"rke","url":"/docs/rke/latest/en/config-options/add-ons/network-plugins/custom-network-plugin-example/","weight":1,"wordcount":415},{"authors":null,"categories":null,"content":"When you create a cluster, some alert rules are predefined. These alerts notify you about signs that the cluster could be unhealthy. You can receive these alerts if you configure a notifier for them.\nSeveral of the alerts use Prometheus expressions as the metric that triggers the alert. For more information on how expressions work, you can refer to the Rancher documentation about Prometheus expressions or the Prometheus documentation about querying metrics.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-admin/tools/alerts/default-alerts/","expirydate":-62135596800,"fuzzywordcount":700,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"0c41ba82279f7519244715a543405050","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/tools/alerts/default-alerts/","postref":"0c41ba82279f7519244715a543405050","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/rancher/v2.x/en/cluster-admin/tools/alerts/default-alerts/","section":"rancher","tags":null,"title":"Default Alerts for Cluster Monitoring","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-admin/tools/alerts/default-alerts/","weight":1,"wordcount":605},{"authors":null,"categories":null,"content":"A persistent volume (PV) is a piece of storage in the Kubernetes cluster, while a persistent volume claim (PVC) is a request for storage.\nThere are two ways to use persistent storage in Kubernetes:\n Use an existing persistent volume Dynamically provision new persistent volumes  To use an existing PV, your application will need to use a PVC that is bound to a PV, and the PV should include the minimum resources that the PVC requires.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-admin/volumes-and-storage/how-storage-works/","expirydate":-62135596800,"fuzzywordcount":900,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"c7452c90c9fbd77fa6d802d40ce61bb4","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/volumes-and-storage/how-storage-works/","postref":"c7452c90c9fbd77fa6d802d40ce61bb4","publishdate":"0001-01-01T00:00:00Z","readingtime":5,"relpermalink":"/docs/rancher/v2.x/en/cluster-admin/volumes-and-storage/how-storage-works/","section":"rancher","tags":null,"title":"How Persistent Storage Works","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-admin/volumes-and-storage/how-storage-works/","weight":1,"wordcount":880},{"authors":null,"categories":null,"content":"Resource quotas in Rancher include the same functionality as the native version of Kubernetes. However, in Rancher, resource quotas have been extended so that you can apply them to projects.\nIn a standard Kubernetes deployment, resource quotas are applied to individual namespaces. However, you cannot apply the quota to your namespaces simultaneously with a single action. Instead, the resource quota must be applied multiple times.\nIn the following diagram, a Kubernetes administrator is trying to enforce a resource quota without Rancher.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/project-admin/resource-quotas/quotas-for-projects/","expirydate":-62135596800,"fuzzywordcount":500,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"c1e00a1adfe4c20644eeaf293654f16f","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/project-admin/resource-quotas/quotas-for-projects/","postref":"c1e00a1adfe4c20644eeaf293654f16f","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/rancher/v2.x/en/project-admin/resource-quotas/quotas-for-projects/","section":"rancher","tags":null,"title":"How Resource Quotas Work in Rancher Projects","type":"rancher","url":"/docs/rancher/v2.x/en/project-admin/resource-quotas/quotas-for-projects/","weight":1,"wordcount":477},{"authors":null,"categories":null,"content":"K3s is very lightweight, but has some minimum requirements as outlined below.\nWhether you&rsquo;re configuring a K3s cluster to run in a Docker or Kubernetes setup, each node running K3s should meet the following minimum requirements. You may need more resources to fit your needs.\nPrerequisites  Two nodes cannot have the same hostname. If all your nodes have the same hostname, use the --with-node-id option to append a random suffix for each node, or otherwise devise a unique name to pass with --node-name or $K3S_NODE_NAME for each node you add to the cluster.","date":-62135596800,"description":"","dir":"k3s/latest/en/installation/installation-requirements/","expirydate":-62135596800,"fuzzywordcount":600,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"641ec44ff0c14e181efaed0ddd57a8f9","permalink":"http://jijeesh.github.io/docs/k3s/latest/en/installation/installation-requirements/","postref":"641ec44ff0c14e181efaed0ddd57a8f9","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/k3s/latest/en/installation/installation-requirements/","section":"k3s","tags":null,"title":"Installation Requirements","type":"k3s","url":"/docs/k3s/latest/en/installation/installation-requirements/","weight":1,"wordcount":584},{"authors":null,"categories":null,"content":"This page describes the software, hardware, and networking requirements for the nodes where the Rancher server will be installed. The Rancher server can be installed on a single node or a high-availability Kubernetes cluster.\n It is important to note that if you install Rancher on a Kubernetes cluster, requirements are different from the node requirements for downstream user clusters, which will run your apps and services.\n Make sure the node(s) for the Rancher server fulfill the following requirements:","date":-62135596800,"description":"Learn the node requirements for each node running Rancher server when you’re configuring  Rancher to run either in a Docker or Kubernetes setup","dir":"rancher/v2.x/en/installation/requirements/","expirydate":-62135596800,"fuzzywordcount":1600,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"ea2897f43e7926e447f12959e3e727de","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/requirements/","postref":"ea2897f43e7926e447f12959e3e727de","publishdate":"0001-01-01T00:00:00Z","readingtime":8,"relpermalink":"/docs/rancher/v2.x/en/installation/requirements/","section":"rancher","tags":null,"title":"Installation Requirements","type":"rancher","url":"/docs/rancher/v2.x/en/installation/requirements/","weight":1,"wordcount":1562},{"authors":null,"categories":null,"content":"Docker is required to be installed on any node that runs the Rancher server.\nThere are a couple of options for installing Docker. One option is to refer to the official Docker documentation about how to install Docker on Linux. The steps will vary based on the Linux distribution.\nAnother option is to use one of Rancher&rsquo;s Docker installation scripts, which are available for most recent versions of Docker.\nFor example, this command could be used to install Docker 18.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/requirements/installing-docker/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"d0afd0796d449b327a6c90cde36d3569","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/requirements/installing-docker/","postref":"d0afd0796d449b327a6c90cde36d3569","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rancher/v2.x/en/installation/requirements/installing-docker/","section":"rancher","tags":null,"title":"Installing Docker","type":"rancher","url":"/docs/rancher/v2.x/en/installation/requirements/installing-docker/","weight":1,"wordcount":113},{"authors":null,"categories":null,"content":"For development and testing environments only, Rancher can be installed by running a single Docker container.\nIn this installation scenario, you&rsquo;ll install Docker on a single Linux host, and then deploy Rancher on your host using a single Docker container.\n Want to use an external load balancer? See Docker Install with an External Load Balancer instead.\n Requirements for OS, Docker, Hardware, and Networking Make sure that your node fulfills the general installation requirements.","date":-62135596800,"description":"For development and testing environments only, use a Docker install. Install Docker on a single Linux host, and deploy Rancher with a single Docker container.","dir":"rancher/v2.x/en/installation/other-installation-methods/single-node-docker/","expirydate":-62135596800,"fuzzywordcount":1000,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"23f1e2450e3bbf1d6210e308edd9e1b0","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/other-installation-methods/single-node-docker/","postref":"23f1e2450e3bbf1d6210e308edd9e1b0","publishdate":"0001-01-01T00:00:00Z","readingtime":5,"relpermalink":"/docs/rancher/v2.x/en/installation/other-installation-methods/single-node-docker/","section":"rancher","tags":null,"title":"Installing Rancher on a Single Node Using Docker","type":"rancher","url":"/docs/rancher/v2.x/en/installation/other-installation-methods/single-node-docker/","weight":1,"wordcount":930},{"authors":null,"categories":null,"content":"After Helm 3 was released, the Rancher installation instructions were updated to use Helm 3.\nIf you are using Helm 2, we recommend migrating to Helm 3 because it is simpler to use and more secure than Helm 2.\nThis section provides a copy of the older high-availability Kubernetes Rancher installation instructions that used Helm 2, and it is intended to be used if upgrading to Helm 3 is not feasible.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/options/helm2/","expirydate":-62135596800,"fuzzywordcount":500,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"dcb033c399d213b004beaf89b43ecb48","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/helm2/","postref":"dcb033c399d213b004beaf89b43ecb48","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/rancher/v2.x/en/installation/options/helm2/","section":"rancher","tags":null,"title":"Kubernetes Installation Using Helm 2","type":"rancher","url":"/docs/rancher/v2.x/en/installation/options/helm2/","weight":1,"wordcount":449},{"authors":null,"categories":null,"content":"Rancher v2.x is built on the Kubernetes container orchestrator. This shift in underlying technology for v2.x is a large departure from v1.6, which supported several popular container orchestrators. Since Rancher is now based entirely on Kubernetes, it&rsquo;s helpful to learn the Kubernetes basics.\nThe following table introduces and defines some key Kubernetes concepts.\n   Concept Definition     Cluster A collection of machines that run containerized applications managed by Kubernetes.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/v1.6-migration/kub-intro/","expirydate":-62135596800,"fuzzywordcount":300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"fff0b1b7867e1b41ec2dbbcecc4c7305","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/v1.6-migration/kub-intro/","postref":"fff0b1b7867e1b41ec2dbbcecc4c7305","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/v1.6-migration/kub-intro/","section":"rancher","tags":null,"title":"Kubernetes Introduction","type":"rancher","url":"/docs/rancher/v2.x/en/v1.6-migration/kub-intro/","weight":1,"wordcount":237},{"authors":null,"categories":null,"content":"This page describes the requirements for the nodes where your apps and services will be installed.\nIn this section, &ldquo;user cluster&rdquo; refers to a cluster running your apps, which should be separate from the cluster (or single node) running Rancher.\n If Rancher is installed on a high-availability Kubernetes cluster, the Rancher server cluster and user clusters have different requirements. For Rancher installation requirements, refer to the node requirements in the installation section.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-provisioning/node-requirements/","expirydate":-62135596800,"fuzzywordcount":2000,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"bf33d48813329eb0c11cd81b52a4dff5","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/node-requirements/","postref":"bf33d48813329eb0c11cd81b52a4dff5","publishdate":"0001-01-01T00:00:00Z","readingtime":10,"relpermalink":"/docs/rancher/v2.x/en/cluster-provisioning/node-requirements/","section":"rancher","tags":null,"title":"Node Requirements for User Clusters","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-provisioning/node-requirements/","weight":1,"wordcount":1962},{"authors":null,"categories":null,"content":"Notifiers are services that inform you of alert events. You can configure notifiers to send alert notifications to staff best suited to take corrective action.\nNotifiers are configured at the cluster level. This model ensures that only cluster owners need to configure notifiers, leaving project owners to simply configure alerts in the scope of their projects. You don&rsquo;t need to dispense privileges like SMTP server access or cloud account access.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-admin/tools/notifiers/","expirydate":-62135596800,"fuzzywordcount":700,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"ceab60cc1b1470ad203e5fc5d425f448","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/tools/notifiers/","postref":"ceab60cc1b1470ad203e5fc5d425f448","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/docs/rancher/v2.x/en/cluster-admin/tools/notifiers/","section":"rancher","tags":null,"title":"Notifiers","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-admin/tools/notifiers/","weight":1,"wordcount":675},{"authors":null,"categories":null,"content":"One-time snapshots are handled differently depending on your version of RKE.\n To save a snapshot of etcd from each etcd node in the cluster config file, run the rke etcd snapshot-save command.\nThe snapshot is saved in /opt/rke/etcd-snapshots.\nWhen running the command, an additional container is created to take the snapshot. When the snapshot is completed, the container is automatically removed.\nThe one-time snapshot can be uploaded to a S3 compatible backend by using the additional options to specify the S3 backend.","date":-62135596800,"description":"","dir":"rke/latest/en/etcd-snapshots/one-time-snapshots/","expirydate":-62135596800,"fuzzywordcount":700,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"43ff4b9070b408e388d620b1e917d460","permalink":"http://jijeesh.github.io/docs/rke/latest/en/etcd-snapshots/one-time-snapshots/","postref":"43ff4b9070b408e388d620b1e917d460","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/rke/latest/en/etcd-snapshots/one-time-snapshots/","section":"rke","tags":null,"title":"One-time Snapshots","type":"rke","url":"/docs/rke/latest/en/etcd-snapshots/one-time-snapshots/","weight":1,"wordcount":615},{"authors":null,"categories":null,"content":"Rancher is a container management platform built for organizations that deploy containers in production. Rancher makes it easy to run Kubernetes everywhere, meet IT requirements, and empower DevOps teams.\nRun Kubernetes Everywhere Kubernetes has become the container orchestration standard. Most cloud and virtualization vendors now offer it as standard infrastructure. Rancher users have the choice of creating Kubernetes clusters with Rancher Kubernetes Engine (RKE) or cloud Kubernetes services, such as GKE, AKS, and EKS.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/overview/","expirydate":-62135596800,"fuzzywordcount":800,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"26d1e79e0ef0cef0242616a485f385d0","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/overview/","postref":"26d1e79e0ef0cef0242616a485f385d0","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/docs/rancher/v2.x/en/overview/","section":"rancher","tags":null,"title":"Overview","type":"rancher","url":"/docs/rancher/v2.x/en/overview/","weight":1,"wordcount":776},{"authors":null,"categories":null,"content":"Rancher Kubernetes Engine (RKE) is a CNCF-certified Kubernetes distribution that runs entirely within Docker containers. It works on bare-metal and virtualized servers. RKE solves the problem of installation complexity, a common issue in the Kubernetes community. With RKE, the installation and operation of Kubernetes is both simplified and easily automated, and it&rsquo;s entirely independent of the operating system and platform you&rsquo;re running. As long as you can run a supported version of Docker, you can deploy and run Kubernetes with RKE.","date":-62135596800,"description":"RKE solves Kubernetes installation complexity. With RKE, Kubernetes installation is simplified, regardless of what OSs and platforms you’re running.","dir":"rke/latest/en/","expirydate":-62135596800,"fuzzywordcount":100,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"876c0cc22dc880fa29e9cf07474b5257","permalink":"http://jijeesh.github.io/docs/rke/latest/en/","postref":"876c0cc22dc880fa29e9cf07474b5257","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rke/latest/en/","section":"rke","tags":null,"title":"Overview of RKE","type":"rke","url":"/docs/rke/latest/en/","weight":1,"wordcount":81},{"authors":null,"categories":null,"content":"RancherOS is the smallest, easiest way to run Docker in production. Every process in RancherOS is a container managed by Docker. This includes system services such as udev and syslog. Because it only includes the services necessary to run Docker, RancherOS is significantly smaller than most traditional operating systems. By removing unnecessary libraries and services, requirements for security patches and other maintenance are also reduced. This is possible because, with Docker, users typically package all necessary libraries into their containers.","date":-62135596800,"description":"RancherOS is a simplified Linux distribution built from containers, for containers. These documents describe how to install and use RancherOS.","dir":"os/v1.x/en/","expirydate":-62135596800,"fuzzywordcount":500,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"9e337e126a809e439ec5a9ee6dfe0f21","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/","postref":"9e337e126a809e439ec5a9ee6dfe0f21","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/os/v1.x/en/","section":"os","tags":null,"title":"Overview of RancherOS","type":"os","url":"/docs/os/v1.x/en/","weight":1,"wordcount":402},{"authors":null,"categories":null,"content":"RancherOS is the smallest, easiest way to run Docker in production. Every process in RancherOS is a container managed by Docker. This includes system services such as udev and syslog. Because it only includes the services necessary to run Docker, RancherOS is significantly smaller than most traditional operating systems. By removing unnecessary libraries and services, requirements for security patches and other maintenance are also reduced. This is possible because, with Docker, users typically package all necessary libraries into their containers.","date":-62135596800,"description":"RancherOS is a simplified Linux distribution built from containers, for containers. These documents describe how to install and use RancherOS.","dir":"os/v1.x/en/overview/","expirydate":-62135596800,"fuzzywordcount":500,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"9976686b53e6abe1bea61218b104bfb9","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/overview/","postref":"9976686b53e6abe1bea61218b104bfb9","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/os/v1.x/en/overview/","section":"os","tags":null,"title":"Overview of RancherOS","type":"os","url":"/docs/os/v1.x/en/overview/","weight":1,"wordcount":402},{"authors":null,"categories":null,"content":"Available as of v2.2.0\nWhile configuring monitoring at either the cluster level or project level, there are multiple options that can be configured.\n   Option Description     Data Retention How long your Prometheus instance retains monitoring data scraped from Rancher objects before it&rsquo;s purged.   Enable Node Exporter Whether or not to deploy the node exporter.   Node Exporter Host Port The host port on which data is exposed, i.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-admin/tools/monitoring/prometheus/","expirydate":-62135596800,"fuzzywordcount":600,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"1fa1af288ee6983ee98258f7afb83ccb","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/tools/monitoring/prometheus/","postref":"1fa1af288ee6983ee98258f7afb83ccb","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/rancher/v2.x/en/cluster-admin/tools/monitoring/prometheus/","section":"rancher","tags":null,"title":"Prometheus Configuration","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-admin/tools/monitoring/prometheus/","weight":1,"wordcount":501},{"authors":null,"categories":null,"content":"This section explains how to configure Rancher with vSphere credentials, provision nodes in vSphere, and set up Kubernetes clusters on those nodes.\nPrerequisites This section describes the requirements for setting up vSphere so that Rancher can provision VMs and clusters.\nThe node templates are documented and tested with the vSphere Web Services API version 6.5.\n Create credentials in vSphere Network permissions Valid ESXi License for vSphere API Access  Create Credentials in vSphere Before proceeding to create a cluster, you must ensure that you have a vSphere user with sufficient permissions.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-provisioning/rke-clusters/node-pools/vsphere/provisioning-vsphere-clusters/","expirydate":-62135596800,"fuzzywordcount":2600,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"1c6dea262f361f5402bb7250d6fd3b92","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/node-pools/vsphere/provisioning-vsphere-clusters/","postref":"1c6dea262f361f5402bb7250d6fd3b92","publishdate":"0001-01-01T00:00:00Z","readingtime":13,"relpermalink":"/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/node-pools/vsphere/provisioning-vsphere-clusters/","section":"rancher","tags":null,"title":"Provisioning Kubernetes Clusters in vSphere","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/node-pools/vsphere/provisioning-vsphere-clusters/","weight":1,"wordcount":2592},{"authors":null,"categories":null,"content":"This page contains frequently asked questions about the changes between Rancher v1.x and v2.x, and how to upgrade from Rancher v1.x to v2.x.\nKubernetes What does it mean when you say Rancher v2.x is built on Kubernetes?\nRancher v2.x is a complete container management platform built 100% on Kubernetes leveraging its Custom Resource and Controller framework. All features are written as a CustomResourceDefinition (CRD) which extends the existing Kubernetes API and can leverage native features such as RBAC.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/faq/upgrades-to-2x/","expirydate":-62135596800,"fuzzywordcount":900,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"80c80205d9c05f8709e2d9b0a55cd66b","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/faq/upgrades-to-2x/","postref":"80c80205d9c05f8709e2d9b0a55cd66b","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/docs/rancher/v2.x/en/faq/upgrades-to-2x/","section":"rancher","tags":null,"title":"Questions about Upgrading to Rancher v2.x","type":"rancher","url":"/docs/rancher/v2.x/en/faq/upgrades-to-2x/","weight":1,"wordcount":801},{"authors":null,"categories":null,"content":"If you have a specific RanchersOS machine requirements, please check out our guides on running RancherOS. With the rest of this guide, we&rsquo;ll start up a RancherOS using Docker machine and show you some of what RancherOS can do.\nLaunching RancherOS using Docker Machine Before moving forward, you&rsquo;ll need to have Docker Machine and VirtualBox installed. Once you have VirtualBox and Docker Machine installed, it&rsquo;s just one command to get RancherOS running.","date":-62135596800,"description":"","dir":"os/v1.x/en/quick-start-guide/","expirydate":-62135596800,"fuzzywordcount":800,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"402203ac56263b8fcfe8c86012864335","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/quick-start-guide/","postref":"402203ac56263b8fcfe8c86012864335","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/docs/os/v1.x/en/quick-start-guide/","section":"os","tags":null,"title":"Quick Start","type":"os","url":"/docs/os/v1.x/en/quick-start-guide/","weight":1,"wordcount":776},{"authors":null,"categories":null,"content":"What&rsquo;s New? Rancher was originally built to work with multiple orchestrators, and it included its own orchestrator called Cattle. With the rise of Kubernetes in the marketplace, Rancher now exclusively deploys and manages multiple Kubernetes clusters running anywhere, on any provider. It can provision Kubernetes from a hosted provider, provision compute nodes and then install Kubernetes onto them, or inherit existing Kubernetes clusters running anywhere.\nOne Rancher server installation can manage hundreds of Kubernetes clusters from the same interface.","date":-62135596800,"description":"Rancher adds significant value on top of Kubernetes: managing hundreds of clusters from one interface, centralizing RBAC, enabling monitoring and alerting. Read more.","dir":"rancher/v2.x/en/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"f83a79e40f336c447241dd6aeac2d8cb","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/","postref":"f83a79e40f336c447241dd6aeac2d8cb","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rancher/v2.x/en/","section":"rancher","tags":null,"title":"Rancher 2.x","type":"rancher","url":"/docs/rancher/v2.x/en/","weight":1,"wordcount":187},{"authors":null,"categories":null,"content":"There are three roles that can be assigned to nodes: etcd, controlplane and worker.\nSeparating Worker Nodes from Nodes with Other Roles When designing your cluster(s), you have two options:\n Use dedicated nodes for each role. This ensures resource availability for the components needed for the specified role. It also strictly isolates network traffic between each of the roles according to the port requirements. Assign the etcd and controlplane roles to the same nodes.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-provisioning/production/recommended-architecture/","expirydate":-62135596800,"fuzzywordcount":500,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"af153e1fd033be8a590359ccf8283a01","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/production/recommended-architecture/","postref":"af153e1fd033be8a590359ccf8283a01","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/rancher/v2.x/en/cluster-provisioning/production/recommended-architecture/","section":"rancher","tags":null,"title":"Recommended Cluster Architecture","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-provisioning/production/recommended-architecture/","weight":1,"wordcount":484},{"authors":null,"categories":null,"content":"This section describes the roles for etcd nodes, controlplane nodes, and worker nodes in Kubernetes, and how the roles work together in a cluster.\nThis diagram is applicable to Kubernetes clusters launched with Rancher using RKE..\n Lines show the traffic flow between components. Colors are used purely for visual aid\netcd Nodes with the etcd role run etcd, which is a consistent and highly available key value store used as Kubernetes’ backing store for all cluster data.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-provisioning/production/nodes-and-roles/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"3ac825645dc3c34b5674605f89fbdfee","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/production/nodes-and-roles/","postref":"3ac825645dc3c34b5674605f89fbdfee","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/cluster-provisioning/production/nodes-and-roles/","section":"rancher","tags":null,"title":"Roles for Nodes in Kubernetes","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-provisioning/production/nodes-and-roles/","weight":1,"wordcount":355},{"authors":null,"categories":null,"content":"Available as of v2.4.0-alpha1\nRancher can run a security scan to check whether Kubernetes is deployed according to security best practices as defined in the CIS Kubernetes Benchmark.\nThe Center for Internet Security (CIS) is a 501&copy;(3) nonprofit organization, formed in October 2000, with a mission is to &ldquo;identify, develop, validate, promote, and sustain best practice solutions for cyber defense and build and lead communities to enable an environment of trust in cyberspace&rdquo;.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/security/security-scan/","expirydate":-62135596800,"fuzzywordcount":700,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"b382875af487cd260d5a8c7a9e3ded7c","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/security/security-scan/","postref":"b382875af487cd260d5a8c7a9e3ded7c","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/docs/rancher/v2.x/en/security/security-scan/","section":"rancher","tags":null,"title":"Security Scans","type":"rancher","url":"/docs/rancher/v2.x/en/security/security-scan/","weight":1,"wordcount":653},{"authors":null,"categories":null,"content":"This section describes how to set up existing persistent storage for workloads in Rancher.\n This section assumes that you understand the Kubernetes concepts of persistent volumes and persistent volume claims. For more information, refer to the section on how storage works.\n To set up storage, follow these steps:\n Set up persistent storage in an infrastructure provider. Add a persistent volume that refers to the persistent storage. Add a persistent volume claim that refers to the persistent volume.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-admin/volumes-and-storage/attaching-existing-storage/","expirydate":-62135596800,"fuzzywordcount":1000,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"20df283310871c0733d273f582b6c43c","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/volumes-and-storage/attaching-existing-storage/","postref":"20df283310871c0733d273f582b6c43c","publishdate":"0001-01-01T00:00:00Z","readingtime":5,"relpermalink":"/docs/rancher/v2.x/en/cluster-admin/volumes-and-storage/attaching-existing-storage/","section":"rancher","tags":null,"title":"Setting up Existing Storage","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-admin/volumes-and-storage/attaching-existing-storage/","weight":1,"wordcount":919},{"authors":null,"categories":null,"content":"This section contains commands and tips for troubleshooting nodes with the etcd role.\nThis page covers the following topics:\n Checking if the etcd Container is Running etcd Container Logging etcd Cluster and Connectivity Checks  Check etcd Members on all Nodes Check Endpoint Status Check Endpoint Health Check Connectivity on Port TCP/2379 Check Connectivity on Port TCP/2380  etcd Alarms etcd Space Errors Log Level etcd Content  Watch Streaming Events Query etcd Directly  Replacing Unhealthy etcd Nodes  Checking if the etcd Container is Running The container for etcd should have status Up.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/troubleshooting/kubernetes-components/etcd/","expirydate":-62135596800,"fuzzywordcount":2200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"285e8feea78090bb5d15de4adc16101b","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/troubleshooting/kubernetes-components/etcd/","postref":"285e8feea78090bb5d15de4adc16101b","publishdate":"0001-01-01T00:00:00Z","readingtime":11,"relpermalink":"/docs/rancher/v2.x/en/troubleshooting/kubernetes-components/etcd/","section":"rancher","tags":null,"title":"Troubleshooting etcd Nodes","type":"rancher","url":"/docs/rancher/v2.x/en/troubleshooting/kubernetes-components/etcd/","weight":1,"wordcount":2189},{"authors":null,"categories":null,"content":"Rancher relies on users and groups to determine who is allowed to log in to Rancher and which resources they can access. When you configure an external authentication provider, users from that provider will be able to log in to your Rancher server. When a user logs in, the authentication provider will supply your Rancher server with a list of groups to which the user belongs.\nAccess to clusters, projects, multi-cluster apps, and global DNS providers and entries can be controlled by adding either individual users or groups to these resources.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/admin-settings/authentication/user-groups/","expirydate":-62135596800,"fuzzywordcount":900,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"f6dab07788cac69662fb3c55879adff8","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/authentication/user-groups/","postref":"f6dab07788cac69662fb3c55879adff8","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/docs/rancher/v2.x/en/admin-settings/authentication/user-groups/","section":"rancher","tags":null,"title":"Users and Groups","type":"rancher","url":"/docs/rancher/v2.x/en/admin-settings/authentication/user-groups/","weight":1,"wordcount":835},{"authors":null,"categories":null,"content":"You will need to manually enable Istio in each namespace that you want to be tracked or controlled by Istio. When Istio is enabled in a namespace, the Envoy sidecar proxy will be automatically injected into all new workloads that are deployed in the namespace.\nThis namespace setting will only affect new workloads in the namespace. Any preexisting workloads will need to be re-deployed to leverage the sidecar auto injection.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-admin/tools/istio/setup/enable-istio-in-namespace/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"d0be517253f2007697b4b4196028b297","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/tools/istio/setup/enable-istio-in-namespace/","postref":"d0be517253f2007697b4b4196028b297","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/cluster-admin/tools/istio/setup/enable-istio-in-namespace/","section":"rancher","tags":null,"title":"2. Enable Istio in a Namespace","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-admin/tools/istio/setup/enable-istio-in-namespace/","weight":2,"wordcount":312},{"authors":null,"categories":null,"content":"We recommend using Helm, a Kubernetes package manager, to install Rancher on a dedicated Kubernetes cluster. This is called a high-availability Kubernetes installation because increased availability is achieved by running Rancher on multiple nodes.\nIn a standard installation, Kubernetes is first installed on three nodes that are hosted in an infrastructure provider such as Amazon&rsquo;s EC2 or Google Compute Engine.\nThen Helm is used to install Rancher on top of the Kubernetes cluster.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/how-ha-works/","expirydate":-62135596800,"fuzzywordcount":300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"3acab8f3d91da335f22a462ec1b79d87","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/how-ha-works/","postref":"3acab8f3d91da335f22a462ec1b79d87","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/installation/how-ha-works/","section":"rancher","tags":null,"title":"About High-availability Installations","type":"rancher","url":"/docs/rancher/v2.x/en/installation/how-ha-works/","weight":2,"wordcount":280},{"authors":null,"categories":null,"content":"To keep your clusters and applications healthy and driving your organizational productivity forward, you need to stay informed of events occurring in your clusters and projects, both planned and unplanned. When an event occurs, your alert is triggered, and you are sent a notification. You can then, if necessary, follow up with corrective actions.\nNotifiers and alerts are built on top of the Prometheus Alertmanager. Leveraging these tools, Rancher can notify cluster owners and project owners of events they need to address.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-admin/tools/alerts/","expirydate":-62135596800,"fuzzywordcount":1900,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"7b071e5d9f7d7ae80b162cc0c16aadb4","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/tools/alerts/","postref":"7b071e5d9f7d7ae80b162cc0c16aadb4","publishdate":"0001-01-01T00:00:00Z","readingtime":9,"relpermalink":"/docs/rancher/v2.x/en/cluster-admin/tools/alerts/","section":"rancher","tags":null,"title":"Alerts","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-admin/tools/alerts/","weight":2,"wordcount":1833},{"authors":null,"categories":null,"content":"This section describes how to provision new persistent storage for workloads in Rancher.\n This section assumes that you understand the Kubernetes concepts of storage classes and persistent volume claims. For more information, refer to the section on how storage works.\n To provision new storage for your workloads, follow these steps:\n Add a storage class and configure it to use your storage provider. Add a persistent volume claim that refers to the storage class.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-admin/volumes-and-storage/provisioning-new-storage/","expirydate":-62135596800,"fuzzywordcount":900,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"ee6530b0b5723aa8d0fe54be7becda44","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/volumes-and-storage/provisioning-new-storage/","postref":"ee6530b0b5723aa8d0fe54be7becda44","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/docs/rancher/v2.x/en/cluster-admin/volumes-and-storage/provisioning-new-storage/","section":"rancher","tags":null,"title":"Dynamically Provisioning New Storage in Rancher","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-admin/volumes-and-storage/provisioning-new-storage/","weight":2,"wordcount":834},{"authors":null,"categories":null,"content":"In order to provision nodes with RKE, all nodes must be configured with disk UUIDs. This is required so that attached VMDKs present a consistent UUID to the VM, allowing the disk to be mounted properly.\nDepending on whether you are provisioning the VMs using the vSphere node driver in Rancher or using your own scripts or third-party tools, there are different methods available to enable disk UUIDs for VMs:","date":-62135596800,"description":"","dir":"rke/latest/en/config-options/cloud-providers/vsphere/enabling-uuid/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"218374635c19b05dbcc252d3dcea0c6a","permalink":"http://jijeesh.github.io/docs/rke/latest/en/config-options/cloud-providers/vsphere/enabling-uuid/","postref":"218374635c19b05dbcc252d3dcea0c6a","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rke/latest/en/config-options/cloud-providers/vsphere/enabling-uuid/","section":"rke","tags":null,"title":"Enabling Disk UUIDs for vSphere VMs","type":"rke","url":"/docs/rke/latest/en/config-options/cloud-providers/vsphere/enabling-uuid/","weight":2,"wordcount":198},{"authors":null,"categories":null,"content":"After Helm 3 was released, the Rancher installation instructions were updated to use Helm 3.\nIf you are using Helm 2, we recommend migrating to Helm 3 because it is simpler to use and more secure than Helm 2.\nThis section provides a copy of the older instructions for installing Rancher on a Kubernetes cluster using Helm 2 in an air air gap environment, and it is intended to be used if upgrading to Helm 3 is not feasible.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/options/air-gap-helm2/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"f975ad02b2c674268e82845b604a2c86","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/air-gap-helm2/","postref":"f975ad02b2c674268e82845b604a2c86","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/installation/options/air-gap-helm2/","section":"rancher","tags":null,"title":"Installing Rancher in an Air Gapped Environment with Helm 2","type":"rancher","url":"/docs/rancher/v2.x/en/installation/options/air-gap-helm2/","weight":2,"wordcount":383},{"authors":null,"categories":null,"content":"Node drivers are used to provision hosts, which Rancher uses to launch and manage Kubernetes clusters. A node driver is the same as a Docker Machine driver. The availability of which node driver to display when creating node templates is defined based on the node driver&rsquo;s status. Only active node drivers will be displayed as an option for creating node templates. By default, Rancher is packaged with many existing Docker Machine drivers, but you can also create custom node drivers to add to Rancher.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/admin-settings/drivers/node-drivers/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"75329fd42bc8d8b8fc58c5082af3be13","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/drivers/node-drivers/","postref":"75329fd42bc8d8b8fc58c5082af3be13","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/admin-settings/drivers/node-drivers/","section":"rancher","tags":null,"title":"Node Drivers","type":"rancher","url":"/docs/rancher/v2.x/en/admin-settings/drivers/node-drivers/","weight":2,"wordcount":326},{"authors":null,"categories":null,"content":"Although the Namespace Default Limit propagates from the project to each namespace, in some cases, you may need to increase (or decrease) the performance for a specific namespace. In this situation, you can override the default limits by editing the namespace.\nIn the diagram below, the Rancher administrator has a resource quota in effect for their project. However, the administrator wants to override the namespace limits for Namespace 3 so that it performs better.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/project-admin/resource-quotas/override-namespace-default/","expirydate":-62135596800,"fuzzywordcount":300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"6856b7abc028181c31c7697c173cc2ba","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/project-admin/resource-quotas/override-namespace-default/","postref":"6856b7abc028181c31c7697c173cc2ba","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/project-admin/resource-quotas/override-namespace-default/","section":"rancher","tags":null,"title":"Overriding the Default Limit for a Namespace","type":"rancher","url":"/docs/rancher/v2.x/en/project-admin/resource-quotas/override-namespace-default/","weight":2,"wordcount":260},{"authors":null,"categories":null,"content":"Recurring snapshots are handled differently based on your version of RKE.\n To schedule automatic recurring etcd snapshots, you can enable the etcd-snapshot service with extra configuration options. etcd-snapshot runs in a service container alongside the etcd container. By default, the etcd-snapshot service takes a snapshot for every node that has the etcd role and stores them to local disk in /opt/rke/etcd-snapshots.\nIf you set up the options for S3, the snapshot will also be uploaded to the S3 backend.","date":-62135596800,"description":"","dir":"rke/latest/en/etcd-snapshots/recurring-snapshots/","expirydate":-62135596800,"fuzzywordcount":800,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"f89cc6e7cfe36095a93a4380f609da6d","permalink":"http://jijeesh.github.io/docs/rke/latest/en/etcd-snapshots/recurring-snapshots/","postref":"f89cc6e7cfe36095a93a4380f609da6d","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/docs/rke/latest/en/etcd-snapshots/recurring-snapshots/","section":"rke","tags":null,"title":"Recurring Snapshots","type":"rke","url":"/docs/rke/latest/en/etcd-snapshots/recurring-snapshots/","weight":2,"wordcount":757},{"authors":null,"categories":null,"content":"This section describes how to enable Istio and start using it in your projects.\nThis section assumes that you have Rancher installed, and you have a Rancher-provisioned Kubernetes cluster where you would like to set up Istio.\nIf you use Istio for traffic management, you will need to allow external traffic to the cluster. In that case, you will need to follow all of the steps below.\n Quick Setup If you don&rsquo;t need external traffic to reach Istio, and you just want to set up Istio for monitoring and tracing traffic within the cluster, skip the steps for setting up the Istio gateway and setting up Istio&rsquo;s components for traffic management.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-admin/tools/istio/setup/","expirydate":-62135596800,"fuzzywordcount":300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"3bf2ce9e77f2e6ea0f8af6df1df287b1","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/tools/istio/setup/","postref":"3bf2ce9e77f2e6ea0f8af6df1df287b1","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/cluster-admin/tools/istio/setup/","section":"rancher","tags":null,"title":"Setup Guide","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-admin/tools/istio/setup/","weight":2,"wordcount":220},{"authors":null,"categories":null,"content":"This section applies to nodes with the controlplane role.\nCheck if the Controlplane Containers are Running There are three specific containers launched on nodes with the controlplane role:\n kube-apiserver kube-controller-manager kube-scheduler  The containers should have status Up. The duration shown after Up is the time the container has been running.\ndocker ps -a -f=name='kube-apiserver|kube-controller-manager|kube-scheduler'  Example output:\nCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 26c7159abbcc rancher/hyperkube:v1.11.5-rancher1 &quot;/opt/rke-tools/en.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/troubleshooting/kubernetes-components/controlplane/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"a2a79babdbb966b8426ea0ee552c08c8","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/troubleshooting/kubernetes-components/controlplane/","postref":"a2a79babdbb966b8426ea0ee552c08c8","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rancher/v2.x/en/troubleshooting/kubernetes-components/controlplane/","section":"rancher","tags":null,"title":"Troubleshooting Controlplane Nodes","type":"rancher","url":"/docs/rancher/v2.x/en/troubleshooting/kubernetes-components/controlplane/","weight":2,"wordcount":162},{"authors":null,"categories":null,"content":"Available as of v2.3.0\nThis feature enables a UI that lets you create, read, update and delete virtual services and destination rules, which are traffic management features of Istio.\n Prerequisite: Turning on this feature does not enable Istio. A cluster administrator needs to enable Istio for the cluster in order to use the feature.\n To enable or disable this feature, refer to the instructions on the main page about enabling experimental features.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/options/feature-flags/istio-virtual-service-ui/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"4f124fc6f75e00100aa9d321f5ea4d7e","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/feature-flags/istio-virtual-service-ui/","postref":"4f124fc6f75e00100aa9d321f5ea4d7e","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/installation/options/feature-flags/istio-virtual-service-ui/","section":"rancher","tags":null,"title":"UI for Istio Virtual Services and Destination Rules","type":"rancher","url":"/docs/rancher/v2.x/en/installation/options/feature-flags/istio-virtual-service-ui/","weight":2,"wordcount":336},{"authors":null,"categories":null,"content":"Available as of v2.2.0\nAfter you&rsquo;ve enabled monitoring at either the cluster level or project level, you will want to be start viewing the data being collected. There are multiple ways to view this data.\nRancher Dashboard  Note: This is only available if you&rsquo;ve enabled monitoring at the cluster level. Project specific analytics must be viewed using the project&rsquo;s Grafana instance.\n Rancher&rsquo;s dashboards are available at multiple locations:","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-admin/tools/monitoring/viewing-metrics/","expirydate":-62135596800,"fuzzywordcount":800,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"0a9be7308ac272209b8a93db206daaa6","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/tools/monitoring/viewing-metrics/","postref":"0a9be7308ac272209b8a93db206daaa6","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/docs/rancher/v2.x/en/cluster-admin/tools/monitoring/viewing-metrics/","section":"rancher","tags":null,"title":"Viewing Metrics","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-admin/tools/monitoring/viewing-metrics/","weight":2,"wordcount":796},{"authors":null,"categories":null,"content":"Prerequisite: Your cluster needs a worker node that can designated for Istio. The worker node should meet the resource requirements.\n This section describes how use node selectors to configure Istio components to be deployed on a designated node.\nIn larger deployments, it is strongly advised that Istio&rsquo;s infrastructure be placed on dedicated nodes in the cluster by adding a node selector for each Istio component.\nAdding a Label to the Istio Node First, add a label to the node where Istio components should be deployed.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-admin/tools/istio/setup/node-selectors/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"4655c9f7b94fb96277dcd9281c714a3d","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/tools/istio/setup/node-selectors/","postref":"4655c9f7b94fb96277dcd9281c714a3d","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/cluster-admin/tools/istio/setup/node-selectors/","section":"rancher","tags":null,"title":"3. Select the Nodes Where Istio Components Will be Deployed","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-admin/tools/istio/setup/node-selectors/","weight":3,"wordcount":315},{"authors":null,"categories":null,"content":"Kubernetes cluster. If you are installing Rancher on a single node, the main architecture recommendation that applies to your installation is that the node running Rancher should be separate from downstream clusters.\nThis section covers the following topics:\n Separation of Rancher and User Clusters Why HA is Better for Rancher in Production Recommended Load Balancer Configuration for Kubernetes Installations Environment for Kubernetes Installations Recommended Node Roles for Kubernetes Installations Architecture for an Authorized Cluster Endpoint  Separation of Rancher and User Clusters A user cluster is a downstream Kubernetes cluster that runs your apps and services.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/overview/architecture-recommendations/","expirydate":-62135596800,"fuzzywordcount":1000,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"647f60f83d125165de02f8ace3e5f1cf","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/overview/architecture-recommendations/","postref":"647f60f83d125165de02f8ace3e5f1cf","publishdate":"0001-01-01T00:00:00Z","readingtime":5,"relpermalink":"/docs/rancher/v2.x/en/overview/architecture-recommendations/","section":"rancher","tags":null,"title":"Architecture Recommendations","type":"rancher","url":"/docs/rancher/v2.x/en/overview/architecture-recommendations/","weight":3,"wordcount":910},{"authors":null,"categories":null,"content":"Available as of v2.2.0\nCluster metrics display the hardware utilization for all nodes in your cluster, regardless of its role. They give you a global monitoring insight into the cluster.\nSome of the biggest metrics to look out for:\n CPU Utilization\nHigh load either indicates that your cluster is running efficiently or that you&rsquo;re running out of CPU resources.\n Disk Utilization\nBe on the lookout for increased read and write rates on nodes nearing their disk capacity.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-admin/tools/monitoring/cluster-metrics/","expirydate":-62135596800,"fuzzywordcount":900,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"4ec4f42b91d97b6d1d92e8e8504dfb2d","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/tools/monitoring/cluster-metrics/","postref":"4ec4f42b91d97b6d1d92e8e8504dfb2d","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/docs/rancher/v2.x/en/cluster-admin/tools/monitoring/cluster-metrics/","section":"rancher","tags":null,"title":"Cluster Metrics","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-admin/tools/monitoring/cluster-metrics/","weight":3,"wordcount":842},{"authors":null,"categories":null,"content":"As of Rancher v2.0.4, disk UUIDs are enabled in vSphere node templates by default.\nFor Rancher prior to v2.0.4, we recommend configuring a vSphere node template to automatically enable disk UUIDs because they are required for Rancher to manipulate vSphere resources.\nTo enable disk UUIDs for all VMs created for a cluster,\n Navigate to the Node Templates in the Rancher UI while logged in as an administrator.\n Add or edit an existing vSphere node template.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-provisioning/rke-clusters/node-pools/vsphere/provisioning-vsphere-clusters/enabling-uuids/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"5140089502300da17ae20e142cc03984","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/node-pools/vsphere/provisioning-vsphere-clusters/enabling-uuids/","postref":"5140089502300da17ae20e142cc03984","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/node-pools/vsphere/provisioning-vsphere-clusters/enabling-uuids/","section":"rancher","tags":null,"title":"Enabling Disk UUIDs in Node Templates","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/node-pools/vsphere/provisioning-vsphere-clusters/enabling-uuids/","weight":3,"wordcount":106},{"authors":null,"categories":null,"content":"This section is about installations of Rancher server in an air gapped environment. An air gapped environment could be where Rancher server will be installed offline, behind a firewall, or behind a proxy.\nThroughout the installations instructions, there will be tabs for either a high availability Kubernetes installation or a single-node Docker installation.\nAir Gapped Kubernetes Installations This section covers how to install Rancher on a Kubernetes cluster in an air gapped environment.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/other-installation-methods/air-gap/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"674ce18461c766b1388d809c13aa3586","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/other-installation-methods/air-gap/","postref":"674ce18461c766b1388d809c13aa3586","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/installation/other-installation-methods/air-gap/","section":"rancher","tags":null,"title":"Installing Rancher in an Air Gapped Environment","type":"rancher","url":"/docs/rancher/v2.x/en/installation/other-installation-methods/air-gap/","weight":3,"wordcount":304},{"authors":null,"categories":null,"content":"For production environments, we recommend installing Rancher in a high-availability configuration so that your user base can always access Rancher Server. When installed in a Kubernetes cluster, Rancher will integrate with the cluster&rsquo;s etcd database and take advantage of Kubernetes scheduling for high-availability.\nThis section describes how to first use RKE to create and manage a cluster, then install Rancher onto that cluster. For this type of architecture, you will need to deploy three VMs in the infrastructure provider of your choice.","date":-62135596800,"description":"For production environments, install Rancher in a high-availability configuration. Read the guide for setting up a 3-node cluster and still install Rancher using a Helm chart.","dir":"rancher/v2.x/en/installation/k8s-install/","expirydate":-62135596800,"fuzzywordcount":700,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"fe9f613e8d2bfd1caaa338b67f0959d0","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/k8s-install/","postref":"fe9f613e8d2bfd1caaa338b67f0959d0","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/docs/rancher/v2.x/en/installation/k8s-install/","section":"rancher","tags":null,"title":"Installing Rancher on a Kubernetes Cluster","type":"rancher","url":"/docs/rancher/v2.x/en/installation/k8s-install/","weight":3,"wordcount":674},{"authors":null,"categories":null,"content":"Logging is helpful because it allows you to:\n Capture and analyze the state of your cluster Look for trends in your environment Save your logs to a safe location outside of your cluster Stay informed of events like a container crashing, a pod eviction, or a node dying More easily debug and troubleshoot problems  Rancher supports integration with the following services:\n Elasticsearch Splunk Kafka Syslog Fluentd  This section covers the following topics:","date":-62135596800,"description":"Rancher integrates with popular logging services. Learn the requirements and benefits of integrating with logging services, and enable logging on your cluster.","dir":"rancher/v2.x/en/cluster-admin/tools/logging/","expirydate":-62135596800,"fuzzywordcount":800,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"bfe4bdcd4c97efdaa0ea5202b101c246","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/tools/logging/","postref":"bfe4bdcd4c97efdaa0ea5202b101c246","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/docs/rancher/v2.x/en/cluster-admin/tools/logging/","section":"rancher","tags":null,"title":"Rancher Integration with Logging Services","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-admin/tools/logging/","weight":3,"wordcount":739},{"authors":null,"categories":null,"content":"The details of restoring your cluster from backup are different depending on your version of RKE.\n If there is a disaster with your Kubernetes cluster, you can use rke etcd snapshot-restore to recover your etcd. This command reverts etcd to a specific snapshot and should be run on an etcd node of the the specific cluster that has suffered the disaster.\nThe following actions will be performed when you run the command:","date":-62135596800,"description":"","dir":"rke/latest/en/etcd-snapshots/restoring-from-backup/","expirydate":-62135596800,"fuzzywordcount":800,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"9cc89efa79354256ea80d0d065155dff","permalink":"http://jijeesh.github.io/docs/rke/latest/en/etcd-snapshots/restoring-from-backup/","postref":"9cc89efa79354256ea80d0d065155dff","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/docs/rke/latest/en/etcd-snapshots/restoring-from-backup/","section":"rke","tags":null,"title":"Restoring from Backup","type":"rke","url":"/docs/rke/latest/en/etcd-snapshots/restoring-from-backup/","weight":3,"wordcount":705},{"authors":null,"categories":null,"content":"This section describes the permissions required to access Istio features and how to configure access to the Kiali and Jaeger visualizations.\nCluster-level Access By default, only cluster administrators can:\n Enable Istio for the cluster Configure resource allocations for Istio View each UI for Prometheus, Grafana, Kiali, and Jaeger  Project-level Access After Istio is enabled in a cluster, project owners and members have permission to:\n Enable and disable Istio sidecar auto-injection for namespaces Add the Istio sidecar to workloads View the traffic metrics and traffic graph for the cluster View the Kiali and Jaeger visualizations if cluster administrators give access to project members Configure Istio&rsquo;s resources (such as the gateway, destination rules, or virtual services) with kubectl (This does not apply to read-only project members)  Access to Visualizations By default, the Kiali and Jaeger visualizations are restricted to the cluster owner because the information in them could be sensitive.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-admin/tools/istio/rbac/","expirydate":-62135596800,"fuzzywordcount":500,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"00cafb755794ec85ad988980b40a35f0","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/tools/istio/rbac/","postref":"00cafb755794ec85ad988980b40a35f0","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/rancher/v2.x/en/cluster-admin/tools/istio/rbac/","section":"rancher","tags":null,"title":"Role-based Access Control","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-admin/tools/istio/rbac/","weight":3,"wordcount":437},{"authors":null,"categories":null,"content":"Available as of v2.2.0\nWhen setting resource quotas, if you set anything related to CPU or Memory (i.e. limits or reservations) on a project / namespace, all containers will require a respective CPU or Memory field set during creation. See the Kubernetes documentation for more details on why this is required.\nTo avoid setting these limits on each and every container during workload creation, a default container resource limit can be specified on the namespace.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/project-admin/resource-quotas/override-container-default/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"d3c388dd0dc8283dcf5fa2b2cc3b946c","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/project-admin/resource-quotas/override-container-default/","postref":"d3c388dd0dc8283dcf5fa2b2cc3b946c","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/project-admin/resource-quotas/override-container-default/","section":"rancher","tags":null,"title":"Setting Container Default Resource Limits","type":"rancher","url":"/docs/rancher/v2.x/en/project-admin/resource-quotas/override-container-default/","weight":3,"wordcount":389},{"authors":null,"categories":null,"content":"The nginx-proxy container is deployed on every node that does not have the controlplane role. It provides access to all the nodes with the controlplane role by dynamically generating the NGINX configuration based on available nodes with the controlplane role.\nCheck if the Container is Running The container is called nginx-proxy and should have status Up. The duration shown after Up is the time the container has been running.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/troubleshooting/kubernetes-components/nginx-proxy/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"7ee6077d70f1996b89da79fc651981cd","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/troubleshooting/kubernetes-components/nginx-proxy/","postref":"7ee6077d70f1996b89da79fc651981cd","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rancher/v2.x/en/troubleshooting/kubernetes-components/nginx-proxy/","section":"rancher","tags":null,"title":"Troubleshooting nginx-proxy","type":"rancher","url":"/docs/rancher/v2.x/en/troubleshooting/kubernetes-components/nginx-proxy/","weight":3,"wordcount":189},{"authors":null,"categories":null,"content":"When running larger Rancher installations with 15 or more clusters it is recommended to increase the default keyspace for etcd from the default 2GB. The maximum setting is 8GB and the host should have enough RAM to keep the entire dataset in memory. When increasing this value you should also increase the size of the host. The keyspace size can also be adjusted in smaller installations if you anticipate a high rate of change of pods during the garbage collection interval.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/options/etcd/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"cd12c77835d18f80838e94ab8e6b4198","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/etcd/","postref":"cd12c77835d18f80838e94ab8e6b4198","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/installation/options/etcd/","section":"rancher","tags":null,"title":"Tuning etcd for Large Installations","type":"rancher","url":"/docs/rancher/v2.x/en/installation/options/etcd/","weight":3,"wordcount":314},{"authors":null,"categories":null,"content":"If RancherOS has released a new version and you want to learn how to upgrade your OS, we make it easy using the ros os command.\nSince RancherOS is a kernel and initrd, the upgrade process is downloading a new kernel and initrd, and updating the boot loader to point to it. The old kernel and initrd are not removed. If there is a problem with your upgrade, you can select the old kernel from the Syslinux bootloader.","date":-62135596800,"description":"","dir":"os/v1.x/en/upgrading/","expirydate":-62135596800,"fuzzywordcount":900,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"30ffa52aa547de4502841f3a9f03da17","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/upgrading/","postref":"30ffa52aa547de4502841f3a9f03da17","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/docs/os/v1.x/en/upgrading/","section":"os","tags":null,"title":"Upgrading","type":"os","url":"/docs/os/v1.x/en/upgrading/","weight":3,"wordcount":845},{"authors":null,"categories":null,"content":"This section shows an example of how to configure the vSphere cloud provider.\nThe vSphere cloud provider must be enabled to allow dynamic provisioning of volumes.\nFor more details on deploying a Kubernetes cluster on vSphere, refer to the official cloud provider documentation.\n Note: This documentation reflects the new vSphere Cloud Provider configuration schema introduced in Kubernetes v1.9 which differs from previous versions.\n vSphere Configuration Example Given the following:","date":-62135596800,"description":"","dir":"rke/latest/en/config-options/cloud-providers/vsphere/config-reference/","expirydate":-62135596800,"fuzzywordcount":800,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"aaa8d114716e8719f58709e031a10bf0","permalink":"http://jijeesh.github.io/docs/rke/latest/en/config-options/cloud-providers/vsphere/config-reference/","postref":"aaa8d114716e8719f58709e031a10bf0","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/docs/rke/latest/en/config-options/cloud-providers/vsphere/config-reference/","section":"rke","tags":null,"title":"vSphere Configuration Reference","type":"rke","url":"/docs/rke/latest/en/config-options/cloud-providers/vsphere/config-reference/","weight":3,"wordcount":726},{"authors":null,"categories":null,"content":"Prerequisite: To enable Istio for a workload, the cluster and namespace must have Istio enabled.\n Enabling Istio in a namespace only enables automatic sidecar injection for new workloads. To enable the Envoy sidecar for existing workloads, you need to enable it manually for each workload.\nTo inject the Istio sidecar on an existing workload in the namespace, go to the workload, click the Ellipsis (&hellip;), and click Redeploy.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-admin/tools/istio/setup/deploy-workloads/","expirydate":-62135596800,"fuzzywordcount":500,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"0bd8e196e425c8529c53998d29092f1d","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/tools/istio/setup/deploy-workloads/","postref":"0bd8e196e425c8529c53998d29092f1d","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/cluster-admin/tools/istio/setup/deploy-workloads/","section":"rancher","tags":null,"title":"4. Add Deployments and Services with the Istio Sidecar","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-admin/tools/istio/setup/deploy-workloads/","weight":4,"wordcount":404},{"authors":null,"categories":null,"content":"Developing Development is easiest done with QEMU on Linux. OS X works too, although QEMU doesn&rsquo;t have KVM support. If you are running Linux in a virtual machine, then we recommend you run VMWare Fusion/Workstation and enable VT-x support. Then, QEMU will have KVM support and run sufficiently fast inside your Linux VM.\nBuilding Requirements:  bash make Docker 1.10.3+  $ make  The build will run in Docker containers, and when the build is done, the vmlinuz, initrd, and ISO should be in dist/artifacts.","date":-62135596800,"description":"","dir":"os/v1.x/en/about/","expirydate":-62135596800,"fuzzywordcount":500,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"56a0b24be9898c87fdaf6e996e78f3ec","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/about/","postref":"56a0b24be9898c87fdaf6e996e78f3ec","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/os/v1.x/en/about/","section":"os","tags":null,"title":"About","type":"os","url":"/docs/os/v1.x/en/about/","weight":4,"wordcount":443},{"authors":null,"categories":null,"content":"This section describes how to disable Istio in a cluster, namespace, or workload.\nDisable Istio in a Cluster To disable Istio,\n From the Global view, navigate to the cluster that you want to disable Istio for. Click Tools &gt; Istio. Click Disable, then click the red button again to confirm the disable action.  Result: The cluster-istio application in the cluster&rsquo;s system project gets removed. The Istio sidecar cannot be deployed on any workloads in the cluster.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-admin/tools/istio/disabling-istio/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"9d4793f1f53aecb05fb22be4ece5609a","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/tools/istio/disabling-istio/","postref":"9d4793f1f53aecb05fb22be4ece5609a","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rancher/v2.x/en/cluster-admin/tools/istio/disabling-istio/","section":"rancher","tags":null,"title":"Disabling Istio","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-admin/tools/istio/disabling-istio/","weight":4,"wordcount":177},{"authors":null,"categories":null,"content":"These example scenarios for backup and restore are different based on your version of RKE.\n This walkthrough will demonstrate how to restore an etcd cluster from a local snapshot with the following steps:\n Back up the cluster Simulate a node failure Add a new etcd node to the cluster Restore etcd on the new node from the backup Confirm that cluster operations are restored  In this example, the Kubernetes cluster was deployed on two AWS nodes.","date":-62135596800,"description":"","dir":"rke/latest/en/etcd-snapshots/example-scenarios/","expirydate":-62135596800,"fuzzywordcount":1000,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"d53c9b6d5a4613ebae438c2ab1090dab","permalink":"http://jijeesh.github.io/docs/rke/latest/en/etcd-snapshots/example-scenarios/","postref":"d53c9b6d5a4613ebae438c2ab1090dab","publishdate":"0001-01-01T00:00:00Z","readingtime":5,"relpermalink":"/docs/rke/latest/en/etcd-snapshots/example-scenarios/","section":"rke","tags":null,"title":"Example Scenarios","type":"rke","url":"/docs/rke/latest/en/etcd-snapshots/example-scenarios/","weight":4,"wordcount":938},{"authors":null,"categories":null,"content":"In This Document  Cluster Metrics  Node Metrics  Etcd Metrics Kubernetes Components Metrics Rancher Logging Metrics Workload Metrics  Pod Metrics Container Metrics   Cluster Metrics  CPU Utilization\n   Catalog Expression     Detail 1 - (avg(irate(node_cpu_seconds_total{mode=&quot;idle&quot;}[5m])) by (instance))   Summary 1 - (avg(irate(node_cpu_seconds_total{mode=&quot;idle&quot;}[5m])))    Load Average\n   Catalog Expression     Detail load1sum(node_load1) by (instance) / count(node_cpu_seconds_total{mode=&quot;system&quot;}) by (instance)load5sum(node_load5) by (instance) / count(node_cpu_seconds_total{mode=&quot;system&quot;}) by (instance)load15sum(node_load15) by (instance) / count(node_cpu_seconds_total{mode=&quot;system&quot;}) by (instance)   Summary load1sum(node_load1) by (instance) / count(node_cpu_seconds_total{mode=&quot;system&quot;})load5sum(node_load5) by (instance) / count(node_cpu_seconds_total{mode=&quot;system&quot;})load15sum(node_load15) by (instance) / count(node_cpu_seconds_total{mode=&quot;system&quot;})    Memory Utilization","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-admin/tools/monitoring/expression/","expirydate":-62135596800,"fuzzywordcount":1200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"cc633033db28ccda9b76c61e92097056","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/tools/monitoring/expression/","postref":"cc633033db28ccda9b76c61e92097056","publishdate":"0001-01-01T00:00:00Z","readingtime":6,"relpermalink":"/docs/rancher/v2.x/en/cluster-admin/tools/monitoring/expression/","section":"rancher","tags":null,"title":"Expression","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-admin/tools/monitoring/expression/","weight":4,"wordcount":1123},{"authors":null,"categories":null,"content":"How Do I Know if My Certificates are in PEM Format? You can recognize the PEM format by the following traits:\n The file begins with the following header:\n-----BEGIN CERTIFICATE----- The header is followed by a long string of characters. Like, really long. The file ends with a footer:\n-----END CERTIFICATE-----  PEM Certificate Example:\n----BEGIN CERTIFICATE----- MIIGVDCCBDygAwIBAgIJAMiIrEm29kRLMA0GCSqGSIb3DQEBCwUAMHkxCzAJBgNV ... more lines VWQqljhfacYPgp8KJUJENQ9h5hZ2nSCrI+W00Jcw4QcEdCI8HL5wmg== -----END CERTIFICATE-----  PEM Certificate Key Example:","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/other-installation-methods/single-node-docker/troubleshooting/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"c61ce70edb92367785e88656f05475d7","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/other-installation-methods/single-node-docker/troubleshooting/","postref":"c61ce70edb92367785e88656f05475d7","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/installation/other-installation-methods/single-node-docker/troubleshooting/","section":"rancher","tags":null,"title":"FAQ and Troubleshooting","type":"rancher","url":"/docs/rancher/v2.x/en/installation/other-installation-methods/single-node-docker/troubleshooting/","weight":4,"wordcount":311},{"authors":null,"categories":null,"content":"Available as of v2.2.0\nUsing Rancher, you can monitor the state and processes of your cluster nodes, Kubernetes components, and software deployments through integration with Prometheus, a leading open-source monitoring solution.\nThis section covers the following topics:\n About Prometheus Monitoring scope Enabling cluster monitoring Resource consumption  Resource consumption of Prometheus pods Resource consumption of other pods   About Prometheus Prometheus provides a time series of your data, which is, according to Prometheus documentation:","date":-62135596800,"description":"Prometheus lets you view metrics from your different Rancher and Kubernetes objects. Learn about the scope of monitoring and how to enable cluster monitoring","dir":"rancher/v2.x/en/cluster-admin/tools/monitoring/","expirydate":-62135596800,"fuzzywordcount":800,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"ff4e98989a906389f459230f4b38c4bd","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/tools/monitoring/","postref":"ff4e98989a906389f459230f4b38c4bd","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/docs/rancher/v2.x/en/cluster-admin/tools/monitoring/","section":"rancher","tags":null,"title":"Integrating Rancher and Prometheus for Cluster Monitoring","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-admin/tools/monitoring/","weight":4,"wordcount":777},{"authors":null,"categories":null,"content":"This page explains concepts related to Kubernetes that are important for understanding how Rancher works. The descriptions below provide a simplified interview of Kubernetes components. For more details, refer to the official documentation on Kubernetes components.\nThis section covers the following topics:\n About Docker About Kubernetes What is a Kubernetes Cluster? Roles for Nodes in Kubernetes Clusters  etcd Nodes Controlplane Nodes Worker Nodes  About Helm  About Docker Docker is the container packaging and runtime standard.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/overview/concepts/","expirydate":-62135596800,"fuzzywordcount":700,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"6a2b5114d36d29dfa7e4fdf407a15fbf","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/overview/concepts/","postref":"6a2b5114d36d29dfa7e4fdf407a15fbf","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/docs/rancher/v2.x/en/overview/concepts/","section":"rancher","tags":null,"title":"Kubernetes Concepts","type":"rancher","url":"/docs/rancher/v2.x/en/overview/concepts/","weight":4,"wordcount":662},{"authors":null,"categories":null,"content":"Docker Installations The single-node Docker installation is for Rancher users that are wanting to test out Rancher. Instead of running on a Kubernetes cluster using Helm, you install the Rancher server component on a single node using a docker run command.\nSince there is only one node and a single Docker container, if the node goes down, there is no copy of the etcd data available on other nodes and you will lose all the data of your Rancher server.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/other-installation-methods/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"60b1f91813abf0faa16790774c0c807c","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/other-installation-methods/","postref":"60b1f91813abf0faa16790774c0c807c","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rancher/v2.x/en/installation/other-installation-methods/","section":"rancher","tags":null,"title":"Other Installation Methods","type":"rancher","url":"/docs/rancher/v2.x/en/installation/other-installation-methods/","weight":4,"wordcount":116},{"authors":null,"categories":null,"content":"When you create a resource quota, you are configuring the pool of resources available to the project. You can set the following resource limits for the following resource types.\n   Resource Type Description     CPU Limit* The maximum amount of CPU (in millicores) allocated to the project/namespace.1   CPU Reservation* The minimum amount of CPU (in millicores) guaranteed to the project/namespace.1   Memory Limit* The maximum amount of memory (in bytes) allocated to the project/namespace.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/project-admin/resource-quotas/quota-type-reference/","expirydate":-62135596800,"fuzzywordcount":300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"b990654f4de8cbb6f00a5b04c4d3f72b","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/project-admin/resource-quotas/quota-type-reference/","postref":"b990654f4de8cbb6f00a5b04c4d3f72b","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/project-admin/resource-quotas/quota-type-reference/","section":"rancher","tags":null,"title":"Resource Quota Type Reference","type":"rancher","url":"/docs/rancher/v2.x/en/project-admin/resource-quotas/quota-type-reference/","weight":4,"wordcount":299},{"authors":null,"categories":null,"content":"This section applies to every node as it includes components that run on nodes with any role.\nCheck if the Containers are Running There are three specific containers launched on nodes with the controlplane role:\n kubelet kube-proxy  The containers should have status Up. The duration shown after Up is the time the container has been running.\ndocker ps -a -f=name='kubelet|kube-proxy'  Example output:\nCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 158d0dcc33a5 rancher/hyperkube:v1.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/troubleshooting/kubernetes-components/worker-and-generic/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"fe7604cfa01bfff3ad81845cef0ed341","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/troubleshooting/kubernetes-components/worker-and-generic/","postref":"fe7604cfa01bfff3ad81845cef0ed341","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rancher/v2.x/en/troubleshooting/kubernetes-components/worker-and-generic/","section":"rancher","tags":null,"title":"Troubleshooting Worker Nodes and Generic Components","type":"rancher","url":"/docs/rancher/v2.x/en/troubleshooting/kubernetes-components/worker-and-generic/","weight":4,"wordcount":112},{"authors":null,"categories":null,"content":"If you are experiencing issues while provisioning a cluster with enabled vSphere Cloud Provider or while creating vSphere volumes for your workloads, you should inspect the logs of the following K8s services:\n controller-manager (Manages volumes in vCenter) kubelet: (Mounts vSphere volumes to pods)  If your cluster is not configured with external Cluster Logging, you will need to SSH into nodes to get the logs of the kube-controller-manager (running on one of the control plane nodes) and the kubelet (pertaining to the node where the stateful pod has been scheduled).","date":-62135596800,"description":"","dir":"rke/latest/en/config-options/cloud-providers/vsphere/troubleshooting/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"c3409d0f51886e47bc88dfd72962fb5f","permalink":"http://jijeesh.github.io/docs/rke/latest/en/config-options/cloud-providers/vsphere/troubleshooting/","postref":"c3409d0f51886e47bc88dfd72962fb5f","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rke/latest/en/config-options/cloud-providers/vsphere/troubleshooting/","section":"rke","tags":null,"title":"Troubleshooting vSphere Clusters","type":"rke","url":"/docs/rke/latest/en/config-options/cloud-providers/vsphere/troubleshooting/","weight":4,"wordcount":158},{"authors":null,"categories":null,"content":"The tables below describe the configuration options available in the vSphere node template:\n Account access Instance options Scheduling options  Account Access The account access parameters are different based on the Rancher version.\n    Parameter Required Description     Cloud Credentials * Your vSphere account access information, stored in a cloud credential.        Parameter Required Description     vCenter or ESXi Server * IP or FQDN of the vCenter or ESXi server used for managing VMs.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-provisioning/rke-clusters/node-pools/vsphere/provisioning-vsphere-clusters/node-template-reference/","expirydate":-62135596800,"fuzzywordcount":700,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"58fa36d188f83f3f2ab613cd37083f28","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/node-pools/vsphere/provisioning-vsphere-clusters/node-template-reference/","postref":"58fa36d188f83f3f2ab613cd37083f28","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/node-pools/vsphere/provisioning-vsphere-clusters/node-template-reference/","section":"rancher","tags":null,"title":"vSphere Node Template Configuration Reference","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/node-pools/vsphere/provisioning-vsphere-clusters/node-template-reference/","weight":4,"wordcount":685},{"authors":null,"categories":null,"content":"The gateway to each cluster can have its own port or load balancer, which is unrelated to a service mesh. By default, each Rancher-provisioned cluster has one NGINX ingress controller allowing traffic into the cluster.\nYou can use the NGINX ingress controller with or without Istio installed. If this is the only gateway to your cluster, Istio will be able to route traffic from service to service, but Istio will not be able to receive traffic from outside the cluster.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-admin/tools/istio/setup/gateway/","expirydate":-62135596800,"fuzzywordcount":800,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"32402e0a179f04c0acc001ed054685bc","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/tools/istio/setup/gateway/","postref":"32402e0a179f04c0acc001ed054685bc","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/docs/rancher/v2.x/en/cluster-admin/tools/istio/setup/gateway/","section":"rancher","tags":null,"title":"5. Set up the Istio Gateway","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-admin/tools/istio/setup/gateway/","weight":5,"wordcount":745},{"authors":null,"categories":null,"content":"When installing Rancher, there are several advanced options that can be enabled:\n Custom CA Certificate API Audit Log TLS Settings Air Gap Persistent Data Running rancher/rancher and rancher/rancher-agent on the Same Node  Custom CA Certificate If you want to configure Rancher to use a CA root certificate to be used when validating services, you would start the Rancher container sharing the directory that contains the CA root certificate.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/other-installation-methods/single-node-docker/advanced/","expirydate":-62135596800,"fuzzywordcount":600,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"ef24cf03073b0102d091ebecca3c29f1","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/other-installation-methods/single-node-docker/advanced/","postref":"ef24cf03073b0102d091ebecca3c29f1","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/rancher/v2.x/en/installation/other-installation-methods/single-node-docker/advanced/","section":"rancher","tags":null,"title":"Advanced Options for Docker Installs","type":"rancher","url":"/docs/rancher/v2.x/en/installation/other-installation-methods/single-node-docker/advanced/","weight":5,"wordcount":586},{"authors":null,"categories":null,"content":"These example scenarios describe how an organization could use templates to standardize cluster creation.\n Enforcing templates: Administrators might want to enforce one or more template settings for everyone if they want all new Rancher-provisioned clusters to have those settings. Sharing different templates with different users: Administrators might give different templates to basic and advanced users, so that basic users have more restricted options and advanced users have more discretion when creating clusters.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/admin-settings/rke-templates/example-scenarios/","expirydate":-62135596800,"fuzzywordcount":1100,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"88e0d452d01ec6e7cb391c86ac3b808c","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/rke-templates/example-scenarios/","postref":"88e0d452d01ec6e7cb391c86ac3b808c","publishdate":"0001-01-01T00:00:00Z","readingtime":5,"relpermalink":"/docs/rancher/v2.x/en/admin-settings/rke-templates/example-scenarios/","section":"rancher","tags":null,"title":"Example Scenarios","type":"rancher","url":"/docs/rancher/v2.x/en/admin-settings/rke-templates/example-scenarios/","weight":5,"wordcount":1000},{"authors":null,"categories":null,"content":"Important: RKE add-on install is only supported up to Rancher v2.0.8 Please use the Rancher Helm chart to install Rancher on a Kubernetes cluster. For details, see the Kubernetes Install - Installation Outline.\nIf you are currently using the RKE add-on install method, see Migrating from a Kubernetes Install with an RKE Add-on for details on how to move to using the helm chart.\n Below are steps that you can follow to determine what is wrong in your cluster.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/options/helm2/rke-add-on/troubleshooting/generic-troubleshooting/","expirydate":-62135596800,"fuzzywordcount":800,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"da6e48d9fcdac9442c7ce5297d0cbcf0","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/helm2/rke-add-on/troubleshooting/generic-troubleshooting/","postref":"da6e48d9fcdac9442c7ce5297d0cbcf0","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/docs/rancher/v2.x/en/installation/options/helm2/rke-add-on/troubleshooting/generic-troubleshooting/","section":"rancher","tags":null,"title":"Generic troubleshooting","type":"rancher","url":"/docs/rancher/v2.x/en/installation/options/helm2/rke-add-on/troubleshooting/generic-troubleshooting/","weight":5,"wordcount":709},{"authors":null,"categories":null,"content":"Important: RKE add-on install is only supported up to Rancher v2.0.8 Please use the Rancher helm chart to install Rancher on a Kubernetes cluster. For details, see the Kubernetes Install - Installation Outline.\nIf you are currently using the RKE add-on install method, see Migrating from a Kubernetes Install with an RKE Add-on for details on how to move to using the helm chart.\n Below are steps that you can follow to determine what is wrong in your cluster.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/options/rke-add-on/troubleshooting/generic-troubleshooting/","expirydate":-62135596800,"fuzzywordcount":800,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"a6cbcf59cf16789f97424e0e6e9591cb","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/rke-add-on/troubleshooting/generic-troubleshooting/","postref":"a6cbcf59cf16789f97424e0e6e9591cb","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/docs/rancher/v2.x/en/installation/options/rke-add-on/troubleshooting/generic-troubleshooting/","section":"rancher","tags":null,"title":"Generic troubleshooting","type":"rancher","url":"/docs/rancher/v2.x/en/installation/options/rke-add-on/troubleshooting/generic-troubleshooting/","weight":5,"wordcount":709},{"authors":null,"categories":null,"content":"Available as of v2.3.0\nIstio is an open-source tool that makes it easier for DevOps teams to observe, control, troubleshoot, and secure the traffic within a complex network of microservices.\nAs a network of microservices changes and grows, the interactions between them can become more difficult to manage and understand. In such a situation, it is useful to have a service mesh as a separate infrastructure layer. Istio&rsquo;s service mesh lets you manipulate traffic between microservices without changing the microservices directly.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-admin/tools/istio/","expirydate":-62135596800,"fuzzywordcount":1000,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"795176f80950b4125b6150aa24c5507c","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/tools/istio/","postref":"795176f80950b4125b6150aa24c5507c","publishdate":"0001-01-01T00:00:00Z","readingtime":5,"relpermalink":"/docs/rancher/v2.x/en/cluster-admin/tools/istio/","section":"rancher","tags":null,"title":"Istio","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-admin/tools/istio/","weight":5,"wordcount":914},{"authors":null,"categories":null,"content":"In this section:\n Operating System\n General Linux Requirements Red Hat Enterprise Linux (RHEL) / Oracle Enterprise Linux (OEL) / CentOS\n Using upstream Docker Using RHEL/CentOS packaged Docker  Notes about Atomic Nodes\n OpenSSH version Creating a Docker Group   Software\n Ports\n Opening port TCP/6443 using iptables Opening port TCP/6443 using firewalld  SSH Server Configuration\n  Operating System General Linux Requirements RKE runs on almost any Linux OS with Docker installed.","date":-62135596800,"description":"","dir":"rke/latest/en/os/","expirydate":-62135596800,"fuzzywordcount":1700,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"fb141bb2eb74f8adcc69bca54dca556c","permalink":"http://jijeesh.github.io/docs/rke/latest/en/os/","postref":"fb141bb2eb74f8adcc69bca54dca556c","publishdate":"0001-01-01T00:00:00Z","readingtime":8,"relpermalink":"/docs/rke/latest/en/os/","section":"rke","tags":null,"title":"Requirements","type":"rke","url":"/docs/rke/latest/en/os/","weight":5,"wordcount":1652},{"authors":null,"categories":null,"content":"When installing Rancher, there are several advanced options that can be enabled during installation. Within each install guide, these options are presented. Learn more about these options:\n   Advanced Option Available as of     Custom CA Certificate v2.0.0   API Audit Log v2.0.0   TLS Settings v2.1.7   etcd configuration v2.2.0   Local System Charts for Air Gap Installations v2.3.0    ","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/options/","expirydate":-62135596800,"fuzzywordcount":100,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"5bf92da126d1a47d11cadb5d855ea395","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/","postref":"5bf92da126d1a47d11cadb5d855ea395","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rancher/v2.x/en/installation/options/","section":"rancher","tags":null,"title":"Resources, References, and Advanced Options","type":"rancher","url":"/docs/rancher/v2.x/en/installation/options/","weight":5,"wordcount":54},{"authors":null,"categories":null,"content":"As of v0.1.9, the rke-bundle-cert container is removed on both success and failure of a restore. To debug any issues, you will need to look at the logs generated from rke.\nAs of v0.1.8 and below, the rke-bundle-cert container is left over from a failed etcd restore. If you are having an issue with restoring an etcd snapshot then you can do the following on each etcd nodes before attempting to do another restore:","date":-62135596800,"description":"","dir":"rke/latest/en/etcd-snapshots/troubleshooting/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"5666a5ce666365de523c0fc6e85edf06","permalink":"http://jijeesh.github.io/docs/rke/latest/en/etcd-snapshots/troubleshooting/","postref":"5666a5ce666365de523c0fc6e85edf06","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rke/latest/en/etcd-snapshots/troubleshooting/","section":"rke","tags":null,"title":"Troubleshooting","type":"rke","url":"/docs/rke/latest/en/etcd-snapshots/troubleshooting/","weight":5,"wordcount":145},{"authors":null,"categories":null,"content":"A central advantage of traffic management in Istio is that it allows dynamic request routing. Some common applications for dynamic request routing include canary deployments and blue/green deployments. The two key resources in Istio traffic management are virtual services and destination rules.\n Virtual services intercept and direct traffic to your Kubernetes services, allowing you to divide percentages of traffic from a request to different services. You can use them to define a set of routing rules to apply when a host is addressed.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-admin/tools/istio/setup/set-up-traffic-management/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"2e8a2c0c93e2c1493efe8a2ac1939167","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/tools/istio/setup/set-up-traffic-management/","postref":"2e8a2c0c93e2c1493efe8a2ac1939167","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/cluster-admin/tools/istio/setup/set-up-traffic-management/","section":"rancher","tags":null,"title":"6. Set up Istio's Components for Traffic Management","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-admin/tools/istio/setup/set-up-traffic-management/","weight":6,"wordcount":353},{"authors":null,"categories":null,"content":"This section describes how to view the traffic that is being managed by Istio.\nThe Kiali Traffic Graph Rancher integrates a Kiali graph into the Rancher UI. The Kiali graph provides a powerful way to visualize the topology of your Istio service mesh. It shows you which services communicate with each other.\nTo see the traffic graph,\n From the project view in Rancher, click Resources &gt; Istio. Go to the Traffic Graph tab.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-admin/tools/istio/setup/view-traffic/","expirydate":-62135596800,"fuzzywordcount":300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"c284397506de9510db1e71da809829fa","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/tools/istio/setup/view-traffic/","postref":"c284397506de9510db1e71da809829fa","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/cluster-admin/tools/istio/setup/view-traffic/","section":"rancher","tags":null,"title":"7. Generate and View Traffic","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-admin/tools/istio/setup/view-traffic/","weight":7,"wordcount":235},{"authors":null,"categories":null,"content":"This guide will help you quickly launch a cluster with default options. The installation section covers in greater detail how K3s can be set up.\nFor information on how K3s components work together, refer to the architecture section.\n New to Kubernetes? The official Kubernetes docs already have some great tutorials outlining the basics here.\n Install Script K3s provides an installation script that is a convenient way to install it as a service on systemd or openrc based systems.","date":-62135596800,"description":"","dir":"k3s/latest/en/quick-start/","expirydate":-62135596800,"fuzzywordcount":300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"b492db157d4564d8ce87f3dfcffb718c","permalink":"http://jijeesh.github.io/docs/k3s/latest/en/quick-start/","postref":"b492db157d4564d8ce87f3dfcffb718c","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/k3s/latest/en/quick-start/","section":"k3s","tags":null,"title":"Quick-Start Guide","type":"k3s","url":"/docs/k3s/latest/en/quick-start/","weight":10,"wordcount":266},{"authors":null,"categories":null,"content":"Administrators have the permission to create RKE templates, and only administrators can give that permission to other users.\nFor more information on administrator permissions, refer to the documentation on global permissions.\nGiving Users Permission to Create Templates Templates can only be created by users who have the global permission Create RKE Templates.\nAdministrators have the global permission to create templates, and only administrators can give that permission to other users.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/admin-settings/rke-templates/creator-permissions/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"01f60dd5678e3d86954b636b866900d8","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/rke-templates/creator-permissions/","postref":"01f60dd5678e3d86954b636b866900d8","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/admin-settings/rke-templates/creator-permissions/","section":"rancher","tags":null,"title":"Template Creator Permissions","type":"rancher","url":"/docs/rancher/v2.x/en/admin-settings/rke-templates/creator-permissions/","weight":10,"wordcount":377},{"authors":null,"categories":null,"content":"You can upgrade K3s by using the installation script, or by manually installing the binary of the desired version.\n Note: When upgrading, upgrade server nodes first one at a time, then any worker nodes.\n Upgrade K3s Using the Installation Script To upgrade K3s from an older version you can re-run the installation script using the same flags, for example:\ncurl -sfL https://get.k3s.io | sh - If you want to upgrade to specific version you can run the following command:","date":-62135596800,"description":"","dir":"k3s/latest/en/upgrades/basic/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"404392b559b3a865117cf7f2b0e02236","permalink":"http://jijeesh.github.io/docs/k3s/latest/en/upgrades/basic/","postref":"404392b559b3a865117cf7f2b0e02236","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/k3s/latest/en/upgrades/basic/","section":"k3s","tags":null,"title":"Upgrade Basics","type":"k3s","url":"/docs/k3s/latest/en/upgrades/basic/","weight":10,"wordcount":161},{"authors":null,"categories":null,"content":"Note: This feature is available as of v1.17.4+k3s1\n Overview You can manage K3s cluster upgrades using Rancher&rsquo;s system-upgrade-controller. This is a Kubernetes-native approach to cluster upgrades. It leverages a custom resource definition (CRD), the plan, and a controller that schedules upgrades based on the configured plans.\nA plan defines upgrade policies and requirements. This documentation will provide plans with defaults appropriate for upgrading a K3s cluster. For more advanced plan configuration options, please review the CRD.","date":-62135596800,"description":"","dir":"k3s/latest/en/upgrades/automated/","expirydate":-62135596800,"fuzzywordcount":700,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"402252dbbed67c2351ae0ad3802bb155","permalink":"http://jijeesh.github.io/docs/k3s/latest/en/upgrades/automated/","postref":"402252dbbed67c2351ae0ad3802bb155","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/docs/k3s/latest/en/upgrades/automated/","section":"k3s","tags":null,"title":"Automated Upgrades","type":"k3s","url":"/docs/k3s/latest/en/upgrades/automated/","weight":20,"wordcount":665},{"authors":null,"categories":null,"content":"Important: RKE add-on install is only supported up to Rancher v2.0.8 Please use the Rancher Helm chart to install Rancher on a Kubernetes cluster. For details, see the Kubernetes Install - Installation Outline.\nIf you are currently using the RKE add-on install method, see Migrating from a Kubernetes Install with an RKE Add-on for details on how to move to using the helm chart.\n To debug issues around this error, you will need to download the command-line tool kubectl.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/options/helm2/rke-add-on/troubleshooting/job-complete-status/","expirydate":-62135596800,"fuzzywordcount":500,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"21b481300426eca052a0472eb953ac77","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/helm2/rke-add-on/troubleshooting/job-complete-status/","postref":"21b481300426eca052a0472eb953ac77","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/rancher/v2.x/en/installation/options/helm2/rke-add-on/troubleshooting/job-complete-status/","section":"rancher","tags":null,"title":"Failed to get job complete status","type":"rancher","url":"/docs/rancher/v2.x/en/installation/options/helm2/rke-add-on/troubleshooting/job-complete-status/","weight":20,"wordcount":433},{"authors":null,"categories":null,"content":"Important: RKE add-on install is only supported up to Rancher v2.0.8 Please use the Rancher helm chart to install Rancher on a Kubernetes cluster. For details, see the Kubernetes Install - Installation Outline.\nIf you are currently using the RKE add-on install method, see Migrating from a Kubernetes Install with an RKE Add-on for details on how to move to using the helm chart.\n To debug issues around this error, you will need to download the command-line tool kubectl.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/options/rke-add-on/troubleshooting/job-complete-status/","expirydate":-62135596800,"fuzzywordcount":500,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"d5c84037c05ead58d8e2e78134d33a1d","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/rke-add-on/troubleshooting/job-complete-status/","postref":"d5c84037c05ead58d8e2e78134d33a1d","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/rancher/v2.x/en/installation/options/rke-add-on/troubleshooting/job-complete-status/","section":"rancher","tags":null,"title":"Failed to get job complete status","type":"rancher","url":"/docs/rancher/v2.x/en/installation/options/rke-add-on/troubleshooting/job-complete-status/","weight":20,"wordcount":433},{"authors":null,"categories":null,"content":"This section contains instructions for installing K3s in various environments. Please ensure you have met the Installation Requirements before you begin installing K3s.\nInstallation and Configuration Options provides guidance on the options available to you when installing K3s.\nHigh Availability with an External DB details how to set up an HA K3s cluster backed by an external datastore such as MySQL, PostgreSQL, or etcd.\nHigh Availability with Embedded DB (Experimental) details how to set up an HA K3s cluster that leverages a built-in distributed database.","date":-62135596800,"description":"","dir":"k3s/latest/en/installation/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"d59c468f13321097acde0e673611ebb8","permalink":"http://jijeesh.github.io/docs/k3s/latest/en/installation/","postref":"d59c468f13321097acde0e673611ebb8","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/k3s/latest/en/installation/","section":"k3s","tags":null,"title":"Installation","type":"k3s","url":"/docs/k3s/latest/en/installation/","weight":20,"wordcount":135},{"authors":null,"categories":null,"content":"This page focuses on the options that can be used when you set up K3s for the first time:\n Installation script options Installing K3s from the binary Registration options for the K3s server Registration options for the K3s agent  For more advanced options, refer to this page.\nInstallation Script Options As mentioned in the Quick-Start Guide, you can use the installation script available at https://get.k3s.io to install K3s as a service on systemd and openrc based systems.","date":-62135596800,"description":"","dir":"k3s/latest/en/installation/install-options/","expirydate":-62135596800,"fuzzywordcount":1700,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"c8929255ef751ae1d0b48d896700aaf5","permalink":"http://jijeesh.github.io/docs/k3s/latest/en/installation/install-options/","postref":"c8929255ef751ae1d0b48d896700aaf5","publishdate":"0001-01-01T00:00:00Z","readingtime":8,"relpermalink":"/docs/k3s/latest/en/installation/install-options/","section":"k3s","tags":null,"title":"Installation Options","type":"k3s","url":"/docs/k3s/latest/en/installation/install-options/","weight":20,"wordcount":1609},{"authors":null,"categories":null,"content":"The kubeconfig file is used to configure access to the Kubernetes cluster. It is required to be set up properly in order to access the Kubernetes API such as with kubectl or for installing applications with Helm. You may set the kubeconfig by either exporting the KUBECONFIG environment variable or by specifying a flag for kubectl and helm. Refer to the examples below for details.\nLeverage the KUBECONFIG environment variable:","date":-62135596800,"description":"","dir":"k3s/latest/en/cluster-access/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"69589867793601079ff7dd4bbfb3c661","permalink":"http://jijeesh.github.io/docs/k3s/latest/en/cluster-access/","postref":"69589867793601079ff7dd4bbfb3c661","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/k3s/latest/en/cluster-access/","section":"k3s","tags":null,"title":"Cluster Access","type":"k3s","url":"/docs/k3s/latest/en/cluster-access/","weight":21,"wordcount":137},{"authors":null,"categories":null,"content":"Get started with your migration to Rancher v2.x by installing Rancher and configuring your new Rancher environment.\nOutline  A. Install Rancher v2.x B. Configure Authentication C. Provision a Cluster and Project D. Create Stacks  A. Install Rancher v2.x The first step in migrating from v1.6 to v2.x is to install the Rancher v2.x Server side-by-side with your v1.6 Server, as you&rsquo;ll need your old install during the migration process.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/v1.6-migration/get-started/","expirydate":-62135596800,"fuzzywordcount":1000,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"7a990fcbc331890d82699e3752e40014","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/v1.6-migration/get-started/","postref":"7a990fcbc331890d82699e3752e40014","publishdate":"0001-01-01T00:00:00Z","readingtime":5,"relpermalink":"/docs/rancher/v2.x/en/v1.6-migration/get-started/","section":"rancher","tags":null,"title":"1. Get Started","type":"rancher","url":"/docs/rancher/v2.x/en/v1.6-migration/get-started/","weight":25,"wordcount":934},{"authors":null,"categories":null,"content":"After completing your Docker installation of Rancher, we recommend creating backups of it on a regular basis. Having a recent backup will let you recover quickly from an unexpected disaster.\nBefore You Start During the creation of your backup, you&rsquo;ll enter a series of commands, replacing placeholders with data from your environment. These placeholders are denoted with angled brackets and all capital letters (&lt;EXAMPLE&gt;). Here&rsquo;s an example of a command with a placeholder:","date":-62135596800,"description":"","dir":"rancher/v2.x/en/backups/backups/single-node-backups/","expirydate":-62135596800,"fuzzywordcount":500,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"2f9fee7e4eb61f9b20044a4e03735ccd","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/backups/backups/single-node-backups/","postref":"2f9fee7e4eb61f9b20044a4e03735ccd","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/rancher/v2.x/en/backups/backups/single-node-backups/","section":"rancher","tags":null,"title":"Creating Backups for Rancher Installed with Docker","type":"rancher","url":"/docs/rancher/v2.x/en/backups/backups/single-node-backups/","weight":25,"wordcount":436},{"authors":null,"categories":null,"content":"Note: Please reference the Networking page for information about CoreDNS, Traefik, and the Service LB.\n By default, K3s will run with flannel as the CNI, using VXLAN as the default backend. To change the CNI, refer to the section on configuring a custom CNI. To change the flannel backend, refer to the flannel options section.\nFlannel Options The default backend for flannel is VXLAN. To enable encryption, pass the IPSec (Internet Protocol Security) or WireGuard options below.","date":-62135596800,"description":"","dir":"k3s/latest/en/installation/network-options/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"e14d4e3c55635cdd279b31acf91073ca","permalink":"http://jijeesh.github.io/docs/k3s/latest/en/installation/network-options/","postref":"e14d4e3c55635cdd279b31acf91073ca","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/k3s/latest/en/installation/network-options/","section":"k3s","tags":null,"title":"Network Options","type":"k3s","url":"/docs/k3s/latest/en/installation/network-options/","weight":25,"wordcount":323},{"authors":null,"categories":null,"content":"Note: The intent of these guides is to quickly launch a sandbox that you can use to evaluate Rancher. These guides are not intended for production environments. For comprehensive setup instructions, see Installation.\n Howdy buckaroos! Use this section of the docs to jump start your deployment and testing of Rancher 2.x! It contains instructions for a simple Rancher setup and some common use cases. We plan on adding more content to this section in the future.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/quick-start-guide/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"0a95c8a6e8dbe56b71a2ca455083ecf6","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/quick-start-guide/","postref":"0a95c8a6e8dbe56b71a2ca455083ecf6","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rancher/v2.x/en/quick-start-guide/","section":"rancher","tags":null,"title":"Rancher Deployment Quick Start Guides","type":"rancher","url":"/docs/rancher/v2.x/en/quick-start-guide/","weight":25,"wordcount":130},{"authors":null,"categories":null,"content":"This section describes how to upgrade your K3s cluster.\nUpgrade basics describes several techniques for upgrading your cluster manually. It can also be used as a basis for upgrading through third-party Infrastructure-as-Code tools like Terraform.\nAutomated upgrades describes how to perform Kubernetes-native automated upgrades using Rancher&rsquo;s system-upgrade-controller.","date":-62135596800,"description":"","dir":"k3s/latest/en/upgrades/","expirydate":-62135596800,"fuzzywordcount":100,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"92acfae1d08fc25594df5efc7b5bed07","permalink":"http://jijeesh.github.io/docs/k3s/latest/en/upgrades/","postref":"92acfae1d08fc25594df5efc7b5bed07","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/k3s/latest/en/upgrades/","section":"k3s","tags":null,"title":"Upgrades","type":"k3s","url":"/docs/k3s/latest/en/upgrades/","weight":25,"wordcount":47},{"authors":null,"categories":null,"content":"Important: RKE add-on install is only supported up to Rancher v2.0.8 Please use the Rancher Helm chart to install Rancher on a Kubernetes cluster. For details, see the Kubernetes Install - Installation Outline.\nIf you are currently using the RKE add-on install method, see Migrating from a Kubernetes Install with an RKE Add-on for details on how to move to using the helm chart.\n To debug issues around this error, you will need to download the command-line tool kubectl.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/options/helm2/rke-add-on/troubleshooting/404-default-backend/","expirydate":-62135596800,"fuzzywordcount":500,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"451ccd641fb8f493da8406bb77d720d0","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/helm2/rke-add-on/troubleshooting/404-default-backend/","postref":"451ccd641fb8f493da8406bb77d720d0","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/installation/options/helm2/rke-add-on/troubleshooting/404-default-backend/","section":"rancher","tags":null,"title":"404 - default backend","type":"rancher","url":"/docs/rancher/v2.x/en/installation/options/helm2/rke-add-on/troubleshooting/404-default-backend/","weight":30,"wordcount":416},{"authors":null,"categories":null,"content":"Important: RKE add-on install is only supported up to Rancher v2.0.8 Please use the Rancher Helm chart to install Rancher on a Kubernetes cluster. For details, see the Kubernetes Install - Installation Outline.\nIf you are currently using the RKE add-on install method, see Migrating from a Kubernetes Install with an RKE Add-on for details on how to move to using the helm chart.\n To debug issues around this error, you will need to download the command-line tool kubectl.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/options/rke-add-on/troubleshooting/404-default-backend/","expirydate":-62135596800,"fuzzywordcount":500,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"b7c0402f42c893f0bd62c266bd2f15b6","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/rke-add-on/troubleshooting/404-default-backend/","postref":"b7c0402f42c893f0bd62c266bd2f15b6","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/installation/options/rke-add-on/troubleshooting/404-default-backend/","section":"rancher","tags":null,"title":"404 - default backend","type":"rancher","url":"/docs/rancher/v2.x/en/installation/options/rke-add-on/troubleshooting/404-default-backend/","weight":30,"wordcount":416},{"authors":null,"categories":null,"content":"Note: Official support for installing Rancher on a Kubernetes cluster was introduced in our v1.0.0 release.\n This section describes how to install a high-availability K3s cluster with an external database.\nSingle server clusters can meet a variety of use cases, but for environments where uptime of the Kubernetes control plane is critical, you can run K3s in an HA configuration. An HA K3s cluster is comprised of:","date":-62135596800,"description":"","dir":"k3s/latest/en/installation/ha/","expirydate":-62135596800,"fuzzywordcount":800,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"3bf58308a1e073fdf9b09f50e3dbe7e7","permalink":"http://jijeesh.github.io/docs/k3s/latest/en/installation/ha/","postref":"3bf58308a1e073fdf9b09f50e3dbe7e7","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/docs/k3s/latest/en/installation/ha/","section":"k3s","tags":null,"title":"High Availability with an External DB","type":"k3s","url":"/docs/k3s/latest/en/installation/ha/","weight":30,"wordcount":717},{"authors":null,"categories":null,"content":"When deploying an application that needs to retain data, you’ll need to create persistent storage. Persistent storage allows you to store application data external from the pod running your application. This storage practice allows you to maintain application data, even if the application’s pod fails.\nA persistent volume (PV) is a piece of storage in the Kubernetes cluster, while a persistent volume claim (PVC) is a request for storage. For details on how PVs and PVCs work, refer to the official Kubernetes documentation on storage.","date":-62135596800,"description":"","dir":"k3s/latest/en/storage/","expirydate":-62135596800,"fuzzywordcount":500,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"2d3e9810bcde87a4fd9d0503deceb75a","permalink":"http://jijeesh.github.io/docs/k3s/latest/en/storage/","postref":"2d3e9810bcde87a4fd9d0503deceb75a","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/k3s/latest/en/storage/","section":"k3s","tags":null,"title":"Volumes and Storage","type":"k3s","url":"/docs/k3s/latest/en/storage/","weight":30,"wordcount":419},{"authors":null,"categories":null,"content":"If you are an RKE template owner, you can share it with users or groups of users, who can then use the template to create clusters.\nSince RKE templates are specifically shared with users and groups, owners can share different RKE templates with different sets of users.\nWhen you share a template, each user can have one of two access levels:\n Owner: This user can update, delete, and share the templates that they own.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/admin-settings/rke-templates/template-access-and-sharing/","expirydate":-62135596800,"fuzzywordcount":600,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"b0ab071c3acda58123b3932e95bb0ab3","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/rke-templates/template-access-and-sharing/","postref":"b0ab071c3acda58123b3932e95bb0ab3","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/rancher/v2.x/en/admin-settings/rke-templates/template-access-and-sharing/","section":"rancher","tags":null,"title":"Access and Sharing","type":"rancher","url":"/docs/rancher/v2.x/en/admin-settings/rke-templates/template-access-and-sharing/","weight":31,"wordcount":524},{"authors":null,"categories":null,"content":"This section describes how to manage RKE templates and revisions. You an create, share, update, and delete templates from the Global view under Tools &gt; RKE Templates.\nTemplate updates are handled through a revision system. When template owners want to change or update a template, they create a new revision of the template. Individual revisions cannot be edited. However, if you want to prevent a revision from being used to create a new cluster, you can disable it.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/admin-settings/rke-templates/creating-and-revising/","expirydate":-62135596800,"fuzzywordcount":1500,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"c8b45dd29fb4103d7da98933bee2c57a","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/rke-templates/creating-and-revising/","postref":"c8b45dd29fb4103d7da98933bee2c57a","publishdate":"0001-01-01T00:00:00Z","readingtime":7,"relpermalink":"/docs/rancher/v2.x/en/admin-settings/rke-templates/creating-and-revising/","section":"rancher","tags":null,"title":"Creating and Revising Templates","type":"rancher","url":"/docs/rancher/v2.x/en/admin-settings/rke-templates/creating-and-revising/","weight":32,"wordcount":1436},{"authors":null,"categories":null,"content":"This section describes how template administrators can enforce templates in Rancher, restricting the ability of users to create clusters without a template.\nBy default, any standard user in Rancher can create clusters. But when RKE template enforcement is turned on,\n Only an administrator has the ability to create clusters without a template. All standard users must use an RKE template to create a new cluster. Standard users cannot create a cluster without using a template.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/admin-settings/rke-templates/enforcement/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"e0b3b85aefcf33d914854fa6748df8db","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/rke-templates/enforcement/","postref":"e0b3b85aefcf33d914854fa6748df8db","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/admin-settings/rke-templates/enforcement/","section":"rancher","tags":null,"title":"Template Enforcement","type":"rancher","url":"/docs/rancher/v2.x/en/admin-settings/rke-templates/enforcement/","weight":32,"wordcount":346},{"authors":null,"categories":null,"content":"When a user creates an RKE template, each setting in the template has a switch in the Rancher UI that indicates if users can override the setting. This switch marks those settings as Allow User Override.\nAfter a cluster is created with a template, end users can&rsquo;t update any of the settings defined in the template unless the template owner marked them as Allow User Override. However, if the template is updated to a new revision that changes the settings or allows end users to change them, the cluster can be upgraded to a new revision of the template and the changes in the new revision will be applied to the cluster.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/admin-settings/rke-templates/overrides/","expirydate":-62135596800,"fuzzywordcount":300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"d2376eaadb3d2f9e067a2b5ea5a64288","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/rke-templates/overrides/","postref":"d2376eaadb3d2f9e067a2b5ea5a64288","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rancher/v2.x/en/admin-settings/rke-templates/overrides/","section":"rancher","tags":null,"title":"Overriding Template Settings","type":"rancher","url":"/docs/rancher/v2.x/en/admin-settings/rke-templates/overrides/","weight":33,"wordcount":201},{"authors":null,"categories":null,"content":"Note: CNI options are covered in detail on the Installation Network Options page. Please reference that page for details on Flannel and the various flannel backend options or how to set up your own CNI.\n Open Ports Please reference the Installation Requirements page for port information.\nCoreDNS CoreDNS is deployed on start of the agent. To disable, run each server with the --no-deploy coredns option.\nIf you don&rsquo;t install CoreDNS, you will need to install a cluster DNS provider yourself.","date":-62135596800,"description":"","dir":"k3s/latest/en/networking/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"26b85b431b9b78b003c4fe2211ec5fbd","permalink":"http://jijeesh.github.io/docs/k3s/latest/en/networking/","postref":"26b85b431b9b78b003c4fe2211ec5fbd","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/k3s/latest/en/networking/","section":"k3s","tags":null,"title":"Networking","type":"k3s","url":"/docs/k3s/latest/en/networking/","weight":35,"wordcount":301},{"authors":null,"categories":null,"content":"As of v1.0.0, K3s is previewing support for running a highly available control plane without the need for an external database. This means there is no need to manage an external etcd or SQL datastore in order to run a reliable production-grade setup. While this feature is currently experimental, we expect it to be the primary architecture for running HA K3s clusters in the future.\nThis architecture is achieved by embedding a dqlite database within the K3s server process.","date":-62135596800,"description":"","dir":"k3s/latest/en/installation/ha-embedded/","expirydate":-62135596800,"fuzzywordcount":300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"a4354bf686f4aaf800b0a4654dac8ce2","permalink":"http://jijeesh.github.io/docs/k3s/latest/en/installation/ha-embedded/","postref":"a4354bf686f4aaf800b0a4654dac8ce2","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/k3s/latest/en/installation/ha-embedded/","section":"k3s","tags":null,"title":"High Availability with Embedded DB (Experimental)","type":"k3s","url":"/docs/k3s/latest/en/installation/ha-embedded/","weight":40,"wordcount":225},{"authors":null,"categories":null,"content":"K3s release v1.17.0+k3s.1 added support for Helm 3. You can access the Helm 3 documentation here.\nHelm is the package management tool of choice for Kubernetes. Helm charts provide templating syntax for Kubernetes YAML manifest documents. With Helm we can create configurable deployments instead of just using static files. For more information about creating your own catalog of deployments, check out the docs at https://helm.sh/.\nK3s does not require any special configuration to start using Helm 3.","date":-62135596800,"description":"","dir":"k3s/latest/en/helm/","expirydate":-62135596800,"fuzzywordcount":500,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"d0f0148077fcbcae254336178ae124a5","permalink":"http://jijeesh.github.io/docs/k3s/latest/en/helm/","postref":"d0f0148077fcbcae254336178ae124a5","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/k3s/latest/en/helm/","section":"k3s","tags":null,"title":"Helm","type":"k3s","url":"/docs/k3s/latest/en/helm/","weight":42,"wordcount":441},{"authors":null,"categories":null,"content":"This section contains advanced information describing the different ways you can run and manage K3s:\n Auto-deploying manifests Using Docker as the container runtime Secrets Encryption Config (Experimental) Running K3s with RootlessKit (Experimental) Node labels and taints Starting the server with the installation script Additional preparation for Alpine Linux setup Running K3d (K3s in Docker) and docker-compose Raspbian Buster - Enable legacy iptables  Auto-Deploying Manifests Any file found in /var/lib/rancher/k3s/server/manifests will automatically be deployed to Kubernetes in a manner similar to kubectl apply.","date":-62135596800,"description":"","dir":"k3s/latest/en/advanced/","expirydate":-62135596800,"fuzzywordcount":1300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"0d162d4a5dfd0038ae8334e9a7ea0085","permalink":"http://jijeesh.github.io/docs/k3s/latest/en/advanced/","postref":"0d162d4a5dfd0038ae8334e9a7ea0085","publishdate":"0001-01-01T00:00:00Z","readingtime":6,"relpermalink":"/docs/k3s/latest/en/advanced/","section":"k3s","tags":null,"title":"Advanced Options and Configuration","type":"k3s","url":"/docs/k3s/latest/en/advanced/","weight":45,"wordcount":1241},{"authors":null,"categories":null,"content":"You can create a cluster from an RKE template that you created, or from a template that has been shared with you.\nRKE templates can be applied to new clusters.\nAs of Rancher v2.3.3, you can save the configuration of an existing cluster as an RKE template. Then the cluster&rsquo;s settings can only be changed if the template is updated.\nYou can&rsquo;t change a cluster to use a different RKE template.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/admin-settings/rke-templates/applying-templates/","expirydate":-62135596800,"fuzzywordcount":700,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"16562207b3943a8e2e795540a1a4040a","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/rke-templates/applying-templates/","postref":"16562207b3943a8e2e795540a1a4040a","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/rancher/v2.x/en/admin-settings/rke-templates/applying-templates/","section":"rancher","tags":null,"title":"Applying Templates","type":"rancher","url":"/docs/rancher/v2.x/en/admin-settings/rke-templates/applying-templates/","weight":50,"wordcount":632},{"authors":null,"categories":null,"content":"This section contains information about how to create backups of your Rancher data and how to restore them in a disaster scenario.\n Docker Install Backups Kubernetes Install Backups  If you are looking to back up your Rancher launched Kubernetes cluster, please refer here.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/backups/backups/","expirydate":-62135596800,"fuzzywordcount":100,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"f4263d99ee181b8c9a36d0b3e81999c0","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/backups/backups/","postref":"f4263d99ee181b8c9a36d0b3e81999c0","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rancher/v2.x/en/backups/backups/","section":"rancher","tags":null,"title":"Backups","type":"rancher","url":"/docs/rancher/v2.x/en/backups/backups/","weight":50,"wordcount":43},{"authors":null,"categories":null,"content":"The ability to run Kubernetes using a datastore other than etcd sets K3s apart from other Kubernetes distributions. This feature provides flexibility to Kubernetes operators. The available datastore options allow you to select a datastore that best fits your use case. For example:\n If your team doesn&rsquo;t have expertise in operating etcd, you can choose an enterprise-grade SQL database like MySQL or PostgreSQL If you need to run a simple, short-lived cluster in your CI/CD environment, you can use the embedded SQLite database If you wish to deploy Kubernetes on the edge and require a highly available solution but can&rsquo;t afford the operational overhead of managing a database at the edge, you can use K3s&rsquo;s embedded HA datastore built on top of DQLite (currently experimental)  K3s supports the following datastore options:","date":-62135596800,"description":"","dir":"k3s/latest/en/installation/datastore/","expirydate":-62135596800,"fuzzywordcount":800,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"b73388dde213064b66184ebbc0935882","permalink":"http://jijeesh.github.io/docs/k3s/latest/en/installation/datastore/","postref":"b73388dde213064b66184ebbc0935882","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/docs/k3s/latest/en/installation/datastore/","section":"k3s","tags":null,"title":"Cluster Datastore Options","type":"k3s","url":"/docs/k3s/latest/en/installation/datastore/","weight":50,"wordcount":790},{"authors":null,"categories":null,"content":"This section describes how to create backups of your high-availability Rancher install.\n Prerequisites:    Rancher Kubernetes Engine v0.1.7 or later\nThe commands for taking etcd snapshots are only available in RKE v0.1.7 and later.\n  rancher-cluster.yml\nYou'll need the RKE config file that you used for Rancher install, rancher-cluster.yml. You created this file during your initial install. Place this file in same directory as the RKE binary.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/backups/backups/ha-backups/","expirydate":-62135596800,"fuzzywordcount":900,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"be996e2b4a135c2290ede1785f2e6ba6","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/backups/backups/ha-backups/","postref":"be996e2b4a135c2290ede1785f2e6ba6","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/docs/rancher/v2.x/en/backups/backups/ha-backups/","section":"rancher","tags":null,"title":"Creating Backups for Rancher Installed on Kubernetes","type":"rancher","url":"/docs/rancher/v2.x/en/backups/backups/ha-backups/","weight":50,"wordcount":820},{"authors":null,"categories":null,"content":"This section provides an overview of the architecture options of installing Rancher, describing advantages of each option.\nTerminology In this section,\nThe Rancher server manages and provisions Kubernetes clusters. You can interact with downstream Kubernetes clusters through the Rancher server&rsquo;s user interface.\nRKE (Rancher Kubernetes Engine) is a certified Kubernetes distribution and CLI/library which creates and manages a Kubernetes cluster. When you create a cluster in the Rancher UI, it calls RKE as a library to provision Rancher-launched Kubernetes clusters.","date":-62135596800,"description":"Learn how to install Rancher in development and production environments. Read about single node and high availability installation","dir":"rancher/v2.x/en/installation/","expirydate":-62135596800,"fuzzywordcount":800,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"55edcc8f62a5d15ac6265f037f123c82","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/","postref":"55edcc8f62a5d15ac6265f037f123c82","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/docs/rancher/v2.x/en/installation/","section":"rancher","tags":null,"title":"Installing Rancher","type":"rancher","url":"/docs/rancher/v2.x/en/installation/","weight":50,"wordcount":792},{"authors":null,"categories":null,"content":"RKE is a fast, versatile Kubernetes installer that you can use to install Kubernetes on your Linux hosts. You can get started in a couple of quick and easy steps:\n Download the RKE Binary  Alternative RKE MacOS X Install - Homebrew  Prepare the Nodes for the Kubernetes Cluster Creating the Cluster Configuration File Deploying Kubernetes with RKE Save your Files Interacting with your Kubernetes Cluster  Download the RKE binary  From your workstation, open a web browser and navigate to our RKE Releases page.","date":-62135596800,"description":"RKE is a fast, versatile Kubernetes installer you can use to install Kubernetes on your Linux hosts. Learn the simple steps for an RKE Kubernetes installation","dir":"rke/latest/en/installation/","expirydate":-62135596800,"fuzzywordcount":1100,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"1895d83200b04af55aa5817f1ec94763","permalink":"http://jijeesh.github.io/docs/rke/latest/en/installation/","postref":"1895d83200b04af55aa5817f1ec94763","publishdate":"0001-01-01T00:00:00Z","readingtime":5,"relpermalink":"/docs/rke/latest/en/installation/","section":"rke","tags":null,"title":"RKE Kubernetes Installation","type":"rke","url":"/docs/rke/latest/en/installation/","weight":50,"wordcount":1010},{"authors":null,"categories":null,"content":"Available as of v1.0.0\nContainerd can be configured to connect to private registries and use them to pull private images on the node.\nUpon startup, K3s will check to see if a registries.yaml file exists at /etc/rancher/k3s/ and instruct containerd to use any registries defined in the file. If you wish to use a private registry, then you will need to create this file as root on each node that will be using the registry.","date":-62135596800,"description":"","dir":"k3s/latest/en/installation/private-registry/","expirydate":-62135596800,"fuzzywordcount":700,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"def87dee9e0677ac3c236a8a0cc489a3","permalink":"http://jijeesh.github.io/docs/k3s/latest/en/installation/private-registry/","postref":"def87dee9e0677ac3c236a8a0cc489a3","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/k3s/latest/en/installation/private-registry/","section":"k3s","tags":null,"title":"Private Registry Configuration","type":"k3s","url":"/docs/k3s/latest/en/installation/private-registry/","weight":55,"wordcount":600},{"authors":null,"categories":null,"content":"You can install K3s in an air-gapped environment using two different methods. You can either deploy a private registry and mirror docker.io or you can manually deploy images such as for small clusters.\nPrivate Registry Method This document assumes you have already created your nodes in your air-gap environment and have a secure Docker private registry on your bastion host. If you have not yet set up a private Docker registry, refer to the official documentation here.","date":-62135596800,"description":"","dir":"k3s/latest/en/installation/airgap/","expirydate":-62135596800,"fuzzywordcount":800,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"e07cc1b7438ca0df9cc66153c3b994b8","permalink":"http://jijeesh.github.io/docs/k3s/latest/en/installation/airgap/","postref":"e07cc1b7438ca0df9cc66153c3b994b8","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/docs/k3s/latest/en/installation/airgap/","section":"k3s","tags":null,"title":"Air-Gap Install","type":"k3s","url":"/docs/k3s/latest/en/installation/airgap/","weight":60,"wordcount":733},{"authors":null,"categories":null,"content":"Below is an example RKE template configuration file for reference.\nThe YAML in the RKE template uses the same customization that is used when you create an RKE cluster. However, since the YAML is within the context of a Rancher provisioned RKE cluster, the customization from the RKE docs needs to be nested under the rancher_kubernetes_engine directive.\n# # Cluster Config# docker_root_dir:/var/lib/dockerenable_cluster_alerting:false# This setting is not enforced. Clusters# created with this sample template# would have alerting turned off by default,# but end users could still turn alerting# on or off.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/admin-settings/rke-templates/example-yaml/","expirydate":-62135596800,"fuzzywordcount":300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"cf7d2333d0baeae29f8f7050b704aa50","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/rke-templates/example-yaml/","postref":"cf7d2333d0baeae29f8f7050b704aa50","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/admin-settings/rke-templates/example-yaml/","section":"rancher","tags":null,"title":"Example YAML","type":"rancher","url":"/docs/rancher/v2.x/en/admin-settings/rke-templates/example-yaml/","weight":60,"wordcount":223},{"authors":null,"categories":null,"content":"The FAQ is updated periodically and designed to answer the questions our users most frequently ask about K3s.\nIs K3s a suitable replacement for k8s?\nK3s is capable of nearly everything k8s can do. It is just a more lightweight version. See the main docs page for more details.\nHow can I use my own Ingress instead of Traefik?\nSimply start K3s server with --no-deploy=traefik and deploy your ingress.\nDoes K3s support Windows?","date":-62135596800,"description":"","dir":"k3s/latest/en/faq/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"328183b3677e253cdce1bbecac989cde","permalink":"http://jijeesh.github.io/docs/k3s/latest/en/faq/","postref":"328183b3677e253cdce1bbecac989cde","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/k3s/latest/en/faq/","section":"k3s","tags":null,"title":"FAQ","type":"k3s","url":"/docs/k3s/latest/en/faq/","weight":60,"wordcount":105},{"authors":null,"categories":null,"content":"This installation guide will help you to deploy and configure the Kubernetes Dashboard on K3s.\nDeploying the Kubernetes Dashboard GITHUB_URL=https://github.com/kubernetes/dashboard/releases VERSION_KUBE_DASHBOARD=$(curl -w &#39;%{url_effective}&#39; -I -L -s -S ${GITHUB_URL}/latest -o /dev/null | sed -e &#39;s|.*/||&#39;) sudo k3s kubectl create -f https://raw.githubusercontent.com/kubernetes/dashboard/${VERSION_KUBE_DASHBOARD}/aio/deploy/recommended.yaml Dashboard RBAC Configuration  Important: The admin-user created in this guide will have administrative privileges in the Dashboard.\n Create the following resource manifest files:\ndashboard.admin-user.yml\napiVersion:v1kind:ServiceAccountmetadata:name:admin-usernamespace:kubernetes-dashboard dashboard.admin-user-role.yml\napiVersion:rbac.authorization.k8s.io/v1kind:ClusterRoleBindingmetadata:name:admin-userroleRef:apiGroup:rbac.authorization.k8s.iokind:ClusterRolename:cluster-adminsubjects:-kind:ServiceAccountname:admin-usernamespace:kubernetes-dashboard Deploy the admin-user configuration:","date":-62135596800,"description":"","dir":"k3s/latest/en/installation/kube-dashboard/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"67020eeb879d7133683940a864366c4b","permalink":"http://jijeesh.github.io/docs/k3s/latest/en/installation/kube-dashboard/","postref":"67020eeb879d7133683940a864366c4b","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/k3s/latest/en/installation/kube-dashboard/","section":"k3s","tags":null,"title":"Kubernetes Dashboard","type":"k3s","url":"/docs/k3s/latest/en/installation/kube-dashboard/","weight":60,"wordcount":197},{"authors":null,"categories":null,"content":"If you installed K3s using the installation script, a script to uninstall K3s was generated during installation.\nTo uninstall K3s from a server node, run:\n/usr/local/bin/k3s-uninstall.sh  To uninstall K3s from an agent node, run:\n/usr/local/bin/k3s-agent-uninstall.sh  ","date":-62135596800,"description":"","dir":"k3s/latest/en/installation/uninstall/","expirydate":-62135596800,"fuzzywordcount":100,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"a507757de6a4cbcbd5bd41a797611382","permalink":"http://jijeesh.github.io/docs/k3s/latest/en/installation/uninstall/","postref":"a507757de6a4cbcbd5bd41a797611382","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/k3s/latest/en/installation/uninstall/","section":"k3s","tags":null,"title":"Uninstalling K3s","type":"k3s","url":"/docs/k3s/latest/en/installation/uninstall/","weight":61,"wordcount":35},{"authors":null,"categories":null,"content":"The Known Issues are updated periodically and designed to inform you about any issues that may not be immediately addressed in the next upcoming release.\nSnap Docker\nIf you plan to use K3s with docker, Docker installed via a snap package is not recommended as it has been known to cause issues running K3s.\nIptables\nIf you are running iptables in nftables mode instead of legacy you might encounter issues. We recommend utilizing newer iptables (such as 1.","date":-62135596800,"description":"","dir":"k3s/latest/en/known-issues/","expirydate":-62135596800,"fuzzywordcount":100,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"cc1e15f1bb896a023b40480547d652bc","permalink":"http://jijeesh.github.io/docs/k3s/latest/en/known-issues/","postref":"cc1e15f1bb896a023b40480547d652bc","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/k3s/latest/en/known-issues/","section":"k3s","tags":null,"title":"Known Issues","type":"k3s","url":"/docs/k3s/latest/en/known-issues/","weight":70,"wordcount":93},{"authors":null,"categories":null,"content":"Prerequisite: The options below are available only for clusters that are launched using RKE.\n Following an upgrade to the latest version of Rancher, you can update your existing clusters to use the latest supported version of Kubernetes.\nBefore a new version of Rancher is released, it&rsquo;s tested with the latest minor versions of Kubernetes to ensure compatibility. For example, Rancher v2.3.0 is was tested with Kubernetes v1.15.4, v1.14.7, and v1.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-admin/upgrading-kubernetes/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"06abf7da620d25049ed3c00a22b5024f","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/upgrading-kubernetes/","postref":"06abf7da620d25049ed3c00a22b5024f","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rancher/v2.x/en/cluster-admin/upgrading-kubernetes/","section":"rancher","tags":null,"title":"Upgrading Kubernetes","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-admin/upgrading-kubernetes/","weight":70,"wordcount":183},{"authors":null,"categories":null,"content":"Prerequisite: The options below are available only for clusters that are launched using RKE.\n When your cluster is running pods with security-sensitive configurations, assign it a pod security policy, which is a set of rules that monitors the conditions and settings in your pods. If a pod doesn&rsquo;t meet the rules specified in your policy, the policy stops it from running.\nYou can assign a pod security policy when you provision a cluster.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-admin/pod-security-policy/","expirydate":-62135596800,"fuzzywordcount":300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"f228ba163562e53596eb993d8c551da3","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/pod-security-policy/","postref":"f228ba163562e53596eb993d8c551da3","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/cluster-admin/pod-security-policy/","section":"rancher","tags":null,"title":"Adding a Pod Security Policy","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-admin/pod-security-policy/","weight":80,"wordcount":238},{"authors":null,"categories":null,"content":"In Rancher, RKE templates are used to provision Kubernetes and define Rancher settings, while node templates are used to provision nodes.\nTherefore, even if RKE template enforcement is turned on, the end user still has flexibility when picking the underlying hardware when creating a Rancher cluster. The end users of an RKE template can still choose an infrastructure provider and the nodes they want to use.\nIf you want to standardize the hardware in your clusters, use RKE templates conjunction with node templates or with a server provisioning tool such as Terraform.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/admin-settings/rke-templates/rke-templates-and-hardware/","expirydate":-62135596800,"fuzzywordcount":900,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"4695f3f9a9717fd3375d2219f0c666b3","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/rke-templates/rke-templates-and-hardware/","postref":"4695f3f9a9717fd3375d2219f0c666b3","publishdate":"0001-01-01T00:00:00Z","readingtime":5,"relpermalink":"/docs/rancher/v2.x/en/admin-settings/rke-templates/rke-templates-and-hardware/","section":"rancher","tags":null,"title":"RKE Templates and Infrastructure","type":"rancher","url":"/docs/rancher/v2.x/en/admin-settings/rke-templates/rke-templates-and-hardware/","weight":90,"wordcount":872},{"authors":null,"categories":null,"content":"This section is about how to prepare your node(s) to install Rancher for your air gapped environment. An air gapped environment could be where Rancher server will be installed offline, behind a firewall, or behind a proxy. There are tabs for either a high availability (recommended) or a Docker installation.\nPrerequisites  OS, Docker, Hardware, and Networking Make sure that your node(s) fulfill the general installation requirements.\nPrivate Registry Rancher supports air gap installs using a private registry.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/options/air-gap-helm2/prepare-nodes/","expirydate":-62135596800,"fuzzywordcount":800,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"2f45169d3ebcf8c7ec3d119762d1f5c9","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/air-gap-helm2/prepare-nodes/","postref":"2f45169d3ebcf8c7ec3d119762d1f5c9","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/docs/rancher/v2.x/en/installation/options/air-gap-helm2/prepare-nodes/","section":"rancher","tags":null,"title":"1. Prepare your Node(s)","type":"rancher","url":"/docs/rancher/v2.x/en/installation/options/air-gap-helm2/prepare-nodes/","weight":100,"wordcount":758},{"authors":null,"categories":null,"content":"This section is about how to prepare your node(s) to install Rancher for your air gapped environment. An air gapped environment could be where Rancher server will be installed offline, behind a firewall, or behind a proxy. There are tabs for either a high availability (recommended) or a Docker installation.\nPrerequisites  OS, Docker, Hardware, and Networking Make sure that your node(s) fulfill the general installation requirements.\nPrivate Registry Rancher supports air gap installs using a private registry.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/other-installation-methods/air-gap/prepare-nodes/","expirydate":-62135596800,"fuzzywordcount":800,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"d88626c3863c7eb50d4ecfc7ad14d282","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/other-installation-methods/air-gap/prepare-nodes/","postref":"d88626c3863c7eb50d4ecfc7ad14d282","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/docs/rancher/v2.x/en/installation/other-installation-methods/air-gap/prepare-nodes/","section":"rancher","tags":null,"title":"1. Prepare your Node(s)","type":"rancher","url":"/docs/rancher/v2.x/en/installation/other-installation-methods/air-gap/prepare-nodes/","weight":100,"wordcount":758},{"authors":null,"categories":null,"content":"Although your services from v1.6 won&rsquo;t work in Rancher v2.x by default, that doesn&rsquo;t mean you have to start again from square one, manually rebuilding your applications in v2.x. To help with migration from v1.6 to v2.x, Rancher has developed a migration tool. The migration-tools CLI is a utility that helps you recreate your applications in Rancher v2.x. This tool exports your Rancher v1.6 services as Compose files and converts them to a Kubernetes manifest that Rancher v2.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/v1.6-migration/run-migration-tool/","expirydate":-62135596800,"fuzzywordcount":1500,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"a1d8396fcfb3c2bc9f887ab6c7b0bade","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/v1.6-migration/run-migration-tool/","postref":"a1d8396fcfb3c2bc9f887ab6c7b0bade","publishdate":"0001-01-01T00:00:00Z","readingtime":7,"relpermalink":"/docs/rancher/v2.x/en/v1.6-migration/run-migration-tool/","section":"rancher","tags":null,"title":"2. Migrate Your Services","type":"rancher","url":"/docs/rancher/v2.x/en/v1.6-migration/run-migration-tool/","weight":100,"wordcount":1487},{"authors":null,"categories":null,"content":"Interact with Rancher using command line interface (CLI) tools from your workstation.\nRancher CLI Follow the steps in rancher cli.\nEnsure you can run rancher kubectl get pods successfully.\nkubectl Install the kubectl utility. See install kubectl.\nConfigure kubectl by visiting your cluster in the Rancher Web UI then clicking on Kubeconfig, copying contents and putting into your ~/.kube/config file.\nRun kubectl cluster-info or kubectl get pods successfully.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/quick-start-guide/cli/","expirydate":-62135596800,"fuzzywordcount":100,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"5af5ca69f876444015d3350a7405915c","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/quick-start-guide/cli/","postref":"5af5ca69f876444015d3350a7405915c","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rancher/v2.x/en/quick-start-guide/cli/","section":"rancher","tags":null,"title":"CLI with Rancher","type":"rancher","url":"/docs/rancher/v2.x/en/quick-start-guide/cli/","weight":100,"wordcount":68},{"authors":null,"categories":null,"content":"Use one of the following guides to deploy and provision Rancher and a Kubernetes cluster in the provider of your choice.\n DigitalOcean (uses Terraform) AWS (uses Terraform) Azure (uses Terraform) GCP (uses Terraform) Vagrant  If you prefer, the following guide will take you through the same process in individual steps. Use this if you want to run Rancher in a different provider, on prem, or if you would just like to see how easy it is.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/quick-start-guide/deployment/","expirydate":-62135596800,"fuzzywordcount":100,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"a1869e675d7d3fcb29bf389790b8686a","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/quick-start-guide/deployment/","postref":"a1869e675d7d3fcb29bf389790b8686a","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rancher/v2.x/en/quick-start-guide/deployment/","section":"rancher","tags":null,"title":"Deploying Rancher Server","type":"rancher","url":"/docs/rancher/v2.x/en/quick-start-guide/deployment/","weight":100,"wordcount":78},{"authors":null,"categories":null,"content":"This document provides prescriptive guidance for hardening a production installation of Rancher v2.3.5. It outlines the configurations and controls required to address Kubernetes benchmark controls from the Center for Information Security (CIS).\n This hardening guide describes how to secure the nodes in your cluster, and it is recommended to follow this guide before installing Kubernetes.\n This hardening guide is intended to be used with specific versions of the CIS Kubernetes Benchmark, Kubernetes, and Rancher:","date":-62135596800,"description":"","dir":"rancher/v2.x/en/security/hardening-2.3.5/","expirydate":-62135596800,"fuzzywordcount":1100,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"875ee26936aca404e31c6ab88bdb70c0","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/security/hardening-2.3.5/","postref":"875ee26936aca404e31c6ab88bdb70c0","publishdate":"0001-01-01T00:00:00Z","readingtime":5,"relpermalink":"/docs/rancher/v2.x/en/security/hardening-2.3.5/","section":"rancher","tags":null,"title":"Hardening Guide v2.3.5","type":"rancher","url":"/docs/rancher/v2.x/en/security/hardening-2.3.5/","weight":100,"wordcount":1020},{"authors":null,"categories":null,"content":"kubectl is a CLI utility for running commands against Kubernetes clusters. It&rsquo;s required for many maintenance and administrative tasks in Rancher 2.x.\nInstallation See kubectl Installation for installation on your operating system.\nConfiguration When you create a Kubernetes cluster with RKE, RKE creates a kube_config_rancher-cluster.yml in the local directory that contains credentials to connect to your new cluster with tools like kubectl or helm.\nYou can copy this file to $HOME/.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/faq/kubectl/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"5ae0859db21cd059c447a7ddeecb6299","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/faq/kubectl/","postref":"5ae0859db21cd059c447a7ddeecb6299","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rancher/v2.x/en/faq/kubectl/","section":"rancher","tags":null,"title":"Installing and Configuring kubectl","type":"rancher","url":"/docs/rancher/v2.x/en/faq/kubectl/","weight":100,"wordcount":131},{"authors":null,"categories":null,"content":"The commands and steps listed in this section apply to the core Kubernetes components on Rancher Launched Kubernetes clusters.\nThis section includes troubleshooting tips in the following categories:\n Troubleshooting etcd Nodes Troubleshooting Controlplane Nodes Troubleshooting nginx-proxy Nodes Troubleshooting Worker Nodes and Generic Components  Kubernetes Component Diagram  Lines show the traffic flow between components. Colors are used purely for visual aid","date":-62135596800,"description":"","dir":"rancher/v2.x/en/troubleshooting/kubernetes-components/","expirydate":-62135596800,"fuzzywordcount":100,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"625cf40771dcef51be3d0515b4fc4904","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/troubleshooting/kubernetes-components/","postref":"625cf40771dcef51be3d0515b4fc4904","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rancher/v2.x/en/troubleshooting/kubernetes-components/","section":"rancher","tags":null,"title":"Kubernetes Components","type":"rancher","url":"/docs/rancher/v2.x/en/troubleshooting/kubernetes-components/","weight":100,"wordcount":60},{"authors":null,"categories":null,"content":"The migration-tools CLI includes multiple commands and options to assist your migration from Rancher v1.6 to Rancher v2.x.\nDownload The migration-tools CLI for your platform can be downloaded from our GitHub releases page. The tool is available for Linux, Mac, and Windows platforms.\nUsage migration-tools [global options] command [command options] [arguments...]  Migration Tools Global Options The migration-tools CLI includes a handful of global options.\n   Global Option Description     --debug Enables debug logging.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/v1.6-migration/run-migration-tool/migration-tools-ref/","expirydate":-62135596800,"fuzzywordcount":600,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"9a5f2bdfeb113306471147a44340e16c","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/v1.6-migration/run-migration-tool/migration-tools-ref/","postref":"9a5f2bdfeb113306471147a44340e16c","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/rancher/v2.x/en/v1.6-migration/run-migration-tool/migration-tools-ref/","section":"rancher","tags":null,"title":"Migration Tools CLI Reference","type":"rancher","url":"/docs/rancher/v2.x/en/v1.6-migration/run-migration-tool/migration-tools-ref/","weight":100,"wordcount":523},{"authors":null,"categories":null,"content":"The following steps will quickly deploy a Rancher Server on AWS with a single node cluster attached.\nPrerequisites  Note Deploying to Amazon AWS will incur charges.\n  Amazon AWS Account: An Amazon AWS Account is required to create resources for deploying Rancher and Kubernetes. Amazon AWS Access Key: Use this link to follow a tutorial to create an Amazon AWS Access Key if you don&rsquo;t have one yet.","date":-62135596800,"description":"Read this step by step Rancher AWS guide to quickly deploy a Rancher Server with a single node cluster attached.","dir":"rancher/v2.x/en/quick-start-guide/deployment/amazon-aws-qs/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"8b8fce7f4244f74212af6a0500dc7e5b","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/quick-start-guide/deployment/amazon-aws-qs/","postref":"8b8fce7f4244f74212af6a0500dc7e5b","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/quick-start-guide/deployment/amazon-aws-qs/","section":"rancher","tags":null,"title":"Rancher AWS Quick Start Guide","type":"rancher","url":"/docs/rancher/v2.x/en/quick-start-guide/deployment/amazon-aws-qs/","weight":100,"wordcount":354},{"authors":null,"categories":null,"content":"The following steps will quickly deploy a Rancher server on Azure in a single-node RKE Kubernetes cluster, with a single-node downstream Kubernetes cluster attached.\nPrerequisites  Note Deploying to Microsoft Azure will incur charges.\n  Microsoft Azure Account: A Microsoft Azure Account is required to create resources for deploying Rancher and Kubernetes. Microsoft Azure Subscription: Use this link to follow a tutorial to create a Microsoft Azure subscription if you don&rsquo;t have one yet.","date":-62135596800,"description":"Read this step by step Rancher Azure guide to quickly deploy a Rancher Server with a single node cluster attached.","dir":"rancher/v2.x/en/quick-start-guide/deployment/microsoft-azure-qs/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"ed3e427133d8675b11b2aa7212e6f563","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/quick-start-guide/deployment/microsoft-azure-qs/","postref":"ed3e427133d8675b11b2aa7212e6f563","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/quick-start-guide/deployment/microsoft-azure-qs/","section":"rancher","tags":null,"title":"Rancher Azure Quick Start Guide","type":"rancher","url":"/docs/rancher/v2.x/en/quick-start-guide/deployment/microsoft-azure-qs/","weight":100,"wordcount":389},{"authors":null,"categories":null,"content":"There are two recommended deployment strategies. Each one has its own pros and cons. Read more about which one would fit best for your use case:\n Hub and Spoke Regional  Hub &amp; Spoke Strategy In this deployment scenario, there is a single Rancher control plane managing Kubernetes clusters across the globe. The control plane would be run on a high-availability Kubernetes cluster, and there would be impact due to latencies.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/best-practices/deployment-strategies/","expirydate":-62135596800,"fuzzywordcount":300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"df7ff611828d744f47c6d3f9dfbea5d2","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/best-practices/deployment-strategies/","postref":"df7ff611828d744f47c6d3f9dfbea5d2","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/best-practices/deployment-strategies/","section":"rancher","tags":null,"title":"Rancher Deployment Strategies","type":"rancher","url":"/docs/rancher/v2.x/en/best-practices/deployment-strategies/","weight":100,"wordcount":228},{"authors":null,"categories":null,"content":"The following steps will quickly deploy a Rancher Server on DigitalOcean with a single node cluster attached.\nPrerequisites  Note Deploying to DigitalOcean will incur charges.\n  DigitalOcean Account: You will require an account on DigitalOcean as this is where the server and cluster will run. DigitalOcean Access Key: Use this link to create a DigitalOcean Access Key if you don&rsquo;t have one. Terraform: Used to provision the server and cluster to DigitalOcean.","date":-62135596800,"description":"Read this step by step Rancher DigitalOcean guide to quickly deploy a Rancher Server with a single node cluster attached.","dir":"rancher/v2.x/en/quick-start-guide/deployment/digital-ocean-qs/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"a367a7ee1e39cd55accc5a21fd979bb0","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/quick-start-guide/deployment/digital-ocean-qs/","postref":"a367a7ee1e39cd55accc5a21fd979bb0","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/quick-start-guide/deployment/digital-ocean-qs/","section":"rancher","tags":null,"title":"Rancher DigitalOcean Quick Start Guide","type":"rancher","url":"/docs/rancher/v2.x/en/quick-start-guide/deployment/digital-ocean-qs/","weight":100,"wordcount":321},{"authors":null,"categories":null,"content":"The following steps will quickly deploy a Rancher server on GCP in a single-node RKE Kubernetes cluster, with a single-node downstream Kubernetes cluster attached.\nPrerequisites  Note Deploying to Google GCP will incur charges.\n  Google GCP Account: A Google GCP Account is required to create resources for deploying Rancher and Kubernetes. Google GCP Project: Use this link to follow a tutorial to create a GCP Project if you don&rsquo;t have one yet.","date":-62135596800,"description":"Read this step by step Rancher GCP guide to quickly deploy a Rancher Server with a single node cluster attached.","dir":"rancher/v2.x/en/quick-start-guide/deployment/google-gcp-qs/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"b8e409a57ac79319ff70550c2bebe3a3","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/quick-start-guide/deployment/google-gcp-qs/","postref":"b8e409a57ac79319ff70550c2bebe3a3","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/quick-start-guide/deployment/google-gcp-qs/","section":"rancher","tags":null,"title":"Rancher GCP Quick Start Guide","type":"rancher","url":"/docs/rancher/v2.x/en/quick-start-guide/deployment/google-gcp-qs/","weight":100,"wordcount":360},{"authors":null,"categories":null,"content":"RancherOS runs on virtualization platforms, cloud providers and bare metal servers. We also support running a local VM on your laptop. To start running RancherOS as quickly as possible, follow our Quick Start Guide.\nPlatforms Workstation Docker Machine\nBoot from ISO\nCloud Amazon EC2\nGoogle Compute Engine\nDigitalOcean\nAzure\nOpenStack\nVMware ESXi\nAliyun\nBare Metal &amp; Virtual Servers PXE\nInstall to Hard Disk\nRaspberry Pi","date":-62135596800,"description":"","dir":"os/v1.x/en/installation/running-rancheros/","expirydate":-62135596800,"fuzzywordcount":100,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"31e4584d466386e6fad84c9667a1a88a","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/running-rancheros/","postref":"31e4584d466386e6fad84c9667a1a88a","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/os/v1.x/en/installation/running-rancheros/","section":"os","tags":null,"title":"Running RancherOS","type":"os","url":"/docs/os/v1.x/en/installation/running-rancheros/","weight":100,"wordcount":65},{"authors":null,"categories":null,"content":"Failed to set up SSH tunneling for host [xxx.xxx.xxx.xxx]: Can&rsquo;t retrieve Docker Info Failed to dial to /var/run/docker.sock: ssh: rejected: administratively prohibited (open failed)  User specified to connect with does not have permission to access the Docker socket. This can be checked by logging into the host and running the command docker ps:  $ ssh -i ssh_privatekey_file user@server user@server$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES  See Manage Docker as a non-root user how to set this up properly.","date":-62135596800,"description":"","dir":"rke/latest/en/troubleshooting/ssh-connectivity-errors/","expirydate":-62135596800,"fuzzywordcount":500,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"e6799618dfe960a5e1d3081fd61a2b90","permalink":"http://jijeesh.github.io/docs/rke/latest/en/troubleshooting/ssh-connectivity-errors/","postref":"e6799618dfe960a5e1d3081fd61a2b90","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rke/latest/en/troubleshooting/ssh-connectivity-errors/","section":"rke","tags":null,"title":"SSH Connectivity Errors","type":"rke","url":"/docs/rke/latest/en/troubleshooting/ssh-connectivity-errors/","weight":100,"wordcount":421},{"authors":null,"categories":null,"content":"A high-availability Kubernetes installation, defined as an installation of Rancher on a Kubernetes cluster with at least three nodes, should be used in any production installation of Rancher, as well as any installation deemed &ldquo;important.&rdquo; Multiple Rancher instances running on multiple nodes ensure high availability that cannot be accomplished with a single node environment.\nWhen you set up your high-availability Rancher installation, consider the following:\nRun Rancher on a Separate Cluster Don&rsquo;t run other workloads or microservices in the Kubernetes cluster that Rancher is installed on.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/best-practices/deployment-types/","expirydate":-62135596800,"fuzzywordcount":700,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"779caa1a58dd4d4a8d6b32018f04a60e","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/best-practices/deployment-types/","postref":"779caa1a58dd4d4a8d6b32018f04a60e","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/rancher/v2.x/en/best-practices/deployment-types/","section":"rancher","tags":null,"title":"Tips for Running Rancher","type":"rancher","url":"/docs/rancher/v2.x/en/best-practices/deployment-types/","weight":100,"wordcount":603},{"authors":null,"categories":null,"content":"Running well built containers can greatly impact the overall performance and security of your environment.\nBelow are a few tips for setting up your containers.\nFor a more detailed discussion of security for containers, you can also refer to Rancher&rsquo;s Guide to Container Security.\nUse a Common Container OS When possible, you should try to standardize on a common container base OS.\nSmaller distributions such as Alpine and BusyBox reduce container image size and generally have a smaller attack/vulnerability surface.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/best-practices/containers/","expirydate":-62135596800,"fuzzywordcount":600,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"490461ee8eb2c67437722971967815a7","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/best-practices/containers/","postref":"490461ee8eb2c67437722971967815a7","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/rancher/v2.x/en/best-practices/containers/","section":"rancher","tags":null,"title":"Tips for Setting Up Containers","type":"rancher","url":"/docs/rancher/v2.x/en/best-practices/containers/","weight":100,"wordcount":576},{"authors":null,"categories":null,"content":"After RKE has deployed Kubernetes, you can upgrade the versions of the components in your Kubernetes cluster, the definition of the Kubernetes services or the add-ons.\nThe default Kubernetes version for each RKE version can be found in the RKE release notes.\nYou can also select a newer version of Kubernetes to install for your cluster. Downgrading Kubernetes is not supported.\nEach version of RKE has a specific list of supported Kubernetes versions.","date":-62135596800,"description":"","dir":"rke/latest/en/upgrades/","expirydate":-62135596800,"fuzzywordcount":800,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"f1695b120b0f3bd1dfacffb481ee616b","permalink":"http://jijeesh.github.io/docs/rke/latest/en/upgrades/","postref":"f1695b120b0f3bd1dfacffb481ee616b","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/docs/rke/latest/en/upgrades/","section":"rke","tags":null,"title":"Upgrades","type":"rke","url":"/docs/rke/latest/en/upgrades/","weight":100,"wordcount":713},{"authors":null,"categories":null,"content":"Prerequisite You have a running cluster with at least 1 node.\n1. Deploying a Workload You&rsquo;re ready to create your first workload. A workload is an object that includes pods along with other files and info needed to deploy your application.\nFor this workload, you&rsquo;ll be deploying the application Rancher Hello-World.\n From the Clusters page, open the cluster that you just created.\n From the main menu of the Dashboard, select Projects/Namespaces.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/quick-start-guide/workload/quickstart-deploy-workload-ingress/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"5b0c1a57874212c089cb848df2941123","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/quick-start-guide/workload/quickstart-deploy-workload-ingress/","postref":"5b0c1a57874212c089cb848df2941123","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/quick-start-guide/workload/quickstart-deploy-workload-ingress/","section":"rancher","tags":null,"title":"Workload with Ingress Quick Start","type":"rancher","url":"/docs/rancher/v2.x/en/quick-start-guide/workload/quickstart-deploy-workload-ingress/","weight":100,"wordcount":370},{"authors":null,"categories":null,"content":"This document provides prescriptive guidance for hardening a production installation of Rancher v2.3.3. It outlines the configurations and controls required to address Kubernetes benchmark controls from the Center for Information Security (CIS).\n This hardening guide describes how to secure the nodes in your cluster, and it is recommended to follow this guide before installing Kubernetes.\n This hardening guide is intended to be used with specific versions of the CIS Kubernetes Benchmark, Kubernetes, and Rancher:","date":-62135596800,"description":"","dir":"rancher/v2.x/en/security/hardening-2.3.3/","expirydate":-62135596800,"fuzzywordcount":4600,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"85d8db5b677021564029eb47002fd54d","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/security/hardening-2.3.3/","postref":"85d8db5b677021564029eb47002fd54d","publishdate":"0001-01-01T00:00:00Z","readingtime":22,"relpermalink":"/docs/rancher/v2.x/en/security/hardening-2.3.3/","section":"rancher","tags":null,"title":"Hardening Guide v2.3.3","type":"rancher","url":"/docs/rancher/v2.x/en/security/hardening-2.3.3/","weight":101,"wordcount":4543},{"authors":null,"categories":null,"content":"The commands/steps listed on this page can be used to check the most important Kubernetes resources and apply to Rancher Launched Kubernetes clusters.\nMake sure you configured the correct kubeconfig (for example, export KUBECONFIG=$PWD/kube_config_rancher-cluster.yml for Rancher HA) or are using the embedded kubectl via the UI.\nNodes Get nodes Run the command below and check the following:\n All nodes in your cluster should be listed, make sure there is not one missing.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/troubleshooting/kubernetes-resources/","expirydate":-62135596800,"fuzzywordcount":900,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"79ca814011b9031695e4a8b94e92eea0","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/troubleshooting/kubernetes-resources/","postref":"79ca814011b9031695e4a8b94e92eea0","publishdate":"0001-01-01T00:00:00Z","readingtime":5,"relpermalink":"/docs/rancher/v2.x/en/troubleshooting/kubernetes-resources/","section":"rancher","tags":null,"title":"Kubernetes resources","type":"rancher","url":"/docs/rancher/v2.x/en/troubleshooting/kubernetes-resources/","weight":101,"wordcount":896},{"authors":null,"categories":null,"content":"Rancher allows you to set up numerous combinations of configurations. Some configurations are more appropriate for development and testing, while there are other best practices for production environments for maximum availability and fault tolerance. The following best practices should be followed for production.\nTips for Preventing and Handling Problems These tips can help you solve problems before they happen.\nRun Rancher on a Supported OS and Supported Docker Version Rancher is container-based and can potentially run on any Linux-based operating system.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/best-practices/management/","expirydate":-62135596800,"fuzzywordcount":1900,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"fd069a8ecf19f8e9569c8e4eefa6b675","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/best-practices/management/","postref":"fd069a8ecf19f8e9569c8e4eefa6b675","publishdate":"0001-01-01T00:00:00Z","readingtime":9,"relpermalink":"/docs/rancher/v2.x/en/best-practices/management/","section":"rancher","tags":null,"title":"Tips for Scaling, Security and Reliability","type":"rancher","url":"/docs/rancher/v2.x/en/best-practices/management/","weight":101,"wordcount":1836},{"authors":null,"categories":null,"content":"Before we get started, you&rsquo;ll need to make sure that you have docker machine installed. Download it directly from the docker machine releases. You also need to know the memory requirements.\n Note: If you create a RancherOS instance using Docker Machine, you will not be able to upgrade your version of RancherOS.\n Downloading RancherOS Get the latest ISO artifact from the RancherOS releases.\n   Machine Driver Recommended RancherOS version ISO File     VirtualBox &gt;=v1.","date":-62135596800,"description":"","dir":"os/v1.x/en/installation/running-rancheros/workstation/docker-machine/","expirydate":-62135596800,"fuzzywordcount":800,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"b2a66f864c35bf098da5ff25657be028","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/running-rancheros/workstation/docker-machine/","postref":"b2a66f864c35bf098da5ff25657be028","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/docs/os/v1.x/en/installation/running-rancheros/workstation/docker-machine/","section":"os","tags":null,"title":"Using Docker Machine","type":"os","url":"/docs/os/v1.x/en/installation/running-rancheros/workstation/docker-machine/","weight":101,"wordcount":759},{"authors":null,"categories":null,"content":"The RancherOS ISO file can be used to create a fresh RancherOS install on KVM, VMware, VirtualBox, Hyper-V, Proxmox VE, or bare metal servers. You can download the rancheros.iso file from our releases page.\nSome hypervisors may require a built-in agent to communicate with the guest, for this, RancherOS precompiles some ISO files.\n   Hypervisor ISO     VMware rancheros-vmware.iso   Hyper-V rancheros-hyperv.iso   Proxmox VE rancheros-proxmoxve.","date":-62135596800,"description":"","dir":"os/v1.x/en/installation/running-rancheros/workstation/boot-from-iso/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"d85762c6c2d76f05e6a60216be0c6dc9","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/running-rancheros/workstation/boot-from-iso/","postref":"d85762c6c2d76f05e6a60216be0c6dc9","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/os/v1.x/en/installation/running-rancheros/workstation/boot-from-iso/","section":"os","tags":null,"title":"Booting from ISO","type":"os","url":"/docs/os/v1.x/en/installation/running-rancheros/workstation/boot-from-iso/","weight":102,"wordcount":145},{"authors":null,"categories":null,"content":"This document provides prescriptive guidance for hardening a production installation of Rancher v2.3.0-v2.3.2. It outlines the configurations and controls required to address Kubernetes benchmark controls from the Center for Information Security (CIS).\n This hardening guide describes how to secure the nodes in your cluster, and it is recommended to follow this guide before installing Kubernetes.\n This hardening guide is intended to be used with specific versions of the CIS Kubernetes Benchmark, Kubernetes, and Rancher:","date":-62135596800,"description":"","dir":"rancher/v2.x/en/security/hardening-2.3/","expirydate":-62135596800,"fuzzywordcount":3600,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"857069ab01ba30fa7098945271305fba","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/security/hardening-2.3/","postref":"857069ab01ba30fa7098945271305fba","publishdate":"0001-01-01T00:00:00Z","readingtime":17,"relpermalink":"/docs/rancher/v2.x/en/security/hardening-2.3/","section":"rancher","tags":null,"title":"Hardening Guide v2.3","type":"rancher","url":"/docs/rancher/v2.x/en/security/hardening-2.3/","weight":102,"wordcount":3597},{"authors":null,"categories":null,"content":"The commands/steps listed on this page can be used to check networking related issues in your cluster.\nMake sure you configured the correct kubeconfig (for example, export KUBECONFIG=$PWD/kube_config_rancher-cluster.yml for Rancher HA) or are using the embedded kubectl via the UI.\nDouble check if all the required ports are opened in your (host) firewall Double check if all the required ports are opened in your (host) firewall. The overlay network uses UDP in comparison to all other required ports which are TCP.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/troubleshooting/networking/","expirydate":-62135596800,"fuzzywordcount":800,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"7a97501d67e939493ae5d7d12dba85ce","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/troubleshooting/networking/","postref":"7a97501d67e939493ae5d7d12dba85ce","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/docs/rancher/v2.x/en/troubleshooting/networking/","section":"rancher","tags":null,"title":"Networking","type":"rancher","url":"/docs/rancher/v2.x/en/troubleshooting/networking/","weight":102,"wordcount":719},{"authors":null,"categories":null,"content":"The commands/steps listed on this page can be used to check name resolution issues in your cluster.\nMake sure you configured the correct kubeconfig (for example, export KUBECONFIG=$PWD/kube_config_rancher-cluster.yml for Rancher HA) or are using the embedded kubectl via the UI.\nBefore running the DNS checks, check the default DNS provider for your cluster and make sure that the overlay network is functioning correctly as this can also be the reason why DNS resolution (partly) fails.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/troubleshooting/dns/","expirydate":-62135596800,"fuzzywordcount":1100,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"5c3544bf95424dcc6ebeb5701517e45d","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/troubleshooting/dns/","postref":"5c3544bf95424dcc6ebeb5701517e45d","publishdate":"0001-01-01T00:00:00Z","readingtime":6,"relpermalink":"/docs/rancher/v2.x/en/troubleshooting/dns/","section":"rancher","tags":null,"title":"DNS","type":"rancher","url":"/docs/rancher/v2.x/en/troubleshooting/dns/","weight":103,"wordcount":1092},{"authors":null,"categories":null,"content":"This document provides prescriptive guidance for hardening a production installation of Rancher v2.2.x. It outlines the configurations and controls required to address Kubernetes benchmark controls from the Center for Information Security (CIS).\n This hardening guide describes how to secure the nodes in your cluster, and it is recommended to follow this guide before installing Kubernetes.\n This hardening guide is intended to be used with specific versions of the CIS Kubernetes Benchmark, Kubernetes, and Rancher:","date":-62135596800,"description":"","dir":"rancher/v2.x/en/security/hardening-2.2/","expirydate":-62135596800,"fuzzywordcount":3000,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"04797ddb336c137c2e1b4b6fb6d99680","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/security/hardening-2.2/","postref":"04797ddb336c137c2e1b4b6fb6d99680","publishdate":"0001-01-01T00:00:00Z","readingtime":15,"relpermalink":"/docs/rancher/v2.x/en/security/hardening-2.2/","section":"rancher","tags":null,"title":"Hardening Guide v2.2","type":"rancher","url":"/docs/rancher/v2.x/en/security/hardening-2.2/","weight":103,"wordcount":2996},{"authors":null,"categories":null,"content":"This document provides prescriptive guidance for hardening a production installation of Rancher v2.1.x. It outlines the configurations and controls required to address Kubernetes benchmark controls from the Center for Information Security (CIS).\n This hardening guide describes how to secure the nodes in your cluster, and it is recommended to follow this guide before installing Kubernetes.\n This hardening guide is intended to be used with specific versions of the CIS Kubernetes Benchmark, Kubernetes, and Rancher:","date":-62135596800,"description":"","dir":"rancher/v2.x/en/security/hardening-2.1/","expirydate":-62135596800,"fuzzywordcount":2800,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"9f0b1529071cb8b815164bdd76f57d42","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/security/hardening-2.1/","postref":"9f0b1529071cb8b815164bdd76f57d42","publishdate":"0001-01-01T00:00:00Z","readingtime":14,"relpermalink":"/docs/rancher/v2.x/en/security/hardening-2.1/","section":"rancher","tags":null,"title":"Hardening Guide v2.1","type":"rancher","url":"/docs/rancher/v2.x/en/security/hardening-2.1/","weight":104,"wordcount":2782},{"authors":null,"categories":null,"content":"The commands/steps listed on this page can be used to check your Rancher Kubernetes Installation.\nMake sure you configured the correct kubeconfig (for example, export KUBECONFIG=$PWD/kube_config_rancher-cluster.yml).\nCheck Rancher pods Rancher pods are deployed as a Deployment in the cattle-system namespace.\nCheck if the pods are running on all nodes:\nkubectl -n cattle-system get pods -l app=rancher -o wide  Example output:\nNAME READY STATUS RESTARTS AGE IP NODE rancher-7dbd7875f7-n6t5t 1/1 Running 0 8m x.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/troubleshooting/rancherha/","expirydate":-62135596800,"fuzzywordcount":300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"19a0a58c2874459db34bc2613208a43a","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/troubleshooting/rancherha/","postref":"19a0a58c2874459db34bc2613208a43a","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/troubleshooting/rancherha/","section":"rancher","tags":null,"title":"Rancher HA","type":"rancher","url":"/docs/rancher/v2.x/en/troubleshooting/rancherha/","weight":104,"wordcount":261},{"authors":null,"categories":null,"content":"RancherOS is available as an Amazon Web Services AMI, and can be easily run on EC2. You can launch RancherOS either using the AWS Command Line Interface (CLI) or using the AWS console.\nLaunching RancherOS through the AWS CLI If you haven&rsquo;t installed the AWS CLI, follow the instructions on the AWS CLI page to install the CLI and configure access key and secret keys.\nOnce you&rsquo;ve installed your AWS CLI, use this command to launch an EC2 instance with the RancherOS AMI.","date":-62135596800,"description":"","dir":"os/v1.x/en/installation/running-rancheros/cloud/aws/","expirydate":-62135596800,"fuzzywordcount":500,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"8e4405bc4f06bcc6c117f99c68c19687","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/running-rancheros/cloud/aws/","postref":"8e4405bc4f06bcc6c117f99c68c19687","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/os/v1.x/en/installation/running-rancheros/cloud/aws/","section":"os","tags":null,"title":"Amazon EC2","type":"os","url":"/docs/os/v1.x/en/installation/running-rancheros/cloud/aws/","weight":105,"wordcount":427},{"authors":null,"categories":null,"content":"CIS Kubernetes Benchmark 1.5 - Rancher 2.3.5 with Kubernetes 1.15 Click here to download a PDF version of this document\nOverview This document is a companion to the Rancher v2.3.5 security hardening guide. The hardening guide provides prescriptive guidance for hardening a production installation of Rancher, and this benchmark guide is meant to help you evaluate the level of security of the hardened cluster against each control in the benchmark.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/security/benchmark-2.3.5/","expirydate":-62135596800,"fuzzywordcount":6200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"209f5510e9c78daffe5a39cd15288f62","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/security/benchmark-2.3.5/","postref":"209f5510e9c78daffe5a39cd15288f62","publishdate":"0001-01-01T00:00:00Z","readingtime":29,"relpermalink":"/docs/rancher/v2.x/en/security/benchmark-2.3.5/","section":"rancher","tags":null,"title":"CIS Benchmark Rancher Self-Assessment Guide - v2.3.5","type":"rancher","url":"/docs/rancher/v2.x/en/security/benchmark-2.3.5/","weight":105,"wordcount":6142},{"authors":null,"categories":null,"content":"The commands/steps listed on this page can be used to check clusters that you are importing or that are imported in Rancher.\nMake sure you configured the correct kubeconfig (for example, export KUBECONFIG=$PWD/kubeconfig_from_imported_cluster.yml)\nRancher agents Communication to the cluster (Kubernetes API via cattle-cluster-agent) and communication to the nodes is done through Rancher agents.\nIf the cattle-cluster-agent cannot connect to the configured server-url, the cluster will remain in Pending state, showing Waiting for full cluster configuration.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/troubleshooting/imported-clusters/","expirydate":-62135596800,"fuzzywordcount":300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"1bdd6bb14c90e4a5edc0e56d3f55f0ef","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/troubleshooting/imported-clusters/","postref":"1bdd6bb14c90e4a5edc0e56d3f55f0ef","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/troubleshooting/imported-clusters/","section":"rancher","tags":null,"title":"Imported clusters","type":"rancher","url":"/docs/rancher/v2.x/en/troubleshooting/imported-clusters/","weight":105,"wordcount":239},{"authors":null,"categories":null,"content":"This document is a companion to the Rancher v2.3.3 security hardening guide. The hardening guide provides prescriptive guidance for hardening a production installation of Rancher, and this benchmark guide is meant to help you evaluate the level of security of the hardened cluster against each control in the benchmark.\nThis guide corresponds to specific versions of the hardening guide, Rancher, Kubernetes, and the CIS Benchmark:\n   Self Assessment Guide Version Rancher Version Hardening Guide Version Kubernetes Version CIS Benchmark Version     Self Assessment Guide v2.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/security/benchmark-2.3.3/","expirydate":-62135596800,"fuzzywordcount":5400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"fedb85fe9674fdd74112f98a0d91f3a0","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/security/benchmark-2.3.3/","postref":"fedb85fe9674fdd74112f98a0d91f3a0","publishdate":"0001-01-01T00:00:00Z","readingtime":26,"relpermalink":"/docs/rancher/v2.x/en/security/benchmark-2.3.3/","section":"rancher","tags":null,"title":"CIS Benchmark Rancher Self-Assessment Guide - Rancher v2.3.3","type":"rancher","url":"/docs/rancher/v2.x/en/security/benchmark-2.3.3/","weight":106,"wordcount":5389},{"authors":null,"categories":null,"content":"Note: Due to the maximum transmission unit (MTU) of 1460 bytes on GCE, you will need to configure your network interfaces and both the Docker and System Docker to use a MTU of 1460 bytes or you will encounter weird networking related errors.\n Adding the RancherOS Image into GCE RancherOS is available as an image in GCE, and can be easily run in Google Compute Engine (GCE). Let’s walk through how to upload GCE image.","date":-62135596800,"description":"","dir":"os/v1.x/en/installation/running-rancheros/cloud/gce/","expirydate":-62135596800,"fuzzywordcount":1000,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"9a5b2e4c0b2d99d5227efb4823b18aec","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/running-rancheros/cloud/gce/","postref":"9a5b2e4c0b2d99d5227efb4823b18aec","publishdate":"0001-01-01T00:00:00Z","readingtime":5,"relpermalink":"/docs/os/v1.x/en/installation/running-rancheros/cloud/gce/","section":"os","tags":null,"title":"Google Compute Engine (GCE)","type":"os","url":"/docs/os/v1.x/en/installation/running-rancheros/cloud/gce/","weight":106,"wordcount":980},{"authors":null,"categories":null,"content":"This document is a companion to the Rancher v2.3 security hardening guide. The hardening guide provides prescriptive guidance for hardening a production installation of Rancher, and this benchmark guide is meant to help you evaluate the level of security of the hardened cluster against each control in the benchmark.\nThis guide corresponds to specific versions of the hardening guide, Rancher, Kubernetes, and the CIS Benchmark:\n   Self Assessment Guide Version Rancher Version Hardening Guide Version Kubernetes Version CIS Benchmark Version     Self Assessment Guide v2.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/security/benchmark-2.3/","expirydate":-62135596800,"fuzzywordcount":5400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"03ba243ca5fb93e295ce8f9f67856dc5","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/security/benchmark-2.3/","postref":"03ba243ca5fb93e295ce8f9f67856dc5","publishdate":"0001-01-01T00:00:00Z","readingtime":26,"relpermalink":"/docs/rancher/v2.x/en/security/benchmark-2.3/","section":"rancher","tags":null,"title":"CIS Benchmark Rancher Self-Assessment Guide v2.3","type":"rancher","url":"/docs/rancher/v2.x/en/security/benchmark-2.3/","weight":107,"wordcount":5352},{"authors":null,"categories":null,"content":"RancherOS is available in the Digital Ocean portal. RancherOS is a member of container distributions and you can find it easily.\n Note Deploying to Digital Ocean will incur charges.\n To start a RancherOS Droplet on Digital Ocean:\n In the Digital Ocean portal, go to the project view. Click New Droplet. Click Create Droplet. Click the Container distributions tab. Click RancherOS. Choose a plan. Make sure your Droplet has the minimum hardware requirements for RancherOS.","date":-62135596800,"description":"","dir":"os/v1.x/en/installation/running-rancheros/cloud/do/","expirydate":-62135596800,"fuzzywordcount":300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"a6c04f8a605e25e524a8fa020f59c48d","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/running-rancheros/cloud/do/","postref":"a6c04f8a605e25e524a8fa020f59c48d","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/os/v1.x/en/installation/running-rancheros/cloud/do/","section":"os","tags":null,"title":"Digital Ocean","type":"os","url":"/docs/os/v1.x/en/installation/running-rancheros/cloud/do/","weight":107,"wordcount":239},{"authors":null,"categories":null,"content":"This document is a companion to the Rancher v2.2 security hardening guide. The hardening guide provides prescriptive guidance for hardening a production installation of Rancher, and this benchmark guide is meant to help you evaluate the level of security of the hardened cluster against each control in the benchmark.\nThis guide corresponds to specific versions of the hardening guide, Rancher, Kubernetes, and the CIS Benchmark:\n   Self Assessment Guide Version Rancher Version Hardening Guide Version Kubernetes Version CIS Benchmark Version     Self Assessment Guide v2.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/security/benchmark-2.2/","expirydate":-62135596800,"fuzzywordcount":5500,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"d23dc73fbd40406b6025b3007cc56bd5","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/security/benchmark-2.2/","postref":"d23dc73fbd40406b6025b3007cc56bd5","publishdate":"0001-01-01T00:00:00Z","readingtime":26,"relpermalink":"/docs/rancher/v2.x/en/security/benchmark-2.2/","section":"rancher","tags":null,"title":"CIS Benchmark Rancher Self-Assessment Guide v2.2","type":"rancher","url":"/docs/rancher/v2.x/en/security/benchmark-2.2/","weight":108,"wordcount":5477},{"authors":null,"categories":null,"content":"As of v1.1.0, RancherOS automatically detects that it is running on VMware ESXi, and automatically adds the open-vm-tools service to be downloaded and started, and uses guestinfo keys to set the cloud-init data.\nAs of v1.5.0, RancherOS releases anything required for VMware, which includes initrd, a standard ISO for VMware, a vmdk image, and a specific ISO to be used with Docker Machine. The open-vm-tools is built in to RancherOS, there is no need to download it.","date":-62135596800,"description":"","dir":"os/v1.x/en/installation/running-rancheros/cloud/vmware-esxi/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"a8b24eff65bdb6d412f6bed79975e9e8","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/running-rancheros/cloud/vmware-esxi/","postref":"a8b24eff65bdb6d412f6bed79975e9e8","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/os/v1.x/en/installation/running-rancheros/cloud/vmware-esxi/","section":"os","tags":null,"title":"VMware ESXi","type":"os","url":"/docs/os/v1.x/en/installation/running-rancheros/cloud/vmware-esxi/","weight":108,"wordcount":182},{"authors":null,"categories":null,"content":"This document is a companion to the Rancher v2.1 security hardening guide. The hardening guide provides prescriptive guidance for hardening a production installation of Rancher, and this benchmark guide is meant to help you evaluate the level of security of the hardened cluster against each control in the benchmark.\nThis guide corresponds to specific versions of the hardening guide, Rancher, Kubernetes, and the CIS Benchmark:\n   Self Assessment Guide Version Rancher Version Hardening Guide Version Kubernetes Version CIS Benchmark Version     Self Assessment Guide v2.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/security/benchmark-2.1/","expirydate":-62135596800,"fuzzywordcount":5400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"f2a4757bf2e2a04ab1149390ccf6dcef","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/security/benchmark-2.1/","postref":"f2a4757bf2e2a04ab1149390ccf6dcef","publishdate":"0001-01-01T00:00:00Z","readingtime":25,"relpermalink":"/docs/rancher/v2.x/en/security/benchmark-2.1/","section":"rancher","tags":null,"title":"CIS Benchmark Rancher Self-Assessment Guide v2.1","type":"rancher","url":"/docs/rancher/v2.x/en/security/benchmark-2.1/","weight":109,"wordcount":5311},{"authors":null,"categories":null,"content":"As of v0.5.0, RancherOS releases include an Openstack image that can be found on our releases page. The image format is QCOW3 that is backward compatible with QCOW2.\nWhen launching an instance using the image, you must enable Advanced Options -&gt; Configuration Drive and in order to use a cloud-config file.","date":-62135596800,"description":"","dir":"os/v1.x/en/installation/running-rancheros/cloud/openstack/","expirydate":-62135596800,"fuzzywordcount":100,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"a674c29750b37079ebfcde751bce1bdc","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/running-rancheros/cloud/openstack/","postref":"a674c29750b37079ebfcde751bce1bdc","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/os/v1.x/en/installation/running-rancheros/cloud/openstack/","section":"os","tags":null,"title":"OpenStack","type":"os","url":"/docs/os/v1.x/en/installation/running-rancheros/cloud/openstack/","weight":109,"wordcount":51},{"authors":null,"categories":null,"content":"RancherOS has been published in Azure Marketplace, you can get it from here.\nLaunching RancherOS through the Azure Portal Using the new Azure Resource Management portal, click on Marketplace. Search for RancherOS. Click on Create.\nFollow the steps to create a virtual machine.\nIn the Basics step, provide a name for the VM, use rancher as the user name and select the SSH public key option of authenticating. Add your ssh public key into the appropriate field.","date":-62135596800,"description":"","dir":"os/v1.x/en/installation/running-rancheros/cloud/azure/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"27a2420fadd1837376e30d7a506f967e","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/running-rancheros/cloud/azure/","postref":"27a2420fadd1837376e30d7a506f967e","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/os/v1.x/en/installation/running-rancheros/cloud/azure/","section":"os","tags":null,"title":"Azure","type":"os","url":"/docs/os/v1.x/en/installation/running-rancheros/cloud/azure/","weight":110,"wordcount":350},{"authors":null,"categories":null,"content":"Adding the RancherOS Image into Aliyun RancherOS is available as an image in Aliyun, and can be easily run in Elastic Compute Service (ECS). Let’s walk through how to upload the ECS image.\n Download the most recent RancherOS image. The image rancheros-aliyun.vhd can be found in the release artifacts. Follow Aliyun&rsquo;s instructions on how to upload the image. Before the image can be added, it must be uploaded into an OSS bucket.","date":-62135596800,"description":"","dir":"os/v1.x/en/installation/running-rancheros/cloud/aliyun/","expirydate":-62135596800,"fuzzywordcount":300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"c94540b4c2994311ca8028bba2ec057c","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/running-rancheros/cloud/aliyun/","postref":"c94540b4c2994311ca8028bba2ec057c","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/os/v1.x/en/installation/running-rancheros/cloud/aliyun/","section":"os","tags":null,"title":"Aliyun","type":"os","url":"/docs/os/v1.x/en/installation/running-rancheros/cloud/aliyun/","weight":111,"wordcount":203},{"authors":null,"categories":null,"content":"RancherOS comes with a simple installer that will install RancherOS on a given target disk. To install RancherOS on a new disk, you can use the ros install command. Before installing, you&rsquo;ll need to have already booted RancherOS from ISO. Please be sure to pick the rancheros.iso from our release page.\nUsing ros install to Install RancherOS The ros install command orchestrates the installation from the rancher/os container. You will need to have already created a cloud-config file and found the target disk.","date":-62135596800,"description":"","dir":"os/v1.x/en/installation/running-rancheros/server/install-to-disk/","expirydate":-62135596800,"fuzzywordcount":700,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"4a1d60b7e9d1205c7625bbc25de910a6","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/running-rancheros/server/install-to-disk/","postref":"4a1d60b7e9d1205c7625bbc25de910a6","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/docs/os/v1.x/en/installation/running-rancheros/server/install-to-disk/","section":"os","tags":null,"title":"Installing to Disk","type":"os","url":"/docs/os/v1.x/en/installation/running-rancheros/server/install-to-disk/","weight":111,"wordcount":699},{"authors":null,"categories":null,"content":"#!ipxe # Boot a persistent RancherOS to RAM # Location of Kernel/Initrd images set base-url http://releases.rancher.com/os/latest kernel ${base-url}/vmlinuz rancher.state.dev=LABEL=RANCHER_STATE rancher.state.autoformat=[/dev/sda] rancher.state.wait rancher.cloud_init.datasources=[url:http://example.com/cloud-config] initrd ${base-url}/initrd boot  If you want to autoformat the disk when booting by iPXE, you should add the rancher.state.autoformat part to kernel cmdline. However, this does not install the bootloader to disk, so you cannot upgrade RancherOS.\nIf you don&rsquo;t add rancher.state.autoformat, RancherOS will run completely in memory, you can execute ros install to install to disk.","date":-62135596800,"description":"","dir":"os/v1.x/en/installation/running-rancheros/server/pxe/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"0ffcc26a0a3ef4f9a457ba4a2c03ef49","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/running-rancheros/server/pxe/","postref":"0ffcc26a0a3ef4f9a457ba4a2c03ef49","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/os/v1.x/en/installation/running-rancheros/server/pxe/","section":"os","tags":null,"title":"iPXE","type":"os","url":"/docs/os/v1.x/en/installation/running-rancheros/server/pxe/","weight":112,"wordcount":306},{"authors":null,"categories":null,"content":"As of v0.5.0, RancherOS releases include a Raspberry Pi image that can be found on our releases page. The official Raspberry Pi documentation contains instructions on how to install operating system images.\nWhen installing, there is no ability to pass in a cloud-config. You will need to boot up, change the configuration and then reboot to apply those changes.\nCurrently, only Raspberry Pi 3 is tested and known to work.","date":-62135596800,"description":"","dir":"os/v1.x/en/installation/running-rancheros/server/raspberry-pi/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"b05372d695ee0bdce3b14d864ca86422","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/running-rancheros/server/raspberry-pi/","postref":"b05372d695ee0bdce3b14d864ca86422","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/os/v1.x/en/installation/running-rancheros/server/raspberry-pi/","section":"os","tags":null,"title":"Raspberry Pi","type":"os","url":"/docs/os/v1.x/en/installation/running-rancheros/server/raspberry-pi/","weight":113,"wordcount":364},{"authors":null,"categories":null,"content":"There are two ways that RancherOS can be configured.\n A cloud-config file can be used to provide configuration when first booting RancherOS. Manually changing configuration with the ros config command.  Typically, when you first boot the server, you pass in a cloud-config file to configure the initialization of the server. After the first boot, if you have any changes for the configuration, it&rsquo;s recommended that you use ros config to set the necessary configuration properties.","date":-62135596800,"description":"","dir":"os/v1.x/en/installation/configuration/","expirydate":-62135596800,"fuzzywordcount":600,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"f02e7306e1100c5dc7c788835e7c7759","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/configuration/","postref":"f02e7306e1100c5dc7c788835e7c7759","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/os/v1.x/en/installation/configuration/","section":"os","tags":null,"title":"Configuration","type":"os","url":"/docs/os/v1.x/en/installation/configuration/","weight":120,"wordcount":567},{"authors":null,"categories":null,"content":"The default console keeps time in the Coordinated Universal Time (UTC) zone and synchronizes clocks with the Network Time Protocol (NTP). The Network Time Protocol daemon (ntpd) is an operating system program that maintains the system time in synchronization with time servers using the NTP.\nRancherOS can run ntpd in the System Docker container. You can update its configurations by updating /etc/ntp.conf. For an example of how to update a file such as /etc/ntp.","date":-62135596800,"description":"","dir":"os/v1.x/en/installation/configuration/date-and-timezone/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"79b2882d457d3e7815ac2e720adbaccf","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/configuration/date-and-timezone/","postref":"79b2882d457d3e7815ac2e720adbaccf","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/os/v1.x/en/installation/configuration/date-and-timezone/","section":"os","tags":null,"title":"Date and time zone","type":"os","url":"/docs/os/v1.x/en/installation/configuration/date-and-timezone/","weight":121,"wordcount":175},{"authors":null,"categories":null,"content":"Available as of v1.3\nWhen you have built your own docker registries, and have cached the rancher/os and other os-services images, something like a normal docker pull rancher/os can be cached as docker pull dockerhub.mycompanyname.com/docker.io/rancher/os.\nHowever, you need a way to inject a prefix into RancherOS for installation or service pulls. RancherOS supports a global prefix you can add to force ROS to always use your mirror.\nYou can config a global image prefix:","date":-62135596800,"description":"","dir":"os/v1.x/en/installation/configuration/images-prefix/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"6528d43711f938a8862042af597784e7","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/configuration/images-prefix/","postref":"6528d43711f938a8862042af597784e7","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/os/v1.x/en/installation/configuration/images-prefix/","section":"os","tags":null,"title":"Images prefix","type":"os","url":"/docs/os/v1.x/en/installation/configuration/images-prefix/","weight":121,"wordcount":143},{"authors":null,"categories":null,"content":"RancherOS supports adding SSH keys through the cloud-config file. Within the cloud-config file, you simply add the ssh keys within the ssh_authorized_keys key.\n#cloud-configssh_authorized_keys:-ssh-rsaAAA...ZZZexample1@rancher-ssh-rsaBBB...ZZZexample2@rancher When we pass the cloud-config file during the ros install command, it will allow these ssh keys to be associated with the rancher user. You can ssh into RancherOS using the key.\n$ ssh -i /path/to/private/key rancher@&lt;ip-address&gt;  Please note that OpenSSH 7.0 and greater similarly disable the ssh-dss (DSA) public key algorithm.","date":-62135596800,"description":"","dir":"os/v1.x/en/installation/configuration/ssh-keys/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"dc13599f15e5df8955d64c52f38d15af","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/configuration/ssh-keys/","postref":"dc13599f15e5df8955d64c52f38d15af","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/os/v1.x/en/installation/configuration/ssh-keys/","section":"os","tags":null,"title":"SSH Settings","type":"os","url":"/docs/os/v1.x/en/installation/configuration/ssh-keys/","weight":121,"wordcount":156},{"authors":null,"categories":null,"content":"You can automate writing files to disk using the write_files cloud-config directive.\n#cloud-configwrite_files:-path:/etc/rc.localpermissions:&#34;0755&#34;owner:rootcontent:| #!/bin/bashecho&#34;I&#39;m doing things on start&#34; Writing Files in Specific System Services By default, the write_files directive will create files in the console container. To write files in other system services, the container key can be used. For example, the container key could be used to write to /etc/ntp.conf in the NTP system service.\n#cloud-configwrite_files:-container:ntppath:/etc/ntp.confpermissions:&#34;0644&#34;owner:rootcontent:| server 0.pool.ntp.org iburstserver1.","date":-62135596800,"description":"","dir":"os/v1.x/en/installation/configuration/write-files/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"93eb85ae4461c2af95fc8b4af1402699","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/configuration/write-files/","postref":"93eb85ae4461c2af95fc8b4af1402699","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/os/v1.x/en/installation/configuration/write-files/","section":"os","tags":null,"title":"Writing Files","type":"os","url":"/docs/os/v1.x/en/installation/configuration/write-files/","weight":122,"wordcount":116},{"authors":null,"categories":null,"content":"You can automate running commands on boot using the runcmd cloud-config directive. Commands can be specified as either a list or a string. In the latter case, the command is executed with sh.\n#cloud-configruncmd:-[touch,/home/rancher/test1]-echo&#34;test&#34;&gt;/home/rancher/test2 Commands specified using runcmd will be executed within the context of the console container.\nRunning Docker commands When using runcmd, RancherOS will wait for all commands to complete before starting Docker. As a result, any docker run command should not be placed under runcmd.","date":-62135596800,"description":"","dir":"os/v1.x/en/installation/configuration/running-commands/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"08d2f6537fb0f3227a78762d1172b1d1","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/configuration/running-commands/","postref":"08d2f6537fb0f3227a78762d1172b1d1","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/os/v1.x/en/installation/configuration/running-commands/","section":"os","tags":null,"title":"Running Commands","type":"os","url":"/docs/os/v1.x/en/installation/configuration/running-commands/","weight":123,"wordcount":149},{"authors":null,"categories":null,"content":"You can set the hostname of the host using cloud-config. The example below shows how to configure it.","date":-62135596800,"description":"","dir":"os/v1.x/en/installation/configuration/hostname/","expirydate":-62135596800,"fuzzywordcount":100,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"a8501d3bfba34ff5815d7b893d2da654","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/configuration/hostname/","postref":"a8501d3bfba34ff5815d7b893d2da654","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/os/v1.x/en/installation/configuration/hostname/","section":"os","tags":null,"title":"Setting the Hostname","type":"os","url":"/docs/os/v1.x/en/installation/configuration/hostname/","weight":124,"wordcount":19},{"authors":null,"categories":null,"content":"When booting from the ISO, RancherOS starts with the default console, which is based on busybox.\nYou can select which console you want RancherOS to start with using the cloud-config.\nEnabling Consoles using Cloud-Config When launching RancherOS with a cloud-config file, you can select which console you want to use.\nCurrently, the list of available consoles are:\n default alpine centos debian fedora ubuntu  Here is an example cloud-config file that can be used to enable the debian console.","date":-62135596800,"description":"","dir":"os/v1.x/en/installation/configuration/switching-consoles/","expirydate":-62135596800,"fuzzywordcount":600,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"e35b51a8ab94553386c12d70cea3be15","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/configuration/switching-consoles/","postref":"e35b51a8ab94553386c12d70cea3be15","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/os/v1.x/en/installation/configuration/switching-consoles/","section":"os","tags":null,"title":"Switching Consoles","type":"os","url":"/docs/os/v1.x/en/installation/configuration/switching-consoles/","weight":125,"wordcount":527},{"authors":null,"categories":null,"content":"In RancherOS, you can configure System Docker and Docker daemons by using cloud-config.\nConfiguring Docker In your cloud-config, Docker configuration is located under the rancher.docker key.\n#cloud-configrancher:docker:tls:truetls_args:-&#34;--tlsverify&#34;-&#34;--tlscacert=/etc/docker/tls/ca.pem&#34;-&#34;--tlscert=/etc/docker/tls/server-cert.pem&#34;-&#34;--tlskey=/etc/docker/tls/server-key.pem&#34;-&#34;-H=0.0.0.0:2376&#34;storage_driver:overlay You can also customize Docker after it&rsquo;s been started using ros config.\n$ sudo ros config set rancher.docker.storage_driver overlay  User Docker settings Many of the standard Docker daemon arguments can be placed under the rancher.docker key. The command needed to start the Docker daemon will be generated based on these arguments.","date":-62135596800,"description":"","dir":"os/v1.x/en/installation/configuration/docker/","expirydate":-62135596800,"fuzzywordcount":1100,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"93f80aefc0139661e4be57742b5ef99b","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/configuration/docker/","postref":"93f80aefc0139661e4be57742b5ef99b","publishdate":"0001-01-01T00:00:00Z","readingtime":6,"relpermalink":"/docs/os/v1.x/en/installation/configuration/docker/","section":"os","tags":null,"title":"Configuring Docker or System Docker","type":"os","url":"/docs/os/v1.x/en/installation/configuration/docker/","weight":126,"wordcount":1080},{"authors":null,"categories":null,"content":"ros tls generate is used to generate both the client and server TLS certificates for Docker.\nRemember, all ros commands need to be used with sudo or as a root user.\nEnd to end example Enable TLS for Docker and Generate Server Certificate To have docker secured by TLS you need to set rancher.docker.tls to true, and generate a set of server and client keys and certificates:\n$ sudo ros config set rancher.","date":-62135596800,"description":"","dir":"os/v1.x/en/installation/configuration/setting-up-docker-tls/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"d79f5fc861105683e86dd2fbfcc29a62","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/configuration/setting-up-docker-tls/","postref":"d79f5fc861105683e86dd2fbfcc29a62","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/os/v1.x/en/installation/configuration/setting-up-docker-tls/","section":"os","tags":null,"title":"Setting up Docker TLS","type":"os","url":"/docs/os/v1.x/en/installation/configuration/setting-up-docker-tls/","weight":127,"wordcount":313},{"authors":null,"categories":null,"content":"When launching services through a cloud-config, it is sometimes necessary to pull a private image from DockerHub or from a private registry. Authentication for these can be embedded in your cloud-config.\nFor example, to add authentication for DockerHub:\n#cloud-configrancher:registry_auths:https://index.docker.io/v1/:auth:dXNlcm5hbWU6cGFzc3dvcmQ= The auth key is generated by base64 encoding a string of the form username:password. The docker login command can be used to generate an auth key. After running the command and authenticating successfully, the key can be found in the $HOME/.","date":-62135596800,"description":"","dir":"os/v1.x/en/installation/configuration/private-registries/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"86f43e4fa9394196218018bbd9b3bc7a","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/configuration/private-registries/","postref":"86f43e4fa9394196218018bbd9b3bc7a","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/os/v1.x/en/installation/configuration/private-registries/","section":"os","tags":null,"title":"Private Registries","type":"os","url":"/docs/os/v1.x/en/installation/configuration/private-registries/","weight":128,"wordcount":193},{"authors":null,"categories":null,"content":"The version of User Docker used in RancherOS can be configured using a cloud-config file or by using the ros engine command.\n Note: There are known issues in Docker when switching between versions. For production systems, we recommend setting the Docker engine only once using a cloud-config.\n Available Docker engines The ros engine list command can be used to show which Docker engines are available to switch to.","date":-62135596800,"description":"","dir":"os/v1.x/en/installation/configuration/switching-docker-versions/","expirydate":-62135596800,"fuzzywordcount":500,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"6a399dacb71f56f413a7bc5da13d59e7","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/configuration/switching-docker-versions/","postref":"6a399dacb71f56f413a7bc5da13d59e7","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/os/v1.x/en/installation/configuration/switching-docker-versions/","section":"os","tags":null,"title":"Switching Docker Versions","type":"os","url":"/docs/os/v1.x/en/installation/configuration/switching-docker-versions/","weight":129,"wordcount":471},{"authors":null,"categories":null,"content":"Currently, we don&rsquo;t support adding other users besides rancher.\nYou can add users in the console container, but these users will only exist as long as the console container exists. It only makes sense to add users in a persistent consoles.\nIf you want the console user to be able to ssh into RancherOS, you need to add them to the docker group.","date":-62135596800,"description":"","dir":"os/v1.x/en/installation/configuration/users/","expirydate":-62135596800,"fuzzywordcount":100,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"6957658d4bb71ed971dceb87c74d65aa","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/configuration/users/","postref":"6957658d4bb71ed971dceb87c74d65aa","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/os/v1.x/en/installation/configuration/users/","section":"os","tags":null,"title":"Users","type":"os","url":"/docs/os/v1.x/en/installation/configuration/users/","weight":130,"wordcount":63},{"authors":null,"categories":null,"content":"The resize_device cloud config option can be used to automatically extend the first partition (assuming its ext4) to fill the size of it&rsquo;s device.\nOnce the partition has been resized to fill the device, a /var/lib/rancher/resizefs.done file will be written to prevent the resize tools from being run again. If you need it to run again, delete that file and reboot.\n#cloud-configrancher:resize_device:/dev/sda This behavior is the default when launching RancherOS on AWS.","date":-62135596800,"description":"","dir":"os/v1.x/en/installation/configuration/resizing-device-partition/","expirydate":-62135596800,"fuzzywordcount":100,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"408a84cb67c67f60d4d10fe94030dc55","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/configuration/resizing-device-partition/","postref":"408a84cb67c67f60d4d10fe94030dc55","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/os/v1.x/en/installation/configuration/resizing-device-partition/","section":"os","tags":null,"title":"Resizing a Device Partition","type":"os","url":"/docs/os/v1.x/en/installation/configuration/resizing-device-partition/","weight":131,"wordcount":72},{"authors":null,"categories":null,"content":"The rancher.sysctl cloud-config key can be used to control sysctl parameters. This works in a manner similar to /etc/sysctl.conf for other Linux distros.\n#cloud-config rancher: sysctl: net.ipv4.conf.default.rp_filter: 1  You can either add these settings to your cloud-init.yml, or use sudo ros config merge -i somefile.yml to merge settings into your existing system.","date":-62135596800,"description":"","dir":"os/v1.x/en/installation/configuration/sysctl/","expirydate":-62135596800,"fuzzywordcount":100,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"e51b02b4aeaa6663f789b49034a95769","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/configuration/sysctl/","postref":"e51b02b4aeaa6663f789b49034a95769","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/os/v1.x/en/installation/configuration/sysctl/","section":"os","tags":null,"title":"Sysctl Settings","type":"os","url":"/docs/os/v1.x/en/installation/configuration/sysctl/","weight":132,"wordcount":52},{"authors":null,"categories":null,"content":"RancherOS parses the Linux kernel boot cmdline to add any keys it understands to its configuration. This allows you to modify what cloud-init sources it will use on boot, to enable rancher.debug logging, or to almost any other configuration setting.\nThere are two ways to set or modify persistent kernel parameters, in-place (editing the file and reboot) or during installation to disk.\nIn-place editing Available as of v1.1\nTo edit the kernel boot parameters of an already installed RancherOS system, use the new sudo ros config syslinux editing command (uses vi).","date":-62135596800,"description":"","dir":"os/v1.x/en/installation/configuration/adding-kernel-parameters/","expirydate":-62135596800,"fuzzywordcount":500,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"f8b1bf0eb8375305a89a62cc2f8e6ed2","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/configuration/adding-kernel-parameters/","postref":"f8b1bf0eb8375305a89a62cc2f8e6ed2","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/os/v1.x/en/installation/configuration/adding-kernel-parameters/","section":"os","tags":null,"title":"Kernel boot parameters","type":"os","url":"/docs/os/v1.x/en/installation/configuration/adding-kernel-parameters/","weight":133,"wordcount":460},{"authors":null,"categories":null,"content":"Since RancherOS v0.8, we build our own kernels using an unmodified kernel.org LTS kernel. We provide both loading kernel modules with parameters and loading extra kernel modules for you.\nLoading Kernel Modules with parameters Available as of v1.4\nThe rancher.modules can help you to set kernel modules or module parameters.\nAs an example, I&rsquo;m going to set a parameter for kernel module ndb\nsudo ros config set rancher.modules &quot;['nbd nbds_max=1024', 'nfs']&quot;  Or","date":-62135596800,"description":"","dir":"os/v1.x/en/installation/configuration/loading-kernel-modules/","expirydate":-62135596800,"fuzzywordcount":500,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"f3afe1ced70f1bfe0379ab1091919eff","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/configuration/loading-kernel-modules/","postref":"f3afe1ced70f1bfe0379ab1091919eff","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/os/v1.x/en/installation/configuration/loading-kernel-modules/","section":"os","tags":null,"title":"Loading Kernel Modules","type":"os","url":"/docs/os/v1.x/en/installation/configuration/loading-kernel-modules/","weight":134,"wordcount":447},{"authors":null,"categories":null,"content":"To compile any kernel modules, you will need to download the kernel headers. The kernel headers are available in the form of a system service. Since the kernel headers are a system service, they need to be enabled using the ros service command.\nInstalling Kernel Headers The following commands can be used to install kernel headers for usage by containers in Docker or System Docker.\nDocker $ sudo ros service enable kernel-headers $ sudo ros service up kernel-headers  System Docker $ sudo ros service enable kernel-headers-system-docker $ sudo ros service up kernel-headers-system-docker  The ros service commands will install the kernel headers in /lib/modules/$(uname -r)/build.","date":-62135596800,"description":"","dir":"os/v1.x/en/installation/configuration/kernel-modules-kernel-headers/","expirydate":-62135596800,"fuzzywordcount":300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"4388eefd47e54d899c58af2e75a3ef82","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/configuration/kernel-modules-kernel-headers/","postref":"4388eefd47e54d899c58af2e75a3ef82","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/os/v1.x/en/installation/configuration/kernel-modules-kernel-headers/","section":"os","tags":null,"title":"Installing Kernel Modules that require Kernel Headers","type":"os","url":"/docs/os/v1.x/en/installation/configuration/kernel-modules-kernel-headers/","weight":135,"wordcount":245},{"authors":null,"categories":null,"content":"Available as of v1.5\nIn RancherOS, you can set rancher.password as a kernel parameter and auto-login to be enabled, but there may be some cases where we want to disable both of these options. Both of these options can be disabled in the cloud-config or as part of a ros command.\nHow to Disabling Options If RancherOS has already been started, you can use ros config set to update that you want to disable","date":-62135596800,"description":"","dir":"os/v1.x/en/installation/configuration/disable-access-to-system/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"37cbbc16783e2004366eb250c9163bc2","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/configuration/disable-access-to-system/","postref":"37cbbc16783e2004366eb250c9163bc2","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/os/v1.x/en/installation/configuration/disable-access-to-system/","section":"os","tags":null,"title":"Disabling Access to RancherOS","type":"os","url":"/docs/os/v1.x/en/installation/configuration/disable-access-to-system/","weight":136,"wordcount":118},{"authors":null,"categories":null,"content":"In the air gap environment, the Docker registry, RancherOS repositories URL, and the RancherOS upgrade URL should be configured to ensure the OS can pull images, update OS services, and upgrade the OS.\nConfiguring a Private Docker Registry You should use a private Docker registry so that user-docker and system-docker can pull images.\n Add the private Docker registry domain to the images prefix. Set the private registry certificates for user-docker.","date":-62135596800,"description":"","dir":"os/v1.x/en/installation/configuration/airgap-configuration/","expirydate":-62135596800,"fuzzywordcount":500,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"01949cf27f78d5448b699d6cf06f82d9","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/configuration/airgap-configuration/","postref":"01949cf27f78d5448b699d6cf06f82d9","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/os/v1.x/en/installation/configuration/airgap-configuration/","section":"os","tags":null,"title":"Air Gap Configuration","type":"os","url":"/docs/os/v1.x/en/installation/configuration/airgap-configuration/","weight":138,"wordcount":428},{"authors":null,"categories":null,"content":"A system service is a container that can be run in either System Docker or Docker. Rancher provides services that are already available in RancherOS by adding them to the os-services repo. Anything in the index.yml file from the repository for the tagged release will be an available system service when using the ros service list command.\nEnabling and Starting System Services For any services that are listed from the ros service list, they can be enabled by running a single command.","date":-62135596800,"description":"","dir":"os/v1.x/en/installation/system-services/adding-system-services/","expirydate":-62135596800,"fuzzywordcount":300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"c0dfd15f9771ffed75af9c5d66b63ef9","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/system-services/adding-system-services/","postref":"c0dfd15f9771ffed75af9c5d66b63ef9","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/os/v1.x/en/installation/system-services/adding-system-services/","section":"os","tags":null,"title":"System Services","type":"os","url":"/docs/os/v1.x/en/installation/system-services/adding-system-services/","weight":140,"wordcount":238},{"authors":null,"categories":null,"content":"You can also create your own system service in Docker Compose format. After creating your own custom service, you can launch it in RancherOS in a couple of methods. The service could be directly added to the cloud-config, or a docker-compose.yml file could be saved at a http(s) url location or in a specific directory of RancherOS.\nLaunching Services through Cloud-Config If you want to boot RancherOS with a system service running, you can add the service to the cloud-config that is passed to RancherOS.","date":-62135596800,"description":"","dir":"os/v1.x/en/installation/system-services/custom-system-services/","expirydate":-62135596800,"fuzzywordcount":1100,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"6911b64d256a604acb17ca57c75d3bb5","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/system-services/custom-system-services/","postref":"6911b64d256a604acb17ca57c75d3bb5","publishdate":"0001-01-01T00:00:00Z","readingtime":5,"relpermalink":"/docs/os/v1.x/en/installation/system-services/custom-system-services/","section":"os","tags":null,"title":"Custom System Services","type":"os","url":"/docs/os/v1.x/en/installation/system-services/custom-system-services/","weight":141,"wordcount":1054},{"authors":null,"categories":null,"content":"A few services are containers in created state. Their purpose is to provide volumes for other services.\nuser-volumes Provides user accessible persistent storage directories, used by console service:\n/home /opt /var/lib/kubelet - Added as of v1.2  Available as of v1.2\nIf you want to change user-volumes, for example, add /etc/kubernetes directory:\n$ sudo ros config set rancher.services.user-volumes.volumes [/home:/home,/opt:/opt,/var/lib/kubelet:/var/lib/kubelet,/etc/kubernetes:/etc/kubernetes] $ sudo reboot  Please note that after the restart, the new persistence directory can take effect.","date":-62135596800,"description":"","dir":"os/v1.x/en/installation/system-services/system-docker-volumes/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"2762c16457b9f1f25e00dbc9de397c45","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/system-services/system-docker-volumes/","postref":"2762c16457b9f1f25e00dbc9de397c45","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/os/v1.x/en/installation/system-services/system-docker-volumes/","section":"os","tags":null,"title":"System Docker Volumes","type":"os","url":"/docs/os/v1.x/en/installation/system-services/system-docker-volumes/","weight":142,"wordcount":149},{"authors":null,"categories":null,"content":"The environment key can be used to customize system services. When a value is not assigned, RancherOS looks up the value from the rancher.environment key.\nIn the example below, ETCD_DISCOVERY will be set to https://discovery.etcd.io/d1cd18f5ee1c1e2223aed6a1734719f7 for the etcd service.\nrancher:environment:ETCD_DISCOVERY:https://discovery.etcd.io/d1cd18f5ee1c1e2223aed6a1734719f7services:etcd:...environment:-ETCD_DISCOVERY Wildcard globbing is also supported. In the example below, ETCD_DISCOVERY will be set as in the previous example, along with any other environment variables beginning with ETCD_.\nrancher:environment:ETCD_DISCOVERY:https://discovery.etcd.io/d1cd18f5ee1c1e2223aed6a1734719f7services:etcd:...environment:-ETCD_* Available as of v1.","date":-62135596800,"description":"","dir":"os/v1.x/en/installation/system-services/environment/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"8498e0bdb2c0134adedeb9eb8ef97903","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/system-services/environment/","postref":"8498e0bdb2c0134adedeb9eb8ef97903","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/os/v1.x/en/installation/system-services/environment/","section":"os","tags":null,"title":"Environment","type":"os","url":"/docs/os/v1.x/en/installation/system-services/environment/","weight":143,"wordcount":103},{"authors":null,"categories":null,"content":"In order to start interacting with your Kubernetes cluster, you will use a different binary called kubectl. You will need to install kubectl on your local machine.\nA kubeconfig file is a file used to configure access to Kubernetes when used in conjunction with the kubectl commandline tool (or other clients).\nFor more details on how kubeconfig and kubectl work together, see the Kubernetes documentation.\nWhen you deployed Kubernetes, a kubeconfig is automatically generated for your RKE cluster.","date":-62135596800,"description":"","dir":"rke/latest/en/kubeconfig/","expirydate":-62135596800,"fuzzywordcount":300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"91909900c240572db80f9163ac94d227","permalink":"http://jijeesh.github.io/docs/rke/latest/en/kubeconfig/","postref":"91909900c240572db80f9163ac94d227","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rke/latest/en/kubeconfig/","section":"rke","tags":null,"title":"Kubeconfig File","type":"rke","url":"/docs/rke/latest/en/kubeconfig/","weight":145,"wordcount":216},{"authors":null,"categories":null,"content":"Available as of v0.1.7\nRKE clusters can be configured to automatically take snapshots of etcd. In a disaster scenario, you can restore these snapshots, which are stored on other nodes in the cluster. Snapshots are always saved locally in /opt/rke/etcd-snapshots.\nAvailable as of v0.2.0\nRKE can upload your snapshots to a S3 compatible backend.\nNote: As of RKE v0.2.0, the pki.bundle.tar.gz file is no longer required because of a change in how the Kubernetes cluster state is stored.","date":-62135596800,"description":"","dir":"rke/latest/en/etcd-snapshots/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"3061f9d2c42187f51ecdc3849d0f523f","permalink":"http://jijeesh.github.io/docs/rke/latest/en/etcd-snapshots/","postref":"3061f9d2c42187f51ecdc3849d0f523f","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rke/latest/en/etcd-snapshots/","section":"rke","tags":null,"title":"Backups and Disaster Recovery","type":"rke","url":"/docs/rke/latest/en/etcd-snapshots/","weight":150,"wordcount":146},{"authors":null,"categories":null,"content":"To launch RancherOS, we have built-in system services. They are defined in the Docker Compose format, and can be found in the default system config file, /usr/share/ros/os-config.yml. You can add your own system services or override services in the cloud-config.\npreload-user-images Read more about image preloading.\nnetwork During this service, networking is set up, e.g. hostname, interfaces, and DNS.\nIt is configured by hostname and rancher.networksettings in cloud-config.\nntp Runs ntpd in a System Docker container.","date":-62135596800,"description":"","dir":"os/v1.x/en/installation/boot-process/built-in-system-services/","expirydate":-62135596800,"fuzzywordcount":300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"ee3ff854a000286d2edf9c4c8c4826e5","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/boot-process/built-in-system-services/","postref":"ee3ff854a000286d2edf9c4c8c4826e5","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/os/v1.x/en/installation/boot-process/built-in-system-services/","section":"os","tags":null,"title":"Built-in System Services","type":"os","url":"/docs/os/v1.x/en/installation/boot-process/built-in-system-services/","weight":150,"wordcount":296},{"authors":null,"categories":null,"content":"Available as of v0.2.0\nCertificates are an important part of Kubernetes clusters and are used for all Kubernetes cluster components. RKE has a rke cert command to help work with certificates.\n Ability to generate certificate sign requests for the Kubernetes components Rotate Auto-Generated Certificates  Generating Certificate Signing Requests (CSRs) and Keys If you want to create and sign the certificates by a real Certificate Authority (CA), you can use RKE to generate a set of Certificate Signing Requests (CSRs) and keys.","date":-62135596800,"description":"","dir":"rke/latest/en/cert-mgmt/","expirydate":-62135596800,"fuzzywordcount":800,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"329b33986c9be98da5e7c4667b4236f5","permalink":"http://jijeesh.github.io/docs/rke/latest/en/cert-mgmt/","postref":"329b33986c9be98da5e7c4667b4236f5","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/docs/rke/latest/en/cert-mgmt/","section":"rke","tags":null,"title":"Certificate Management","type":"rke","url":"/docs/rke/latest/en/cert-mgmt/","weight":150,"wordcount":720},{"authors":null,"categories":null,"content":"Available as of v0.2.0\nBy default, Kubernetes clusters require certificates and RKE auto-generates the certificates for all the Kubernetes services. RKE can also use custom certificates for these Kubernetes services.\nWhen deploying Kubernetes with RKE, there are two additional options that can be used with rke up so that RKE uses custom certificates.\n   Option Description     --custom-certs Use custom certificates from a cert dir.","date":-62135596800,"description":"","dir":"rke/latest/en/installation/certs/","expirydate":-62135596800,"fuzzywordcount":500,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"e2bd8cf0fd5a96e973f37e04e4d38d7e","permalink":"http://jijeesh.github.io/docs/rke/latest/en/installation/certs/","postref":"e2bd8cf0fd5a96e973f37e04e4d38d7e","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/rke/latest/en/installation/certs/","section":"rke","tags":null,"title":"Custom Certificates","type":"rke","url":"/docs/rke/latest/en/installation/certs/","weight":150,"wordcount":471},{"authors":null,"categories":null,"content":"Upgrading Rancher  Upgrades  Rolling Back Unsuccessful Upgrades In the event that your Rancher Server does not upgrade successfully, you can rollback to your installation prior to upgrade:\n Rollbacks for Rancher installed with Docker Rollbacks for Rancher installed on a Kubernetes cluster   Note: If you are rolling back to versions in either of these scenarios, you must follow some extra instructions in order to get your clusters working.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/upgrades/","expirydate":-62135596800,"fuzzywordcount":100,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"088d737a1b7dfd559c26ca53bd729ce5","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/upgrades/","postref":"088d737a1b7dfd559c26ca53bd729ce5","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rancher/v2.x/en/upgrades/","section":"rancher","tags":null,"title":"Upgrades and Rollbacks","type":"rancher","url":"/docs/rancher/v2.x/en/upgrades/","weight":150,"wordcount":93},{"authors":null,"categories":null,"content":"Userdata and metadata can be fetched from a cloud provider, VM runtime, or management service during the RancherOS boot process. Since v0.8.0, this process occurs while RancherOS is still running from memory and before System Docker starts. It is configured by the rancher.cloud_init.datasources configuration parameter. For cloud-provider specific images, such as AWS and GCE, the datasource is pre-configured.\nUserdata Userdata is a file given by users when launching RancherOS hosts.","date":-62135596800,"description":"","dir":"os/v1.x/en/installation/boot-process/cloud-init/","expirydate":-62135596800,"fuzzywordcount":300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"2988028b06b74a57451401e0c2f0ddbd","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/boot-process/cloud-init/","postref":"2988028b06b74a57451401e0c2f0ddbd","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/os/v1.x/en/installation/boot-process/cloud-init/","section":"os","tags":null,"title":"Cloud-Init","type":"os","url":"/docs/os/v1.x/en/installation/boot-process/cloud-init/","weight":151,"wordcount":288},{"authors":null,"categories":null,"content":"On boot, RancherOS scans /var/lib/rancher/preload/docker and /var/lib/rancher/preload/system-docker directories and tries to load container image archives it finds there, with docker load and system-docker load.\nThe archives are .tar files, optionally compressed with xz or gzip. These can be produced by docker save command, e.g.:\n$ docker save my-image1 my-image2 some-other/image3 | xz &gt; my-images.tar.xz  The resulting files should be placed into /var/lib/rancher/preload/docker or /var/lib/rancher/preload/system-docker (depending on whether you want it preloaded into Docker or System Docker).","date":-62135596800,"description":"","dir":"os/v1.x/en/installation/boot-process/image-preloading/","expirydate":-62135596800,"fuzzywordcount":300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"3fab2b11b5efa17372cf7d8fa74788a0","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/boot-process/image-preloading/","postref":"3fab2b11b5efa17372cf7d8fa74788a0","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/os/v1.x/en/installation/boot-process/image-preloading/","section":"os","tags":null,"title":"Image Preloading","type":"os","url":"/docs/os/v1.x/en/installation/boot-process/image-preloading/","weight":152,"wordcount":207},{"authors":null,"categories":null,"content":"System services RancherOS uses containers for its system services. This means the logs for syslog, acipd, system-cron, udev, network, ntp, console and the user Docker are available using sudo ros service logs &lt;service-name&gt;.\nBoot logging Since v1.1.0, the init process&rsquo;s logs are copied to /var/log/boot after the user-space filesystem is made available. These can be used to diagnose initialisation, network, and cloud-init issues.\nRemote Syslog logging The Linux kernel has a netconsole logging facility that allows it to send the Kernel level logs to a remote Syslog server.","date":-62135596800,"description":"","dir":"os/v1.x/en/installation/boot-process/logging/","expirydate":-62135596800,"fuzzywordcount":300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"d5534c32b1605adb8782cce9f3082e73","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/boot-process/logging/","postref":"d5534c32b1605adb8782cce9f3082e73","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/os/v1.x/en/installation/boot-process/logging/","section":"os","tags":null,"title":"System Logging","type":"os","url":"/docs/os/v1.x/en/installation/boot-process/logging/","weight":153,"wordcount":263},{"authors":null,"categories":null,"content":"RancherOS will store its state in a single partition specified by the dev field. The field can be a device such as /dev/sda1 or a logical name such LABEL=state or UUID=123124. The default value is LABEL=RANCHER_STATE. The file system type of that partition can be set to auto or a specific file system type such as ext4.\n#cloud-configrancher:state:fstype:autodev:LABEL=RANCHER_STATE For other labels such as RANCHER_BOOT and RANCHER_OEM and RANCHER_SWAP, please refer to Custom partition layout.","date":-62135596800,"description":"","dir":"os/v1.x/en/installation/storage/state-partition/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"f7384c6a8e28b6c35c13a5bf2bb6261f","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/storage/state-partition/","postref":"f7384c6a8e28b6c35c13a5bf2bb6261f","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/os/v1.x/en/installation/storage/state-partition/","section":"os","tags":null,"title":"Persistent State Partition","type":"os","url":"/docs/os/v1.x/en/installation/storage/state-partition/","weight":160,"wordcount":144},{"authors":null,"categories":null,"content":"Additional mounts can be specified as part of your cloud-config. These mounts are applied within the console container. Here&rsquo;s a simple example that mounts /dev/vdb to /mnt/s.\n#cloud-configmounts:-[&#34;/dev/vdb&#34;,&#34;/mnt/s&#34;,&#34;ext4&#34;,&#34;&#34;] Important: Be aware, the 4th parameter is mandatory and cannot be omitted (server crashes). It also yet cannot be defaults\nAs you will use the ros cli most probably, it would look like this:\nros config set mounts '[[&quot;/dev/vdb&quot;,&quot;/mnt/s&quot;,&quot;ext4&quot;,&quot;&quot;]]'  hint: You need to pre-format the disks, rancher-os will not do this for you.","date":-62135596800,"description":"","dir":"os/v1.x/en/installation/storage/additional-mounts/","expirydate":-62135596800,"fuzzywordcount":300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"3d6467a63039dfda7d376e369a7abbb1","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/storage/additional-mounts/","postref":"3d6467a63039dfda7d376e369a7abbb1","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/os/v1.x/en/installation/storage/additional-mounts/","section":"os","tags":null,"title":"Additional Mounts","type":"os","url":"/docs/os/v1.x/en/installation/storage/additional-mounts/","weight":161,"wordcount":231},{"authors":null,"categories":null,"content":"Installing the ZFS service The zfs service will install the kernel-headers for your kernel (if you build your own kernel, you&rsquo;ll need to replicate this service), and then download the ZFS on Linux source, and build and install it. Then it will build a zfs-tools image that will be used to give you access to the zfs tools.\nThe only restriction is that you must mount your zpool into /mnt, as this is the only shared mount directory that will be accessible throughout the system-docker managed containers (including the console).","date":-62135596800,"description":"","dir":"os/v1.x/en/installation/storage/using-zfs/","expirydate":-62135596800,"fuzzywordcount":700,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"8dc60762dd28511a14e9e707e448eb34","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/storage/using-zfs/","postref":"8dc60762dd28511a14e9e707e448eb34","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/docs/os/v1.x/en/installation/storage/using-zfs/","section":"os","tags":null,"title":"Using ZFS","type":"os","url":"/docs/os/v1.x/en/installation/storage/using-zfs/","weight":162,"wordcount":667},{"authors":null,"categories":null,"content":"Using ros config, you can configure specific interfaces. Wildcard globbing is supported so eth* will match eth1 and eth2. The available options you can configure are address, gateway, mtu, and dhcp.\n$ sudo ros config set rancher.network.interfaces.eth1.address 172.68.1.100/24 $ sudo ros config set rancher.network.interfaces.eth1.gateway 172.68.1.1 $ sudo ros config set rancher.network.interfaces.eth1.mtu 1500 $ sudo ros config set rancher.network.interfaces.eth1.dhcp false  If you wanted to configure the interfaces through the cloud config file, you&rsquo;ll need to place interface configurations within the rancher key.","date":-62135596800,"description":"","dir":"os/v1.x/en/installation/networking/interfaces/","expirydate":-62135596800,"fuzzywordcount":800,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"9c351bbacc2c86f6b8a2d0f456b78f28","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/networking/interfaces/","postref":"9c351bbacc2c86f6b8a2d0f456b78f28","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/docs/os/v1.x/en/installation/networking/interfaces/","section":"os","tags":null,"title":"Configuring Network Interfaces","type":"os","url":"/docs/os/v1.x/en/installation/networking/interfaces/","weight":170,"wordcount":739},{"authors":null,"categories":null,"content":"If you wanted to configure the DNS through the cloud config file, you&rsquo;ll need to place DNS configurations within the rancher key.\n#cloud-config#Remember, any changes for rancher will be within the rancher keyrancher:network:dns:search:-mydomain.com-example.com Using ros config, you can set the nameservers, and search, which directly map to the fields of the same name in /etc/resolv.conf.\n$ sudo ros config set rancher.network.dns.search &quot;['mydomain.com','example.com']&quot; $ sudo ros config get rancher.network.dns.search - mydomain.com - example.","date":-62135596800,"description":"","dir":"os/v1.x/en/installation/networking/dns/","expirydate":-62135596800,"fuzzywordcount":100,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"e8a4f0f2bafcfc77f53ee5fe05ba126e","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/networking/dns/","postref":"e8a4f0f2bafcfc77f53ee5fe05ba126e","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/os/v1.x/en/installation/networking/dns/","section":"os","tags":null,"title":"Configuring DNS","type":"os","url":"/docs/os/v1.x/en/installation/networking/dns/","weight":171,"wordcount":72},{"authors":null,"categories":null,"content":"HTTP proxy settings can be set directly under the network key. This will automatically configure proxy settings for both Docker and System Docker.\n#cloud-configrancher:network:http_proxy:https://myproxy.example.comhttps_proxy:https://myproxy.example.comno_proxy:localhost,127.0.0.1 \n Note: System Docker proxy settings will not be applied until after a reboot.\n To add the HTTP_PROXY, HTTPS_PROXY, and NO_PROXY environment variables to a system service, specify each under the environment key for the service.","date":-62135596800,"description":"","dir":"os/v1.x/en/installation/networking/proxy-settings/","expirydate":-62135596800,"fuzzywordcount":100,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"02511238c8d55702d17185c2886c0902","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/networking/proxy-settings/","postref":"02511238c8d55702d17185c2886c0902","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/os/v1.x/en/installation/networking/proxy-settings/","section":"os","tags":null,"title":"Configuring Proxy Settings","type":"os","url":"/docs/os/v1.x/en/installation/networking/proxy-settings/","weight":172,"wordcount":60},{"authors":null,"categories":null,"content":"Adding/Removing Nodes RKE supports adding/removing nodes for worker and controlplane hosts.\nIn order to add additional nodes, you update the original cluster.yml file with any additional nodes and specify their role in the Kubernetes cluster.\nIn order to remove nodes, remove the node information from the nodes list in the original cluster.yml.\nAfter you&rsquo;ve made changes to add/remove nodes, run rke up with the updated cluster.yml.\nAdding/Removing Worker Nodes You can add/remove only worker nodes, by running rke up --update-only.","date":-62135596800,"description":"RKE supports adding/removing nodes for worker and controlplane hosts. Learn about the changes you need to make to the cluster.yml in order to add/remove nodes","dir":"rke/latest/en/managing-clusters/","expirydate":-62135596800,"fuzzywordcount":300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"3bd09b4ae1ca578b69e752a1a8a51837","permalink":"http://jijeesh.github.io/docs/rke/latest/en/managing-clusters/","postref":"3bd09b4ae1ca578b69e752a1a8a51837","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rke/latest/en/managing-clusters/","section":"rke","tags":null,"title":"Adding and Removing Nodes","type":"rke","url":"/docs/rke/latest/en/managing-clusters/","weight":175,"wordcount":255},{"authors":null,"categories":null,"content":"When booting from the ISO, RancherOS starts with the default console, which is based on busybox.\nYou can select which console you want RancherOS to start with using the cloud-config.\nEnabling Consoles using Cloud-Config When launching RancherOS with a cloud-config file, you can select which console you want to use.\nCurrently, the list of available consoles are:\n default alpine centos debian fedora ubuntu  Here is an example cloud-config file that can be used to enable the debian console.","date":-62135596800,"description":"","dir":"os/v1.x/en/installation/custom-builds/custom-console/","expirydate":-62135596800,"fuzzywordcount":600,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"61d61e6c53eea16c58d0b8f2d09e5f07","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/custom-builds/custom-console/","postref":"61d61e6c53eea16c58d0b8f2d09e5f07","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/os/v1.x/en/installation/custom-builds/custom-console/","section":"os","tags":null,"title":"Custom Console","type":"os","url":"/docs/os/v1.x/en/installation/custom-builds/custom-console/","weight":180,"wordcount":527},{"authors":null,"categories":null,"content":"Kernel version in RancherOS RancherOS basically uses the standard Linux kernel, but we maintain a kernel config ourselves. Due to various feature support and security fixes, we are constantly updating the kernel version.\n   RancherOS Kernel     &lt;=v0.7.1 4.4.x   &lt;=v1.3.0 4.9.x   &gt;=v1.4.0 4.14.x    Building and Packaging a Kernel to be used in RancherOS We build the kernel for RancherOS at the os-kernel repository.","date":-62135596800,"description":"","dir":"os/v1.x/en/installation/custom-builds/custom-kernels/","expirydate":-62135596800,"fuzzywordcount":600,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"86ac91fbeefe43408c72ea607498af93","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/custom-builds/custom-kernels/","postref":"86ac91fbeefe43408c72ea607498af93","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/os/v1.x/en/installation/custom-builds/custom-kernels/","section":"os","tags":null,"title":"Custom Kernels","type":"os","url":"/docs/os/v1.x/en/installation/custom-builds/custom-kernels/","weight":181,"wordcount":525},{"authors":null,"categories":null,"content":"It&rsquo;s easy to build your own RancherOS ISO.\nCreate a clone of the main RancherOS repository to your local machine with a git clone.\n$ git clone https://github.com/rancher/os.git  In the root of the repository, the &ldquo;General Configuration&rdquo; section of Dockerfile.dapper can be updated to use custom kernels. After you&rsquo;ve saved your edits, run make in the root directory. After the build has completed, a ./dist/artifacts directory will be created with the custom built RancherOS release files.","date":-62135596800,"description":"","dir":"os/v1.x/en/installation/custom-builds/custom-rancheros-iso/","expirydate":-62135596800,"fuzzywordcount":500,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"24e898012a6c71e2b0c4c328bff977d0","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/custom-builds/custom-rancheros-iso/","postref":"24e898012a6c71e2b0c4c328bff977d0","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/os/v1.x/en/installation/custom-builds/custom-rancheros-iso/","section":"os","tags":null,"title":"Custom RancherOS ISO","type":"os","url":"/docs/os/v1.x/en/installation/custom-builds/custom-rancheros-iso/","weight":182,"wordcount":440},{"authors":null,"categories":null,"content":"Use your infrastructure provider of choice to provision three nodes and a load balancer endpoint for your RKE install.\n Note: These nodes must be in the same region. You may place these servers in separate availability zones (datacenter).\n Requirements for OS, Docker, Hardware, and Networking Make sure that your nodes fulfill the general installation requirements.\nView the OS requirements for RKE at RKE Requirements.\nLoad Balancer RKE will configure an Ingress controller pod, on each of your nodes.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/k8s-install/create-nodes-lb/","expirydate":-62135596800,"fuzzywordcount":300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"78ad69b918932d031c5dfab7d3653dce","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/k8s-install/create-nodes-lb/","postref":"78ad69b918932d031c5dfab7d3653dce","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/installation/k8s-install/create-nodes-lb/","section":"rancher","tags":null,"title":"1. Create Nodes and Load Balancer","type":"rancher","url":"/docs/rancher/v2.x/en/installation/k8s-install/create-nodes-lb/","weight":185,"wordcount":217},{"authors":null,"categories":null,"content":"Use your provider of choice to provision 3 nodes and a Load Balancer endpoint for your RKE install.\n Note: These nodes must be in the same region/datacenter. You may place these servers in separate availability zones.\n Node Requirements View the supported operating systems and hardware/software/networking requirements for nodes running Rancher at Node Requirements.\nView the OS requirements for RKE at RKE Requirements\nLoad Balancer RKE will configure an Ingress controller pod, on each of your nodes.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/options/helm2/create-nodes-lb/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"9004ef3e1dca5d354f21822e70032718","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/helm2/create-nodes-lb/","postref":"9004ef3e1dca5d354f21822e70032718","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rancher/v2.x/en/installation/options/helm2/create-nodes-lb/","section":"rancher","tags":null,"title":"1. Create Nodes and Load Balancer","type":"rancher","url":"/docs/rancher/v2.x/en/installation/options/helm2/create-nodes-lb/","weight":185,"wordcount":185},{"authors":null,"categories":null,"content":"Use RKE to install Kubernetes with a high availability etcd configuration.\n Note: For systems without direct internet access see Air Gap: Kubernetes install for install details.\n Create the rancher-cluster.yml File Using the sample below create the rancher-cluster.yml file. Replace the IP Addresses in the nodes list with the IP address or DNS names of the 3 nodes you created.\n Note: If your node has public and internal addresses, it is recommended to set the internal_address: so Kubernetes will use it for intra-cluster communication.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/options/helm2/kubernetes-rke/","expirydate":-62135596800,"fuzzywordcount":600,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"54ba8f0e911367507966ec560747acc8","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/helm2/kubernetes-rke/","postref":"54ba8f0e911367507966ec560747acc8","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/rancher/v2.x/en/installation/options/helm2/kubernetes-rke/","section":"rancher","tags":null,"title":"2. Install Kubernetes with RKE","type":"rancher","url":"/docs/rancher/v2.x/en/installation/options/helm2/kubernetes-rke/","weight":190,"wordcount":590},{"authors":null,"categories":null,"content":"This section describes how to install a Kubernetes cluster on your three nodes according to our best practices for the Rancher server environment. This cluster should be dedicated to run only the Rancher server. We recommend using RKE to install Kubernetes on this cluster. Hosted Kubernetes providers such as EKS should not be used.\nFor systems without direct internet access, refer to Air Gap: Kubernetes install.\n Single-node Installation Tip: In a single-node Kubernetes cluster, the Rancher server does not have high availability, which is important for running Rancher in production.","date":-62135596800,"description":"Learn how to use Rancher Kubernetes Engine (RKE) to install Kubernetes with a high availability etcd configuration.","dir":"rancher/v2.x/en/installation/k8s-install/kubernetes-rke/","expirydate":-62135596800,"fuzzywordcount":800,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"c8d008bd1b118b62f5e0ac0a1c8e602d","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/k8s-install/kubernetes-rke/","postref":"c8d008bd1b118b62f5e0ac0a1c8e602d","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/docs/rancher/v2.x/en/installation/k8s-install/kubernetes-rke/","section":"rancher","tags":null,"title":"2. Set up a Kubernetes Cluster","type":"rancher","url":"/docs/rancher/v2.x/en/installation/k8s-install/kubernetes-rke/","weight":190,"wordcount":744},{"authors":null,"categories":null,"content":"Amazon ECS is supported, which allows RancherOS EC2 instances to join your cluster.\nPre-Requisites Prior to launching RancherOS EC2 instances, the ECS Container Instance IAM Role will need to have been created. This ecsInstanceRole will need to be used when launching EC2 instances. If you have been using ECS, you created this role if you followed the ECS &ldquo;Get Started&rdquo; interactive guide.\nLaunching an instance with ECS RancherOS makes it easy to join your ECS cluster.","date":-62135596800,"description":"","dir":"os/v1.x/en/installation/amazon-ecs/","expirydate":-62135596800,"fuzzywordcount":500,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"16e1d89ca8ede9084677908f8bcdd137","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/installation/amazon-ecs/","postref":"16e1d89ca8ede9084677908f8bcdd137","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/os/v1.x/en/installation/amazon-ecs/","section":"os","tags":null,"title":"Amazon ECS (EC2 Container Service)","type":"os","url":"/docs/os/v1.x/en/installation/amazon-ecs/","weight":190,"wordcount":436},{"authors":null,"categories":null,"content":"Helm is the package management tool of choice for Kubernetes. Helm &ldquo;charts&rdquo; provide templating syntax for Kubernetes YAML manifest documents. With Helm we can create configurable deployments instead of just using static files. For more information about creating your own catalog of deployments, check out the docs at https://helm.sh/. To be able to use Helm, the server-side component tiller needs to be installed on your cluster.\nFor systems without direct internet access, see Helm - Air Gap for install details.","date":-62135596800,"description":"With Helm, you can create configurable deployments instead of using static files. In order to use Helm, the Tiller service needs to be installed on your cluster.","dir":"rancher/v2.x/en/installation/options/helm2/helm-init/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"6f559ac4b547bbf29f3d692d6114c8be","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/helm2/helm-init/","postref":"6f559ac4b547bbf29f3d692d6114c8be","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/installation/options/helm2/helm-init/","section":"rancher","tags":null,"title":"Initialize Helm: Install the Tiller Service","type":"rancher","url":"/docs/rancher/v2.x/en/installation/options/helm2/helm-init/","weight":195,"wordcount":385},{"authors":null,"categories":null,"content":"Prerequisites: You must have a private registry available to use.\nNote: Populating the private registry with images is the same process for HA and Docker installations, the differences in this section is based on whether or not you are planning to provision a Windows cluster or not.\n By default, all images used to provision Kubernetes clusters or launch any tools in Rancher, e.g. monitoring, pipelines, alerts, are pulled from Docker Hub.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/options/air-gap-helm2/populate-private-registry/","expirydate":-62135596800,"fuzzywordcount":1800,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"cb91a1493882f3385298b9866eb2519f","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/air-gap-helm2/populate-private-registry/","postref":"cb91a1493882f3385298b9866eb2519f","publishdate":"0001-01-01T00:00:00Z","readingtime":9,"relpermalink":"/docs/rancher/v2.x/en/installation/options/air-gap-helm2/populate-private-registry/","section":"rancher","tags":null,"title":"2. Collect and Publish Images to your Private Registry","type":"rancher","url":"/docs/rancher/v2.x/en/installation/options/air-gap-helm2/populate-private-registry/","weight":200,"wordcount":1791},{"authors":null,"categories":null,"content":"Prerequisites: You must have a private registry available to use.\nNote: Populating the private registry with images is the same process for HA and Docker installations, the differences in this section is based on whether or not you are planning to provision a Windows cluster or not.\n By default, all images used to provision Kubernetes clusters or launch any tools in Rancher, e.g. monitoring, pipelines, alerts, are pulled from Docker Hub.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/other-installation-methods/air-gap/populate-private-registry/","expirydate":-62135596800,"fuzzywordcount":1900,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"57393d19246ec030ab51ba11ee0fea94","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/other-installation-methods/air-gap/populate-private-registry/","postref":"57393d19246ec030ab51ba11ee0fea94","publishdate":"0001-01-01T00:00:00Z","readingtime":9,"relpermalink":"/docs/rancher/v2.x/en/installation/other-installation-methods/air-gap/populate-private-registry/","section":"rancher","tags":null,"title":"2. Collect and Publish Images to your Private Registry","type":"rancher","url":"/docs/rancher/v2.x/en/installation/other-installation-methods/air-gap/populate-private-registry/","weight":200,"wordcount":1817},{"authors":null,"categories":null,"content":"Rancher is installed using the Helm package manager for Kubernetes. Helm charts provide templating syntax for Kubernetes YAML manifest documents.\nWith Helm, we can create configurable deployments instead of just using static files. For more information about creating your own catalog of deployments, check out the docs at https://helm.sh/.\nFor systems without direct internet access, see Air Gap: Kubernetes install.\nTo choose a Rancher version to install, refer to Choosing a Rancher Version.","date":-62135596800,"description":"Rancher installation is managed using the Helm Kubernetes package manager. Use Helm to install the prerequisites and charts to install Rancher","dir":"rancher/v2.x/en/installation/k8s-install/helm-rancher/","expirydate":-62135596800,"fuzzywordcount":1400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"8b63401440f06751ed1a5fe8d045a1b8","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/k8s-install/helm-rancher/","postref":"8b63401440f06751ed1a5fe8d045a1b8","publishdate":"0001-01-01T00:00:00Z","readingtime":7,"relpermalink":"/docs/rancher/v2.x/en/installation/k8s-install/helm-rancher/","section":"rancher","tags":null,"title":"3. Install Rancher on the Kubernetes Cluster","type":"rancher","url":"/docs/rancher/v2.x/en/installation/k8s-install/helm-rancher/","weight":200,"wordcount":1300},{"authors":null,"categories":null,"content":"Rancher installation is managed using the Helm package manager for Kubernetes. Use helm to install the prerequisite and charts to install Rancher.\nFor systems without direct internet access, see Air Gap: Kubernetes install.\nRefer to the Helm version requirements to choose a version of Helm to install Rancher.\n Note: The installation instructions assume you are using Helm 2. The instructions will be updated for Helm 3 soon. In the meantime, if you want to use Helm 3, refer to these instructions.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/options/helm2/helm-rancher/","expirydate":-62135596800,"fuzzywordcount":1200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"bbb0d6cdac74bbc04b2dff2b7f609e9d","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/helm2/helm-rancher/","postref":"bbb0d6cdac74bbc04b2dff2b7f609e9d","publishdate":"0001-01-01T00:00:00Z","readingtime":6,"relpermalink":"/docs/rancher/v2.x/en/installation/options/helm2/helm-rancher/","section":"rancher","tags":null,"title":"4.  Install Rancher","type":"rancher","url":"/docs/rancher/v2.x/en/installation/options/helm2/helm-rancher/","weight":200,"wordcount":1178},{"authors":null,"categories":null,"content":"These guides walk you through the deployment of an application, including how to expose the application for use outside of the cluster.\n Workload with Ingress Workload with NodePort  ","date":-62135596800,"description":"","dir":"rancher/v2.x/en/quick-start-guide/workload/","expirydate":-62135596800,"fuzzywordcount":100,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"ed7030c5f3edf05aa2e6e0f9c961525d","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/quick-start-guide/workload/","postref":"ed7030c5f3edf05aa2e6e0f9c961525d","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rancher/v2.x/en/quick-start-guide/workload/","section":"rancher","tags":null,"title":"Deploying Workloads","type":"rancher","url":"/docs/rancher/v2.x/en/quick-start-guide/workload/","weight":200,"wordcount":28},{"authors":null,"categories":null,"content":"If your organization uses Elasticsearch, either on premise or in the cloud, you can configure Rancher to send it Kubernetes logs. Afterwards, you can log into your Elasticsearch deployment to view logs.\n Prerequisites: Configure an Elasticsearch deployment.\n Elasticsearch Deployment Configuration  In the Endpoint field, enter the IP address and port of your Elasticsearch instance. You can find this information from the dashboard of your Elasticsearch deployment.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-admin/tools/logging/elasticsearch/","expirydate":-62135596800,"fuzzywordcount":300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"d1755de81529ba384b7870a7fb305abe","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/tools/logging/elasticsearch/","postref":"d1755de81529ba384b7870a7fb305abe","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/cluster-admin/tools/logging/elasticsearch/","section":"rancher","tags":null,"title":"Elasticsearch","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-admin/tools/logging/elasticsearch/","weight":200,"wordcount":280},{"authors":null,"categories":null,"content":"When setting up your cluster.yml for RKE, there are a lot of different options that can be configured to control the behavior of how RKE launches Kubernetes.\nThere are several options that can be configured in cluster configuration option. There are several example yamls that contain all the options.\nConfiguring Nodes  Nodes Ignoring unsupported Docker versions Private Registries Cluster Level SSH Key Path SSH Agent Bastion Host  Configuring Kubernetes Cluster  Cluster Name Kubernetes Version Prefix Path System Images Services Extra Args and Binds and Environment Variables External Etcd Authentication Authorization Rate Limiting Cloud Providers Audit Log Add-ons  Network Plug-ins DNS providers Ingress Controllers Metrics Server User-Defined Add-ons Add-ons Job Timeout   Cluster Level Options Cluster Name By default, the name of your cluster will be local.","date":-62135596800,"description":"There are a lot of different Kubernetes Configuration options you can choose from when setting up your cluster.yml for RKE","dir":"rke/latest/en/config-options/","expirydate":-62135596800,"fuzzywordcount":600,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"d6ac6995243dbf838eb0b3d69324116d","permalink":"http://jijeesh.github.io/docs/rke/latest/en/config-options/","postref":"d6ac6995243dbf838eb0b3d69324116d","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/rke/latest/en/config-options/","section":"rke","tags":null,"title":"Kubernetes Configuration Options","type":"rke","url":"/docs/rke/latest/en/config-options/","weight":200,"wordcount":527},{"authors":null,"categories":null,"content":"Failed to get job complete status Most common reason for this error is that a node is having issues that block the deploy job from completing successfully. See Get node conditions how to check node conditions.\nYou can also retrieve the log from the job to see if it has an indication of the error, make sure you replace rke-network-plugin-deploy-job with the job name from the error:\nExample command to get logs for error Failed to get job complete status for job rke-network-plugin-deploy-job:","date":-62135596800,"description":"","dir":"rke/latest/en/troubleshooting/provisioning-errors/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"33026bc307bf93adff546b8c473f3327","permalink":"http://jijeesh.github.io/docs/rke/latest/en/troubleshooting/provisioning-errors/","postref":"33026bc307bf93adff546b8c473f3327","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rke/latest/en/troubleshooting/provisioning-errors/","section":"rke","tags":null,"title":"Provisioning Errors","type":"rke","url":"/docs/rke/latest/en/troubleshooting/provisioning-errors/","weight":200,"wordcount":196},{"authors":null,"categories":null,"content":"The following steps quickly deploy a Rancher Server with a single node cluster attached.\nPrerequisites  Vagrant: Vagrant is required as this is used to provision the machine based on the Vagrantfile. Virtualbox: The virtual machines that Vagrant provisions need to be provisioned to VirtualBox. At least 4GB of free RAM.  Getting Started  Clone Rancher Quickstart to a folder using git clone https://github.com/rancher/quickstart.\n Go into the folder containing the Vagrantfile by executing cd quickstart/vagrant.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/quick-start-guide/deployment/quickstart-vagrant/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"b932edf31a90a0ea664693faaed9809f","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/quick-start-guide/deployment/quickstart-vagrant/","postref":"b932edf31a90a0ea664693faaed9809f","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rancher/v2.x/en/quick-start-guide/deployment/quickstart-vagrant/","section":"rancher","tags":null,"title":"Vagrant Quick Start","type":"rancher","url":"/docs/rancher/v2.x/en/quick-start-guide/deployment/quickstart-vagrant/","weight":200,"wordcount":173},{"authors":null,"categories":null,"content":"Prerequisite You have a running cluster with at least 1 node.\n1. Deploying a Workload You&rsquo;re ready to create your first workload. A workload is an object that includes pods along with other files and info needed to deploy your application.\nFor this workload, you&rsquo;ll be deploying the application Rancher Hello-World.\n From the Clusters page, open the cluster that you just created.\n From the main menu of the Dashboard, select Projects/Namespaces.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/quick-start-guide/workload/quickstart-deploy-workload-nodeport/","expirydate":-62135596800,"fuzzywordcount":600,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"97396ded9bfa5a1babfc84ef6f7637bf","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/quick-start-guide/workload/quickstart-deploy-workload-nodeport/","postref":"97396ded9bfa5a1babfc84ef6f7637bf","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/rancher/v2.x/en/quick-start-guide/workload/quickstart-deploy-workload-nodeport/","section":"rancher","tags":null,"title":"Workload with NodePort Quick Start","type":"rancher","url":"/docs/rancher/v2.x/en/quick-start-guide/workload/quickstart-deploy-workload-nodeport/","weight":200,"wordcount":518},{"authors":null,"categories":null,"content":"The nodes directive is the only required section in the cluster.yml file. It&rsquo;s used by RKE to specify cluster node(s), ssh credentials used to access the node(s) and which roles these nodes will be in the Kubernetes cluster.\nThis section covers the following topics:\n Node configuration example Kubernetes roles  etcd Controlplane Worker  Node options  Address Internal address Overriding the hostname SSH port SSH users SSH key path SSH key SSH certificate path SSH certificate Docker socket Labels Taints   Node Configuration Example The following example shows node configuration in an example cluster.","date":-62135596800,"description":"","dir":"rke/latest/en/config-options/nodes/","expirydate":-62135596800,"fuzzywordcount":800,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"e85bdbdcd4554d01e1c11545be98eb70","permalink":"http://jijeesh.github.io/docs/rke/latest/en/config-options/nodes/","postref":"e85bdbdcd4554d01e1c11545be98eb70","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/docs/rke/latest/en/config-options/nodes/","section":"rke","tags":null,"title":"Nodes","type":"rke","url":"/docs/rke/latest/en/config-options/nodes/","weight":210,"wordcount":798},{"authors":null,"categories":null,"content":"RKE supports the ability to configure multiple private Docker registries in the cluster.yml. By passing in your registry and credentials, it allows the nodes to pull images from these private registries.\nprivate_registries:-url:registry.comuser:Usernamepassword:password-url:myregistry.comuser:myuserpassword:mypassword  Note: If you are using a Docker Hub registry, you can omit the url or set it to docker.io.\n Default Registry As of v0.1.10, RKE supports specifying a default registry from the list of private registries to be used with all system images .","date":-62135596800,"description":"","dir":"rke/latest/en/config-options/private-registries/","expirydate":-62135596800,"fuzzywordcount":300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"2c0b100a23ba67f954c0407ffbf51f89","permalink":"http://jijeesh.github.io/docs/rke/latest/en/config-options/private-registries/","postref":"2c0b100a23ba67f954c0407ffbf51f89","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rke/latest/en/config-options/private-registries/","section":"rke","tags":null,"title":"Private Registries","type":"rke","url":"/docs/rke/latest/en/config-options/private-registries/","weight":215,"wordcount":241},{"authors":null,"categories":null,"content":"Since RKE uses ssh to connect to nodes, you can configure the cluster.yml so RKE will use a bastion host. Keep in mind that the port requirements for the RKE node move to the configured bastion host. Our private SSH key(s) only needs to reside on the host running RKE. You do not need to copy your private SSH key(s) to the bastion host.\nbastion_host:address:x.x.x.xuser:ubuntuport:22ssh_key_path:/home/user/.ssh/bastion_rsa# or# ssh_key: |-# -----BEGIN RSA PRIVATE KEY-----## -----END RSA PRIVATE KEY-----# Optionally using SSH certificates# ssh_cert_path: /home/user/.","date":-62135596800,"description":"","dir":"rke/latest/en/config-options/bastion-host/","expirydate":-62135596800,"fuzzywordcount":300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"da79072742c38053ca7f383a351ce156","permalink":"http://jijeesh.github.io/docs/rke/latest/en/config-options/bastion-host/","postref":"da79072742c38053ca7f383a351ce156","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rke/latest/en/config-options/bastion-host/","section":"rke","tags":null,"title":"Bastion/Jump Host Configuration","type":"rke","url":"/docs/rke/latest/en/config-options/bastion-host/","weight":220,"wordcount":254},{"authors":null,"categories":null,"content":"When RKE is deploying Kubernetes, there are several images that are pulled. These images are used as Kubernetes system components as well as helping to deploy these system components.\nAs of v0.1.6, the functionality of a couple of the system images were consolidated into a single rancher/rke-tools image to simplify and speed the deployment process.\nYou can configure the network plug-ins, ingress controller and dns provider as well as the options for these add-ons separately in the cluster.","date":-62135596800,"description":"","dir":"rke/latest/en/config-options/system-images/","expirydate":-62135596800,"fuzzywordcount":300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"2ef0af749de822838bf9ae888b9d03f2","permalink":"http://jijeesh.github.io/docs/rke/latest/en/config-options/system-images/","postref":"2ef0af749de822838bf9ae888b9d03f2","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rke/latest/en/config-options/system-images/","section":"rke","tags":null,"title":"System Images","type":"rke","url":"/docs/rke/latest/en/config-options/system-images/","weight":225,"wordcount":247},{"authors":null,"categories":null,"content":"This section describes how to choose a Rancher version.\nFor a high-availability installation of Rancher, which is recommended for production, the Rancher server is installed using a Helm chart on a Kubernetes cluster. Refer to the Helm version requirements to choose a version of Helm to install Rancher.\nFor Docker installations of Rancher, which is used for development and testing, you will install Rancher as a Docker image.\n When installing, upgrading, or rolling back Rancher Server when it is installed on a Kubernetes cluster, Rancher server is installed using a Helm chart on a Kubernetes cluster.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/options/server-tags/","expirydate":-62135596800,"fuzzywordcount":900,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"2ef8db1736828669d1af9d863e717329","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/server-tags/","postref":"2ef8db1736828669d1af9d863e717329","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/docs/rancher/v2.x/en/installation/options/server-tags/","section":"rancher","tags":null,"title":"Choosing a Rancher Version","type":"rancher","url":"/docs/rancher/v2.x/en/installation/options/server-tags/","weight":230,"wordcount":817},{"authors":null,"categories":null,"content":"To deploy Kubernetes, RKE deploys several core components or services in Docker containers on the nodes. Based on the roles of the node, the containers deployed may be different.\nAll services support additional custom arguments, Docker mount binds and extra environment variables.\n   Component Services key name in cluster.yml     etcd etcd   kube-apiserver kube-api   kube-controller-manager kube-controller   kubelet kubelet   kube-scheduler scheduler   kube-proxy kubeproxy    etcd Kubernetes uses etcd as a store for cluster state and data.","date":-62135596800,"description":"To deploy Kubernetes, RKE deploys several default Kubernetes services. Read about etcd, kube-api server, kubelet, kube-proxy and more","dir":"rke/latest/en/config-options/services/","expirydate":-62135596800,"fuzzywordcount":1200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"9f7759e5aac20df93f92e62d7adb5d33","permalink":"http://jijeesh.github.io/docs/rke/latest/en/config-options/services/","postref":"9f7759e5aac20df93f92e62d7adb5d33","publishdate":"0001-01-01T00:00:00Z","readingtime":6,"relpermalink":"/docs/rke/latest/en/config-options/services/","section":"rke","tags":null,"title":"Default Kubernetes Services","type":"rke","url":"/docs/rke/latest/en/config-options/services/","weight":230,"wordcount":1114},{"authors":null,"categories":null,"content":"As of version v0.3.1 RKE adds the support for managing secret data encryption at rest, which is supported by Kubernetes since version v1.13.\nAt-rest data encryption is required for:\n Compliance requirements Additional layer of security Reduce security impact of etcd node compromise Reduce security impact of etcd backups compromise Ability to use external Key Management Systems  RKE provides users with two paths of configuration to enable at-rest data encryption:","date":-62135596800,"description":"","dir":"rke/latest/en/config-options/secrets-encryption/","expirydate":-62135596800,"fuzzywordcount":1300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"4314c4b4266ddd1b1a129d004db26c58","permalink":"http://jijeesh.github.io/docs/rke/latest/en/config-options/secrets-encryption/","postref":"4314c4b4266ddd1b1a129d004db26c58","publishdate":"0001-01-01T00:00:00Z","readingtime":6,"relpermalink":"/docs/rke/latest/en/config-options/secrets-encryption/","section":"rke","tags":null,"title":"Encrypting Secret Data at Rest","type":"rke","url":"/docs/rke/latest/en/config-options/secrets-encryption/","weight":230,"wordcount":1247},{"authors":null,"categories":null,"content":"RKE supports additional service arguments, volume binds and environment variables.\nExtra Args For any of the Kubernetes services, you can update the extra_args to change the existing defaults.\nAs of v0.1.3, using extra_args will add new arguments and override any existing defaults. For example, if you need to modify the default admission plugins list, you need to include the default list and edit it with your changes so all changes are included.","date":-62135596800,"description":"","dir":"rke/latest/en/config-options/services/services-extras/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"966c23a617ee8a579f49c64996cd3c9e","permalink":"http://jijeesh.github.io/docs/rke/latest/en/config-options/services/services-extras/","postref":"966c23a617ee8a579f49c64996cd3c9e","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rke/latest/en/config-options/services/services-extras/","section":"rke","tags":null,"title":"Extra Args, Extra Binds, and Extra Environment Variables","type":"rke","url":"/docs/rke/latest/en/config-options/services/services-extras/","weight":231,"wordcount":190},{"authors":null,"categories":null,"content":"By default, RKE will launch etcd servers, but RKE also supports being able to use an external etcd. RKE only supports connecting to a TLS enabled etcd setup.\n Note: RKE will not accept having external etcd servers in conjunction with nodes with the etcd role.\n services:etcd:path:/etcdclusterexternal_urls:-https://etcd-example.com:2379ca_cert:|- -----BEGIN CERTIFICATE-----xxxxxxxxxx-----ENDCERTIFICATE----- cert: |------BEGINCERTIFICATE----- xxxxxxxxxx-----ENDCERTIFICATE----- key: |------BEGINPRIVATEKEY----- xxxxxxxxxx-----ENDPRIVATEKEY----- External etcd Options Path The path defines the location of where the etcd cluster is on the endpoints.","date":-62135596800,"description":"","dir":"rke/latest/en/config-options/services/external-etcd/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"62aae19c56e4db138fed5746bf263b49","permalink":"http://jijeesh.github.io/docs/rke/latest/en/config-options/services/external-etcd/","postref":"62aae19c56e4db138fed5746bf263b49","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rke/latest/en/config-options/services/external-etcd/","section":"rke","tags":null,"title":"External etcd","type":"rke","url":"/docs/rke/latest/en/config-options/services/external-etcd/","weight":232,"wordcount":110},{"authors":null,"categories":null,"content":"RKE supports x509 authentication strategy. You can additionally define a list of SANs (Subject Alternative Names) to add to the Kubernetes API Server PKI certificates. As an example, this allows you to connect to your Kubernetes cluster API Server through a load balancer instead of a single node.\nauthentication:strategy:x509sans:-&#34;10.18.160.10&#34;-&#34;my-loadbalancer-1234567890.us-west-2.elb.amazonaws.com&#34; RKE also supports the webhook authentication strategy. You can enable both x509 and webhook strategies by using a | separator in the configuration.","date":-62135596800,"description":"","dir":"rke/latest/en/config-options/authentication/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"6b819353b5577db0bc5f32b1cf25b7ad","permalink":"http://jijeesh.github.io/docs/rke/latest/en/config-options/authentication/","postref":"6b819353b5577db0bc5f32b1cf25b7ad","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rke/latest/en/config-options/authentication/","section":"rke","tags":null,"title":"Authentication","type":"rke","url":"/docs/rke/latest/en/config-options/authentication/","weight":235,"wordcount":103},{"authors":null,"categories":null,"content":"Kubernetes supports multiple Authorization Modules. Currently, RKE only supports the RBAC module.\nBy default, RBAC is already enabled. If you wanted to turn off RBAC support, which isn&rsquo;t recommended, you set the authorization mode to none in your cluster.yml.\nauthorization:# Use `mode: none` to disable authorizationmode:rbac","date":-62135596800,"description":"","dir":"rke/latest/en/config-options/authorization/","expirydate":-62135596800,"fuzzywordcount":100,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"26b30479d2961072775806ec77895758","permalink":"http://jijeesh.github.io/docs/rke/latest/en/config-options/authorization/","postref":"26b30479d2961072775806ec77895758","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rke/latest/en/config-options/authorization/","section":"rke","tags":null,"title":"Authorization","type":"rke","url":"/docs/rke/latest/en/config-options/authorization/","weight":240,"wordcount":46},{"authors":null,"categories":null,"content":"Using the EventRateLimit admission control enforces a limit on the number of events that the API Server will accept in a given time period. In a large multi-tenant cluster, there might be a small percentage of tenants that flood the server with event requests, which could have a significant impact on the performance of the cluster overall. Therefore, it is recommended to limit the rate of events that the API server will accept.","date":-62135596800,"description":"","dir":"rke/latest/en/config-options/rate-limiting/","expirydate":-62135596800,"fuzzywordcount":300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"8d1161152ec51972a7ded704648ce3f3","permalink":"http://jijeesh.github.io/docs/rke/latest/en/config-options/rate-limiting/","postref":"8d1161152ec51972a7ded704648ce3f3","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rke/latest/en/config-options/rate-limiting/","section":"rke","tags":null,"title":"Rate Limiting","type":"rke","url":"/docs/rke/latest/en/config-options/rate-limiting/","weight":241,"wordcount":205},{"authors":null,"categories":null,"content":"RKE supports the ability to set your specific cloud provider for your Kubernetes cluster. There are specific cloud configurations for these cloud providers. To enable a cloud provider its name as well as any required configuration options must be provided under the cloud_provider directive in the cluster YML.\n AWS Azure OpenStack vSphere  Outside of this list, RKE also supports the ability to handle any custom cloud provider.","date":-62135596800,"description":"","dir":"rke/latest/en/config-options/cloud-providers/","expirydate":-62135596800,"fuzzywordcount":100,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"c89a4b4084299587321efcfcf72eecd8","permalink":"http://jijeesh.github.io/docs/rke/latest/en/config-options/cloud-providers/","postref":"c89a4b4084299587321efcfcf72eecd8","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rke/latest/en/config-options/cloud-providers/","section":"rke","tags":null,"title":"Cloud Providers","type":"rke","url":"/docs/rke/latest/en/config-options/cloud-providers/","weight":250,"wordcount":67},{"authors":null,"categories":null,"content":"To enable the AWS cloud provider, there are no RKE configuration options. You only need to set the name as aws. In order to use the AWS cloud provider, all cluster nodes must have already been configured with an appropriate IAM role and your AWS resources must be tagged with a cluster ID.\ncloud_provider:name:aws IAM Requirements The nodes used in RKE that will be running the AWS cloud provider must have at least the following IAM policy (rancher-role.","date":-62135596800,"description":"","dir":"rke/latest/en/config-options/cloud-providers/aws/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"cccde453cba2a44f494d9d13ea04f06a","permalink":"http://jijeesh.github.io/docs/rke/latest/en/config-options/cloud-providers/aws/","postref":"cccde453cba2a44f494d9d13ea04f06a","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rke/latest/en/config-options/cloud-providers/aws/","section":"rke","tags":null,"title":"AWS Cloud Provider","type":"rke","url":"/docs/rke/latest/en/config-options/cloud-providers/aws/","weight":251,"wordcount":346},{"authors":null,"categories":null,"content":"Kubernetes auditing provides a security-relevant chronological set of records about a cluster. Kube-apiserver performs auditing. Each request on each stage of its execution generates an event, which is then pre-processed according to a certain policy and written to a backend. The policy determines what’s recorded and the backends persist the records.\nYou might want to configure the audit log as part of compliance with the CIS (Center for Internet Security) Kubernetes Benchmark controls.","date":-62135596800,"description":"","dir":"rke/latest/en/config-options/audit-log/","expirydate":-62135596800,"fuzzywordcount":500,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"909d4610634b8121cc5c36882d1af3ad","permalink":"http://jijeesh.github.io/docs/rke/latest/en/config-options/audit-log/","postref":"909d4610634b8121cc5c36882d1af3ad","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/rke/latest/en/config-options/audit-log/","section":"rke","tags":null,"title":"Audit Log","type":"rke","url":"/docs/rke/latest/en/config-options/audit-log/","weight":251,"wordcount":452},{"authors":null,"categories":null,"content":"If you operate Rancher behind a proxy and you want to access services through the proxy (such as retrieving catalogs), you must provide Rancher information about your proxy. As Rancher is written in Go, it uses the common proxy environment variables as shown below.\nMake sure NO_PROXY contains the network addresses, network address ranges and domains that should be excluded from using the proxy.\n   Environment variable Purpose     HTTP_PROXY Proxy address to use when initiating HTTP connection(s)   HTTPS_PROXY Proxy address to use when initiating HTTPS connection(s)   NO_PROXY Network address(es), network address range(s) and domains to exclude from using the proxy when initiating connection(s)     Note NO_PROXY must be in uppercase to use network range (CIDR) notation.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/other-installation-methods/single-node-docker/proxy/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"8c83a1afceca0a835947f777307efc26","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/other-installation-methods/single-node-docker/proxy/","postref":"8c83a1afceca0a835947f777307efc26","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rancher/v2.x/en/installation/other-installation-methods/single-node-docker/proxy/","section":"rancher","tags":null,"title":"HTTP Proxy Configuration","type":"rancher","url":"/docs/rancher/v2.x/en/installation/other-installation-methods/single-node-docker/proxy/","weight":251,"wordcount":194},{"authors":null,"categories":null,"content":"To enable the Azure cloud provider, besides setting the name as azure, there are specific configuration options that must be set. Additionally, the Azure node name must also match the Kubernetes node name.\ncloud_provider:name:azureazureCloudProvider:aadClientId:xxxxxxxxxaadClientSecret:xxxxxxxxxlocation:xxxxxxxxxresourceGroup:xxxxxxxxxsubnetName:xxxxxxxxxsubscriptionId:xxxxxxxxxvnetName:xxxxxxxxxtenantId:xxxxxxxxxsecurityGroupName:xxxxxxxxx Overriding the hostname Since the Azure node name must match the Kubernetes node name, you override the Kubernetes name on the node by setting the hostname_override for each node. If you do not set the hostname_override, the Kubernetes node name will be set as the address, which will cause the Azure cloud provider to fail.","date":-62135596800,"description":"","dir":"rke/latest/en/config-options/cloud-providers/azure/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"b454bf29a15f19f93efbce75513ff962","permalink":"http://jijeesh.github.io/docs/rke/latest/en/config-options/cloud-providers/azure/","postref":"b454bf29a15f19f93efbce75513ff962","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rke/latest/en/config-options/cloud-providers/azure/","section":"rke","tags":null,"title":"Azure Cloud Provider","type":"rke","url":"/docs/rke/latest/en/config-options/cloud-providers/azure/","weight":252,"wordcount":174},{"authors":null,"categories":null,"content":"For development and testing environments that have a special requirement to terminate TLS/SSL at a load balancer instead of your Rancher Server container, deploy Rancher and configure a load balancer to work with it conjunction. This install procedure walks you through deployment of Rancher using a single container, and then provides a sample configuration for a layer 7 Nginx load balancer.\n Want to skip the external load balancer? See Docker Installation instead.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/other-installation-methods/single-node-docker/single-node-install-external-lb/","expirydate":-62135596800,"fuzzywordcount":1400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"78ff312485955ae74f2340ef270c36b1","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/other-installation-methods/single-node-docker/single-node-install-external-lb/","postref":"78ff312485955ae74f2340ef270c36b1","publishdate":"0001-01-01T00:00:00Z","readingtime":7,"relpermalink":"/docs/rancher/v2.x/en/installation/other-installation-methods/single-node-docker/single-node-install-external-lb/","section":"rancher","tags":null,"title":"Docker Install with External Load Balancer","type":"rancher","url":"/docs/rancher/v2.x/en/installation/other-installation-methods/single-node-docker/single-node-install-external-lb/","weight":252,"wordcount":1397},{"authors":null,"categories":null,"content":"To enable the Openstack cloud provider, besides setting the name as openstack, there are specific configuration options that must be set. The Openstack configuration options are grouped into different sections.\ncloud_provider:name:openstackopenstackCloudProvider:global:username:xxxxxxxxxxxxxxpassword:xxxxxxxxxxxxxxauth-url:https://1.2.3.4/identity/v3tenant-id:xxxxxxxxxxxxxxdomain-id:xxxxxxxxxxxxxxload_balancer:subnet-id:xxxxxxxxxxxxxxblock_storage:ignore-volume-az:trueroute:router-id:xxxxxxxxxxxxxxmetadata:search-order:xxxxxxxxxxxxxx Overriding the hostname The OpenStack cloud provider uses the instance name (as determined from OpenStack metadata) as the name of the Kubernetes Node object, you must override the Kubernetes name on the node by setting the hostname_override for each node.","date":-62135596800,"description":"","dir":"rke/latest/en/config-options/cloud-providers/openstack/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"c1675a129d352b6f2a76272f24816aa9","permalink":"http://jijeesh.github.io/docs/rke/latest/en/config-options/cloud-providers/openstack/","postref":"c1675a129d352b6f2a76272f24816aa9","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rke/latest/en/config-options/cloud-providers/openstack/","section":"rke","tags":null,"title":"Openstack Cloud Provider","type":"rke","url":"/docs/rke/latest/en/config-options/cloud-providers/openstack/","weight":253,"wordcount":300},{"authors":null,"categories":null,"content":"In order to provision Kubernetes clusters in vSphere with the RKE CLI, you must enable the vSphere cloud provider.\nThe vSphere cloud provider must also be enabled in order to provision clusters with Rancher, which uses RKE as a library when provisioning RKE clusters.\nThe vSphere Cloud Provider interacts with VMware infrastructure (vCenter or standalone ESXi server) to provision and manage storage for persistent volumes in a Kubernetes cluster.","date":-62135596800,"description":"","dir":"rke/latest/en/config-options/cloud-providers/vsphere/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"62be3d341a5bd60cd5aeea31813e14b3","permalink":"http://jijeesh.github.io/docs/rke/latest/en/config-options/cloud-providers/vsphere/","postref":"62be3d341a5bd60cd5aeea31813e14b3","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rke/latest/en/config-options/cloud-providers/vsphere/","section":"rke","tags":null,"title":"vSphere Cloud Provider","type":"rke","url":"/docs/rke/latest/en/config-options/cloud-providers/vsphere/","weight":254,"wordcount":330},{"authors":null,"categories":null,"content":"If you want to enable a different cloud provider, RKE allows for custom cloud provider options. A name must be provided and the custom Cloud Provider options can be passed in as a multiline string in customCloudProvider.\nFor example, in order to use the oVirt cloud provider with Kubernetes, here&rsquo;s the following cloud provider information:\n[connection] uri = https://localhost:8443/ovirt-engine/api username = admin@internal password = admin  To add this cloud config file to RKE, the cloud_provider would be need to be set.","date":-62135596800,"description":"","dir":"rke/latest/en/config-options/cloud-providers/custom/","expirydate":-62135596800,"fuzzywordcount":100,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"d6b92be018da221f7dc2e06274d5afc5","permalink":"http://jijeesh.github.io/docs/rke/latest/en/config-options/cloud-providers/custom/","postref":"d6b92be018da221f7dc2e06274d5afc5","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rke/latest/en/config-options/cloud-providers/custom/","section":"rke","tags":null,"title":"Custom Cloud Provider","type":"rke","url":"/docs/rke/latest/en/config-options/cloud-providers/custom/","weight":255,"wordcount":94},{"authors":null,"categories":null,"content":"RKE supports configuring pluggable add-ons in the cluster YML. Add-ons are used to deploy several cluster components including:\n Network plug-ins Ingress controller DNS provider Metrics Server  These add-ons require images that can be found under the system_images directive. For each Kubernetes version, there are default images associated with each add-on, but these can be overridden by changing the image tag in system_images.\nThere are a few things worth noting:","date":-62135596800,"description":"","dir":"rke/latest/en/config-options/add-ons/","expirydate":-62135596800,"fuzzywordcount":300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"d6f7146d9784d17df0468d5bf0f16cc6","permalink":"http://jijeesh.github.io/docs/rke/latest/en/config-options/add-ons/","postref":"d6f7146d9784d17df0468d5bf0f16cc6","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rke/latest/en/config-options/add-ons/","section":"rke","tags":null,"title":"Add-Ons","type":"rke","url":"/docs/rke/latest/en/config-options/add-ons/","weight":260,"wordcount":297},{"authors":null,"categories":null,"content":"RKE provides the following network plug-ins that are deployed as add-ons:\n Flannel Calico Canal Weave   Note: After you launch the cluster, you cannot change your network provider. Therefore, choose which network provider you want to use carefully, as Kubernetes doesn’t allow switching between network providers. Once a cluster is created with a network provider, changing network providers would require you tear down the entire cluster and all its applications.","date":-62135596800,"description":"","dir":"rke/latest/en/config-options/add-ons/network-plugins/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"c909f9e5b19aa7256398c10b8d4b7236","permalink":"http://jijeesh.github.io/docs/rke/latest/en/config-options/add-ons/network-plugins/","postref":"c909f9e5b19aa7256398c10b8d4b7236","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rke/latest/en/config-options/add-ons/network-plugins/","section":"rke","tags":null,"title":"Network Plug-ins","type":"rke","url":"/docs/rke/latest/en/config-options/add-ons/network-plugins/","weight":261,"wordcount":377},{"authors":null,"categories":null,"content":"RKE provides the following DNS providers that can be deployed as add-ons:\n CoreDNS kube-dns     RKE version Kubernetes version Default DNS provider     v0.2.5 and higher v1.14.0 and higher CoreDNS   v0.2.5 and higher v1.13.x and lower kube-dns   v0.2.4 and lower any kube-dns    CoreDNS was made the default in RKE v0.2.5 when using Kubernetes 1.14 and higher.","date":-62135596800,"description":"","dir":"rke/latest/en/config-options/add-ons/dns/","expirydate":-62135596800,"fuzzywordcount":600,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"de0dd1977d3835369ebcc338ad72412f","permalink":"http://jijeesh.github.io/docs/rke/latest/en/config-options/add-ons/dns/","postref":"de0dd1977d3835369ebcc338ad72412f","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/rke/latest/en/config-options/add-ons/dns/","section":"rke","tags":null,"title":"DNS providers","type":"rke","url":"/docs/rke/latest/en/config-options/add-ons/dns/","weight":262,"wordcount":533},{"authors":null,"categories":null,"content":"By default, RKE deploys the NGINX ingress controller on all schedulable nodes.\n Note: As of v0.1.8, only workers are considered schedulable nodes, but prior to v0.1.8, worker and controlplane nodes were considered schedulable nodes.\n RKE will deploy the ingress controller as a DaemonSet with hostnetwork: true, so ports 80, and 443 will be opened on each node where the controller is deployed.\nThe images used for ingress controller is under the system_images directive.","date":-62135596800,"description":"By default, RKE deploys the NGINX ingress controller. Learn how to schedule and disable default k8s ingress controllers, and how to configure NGINX controller","dir":"rke/latest/en/config-options/add-ons/ingress-controllers/","expirydate":-62135596800,"fuzzywordcount":500,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"3275143b2758a8d8da078b7dfd53768f","permalink":"http://jijeesh.github.io/docs/rke/latest/en/config-options/add-ons/ingress-controllers/","postref":"3275143b2758a8d8da078b7dfd53768f","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rke/latest/en/config-options/add-ons/ingress-controllers/","section":"rke","tags":null,"title":"K8s Ingress Controllers","type":"rke","url":"/docs/rke/latest/en/config-options/add-ons/ingress-controllers/","weight":262,"wordcount":419},{"authors":null,"categories":null,"content":"By default, RKE deploys Metrics Server to provide metrics on resources in your cluster.\nRKE will deploy Metrics Server as a Deployment.\nThe image used for Metrics Server is under the system_images directive. For each Kubernetes version, there is a default image associated with the Metrics Server, but these can be overridden by changing the image tag in system_images.\nDisabling the Metrics Server Available as of v0.2.0\nYou can disable the default controller by specifying none to the monitoring provider directive in the cluster configuration.","date":-62135596800,"description":"","dir":"rke/latest/en/config-options/add-ons/metrics-server/","expirydate":-62135596800,"fuzzywordcount":100,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"88010a75db658e125e573a3b40cc1280","permalink":"http://jijeesh.github.io/docs/rke/latest/en/config-options/add-ons/metrics-server/","postref":"88010a75db658e125e573a3b40cc1280","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rke/latest/en/config-options/add-ons/metrics-server/","section":"rke","tags":null,"title":"Metrics Server","type":"rke","url":"/docs/rke/latest/en/config-options/add-ons/metrics-server/","weight":263,"wordcount":86},{"authors":null,"categories":null,"content":"Besides the network plug-in and ingress controllers, you can define any add-on that you want deployed after the Kubernetes cluster is deployed.\nThere are two ways that you can specify an add-on.\n In-line Add-ons Referencing YAML Files for Add-ons   Note: When using user-defined add-ons, you must define a namespace for all your resources, otherwise they will end up in the kube-system namespace.\n RKE uploads the YAML manifest as a configmap to the Kubernetes cluster.","date":-62135596800,"description":"","dir":"rke/latest/en/config-options/add-ons/user-defined-add-ons/","expirydate":-62135596800,"fuzzywordcount":300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"72de33a5c874d10cb2dff7e2b6b04b06","permalink":"http://jijeesh.github.io/docs/rke/latest/en/config-options/add-ons/user-defined-add-ons/","postref":"72de33a5c874d10cb2dff7e2b6b04b06","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rke/latest/en/config-options/add-ons/user-defined-add-ons/","section":"rke","tags":null,"title":"User-Defined Add-Ons","type":"rke","url":"/docs/rke/latest/en/config-options/add-ons/user-defined-add-ons/","weight":263,"wordcount":213},{"authors":null,"categories":null,"content":"NGINX will be configured as Layer 4 load balancer (TCP) that forwards connections to one of your Rancher nodes.\n Note: In this configuration, the load balancer is positioned in front of your nodes. The load balancer can be any host capable of running NGINX.\nOne caveat: do not use one of your Rancher nodes as the load balancer.\n Install NGINX Start by installing NGINX on the node you want to use as a load balancer.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/options/helm2/create-nodes-lb/nginx/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"b99f5258f1bc911f28412b298d40016e","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/helm2/create-nodes-lb/nginx/","postref":"b99f5258f1bc911f28412b298d40016e","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/installation/options/helm2/create-nodes-lb/nginx/","section":"rancher","tags":null,"title":"NGINX","type":"rancher","url":"/docs/rancher/v2.x/en/installation/options/helm2/create-nodes-lb/nginx/","weight":270,"wordcount":355},{"authors":null,"categories":null,"content":"NGINX will be configured as Layer 4 load balancer (TCP) that forwards connections to one of your Rancher nodes.\n Note: In this configuration, the load balancer is positioned in front of your nodes. The load balancer can be any host capable of running NGINX.\nOne caveat: do not use one of your Rancher nodes as the load balancer.\n Install NGINX Start by installing NGINX on the node you want to use as a load balancer.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/k8s-install/create-nodes-lb/nginx/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"cce1842a57f75a14d45d95d734961721","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/k8s-install/create-nodes-lb/nginx/","postref":"cce1842a57f75a14d45d95d734961721","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/installation/k8s-install/create-nodes-lb/nginx/","section":"rancher","tags":null,"title":"Setting up an NGINX Load Balancer","type":"rancher","url":"/docs/rancher/v2.x/en/installation/k8s-install/create-nodes-lb/nginx/","weight":270,"wordcount":356},{"authors":null,"categories":null,"content":"Important: RKE add-on install is only supported up to Rancher v2.0.8 Please use the Rancher helm chart to install Rancher on a Kubernetes cluster. For details, see the Kubernetes Install - Installation Outline.\nIf you are currently using the RKE add-on install method, see Migrating from a High-availability Kubernetes install with an RKE add-on for details on how to move to using the Helm chart.\n This procedure walks you through setting up a 3-node cluster using the Rancher Kubernetes Engine (RKE).","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/options/helm2/rke-add-on/layer-4-lb/","expirydate":-62135596800,"fuzzywordcount":2300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"fe7757b53e9b733328983e43bb9c17e8","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/helm2/rke-add-on/layer-4-lb/","postref":"fe7757b53e9b733328983e43bb9c17e8","publishdate":"0001-01-01T00:00:00Z","readingtime":11,"relpermalink":"/docs/rancher/v2.x/en/installation/options/helm2/rke-add-on/layer-4-lb/","section":"rancher","tags":null,"title":"Kubernetes Install with External Load Balancer (TCP/Layer 4)","type":"rancher","url":"/docs/rancher/v2.x/en/installation/options/helm2/rke-add-on/layer-4-lb/","weight":275,"wordcount":2254},{"authors":null,"categories":null,"content":"Important: RKE add-on install is only supported up to Rancher v2.0.8 Please use the Rancher helm chart to install Rancher on a Kubernetes cluster. For details, see the Kubernetes Install - Installation Outline.\nIf you are currently using the RKE add-on install method, see Migrating from a Kubernetes Install with an RKE Add-on for details on how to move to using the helm chart.\n This procedure walks you through setting up a 3-node cluster using the Rancher Kubernetes Engine (RKE).","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/options/rke-add-on/layer-4-lb/","expirydate":-62135596800,"fuzzywordcount":2300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"4b8a8ba04d4343a65ec89b24c2d3fb8c","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/rke-add-on/layer-4-lb/","postref":"4b8a8ba04d4343a65ec89b24c2d3fb8c","publishdate":"0001-01-01T00:00:00Z","readingtime":11,"relpermalink":"/docs/rancher/v2.x/en/installation/options/rke-add-on/layer-4-lb/","section":"rancher","tags":null,"title":"Kubernetes Install with External Load Balancer (TCP/Layer 4)","type":"rancher","url":"/docs/rancher/v2.x/en/installation/options/rke-add-on/layer-4-lb/","weight":275,"wordcount":2256},{"authors":null,"categories":null,"content":"Kubernetes will create all the objects and services for Rancher, but it will not become available until we populate the tls-rancher-ingress secret in the cattle-system namespace with the certificate and key.\nCombine the server certificate followed by any intermediate certificate(s) needed into a file named tls.crt. Copy your certificate key into a file named tls.key.\nUse kubectl with the tls secret type to create the secrets.\nkubectl -n cattle-system create secret tls tls-rancher-ingress \\ --cert=tls.","date":-62135596800,"description":"Read about how to populate the Kubernetes TLS secret for a Rancher installation","dir":"rancher/v2.x/en/installation/options/helm2/helm-rancher/tls-secrets/","expirydate":-62135596800,"fuzzywordcount":300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"357daccc66618286758c5c8a14747b0a","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/helm2/helm-rancher/tls-secrets/","postref":"357daccc66618286758c5c8a14747b0a","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/installation/options/helm2/helm-rancher/tls-secrets/","section":"rancher","tags":null,"title":"Adding Kubernetes TLS Secrets","type":"rancher","url":"/docs/rancher/v2.x/en/installation/options/helm2/helm-rancher/tls-secrets/","weight":276,"wordcount":222},{"authors":null,"categories":null,"content":"Kubernetes will create all the objects and services for Rancher, but it will not become available until we populate the tls-rancher-ingress secret in the cattle-system namespace with the certificate and key.\nCombine the server certificate followed by any intermediate certificate(s) needed into a file named tls.crt. Copy your certificate key into a file named tls.key.\nUse kubectl with the tls secret type to create the secrets.\nkubectl -n cattle-system create secret tls tls-rancher-ingress \\ --cert=tls.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/options/tls-secrets/","expirydate":-62135596800,"fuzzywordcount":300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"62105fa32580cc17d4a278cdadd9138e","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/tls-secrets/","postref":"62105fa32580cc17d4a278cdadd9138e","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/installation/options/tls-secrets/","section":"rancher","tags":null,"title":"Adding TLS Secrets","type":"rancher","url":"/docs/rancher/v2.x/en/installation/options/tls-secrets/","weight":276,"wordcount":222},{"authors":null,"categories":null,"content":"Common Options    Option Default Value Description     hostname &rdquo; &ldquo; string - the Fully Qualified Domain Name for your Rancher Server   ingress.tls.source &ldquo;rancher&rdquo; string - Where to get the cert for the ingress. - &ldquo;rancher, letsEncrypt, secret&rdquo;   letsEncrypt.email &rdquo; &ldquo; string - Your email address   letsEncrypt.environment &ldquo;production&rdquo; string - Valid options: &ldquo;staging, production&rdquo;   privateCA false bool - Set to true if your cert is signed by a private CA","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/options/helm2/helm-rancher/chart-options/","expirydate":-62135596800,"fuzzywordcount":1500,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"37a6b5aaa2d6954f31155dee704b0f94","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/helm2/helm-rancher/chart-options/","postref":"37a6b5aaa2d6954f31155dee704b0f94","publishdate":"0001-01-01T00:00:00Z","readingtime":7,"relpermalink":"/docs/rancher/v2.x/en/installation/options/helm2/helm-rancher/chart-options/","section":"rancher","tags":null,"title":"Chart Options","type":"rancher","url":"/docs/rancher/v2.x/en/installation/options/helm2/helm-rancher/chart-options/","weight":276,"wordcount":1418},{"authors":null,"categories":null,"content":"Common Options    Option Default Value Description     hostname &rdquo; &ldquo; string - the Fully Qualified Domain Name for your Rancher Server   ingress.tls.source &ldquo;rancher&rdquo; string - Where to get the cert for the ingress. - &ldquo;rancher, letsEncrypt, secret&rdquo;   letsEncrypt.email &rdquo; &ldquo; string - Your email address   letsEncrypt.environment &ldquo;production&rdquo; string - Valid options: &ldquo;staging, production&rdquo;   privateCA false bool - Set to true if your cert is signed by a private CA","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/options/chart-options/","expirydate":-62135596800,"fuzzywordcount":1500,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"65b775c31fe356a8007a0651ad04ca0f","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/chart-options/","postref":"65b775c31fe356a8007a0651ad04ca0f","publishdate":"0001-01-01T00:00:00Z","readingtime":7,"relpermalink":"/docs/rancher/v2.x/en/installation/options/chart-options/","section":"rancher","tags":null,"title":"Helm Chart Options for Kubernetes Installations","type":"rancher","url":"/docs/rancher/v2.x/en/installation/options/chart-options/","weight":276,"wordcount":1425},{"authors":null,"categories":null,"content":"Important: RKE add-on install is only supported up to Rancher v2.0.8 Please use the Rancher Helm chart to install Rancher on a Kubernetes cluster. For details, see the Kubernetes Install - Installation Outline.\nIf you are currently using the RKE add-on install method, see Migrating from a Kubernetes Install with an RKE Add-on for details on how to move to using the helm chart.\n This procedure walks you through setting up a 3-node cluster using the Rancher Kubernetes Engine (RKE).","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/options/helm2/rke-add-on/layer-7-lb/","expirydate":-62135596800,"fuzzywordcount":1900,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"109e8af97c2534fd56f58d4739b98f22","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/helm2/rke-add-on/layer-7-lb/","postref":"109e8af97c2534fd56f58d4739b98f22","publishdate":"0001-01-01T00:00:00Z","readingtime":9,"relpermalink":"/docs/rancher/v2.x/en/installation/options/helm2/rke-add-on/layer-7-lb/","section":"rancher","tags":null,"title":"Kubernetes Install with External Load Balancer (HTTPS/Layer 7)","type":"rancher","url":"/docs/rancher/v2.x/en/installation/options/helm2/rke-add-on/layer-7-lb/","weight":276,"wordcount":1847},{"authors":null,"categories":null,"content":"Important: RKE add-on install is only supported up to Rancher v2.0.8 Please use the Rancher Helm chart to install Rancher on a Kubernetes cluster. For details, see the Kubernetes Install - Installation Outline.\nIf you are currently using the RKE add-on install method, see Migrating from a Kubernetes Install with an RKE Add-on for details on how to move to using the helm chart.\n This procedure walks you through setting up a 3-node cluster using the Rancher Kubernetes Engine (RKE).","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/options/rke-add-on/layer-7-lb/","expirydate":-62135596800,"fuzzywordcount":1900,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"45b7a997e1bd158ff908b17f9f6b6fa1","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/rke-add-on/layer-7-lb/","postref":"45b7a997e1bd158ff908b17f9f6b6fa1","publishdate":"0001-01-01T00:00:00Z","readingtime":9,"relpermalink":"/docs/rancher/v2.x/en/installation/options/rke-add-on/layer-7-lb/","section":"rancher","tags":null,"title":"Kubernetes Install with External Load Balancer (HTTPS/Layer 7)","type":"rancher","url":"/docs/rancher/v2.x/en/installation/options/rke-add-on/layer-7-lb/","weight":276,"wordcount":1851},{"authors":null,"categories":null,"content":"  Important: RKE add-on install is only supported up to Rancher v2.0.8 Please use the Rancher helm chart to install Rancher on a Kubernetes cluster. For details, see the Kubernetes Install - Installation Outline.\nIf you are currently using the RKE add-on install method, see Migrating from a Kubernetes Install with an RKE Add-on for details on how to move to using the helm chart.\n  Kubernetes installation with External Load Balancer (TCP/Layer 4) Kubernetes installation with External Load Balancer (HTTPS/Layer 7) HTTP Proxy Configuration for a Kubernetes installation Troubleshooting RKE Add-on Installs  ","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/options/helm2/rke-add-on/","expirydate":-62135596800,"fuzzywordcount":100,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"1c3969c10bce938db9438a15e5b6ee1f","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/helm2/rke-add-on/","postref":"1c3969c10bce938db9438a15e5b6ee1f","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rancher/v2.x/en/installation/options/helm2/rke-add-on/","section":"rancher","tags":null,"title":"RKE Add-On Install","type":"rancher","url":"/docs/rancher/v2.x/en/installation/options/helm2/rke-add-on/","weight":276,"wordcount":91},{"authors":null,"categories":null,"content":" Important: RKE add-on install is only supported up to Rancher v2.0.8\nPlease use the Rancher helm chart to install Rancher on a Kubernetes cluster. For details, see the Kubernetes Install - Installation Outline.\nIf you are currently using the RKE add-on install method, see Migrating from a Kubernetes Install with an RKE Add-on for details on how to move to using the helm chart.\n  Kubernetes Installation with External Load Balancer (TCP/Layer 4) Kubernetes Installation with External Load Balancer (HTTPS/Layer 7) HTTP Proxy Configuration for a Kubernetes installation Troubleshooting RKE Add-on Installs  ","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/options/rke-add-on/","expirydate":-62135596800,"fuzzywordcount":100,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"05c5d919297b84d2bc333d694beb7ff0","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/rke-add-on/","postref":"05c5d919297b84d2bc333d694beb7ff0","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rancher/v2.x/en/installation/options/rke-add-on/","section":"rancher","tags":null,"title":"RKE Add-On Install","type":"rancher","url":"/docs/rancher/v2.x/en/installation/options/rke-add-on/","weight":276,"wordcount":91},{"authors":null,"categories":null,"content":"Helm commands show forbidden When Helm is initiated in the cluster without specifying the correct ServiceAccount, the command helm init will succeed but you won&rsquo;t be able to execute most of the other helm commands. The following error will be shown:\nError: configmaps is forbidden: User &quot;system:serviceaccount:kube-system:default&quot; cannot list configmaps in the namespace &quot;kube-system&quot;  To resolve this, the server component (tiller) needs to be removed and added with the correct ServiceAccount.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/options/helm2/helm-init/troubleshooting/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"a5e0787c503b1c5270c48cf3d303613f","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/helm2/helm-init/troubleshooting/","postref":"a5e0787c503b1c5270c48cf3d303613f","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rancher/v2.x/en/installation/options/helm2/helm-init/troubleshooting/","section":"rancher","tags":null,"title":"Troubleshooting","type":"rancher","url":"/docs/rancher/v2.x/en/installation/options/helm2/helm-init/troubleshooting/","weight":276,"wordcount":143},{"authors":null,"categories":null,"content":"Where is everything Most of the troubleshooting will be done on objects in these 3 namespaces.\n cattle-system - rancher deployment and pods. ingress-nginx - Ingress controller pods and services. kube-system - tiller and cert-manager pods.  &ldquo;default backend - 404&rdquo; A number of things can cause the ingress-controller not to forward traffic to your rancher instance. Most of the time its due to a bad ssl configuration.\nThings to check","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/options/helm2/helm-rancher/troubleshooting/","expirydate":-62135596800,"fuzzywordcount":600,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"56ec789de407bba763ac0ec3380644e9","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/helm2/helm-rancher/troubleshooting/","postref":"56ec789de407bba763ac0ec3380644e9","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/rancher/v2.x/en/installation/options/helm2/helm-rancher/troubleshooting/","section":"rancher","tags":null,"title":"Troubleshooting","type":"rancher","url":"/docs/rancher/v2.x/en/installation/options/helm2/helm-rancher/troubleshooting/","weight":276,"wordcount":592},{"authors":null,"categories":null,"content":"canal Pods show READY 2&frasl;3 The most common cause of this issue is port 8472/UDP is not open between the nodes. Check your local firewall, network routing or security groups.\nOnce the network issue is resolved, the canal pods should timeout and restart to establish their connections.\nnginx-ingress-controller Pods show RESTARTS The most common cause of this issue is the canal pods have failed to establish the overlay network. See canal Pods show READY 2/3 for troubleshooting.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/options/helm2/kubernetes-rke/troubleshooting/","expirydate":-62135596800,"fuzzywordcount":500,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"3a98b26b0c2c2e2ccf7d07e2bb821c8b","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/helm2/kubernetes-rke/troubleshooting/","postref":"3a98b26b0c2c2e2ccf7d07e2bb821c8b","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/rancher/v2.x/en/installation/options/helm2/kubernetes-rke/troubleshooting/","section":"rancher","tags":null,"title":"Troubleshooting","type":"rancher","url":"/docs/rancher/v2.x/en/installation/options/helm2/kubernetes-rke/troubleshooting/","weight":276,"wordcount":451},{"authors":null,"categories":null,"content":"This section describes how to troubleshoot an installation of Rancher on a Kubernetes cluster.\nRelevant Namespaces Most of the troubleshooting will be done on objects in these 3 namespaces.\n cattle-system - rancher deployment and pods. ingress-nginx - Ingress controller pods and services. kube-system - tiller and cert-manager pods.  &ldquo;default backend - 404&rdquo; A number of things can cause the ingress-controller not to forward traffic to your rancher instance.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/options/troubleshooting/","expirydate":-62135596800,"fuzzywordcount":1100,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"bfad45969d1a96d22a8ff65a920c56d6","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/troubleshooting/","postref":"bfad45969d1a96d22a8ff65a920c56d6","publishdate":"0001-01-01T00:00:00Z","readingtime":5,"relpermalink":"/docs/rancher/v2.x/en/installation/options/troubleshooting/","section":"rancher","tags":null,"title":"Troubleshooting the Rancher Server Kubernetes Cluster","type":"rancher","url":"/docs/rancher/v2.x/en/installation/options/troubleshooting/","weight":276,"wordcount":1058},{"authors":null,"categories":null,"content":"Important: RKE add-on install is only supported up to Rancher v2.0.8 Please use the Rancher helm chart to install Kubernetes Rancher. For details, see the Kubernetes Install - Installation Outline.\nIf you are currently using the RKE add-on install method, see Migrating from a Kubernetes Install with an RKE Add-on for details on how to move to using the helm chart.\n Objectives Configuring an Amazon ALB is a multistage process.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/options/helm2/rke-add-on/layer-7-lb/alb/","expirydate":-62135596800,"fuzzywordcount":500,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"c5896ce49173b544e860518f60d1291f","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/helm2/rke-add-on/layer-7-lb/alb/","postref":"c5896ce49173b544e860518f60d1291f","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/installation/options/helm2/rke-add-on/layer-7-lb/alb/","section":"rancher","tags":null,"title":"Amazon ALB Configuration","type":"rancher","url":"/docs/rancher/v2.x/en/installation/options/helm2/rke-add-on/layer-7-lb/alb/","weight":277,"wordcount":425},{"authors":null,"categories":null,"content":"Important: RKE add-on install is only supported up to Rancher v2.0.8 Please use the Rancher helm chart to install Rancher on a Kubernetes cluster. For details, see the Kubernetes Install - Installation Outline.\nIf you are currently using the RKE add-on install method, see Migrating from a Kubernetes Install with an RKE Add-on for details on how to move to using the helm chart.\n Objectives Configuring an Amazon ALB is a multistage process.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/options/rke-add-on/layer-7-lb/alb/","expirydate":-62135596800,"fuzzywordcount":500,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"46c191836b27aeaafc043ba153958362","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/rke-add-on/layer-7-lb/alb/","postref":"46c191836b27aeaafc043ba153958362","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/rancher/v2.x/en/installation/options/rke-add-on/layer-7-lb/alb/","section":"rancher","tags":null,"title":"Amazon ALB Configuration","type":"rancher","url":"/docs/rancher/v2.x/en/installation/options/rke-add-on/layer-7-lb/alb/","weight":277,"wordcount":428},{"authors":null,"categories":null,"content":"Objectives Configuring an Amazon NLB is a multistage process. We&rsquo;ve broken it down into multiple tasks so that it&rsquo;s easy to follow.\n Create Target Groups\nBegin by creating two target groups for the TCP protocol, one regarding TCP port 443 and one regarding TCP port 80 (providing redirect to TCP port 443). You&rsquo;ll add your Linux nodes to these groups.\n Register Targets\nAdd your Linux nodes to the target groups.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/options/helm2/create-nodes-lb/nlb/","expirydate":-62135596800,"fuzzywordcount":900,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"02a9ac33d744d2c1c269e92304bc12fa","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/helm2/create-nodes-lb/nlb/","postref":"02a9ac33d744d2c1c269e92304bc12fa","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/docs/rancher/v2.x/en/installation/options/helm2/create-nodes-lb/nlb/","section":"rancher","tags":null,"title":"Amazon NLB","type":"rancher","url":"/docs/rancher/v2.x/en/installation/options/helm2/create-nodes-lb/nlb/","weight":277,"wordcount":838},{"authors":null,"categories":null,"content":"Important: RKE add-on install is only supported up to Rancher v2.0.8 Please use the Rancher Helm chart to install Rancher on a Kubernetes cluster. For details, see the Kubernetes Install - Installation Outline.\nIf you are currently using the RKE add-on install method, see Migrating from a High-availability Kubernetes install with an RKE add-on for details on how to move to using the helm chart.\n Objectives Configuring an Amazon NLB is a multistage process.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/options/helm2/rke-add-on/layer-4-lb/nlb/","expirydate":-62135596800,"fuzzywordcount":800,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"34a7f7e6e428cecbb97d4e8eae429575","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/helm2/rke-add-on/layer-4-lb/nlb/","postref":"34a7f7e6e428cecbb97d4e8eae429575","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/docs/rancher/v2.x/en/installation/options/helm2/rke-add-on/layer-4-lb/nlb/","section":"rancher","tags":null,"title":"Amazon NLB Configuration","type":"rancher","url":"/docs/rancher/v2.x/en/installation/options/helm2/rke-add-on/layer-4-lb/nlb/","weight":277,"wordcount":788},{"authors":null,"categories":null,"content":"Important: RKE add-on install is only supported up to Rancher v2.0.8 Please use the Rancher helm chart to install Rancher on a Kubernetes cluster. For details, see the Kubernetes Install - Installation Outline.\nIf you are currently using the RKE add-on install method, see Migrating from a Kubernetes Install with an RKE Add-on for details on how to move to using the helm chart.\n Objectives Configuring an Amazon NLB is a multistage process.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/options/rke-add-on/layer-4-lb/nlb/","expirydate":-62135596800,"fuzzywordcount":800,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"c66d472989a3a3eb6ed168b36bba906c","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/rke-add-on/layer-4-lb/nlb/","postref":"c66d472989a3a3eb6ed168b36bba906c","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/docs/rancher/v2.x/en/installation/options/rke-add-on/layer-4-lb/nlb/","section":"rancher","tags":null,"title":"Amazon NLB Configuration","type":"rancher","url":"/docs/rancher/v2.x/en/installation/options/rke-add-on/layer-4-lb/nlb/","weight":277,"wordcount":787},{"authors":null,"categories":null,"content":"Important: RKE add-on install is only supported up to Rancher v2.0.8 Please use the Rancher Helm chart to install Rancher on a Kubernetes cluster. For details, see the Kubernetes Install - Installation Outline.\nIf you are currently using the RKE add-on install method, see Migrating from a Kubernetes Install with an RKE Add-on for details on how to move to using the helm chart.\n If you operate Rancher behind a proxy and you want to access services through the proxy (such as retrieving catalogs), you must provide Rancher information about your proxy.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/options/helm2/rke-add-on/proxy/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"854ddb290d87a00197ff06b6feaa0e1a","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/helm2/rke-add-on/proxy/","postref":"854ddb290d87a00197ff06b6feaa0e1a","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/installation/options/helm2/rke-add-on/proxy/","section":"rancher","tags":null,"title":"HTTP Proxy Configuration","type":"rancher","url":"/docs/rancher/v2.x/en/installation/options/helm2/rke-add-on/proxy/","weight":277,"wordcount":324},{"authors":null,"categories":null,"content":"Important: RKE add-on install is only supported up to Rancher v2.0.8 Please use the Rancher helm chart to install Rancher on a Kubernetes cluster. For details, see the Kubernetes Install - Installation Outline.\nIf you are currently using the RKE add-on install method, see Migrating from a Kubernetes Install with an RKE Add-on for details on how to move to using the helm chart.\n If you operate Rancher behind a proxy and you want to access services through the proxy (such as retrieving catalogs), you must provide Rancher information about your proxy.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/options/rke-add-on/proxy/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"2050ed4bb27e0fe84a9e28e02ac16321","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/rke-add-on/proxy/","postref":"2050ed4bb27e0fe84a9e28e02ac16321","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/installation/options/rke-add-on/proxy/","section":"rancher","tags":null,"title":"HTTP Proxy Configuration","type":"rancher","url":"/docs/rancher/v2.x/en/installation/options/rke-add-on/proxy/","weight":277,"wordcount":328},{"authors":null,"categories":null,"content":"Important: RKE add-on install is only supported up to Rancher v2.0.8 Please use the Rancher Helm chart to install Rancher on a Kubernetes cluster. For details, see the Kubernetes Install - Installation Outline.\nIf you are currently using the RKE add-on install method, see Migrating from a Kubernetes Install with an RKE Add-on for details on how to move to using the helm chart.\n Install NGINX Start by installing NGINX on your load balancer host.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/options/helm2/rke-add-on/layer-7-lb/nginx/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"b067d89111fb29e5b513e1bb0fbf7833","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/helm2/rke-add-on/layer-7-lb/nginx/","postref":"b067d89111fb29e5b513e1bb0fbf7833","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rancher/v2.x/en/installation/options/helm2/rke-add-on/layer-7-lb/nginx/","section":"rancher","tags":null,"title":"NGINX Configuration","type":"rancher","url":"/docs/rancher/v2.x/en/installation/options/helm2/rke-add-on/layer-7-lb/nginx/","weight":277,"wordcount":141},{"authors":null,"categories":null,"content":"Important: RKE add-on install is only supported up to Rancher v2.0.8 Please use the Rancher Helm chart to install Rancher on a Kubernetes cluster. For details, see the Kubernetes Install - Installation Outline.\nIf you are currently using the RKE add-on install method, see Migrating from a Kubernetes Install with an RKE Add-on for details on how to move to using the helm chart.\n Install NGINX Start by installing NGINX on your load balancer host.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/options/rke-add-on/layer-7-lb/nginx/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"dd68e8efb439721e6d7aa12b65c8be11","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/rke-add-on/layer-7-lb/nginx/","postref":"dd68e8efb439721e6d7aa12b65c8be11","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rancher/v2.x/en/installation/options/rke-add-on/layer-7-lb/nginx/","section":"rancher","tags":null,"title":"NGINX Configuration","type":"rancher","url":"/docs/rancher/v2.x/en/installation/options/rke-add-on/layer-7-lb/nginx/","weight":277,"wordcount":141},{"authors":null,"categories":null,"content":"This how-to guide describes how to set up a load balancer in Amazon&rsquo;s EC2 service that will direct traffic to multiple instances on EC2.\n Note: Rancher only supports using the Amazon NLB when terminating traffic in tcp mode for port 443 rather than tls mode. This is due to the fact that the NLB does not inject the correct headers into requests when terminated at the NLB. This means that if you want to use certificates managed by the Amazon Certificate Manager (ACM), you should use an ELB or ALB.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/k8s-install/create-nodes-lb/nlb/","expirydate":-62135596800,"fuzzywordcount":900,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"ee18e38cf7221a6811e21390ea084a6c","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/k8s-install/create-nodes-lb/nlb/","postref":"ee18e38cf7221a6811e21390ea084a6c","publishdate":"0001-01-01T00:00:00Z","readingtime":5,"relpermalink":"/docs/rancher/v2.x/en/installation/k8s-install/create-nodes-lb/nlb/","section":"rancher","tags":null,"title":"Setting up an Amazon NLB Load Balancer","type":"rancher","url":"/docs/rancher/v2.x/en/installation/k8s-install/create-nodes-lb/nlb/","weight":277,"wordcount":855},{"authors":null,"categories":null,"content":"This section is about how to prepare to launch a Kubernetes cluster which is used to deploy Rancher server for your air gapped environment.\nSince a Kubernetes Installation requires a Kubernetes cluster, we will create a Kubernetes cluster using Rancher Kubernetes Engine (RKE). Before being able to start your Kubernetes cluster, you&rsquo;ll need to install RKE and create a RKE config file.\n A. Create an RKE Config File B.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/options/air-gap-helm2/launch-kubernetes/","expirydate":-62135596800,"fuzzywordcount":500,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"d23894624ad58c0e19b347b9bd3ae855","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/air-gap-helm2/launch-kubernetes/","postref":"d23894624ad58c0e19b347b9bd3ae855","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/installation/options/air-gap-helm2/launch-kubernetes/","section":"rancher","tags":null,"title":"3. Install Kubernetes with RKE (Kubernetes Installs Only)","type":"rancher","url":"/docs/rancher/v2.x/en/installation/options/air-gap-helm2/launch-kubernetes/","weight":300,"wordcount":403},{"authors":null,"categories":null,"content":"This section is about how to prepare to launch a Kubernetes cluster which is used to deploy Rancher server for your air gapped environment.\nSince a Kubernetes Installation requires a Kubernetes cluster, we will create a Kubernetes cluster using Rancher Kubernetes Engine (RKE). Before being able to start your Kubernetes cluster, you&rsquo;ll need to install RKE and create a RKE config file.\n A. Create an RKE Config File B.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/other-installation-methods/air-gap/launch-kubernetes/","expirydate":-62135596800,"fuzzywordcount":500,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"276cd56d80ce159e4cb099f817bf64f4","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/other-installation-methods/air-gap/launch-kubernetes/","postref":"276cd56d80ce159e4cb099f817bf64f4","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/installation/other-installation-methods/air-gap/launch-kubernetes/","section":"rancher","tags":null,"title":"3. Install Kubernetes with RKE (Kubernetes Installs Only)","type":"rancher","url":"/docs/rancher/v2.x/en/installation/other-installation-methods/air-gap/launch-kubernetes/","weight":300,"wordcount":416},{"authors":null,"categories":null,"content":"Important: RKE add-on install is only supported up to Rancher v2.0.8\nPlease use the Rancher Helm chart to install Rancher on a Kubernetes cluster. For details, see the Kubernetes Install - Installation Outline.\nIf you are currently using the RKE add-on install method, see Migrating from a Kubernetes Install with an RKE Add-on for details on how to move to using the helm chart.\n If you&rsquo;re using RKE to install Rancher, you can use directives to enable API Auditing for your Rancher install.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/options/helm2/rke-add-on/api-auditing/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"289c53658fb5afbdaa137b6065d940ae","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/helm2/rke-add-on/api-auditing/","postref":"289c53658fb5afbdaa137b6065d940ae","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rancher/v2.x/en/installation/options/helm2/rke-add-on/api-auditing/","section":"rancher","tags":null,"title":"Enable API Auditing","type":"rancher","url":"/docs/rancher/v2.x/en/installation/options/helm2/rke-add-on/api-auditing/","weight":300,"wordcount":196},{"authors":null,"categories":null,"content":"Important: RKE add-on install is only supported up to Rancher v2.0.8\nPlease use the Rancher Helm chart to install Rancher on a Kubernetes cluster. For details, see the Kubernetes Install - Installation Outline.\nIf you are currently using the RKE add-on install method, see Migrating from a Kubernetes Install with an RKE Add-on for details on how to move to using the helm chart.\n If you&rsquo;re using RKE to install Rancher, you can use directives to enable API Auditing for your Rancher install.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/options/rke-add-on/api-auditing/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"ab2941001ee84ad020b1409df6d467c2","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/rke-add-on/api-auditing/","postref":"ab2941001ee84ad020b1409df6d467c2","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rancher/v2.x/en/installation/options/rke-add-on/api-auditing/","section":"rancher","tags":null,"title":"Enable API Auditing","type":"rancher","url":"/docs/rancher/v2.x/en/installation/options/rke-add-on/api-auditing/","weight":300,"wordcount":196},{"authors":null,"categories":null,"content":"There are lots of different configuration options that can be set in the cluster configuration file for RKE. Here are some examples of files:\n Note for Rancher 2 users If you are configuring Cluster Options using a Config File when creating Rancher Launched Kubernetes, the names of services should contain underscores only: kube_api and kube_controller. This only applies to Rancher v2.0.5 and v2.0.6.\n Minimal cluster.yml example nodes:-address:1.2.3.4user:ubunturole:-controlplane-etcd-worker Full cluster.","date":-62135596800,"description":"","dir":"rke/latest/en/example-yamls/","expirydate":-62135596800,"fuzzywordcount":700,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"ceb9a17bbb645fd3eee4969d249ce6d0","permalink":"http://jijeesh.github.io/docs/rke/latest/en/example-yamls/","postref":"ceb9a17bbb645fd3eee4969d249ce6d0","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/docs/rke/latest/en/example-yamls/","section":"rke","tags":null,"title":"Example Cluster.ymls","type":"rke","url":"/docs/rke/latest/en/example-yamls/","weight":300,"wordcount":693},{"authors":null,"categories":null,"content":"Howdy Partner! This tutorial walks you through:\n Installation of Rancher 2.x Creation of your first cluster Deployment of an application, Nginx  Quick Start Outline This Quick Start Guide is divided into different tasks for easier consumption.\n Provision a Linux Host\n Install Rancher\n Log In\n Create the Cluster\n  \n1. Provision a Linux Host Begin creation of a custom cluster by provisioning a Linux host.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/quick-start-guide/deployment/quickstart-manual-setup/","expirydate":-62135596800,"fuzzywordcount":600,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"7f019cf3c74ffec8f68c6b5525163125","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/quick-start-guide/deployment/quickstart-manual-setup/","postref":"7f019cf3c74ffec8f68c6b5525163125","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/rancher/v2.x/en/quick-start-guide/deployment/quickstart-manual-setup/","section":"rancher","tags":null,"title":"Manual Quick Start","type":"rancher","url":"/docs/rancher/v2.x/en/quick-start-guide/deployment/quickstart-manual-setup/","weight":300,"wordcount":533},{"authors":null,"categories":null,"content":"To operate properly, Rancher requires a number of ports to be open on Rancher nodes and on downstream Kubernetes cluster nodes.\nRancher Nodes The following table lists the ports that need to be open to and from nodes that are running the Rancher server container for Docker installs or pods for installing Rancher on Kubernetes.\n   Protocol Port Source Destination Description     TCP 80 Load Balancer / Reverse Proxy  HTTP traffic to Rancher UI / API.","date":-62135596800,"description":"Read about port requirements needed in order for Rancher to operate properly, both for Rancher nodes and downstream Kubernetes cluster nodes","dir":"rancher/v2.x/en/installation/requirements/ports/","expirydate":-62135596800,"fuzzywordcount":1400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"42a96c6651f7437b0c8d6064008ae277","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/requirements/ports/","postref":"42a96c6651f7437b0c8d6064008ae277","publishdate":"0001-01-01T00:00:00Z","readingtime":7,"relpermalink":"/docs/rancher/v2.x/en/installation/requirements/ports/","section":"rancher","tags":null,"title":"Port Requirements","type":"rancher","url":"/docs/rancher/v2.x/en/installation/requirements/ports/","weight":300,"wordcount":1302},{"authors":null,"categories":null,"content":"If your organization uses Splunk, you can configure Rancher to send it Kubernetes logs. Afterwards, you can log into your Splunk server to view logs.\n Prerequisites:\n Configure HTTP event collection for your Splunk Server (Splunk Enterprise or Splunk Cloud). Either create a new token or copy an existing token.  For more information, see Splunk Documentation.\n Splunk Configuration  In the Endpoint field, enter the IP address and port for you Splunk instance (i.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-admin/tools/logging/splunk/","expirydate":-62135596800,"fuzzywordcount":500,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"6145e5fc10edda4a06af568720fe59c2","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/tools/logging/splunk/","postref":"6145e5fc10edda4a06af568720fe59c2","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/cluster-admin/tools/logging/splunk/","section":"rancher","tags":null,"title":"Splunk","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-admin/tools/logging/splunk/","weight":300,"wordcount":420},{"authors":null,"categories":null,"content":"What is required to run RancherOS? RancherOS runs on any laptop, physical, or virtual servers.\nWhat are some commands?    Command Description     docker Good old Docker, use that to run stuff.   system-docker The Docker instance running the system containers. Must run as root or using sudo   ros Control and configure RancherOS    How can I extend my disk size in Amazon?","date":-62135596800,"description":"","dir":"os/v1.x/en/about/faqs/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"e6ac348bb1b3c8956e046be9060fd989","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/about/faqs/","postref":"e6ac348bb1b3c8956e046be9060fd989","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/os/v1.x/en/about/faqs/","section":"os","tags":null,"title":"FAQs","type":"os","url":"/docs/os/v1.x/en/about/faqs/","weight":301,"wordcount":130},{"authors":null,"categories":null,"content":"RancherOS can be used to launch Rancher and be used as the OS to add nodes to Rancher.\nLaunching Agents using Cloud-Config You can easily add hosts into Rancher by using cloud-config to launch the rancher/agent container.\nAfter Rancher is launched and host registration has been saved, you will be able to find use the custom option to add Rancher OS nodes.\n$ sudo docker run --d --privileged -v /var/run/docker.","date":-62135596800,"description":"","dir":"os/v1.x/en/about/running-rancher-on-rancherOS/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"8b5fbdbebf8c3a8db01695948a3e80b2","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/about/running-rancher-on-rancheros/","postref":"8b5fbdbebf8c3a8db01695948a3e80b2","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/os/v1.x/en/about/running-rancher-on-rancheros/","section":"os","tags":null,"title":"Tips on using Rancher v1.x with RancherOS","type":"os","url":"/docs/os/v1.x/en/about/running-rancher-on-rancheros/","weight":302,"wordcount":378},{"authors":null,"categories":null,"content":"Security policy Rancher Labs supports responsible disclosure, and endeavours to resolve all issues in a reasonable time frame. RancherOS is a minimal Linux distribution, built with entirely using open source components.\n Reporting process Please submit possible security issues by emailing security@rancher.com\n Announcements Subscribe to the Rancher announcements forum for release updates.\n   RancherOS Vulnerabilities    ID Description Date Resolution     CVE-2017-6074 Local privilege-escalation using a user after free issue in Datagram Congestion Control Protocol (DCCP).","date":-62135596800,"description":"","dir":"os/v1.x/en/about/security/","expirydate":-62135596800,"fuzzywordcount":800,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"be3e4a807889015a7347f145ce7320cd","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/about/security/","postref":"be3e4a807889015a7347f145ce7320cd","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/docs/os/v1.x/en/about/security/","section":"os","tags":null,"title":"RancherOS Security","type":"os","url":"/docs/os/v1.x/en/about/security/","weight":303,"wordcount":774},{"authors":null,"categories":null,"content":"Test Environment In order to demonstrate how to use the recovery console, we choose a scene that the disk space is full and the OS cannot boot.\n   Term Definition     RancherOS v1.4.0   Platform Virtualbox   Root Disk 2GB   CPU 1C   MEM 2GB    Fill up the disk Start this VM to check disk usage:","date":-62135596800,"description":"","dir":"os/v1.x/en/about/recovery-console/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"de51c046d8e955573db89872245c7792","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/about/recovery-console/","postref":"de51c046d8e955573db89872245c7792","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/os/v1.x/en/about/recovery-console/","section":"os","tags":null,"title":"How to use recovery console","type":"os","url":"/docs/os/v1.x/en/about/recovery-console/","weight":304,"wordcount":357},{"authors":null,"categories":null,"content":"When users use the default ros install, ROS will automatically create one partition on the root disk. It will be the only partition with the label RANCHER_STATE. But sometimes users want to be able to customize the root disk partition to isolate the data.\n The following defaults to MBR mode, GPT mode has not been tested.\n Use RANCHER_STATE partition As mentioned above, the default mode is that ROS will automatically create one partition with the label RANCHER_STATE.","date":-62135596800,"description":"","dir":"os/v1.x/en/about/custom-partition-layout/","expirydate":-62135596800,"fuzzywordcount":700,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"15192c1c39666c179152d124ebd7e6f2","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/about/custom-partition-layout/","postref":"15192c1c39666c179152d124ebd7e6f2","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/docs/os/v1.x/en/about/custom-partition-layout/","section":"os","tags":null,"title":"How to custom partition layout","type":"os","url":"/docs/os/v1.x/en/about/custom-partition-layout/","weight":305,"wordcount":689},{"authors":null,"categories":null,"content":"Processor manufacturers release stability and security updates to the processor microcode. While microcode can be updated through the BIOS, the Linux kernel is also able to apply these updates. These updates provide bug fixes that can be critical to the stability of your system. Without these updates, you may experience spurious crashes or unexpected system halts that can be difficult to track down.\nThe microcode loader supports three loading methods:","date":-62135596800,"description":"","dir":"os/v1.x/en/about/microcode-loader/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"e20ed9018990893810139113595365e3","permalink":"http://jijeesh.github.io/docs/os/v1.x/en/about/microcode-loader/","postref":"e20ed9018990893810139113595365e3","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/os/v1.x/en/about/microcode-loader/","section":"os","tags":null,"title":"How to update microcode","type":"os","url":"/docs/os/v1.x/en/about/microcode-loader/","weight":306,"wordcount":197},{"authors":null,"categories":null,"content":"If you encounter a disaster scenario, you can restore your Rancher Server to your most recent backup.\nBefore You Start During restoration of your backup, you&rsquo;ll enter a series of commands, filling placeholders with data from your environment. These placeholders are denoted with angled brackets and all capital letters (&lt;EXAMPLE&gt;). Here&rsquo;s an example of a command with a placeholder:\ndocker run --volumes-from &lt;RANCHER_CONTAINER_NAME&gt; -v $PWD:/backup \\ busybox sh -c &quot;rm /var/lib/rancher/* -rf &amp;&amp; \\ tar pzxvf /backup/rancher-data-backup-&lt;RANCHER_VERSION&gt;-&lt;DATE&gt;&quot;  In this command, &lt;RANCHER_CONTAINER_NAME&gt; and &lt;RANCHER_VERSION&gt;-&lt;DATE&gt; are environment variables for your Rancher deployment.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/backups/restorations/single-node-restoration/","expirydate":-62135596800,"fuzzywordcount":500,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"9f128b71259f20cf8600a79d9e13c879","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/backups/restorations/single-node-restoration/","postref":"9f128b71259f20cf8600a79d9e13c879","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/rancher/v2.x/en/backups/restorations/single-node-restoration/","section":"rancher","tags":null,"title":"Restoring Backups—Docker Installs","type":"rancher","url":"/docs/rancher/v2.x/en/backups/restorations/single-node-restoration/","weight":365,"wordcount":443},{"authors":null,"categories":null,"content":"This procedure describes how to use RKE to restore a snapshot of the Rancher Kubernetes cluster. The cluster snapshot will include Kubernetes configuration and the Rancher database and state.\nAdditionally, the pki.bundle.tar.gz file usage is no longer required as v0.2.0 has changed how the Kubernetes cluster state is stored.\nRestore Outline  1. Preparation 2. Place Snapshot 3. Configure RKE 4. Restore Database 5. Bring Up the Cluster  1.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/backups/restorations/ha-restoration/","expirydate":-62135596800,"fuzzywordcount":1300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"98460da2fc36dd8e0f557e32dfb329dd","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/backups/restorations/ha-restoration/","postref":"98460da2fc36dd8e0f557e32dfb329dd","publishdate":"0001-01-01T00:00:00Z","readingtime":6,"relpermalink":"/docs/rancher/v2.x/en/backups/restorations/ha-restoration/","section":"rancher","tags":null,"title":"Restoring Backups—Kubernetes installs","type":"rancher","url":"/docs/rancher/v2.x/en/backups/restorations/ha-restoration/","weight":370,"wordcount":1210},{"authors":null,"categories":null,"content":"Important: RKE add-on install is only supported up to Rancher v2.0.8 Please use the Rancher Helm chart to install Rancher on a Kubernetes cluster. For details, see the Kubernetes Install - Installation Outline.\nIf you are currently using the RKE add-on install method, see Migrating from a Kubernetes Install with an RKE Add-on for details on how to move to using the helm chart.\n This section contains common errors seen when setting up a Kubernetes installation.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/options/helm2/rke-add-on/troubleshooting/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"df5f458943e135b0bb127f66c1af36b4","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/helm2/rke-add-on/troubleshooting/","postref":"df5f458943e135b0bb127f66c1af36b4","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rancher/v2.x/en/installation/options/helm2/rke-add-on/troubleshooting/","section":"rancher","tags":null,"title":"Troubleshooting HA RKE Add-On Install","type":"rancher","url":"/docs/rancher/v2.x/en/installation/options/helm2/rke-add-on/troubleshooting/","weight":370,"wordcount":169},{"authors":null,"categories":null,"content":"Important: RKE add-on install is only supported up to Rancher v2.0.8 Please use the Rancher Helm chart to install Rancher on a Kubernetes cluster. For details, see the Kubernetes Install - Installation Outline.\nIf you are currently using the RKE add-on install method, see Migrating from a Kubernetes Install with an RKE Add-on for details on how to move to using the helm chart.\n This section contains common errors seen when setting up a Kubernetes installation.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/options/rke-add-on/troubleshooting/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"e6d4fbe86168543b4dcf200f60e50c9c","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/rke-add-on/troubleshooting/","postref":"e6d4fbe86168543b4dcf200f60e50c9c","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rancher/v2.x/en/installation/options/rke-add-on/troubleshooting/","section":"rancher","tags":null,"title":"Troubleshooting HA RKE Add-On Install","type":"rancher","url":"/docs/rancher/v2.x/en/installation/options/rke-add-on/troubleshooting/","weight":370,"wordcount":169},{"authors":null,"categories":null,"content":"In testing environments, you usually need to route external traffic to your cluster containers by using an unadvertised IP and port number, providing users access to their apps. You can accomplish this goal using port mapping, which exposes a workload (i.e., service) publicly over a specific port, provided you know your node IP address(es). You can either map a port using HostPorts (which exposes a service on a specified port on a single node) or NodePorts (which exposes a service on all nodes on a single port).","date":-62135596800,"description":"","dir":"rancher/v2.x/en/v1.6-migration/expose-services/","expirydate":-62135596800,"fuzzywordcount":1300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"22b216173de8407705833831ca3b76af","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/v1.6-migration/expose-services/","postref":"22b216173de8407705833831ca3b76af","publishdate":"0001-01-01T00:00:00Z","readingtime":6,"relpermalink":"/docs/rancher/v2.x/en/v1.6-migration/expose-services/","section":"rancher","tags":null,"title":"3. Expose Your Services","type":"rancher","url":"/docs/rancher/v2.x/en/v1.6-migration/expose-services/","weight":400,"wordcount":1245},{"authors":null,"categories":null,"content":"Rancher v1.6 provided TCP and HTTP health checks on your nodes and services using its own health check microservice. These health checks monitored your containers to confirm they&rsquo;re operating as intended. If a container failed a health check, Rancher would destroy the unhealthy container and then replicates a healthy one to replace it.\nFor Rancher v2.x, we&rsquo;ve replaced the health check microservice, leveraging instead Kubernetes&rsquo; native health check support.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/v1.6-migration/monitor-apps/","expirydate":-62135596800,"fuzzywordcount":1300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"0ee82650003e83527903d101a489ea9d","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/v1.6-migration/monitor-apps/","postref":"0ee82650003e83527903d101a489ea9d","publishdate":"0001-01-01T00:00:00Z","readingtime":6,"relpermalink":"/docs/rancher/v2.x/en/v1.6-migration/monitor-apps/","section":"rancher","tags":null,"title":"4. Configure Health Checks","type":"rancher","url":"/docs/rancher/v2.x/en/v1.6-migration/monitor-apps/","weight":400,"wordcount":1217},{"authors":null,"categories":null,"content":"This section is about how to deploy Rancher for your air gapped environment. An air gapped environment could be where Rancher server will be installed offline, behind a firewall, or behind a proxy. There are tabs for either a high availability (recommended) or a Docker installation.\n Rancher recommends installing Rancher on a Kubernetes cluster. A highly available Kubernetes Installation is comprised of three nodes running the Rancher server components on a Kubernetes cluster.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/options/air-gap-helm2/install-rancher/","expirydate":-62135596800,"fuzzywordcount":2300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"55260b33d4d486c72b0cad8ed156f54a","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/air-gap-helm2/install-rancher/","postref":"55260b33d4d486c72b0cad8ed156f54a","publishdate":"0001-01-01T00:00:00Z","readingtime":11,"relpermalink":"/docs/rancher/v2.x/en/installation/options/air-gap-helm2/install-rancher/","section":"rancher","tags":null,"title":"4. Install Rancher","type":"rancher","url":"/docs/rancher/v2.x/en/installation/options/air-gap-helm2/install-rancher/","weight":400,"wordcount":2299},{"authors":null,"categories":null,"content":"This section is about how to deploy Rancher for your air gapped environment. An air gapped environment could be where Rancher server will be installed offline, behind a firewall, or behind a proxy. There are tabs for either a high availability (recommended) or a Docker installation.\n Note: These installation instructions assume you are using Helm 3. For migration of installs started with Helm 2, refer to the official Helm 2 to 3 migration docs.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/other-installation-methods/air-gap/install-rancher/","expirydate":-62135596800,"fuzzywordcount":2400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"70099a6420b2d7c36db7fce3101bba24","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/other-installation-methods/air-gap/install-rancher/","postref":"70099a6420b2d7c36db7fce3101bba24","publishdate":"0001-01-01T00:00:00Z","readingtime":12,"relpermalink":"/docs/rancher/v2.x/en/installation/other-installation-methods/air-gap/install-rancher/","section":"rancher","tags":null,"title":"4. Install Rancher","type":"rancher","url":"/docs/rancher/v2.x/en/installation/other-installation-methods/air-gap/install-rancher/","weight":400,"wordcount":2359},{"authors":null,"categories":null,"content":"You might want to use a private Docker registry to share your custom base images within your organization. With a private registry, you can keep a private, consistent, and centralized source of truth for the Docker images that are used in your clusters.\nThere are two main ways to set up private registries in Rancher: by setting up the global default registry through the Settings tab in the global view, and by setting up a private registry in the advanced options in the cluster-level settings.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/admin-settings/config-private-registry/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"fdd2c65f8dcfbcff5f79a9c72a5add96","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/config-private-registry/","postref":"fdd2c65f8dcfbcff5f79a9c72a5add96","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/admin-settings/config-private-registry/","section":"rancher","tags":null,"title":"Configuring a Global Default Private Registry","type":"rancher","url":"/docs/rancher/v2.x/en/admin-settings/config-private-registry/","weight":400,"wordcount":386},{"authors":null,"categories":null,"content":"This section contains the requirements for Helm, which is the tool used to install Rancher on a high-availability Kubernetes cluster.\n The installation instructions have been updated for Helm 3. For migration of installs started with Helm 2, refer to the official Helm 2 to 3 Migration Docs. This section provides a copy of the older high-availability Rancher installation instructions that used Helm 2, and it is intended to be used if upgrading to Helm 3 is not feasible.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/options/helm-version/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"b37b4c128ccdbdf977e9948410d588ab","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/helm-version/","postref":"b37b4c128ccdbdf977e9948410d588ab","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rancher/v2.x/en/installation/options/helm-version/","section":"rancher","tags":null,"title":"Helm Version Requirements","type":"rancher","url":"/docs/rancher/v2.x/en/installation/options/helm-version/","weight":400,"wordcount":131},{"authors":null,"categories":null,"content":"If your organization uses Kafka, you can configure Rancher to send it Kubernetes logs. Afterwards, you can log into your Kafka server to view logs.\n Prerequisite: You must have a Kafka server configured.\n Kafka Server Configuration  Select the type of Endpoint your Kafka server is using:\n Zookeeper: Enter the IP address and port. By default, Zookeeper uses port 2181. Please note that a Zookeeper endpoint cannot enable TLS.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-admin/tools/logging/kafka/","expirydate":-62135596800,"fuzzywordcount":300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"6e9f63e315a7c716ecac1f22e29d02dd","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/tools/logging/kafka/","postref":"6e9f63e315a7c716ecac1f22e29d02dd","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/cluster-admin/tools/logging/kafka/","section":"rancher","tags":null,"title":"Kafka","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-admin/tools/logging/kafka/","weight":400,"wordcount":259},{"authors":null,"categories":null,"content":" SSH Connectivity Errors Provisioning Errors  ","date":-62135596800,"description":"","dir":"rke/latest/en/troubleshooting/","expirydate":-62135596800,"fuzzywordcount":100,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"7b6281a9471118849160e71d1e1d98b9","permalink":"http://jijeesh.github.io/docs/rke/latest/en/troubleshooting/","postref":"7b6281a9471118849160e71d1e1d98b9","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rke/latest/en/troubleshooting/","section":"rke","tags":null,"title":"Troubleshooting","type":"rke","url":"/docs/rke/latest/en/troubleshooting/","weight":400,"wordcount":5},{"authors":null,"categories":null,"content":"In v1.6, objects called services were used to schedule containers to your cluster hosts. Services included the Docker image for an application, along with configuration settings for a desired state.\nIn Rancher v2.x, the equivalent object is known as a workload. Rancher v2.x retains all scheduling functionality from v1.6, but because of the change from Cattle to Kubernetes as the default container orchestrator, the terminology and mechanisms for scheduling workloads has changed.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/v1.6-migration/schedule-workloads/","expirydate":-62135596800,"fuzzywordcount":1700,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"8bcaaca2c539fd4b40a12b81e3bfe8b0","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/v1.6-migration/schedule-workloads/","postref":"8bcaaca2c539fd4b40a12b81e3bfe8b0","publishdate":"0001-01-01T00:00:00Z","readingtime":8,"relpermalink":"/docs/rancher/v2.x/en/v1.6-migration/schedule-workloads/","section":"rancher","tags":null,"title":"5. Schedule Your Services","type":"rancher","url":"/docs/rancher/v2.x/en/v1.6-migration/schedule-workloads/","weight":500,"wordcount":1670},{"authors":null,"categories":null,"content":"Rancher ships with several example repositories that you can use to familiarize yourself with pipelines. We recommend configuring and testing the example repository that most resembles your environment before using pipelines with your own repositories in a production environment. Use this example repository as a sandbox for repo configuration, build demonstration, etc. Rancher includes example repositories for:\n Go Maven php   Note: The example repositories are only available if you have not configured a version control provider.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/k8s-in-rancher/pipelines/example-repos/","expirydate":-62135596800,"fuzzywordcount":500,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"de89ada5135eee597873ad47b7b9b10b","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/k8s-in-rancher/pipelines/example-repos/","postref":"de89ada5135eee597873ad47b7b9b10b","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/k8s-in-rancher/pipelines/example-repos/","section":"rancher","tags":null,"title":"Example Repositories","type":"rancher","url":"/docs/rancher/v2.x/en/k8s-in-rancher/pipelines/example-repos/","weight":500,"wordcount":401},{"authors":null,"categories":null,"content":"If your organization uses Syslog, you can configure Rancher to send it Kubernetes logs. Afterwards, you can log into your Syslog server to view logs.\n Prerequisite: You must have a Syslog server configured.\n If you are using rsyslog, please make sure your rsyslog authentication mode is x509/name.\nSyslog Server Configuration  In the Endpoint field, enter the IP address and port for your Syslog server. Additionally, in the dropdown, select the protocol that your Syslog server uses.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-admin/tools/logging/syslog/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"273160a7df2efcc2f2e062a5b4c96515","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/tools/logging/syslog/","postref":"273160a7df2efcc2f2e062a5b4c96515","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/cluster-admin/tools/logging/syslog/","section":"rancher","tags":null,"title":"Syslog","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-admin/tools/logging/syslog/","weight":500,"wordcount":343},{"authors":null,"categories":null,"content":"Pipelines can be configured either through the UI or using a yaml file in the repository, i.e. .rancher-pipeline.yml or .rancher-pipeline.yaml.\nIn the [pipeline configuration docs](), we provide examples of each available feature within pipelines. Here is a full example for those who want to jump right in.\n# examplestages:-name:Buildsomething# Conditions for stageswhen:branch:masterevent:[push,pull_request]# Multiple steps run concurrentlysteps:-runScriptConfig:image:busyboxshellScript:echo${FIRST_KEY}&amp;&amp;echo${ALIAS_ENV}# Set environment variables in container for the stepenv:FIRST_KEY:VALUESECOND_KEY:VALUE2# Set environment variables from project secretsenvFrom:-sourceName:my-secretsourceKey:secret-keytargetKey:ALIAS_ENV-runScriptConfig:image:busyboxshellScript:date-R# Conditions for stepswhen:branch:[master,dev]event:push-name:Publishmyimagesteps:-publishImageConfig:dockerfilePath:.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/k8s-in-rancher/pipelines/example/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"a2d0c0e0ced3f1e3131b15c4bffe55cb","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/k8s-in-rancher/pipelines/example/","postref":"a2d0c0e0ced3f1e3131b15c4bffe55cb","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rancher/v2.x/en/k8s-in-rancher/pipelines/example/","section":"rancher","tags":null,"title":"Example YAML File","type":"rancher","url":"/docs/rancher/v2.x/en/k8s-in-rancher/pipelines/example/","weight":501,"wordcount":108},{"authors":null,"categories":null,"content":"Service discovery is one of the core functionalities of any container-based environment. Once you have packaged and launched your application, the next step is making it discoverable to other containers in your environment or the external world. This document will describe how to use the service discovery support provided by Rancher v2.x so that you can find them by name.\nThis document will also show you how to link the workloads and services that you migrated into Rancher v2.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/v1.6-migration/discover-services/","expirydate":-62135596800,"fuzzywordcount":800,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"91fb2df0238f1952e330addb7cf34703","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/v1.6-migration/discover-services/","postref":"91fb2df0238f1952e330addb7cf34703","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/docs/rancher/v2.x/en/v1.6-migration/discover-services/","section":"rancher","tags":null,"title":"6. Service Discovery","type":"rancher","url":"/docs/rancher/v2.x/en/v1.6-migration/discover-services/","weight":600,"wordcount":739},{"authors":null,"categories":null,"content":"If your organization uses Fluentd, you can configure Rancher to send it Kubernetes logs. Afterwards, you can log into your Fluentd server to view logs.\n Prerequisites: Configure Fluentd input forward to receive the event stream.\nSee Fluentd Documentation for details.\n Fluentd Configuration You can add multiple Fluentd Servers. If you want to add additional Fluentd servers, click Add Fluentd Server. For each Fluentd server, complete the configuration information:","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-admin/tools/logging/fluentd/","expirydate":-62135596800,"fuzzywordcount":300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"255e941097968c90d9b11aa8550caeb2","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/tools/logging/fluentd/","postref":"255e941097968c90d9b11aa8550caeb2","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/cluster-admin/tools/logging/fluentd/","section":"rancher","tags":null,"title":"Fluentd","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-admin/tools/logging/fluentd/","weight":600,"wordcount":272},{"authors":null,"categories":null,"content":"If your applications are public-facing and consume significant traffic, you should place a load balancer in front of your cluster so that users can always access their apps without service interruption. Typically, you can fulfill a high volume of service requests by horizontally scaling your deployment, which spins up additional application containers as traffic ramps up. However, this technique requires routing that distributes traffic across your nodes efficiently. In cases where you need to accommodate public traffic that scales up and down, you&rsquo;ll need a load balancer.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/v1.6-migration/load-balancing/","expirydate":-62135596800,"fuzzywordcount":1800,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"e4264cec5feef8d807f82ddb6691a6cf","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/v1.6-migration/load-balancing/","postref":"e4264cec5feef8d807f82ddb6691a6cf","publishdate":"0001-01-01T00:00:00Z","readingtime":9,"relpermalink":"/docs/rancher/v2.x/en/v1.6-migration/load-balancing/","section":"rancher","tags":null,"title":"7. Load Balancing","type":"rancher","url":"/docs/rancher/v2.x/en/v1.6-migration/load-balancing/","weight":700,"wordcount":1781},{"authors":null,"categories":null,"content":"This section is devoted to protecting your data in a disaster scenario.\nTo protect yourself from a disaster scenario, you should create backups on a regular basis.\n Rancher Server Backups Backing up Rancher Launched Kubernetes Clusters  In a disaster scenario, you can restore your etcd database by restoring a backup.\n Rancher Server Restorations Restoring Rancher Launched Kubernetes Clusters  ","date":-62135596800,"description":"","dir":"rancher/v2.x/en/backups/","expirydate":-62135596800,"fuzzywordcount":100,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"5f4d4640868252e2bba272f1c3bd93c0","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/backups/","postref":"5f4d4640868252e2bba272f1c3bd93c0","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rancher/v2.x/en/backups/","section":"rancher","tags":null,"title":"Backups and Disaster Recovery","type":"rancher","url":"/docs/rancher/v2.x/en/backups/","weight":1000,"wordcount":58},{"authors":null,"categories":null,"content":"The purpose of this section is to consolidate best practices for Rancher implementations. This also includes recommendations for related technologies, such as Kubernetes, Docker, containers, and more. The objective is to improve the outcome of a Rancher implementation using the operational experience of Rancher and its customers.\nIf you have any questions about how these might apply to your use case, please contact your Customer Success Manager or Support.\nUse the navigation bar on the left to find the current best practices for managing and deploying the Rancher Server.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/best-practices/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"8049ecd06408c66f06905e6c48f9687d","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/best-practices/","postref":"8049ecd06408c66f06905e6c48f9687d","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rancher/v2.x/en/best-practices/","section":"rancher","tags":null,"title":"Best Practices Guide","type":"rancher","url":"/docs/rancher/v2.x/en/best-practices/","weight":1000,"wordcount":141},{"authors":null,"categories":null,"content":"This section describes how to configure custom Windows clusters that are using Host Gateway (L2bridge) mode.\nDisabling Private IP Address Checks If you are using Host Gateway (L2bridge) mode and hosting your nodes on any of the cloud services listed below, you must disable the private IP address checks for both your Linux or Windows hosts on startup. To disable this check for each node, follow the directions provided by each service below.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-provisioning/rke-clusters/windows-clusters/host-gateway-requirements/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"196b8e0a909e9cfac3e52c2587c479d7","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/windows-clusters/host-gateway-requirements/","postref":"196b8e0a909e9cfac3e52c2587c479d7","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/windows-clusters/host-gateway-requirements/","section":"rancher","tags":null,"title":"Networking Requirements for Host Gateway (L2bridge)","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/windows-clusters/host-gateway-requirements/","weight":1000,"wordcount":322},{"authors":null,"categories":null,"content":"This section contains information about how to upgrade your Rancher server to a newer version. Regardless if you installed in an air gap environment or not, the upgrade steps mainly depend on whether you have a single node or high-availability installation of Rancher. Select from the following options:\n Upgrading Rancher installed with Docker Upgrading Rancher installed on a Kubernetes cluster  Known Upgrade Issues The following table lists some of the most noteworthy issues to be considered when upgrading Rancher.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/upgrades/upgrades/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"9d31801c2be2ef4687fa46600bb56496","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/upgrades/upgrades/","postref":"9d31801c2be2ef4687fa46600bb56496","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/upgrades/upgrades/","section":"rancher","tags":null,"title":"Upgrades","type":"rancher","url":"/docs/rancher/v2.x/en/upgrades/upgrades/","weight":1005,"wordcount":352},{"authors":null,"categories":null,"content":"If you lose the data on your Rancher Server, you can restore it if you have backups stored in a safe location.\n Restoring Backups—Docker Installs Restoring Backups—Kubernetes installs  If you are looking to restore your Rancher launched Kubernetes cluster, please refer here.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/backups/restorations/","expirydate":-62135596800,"fuzzywordcount":100,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"b0e1149866492404f66ef2849f67caf1","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/backups/restorations/","postref":"b0e1149866492404f66ef2849f67caf1","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rancher/v2.x/en/backups/restorations/","section":"rancher","tags":null,"title":"Restorations","type":"rancher","url":"/docs/rancher/v2.x/en/backups/restorations/","weight":1010,"wordcount":42},{"authors":null,"categories":null,"content":"This section contains information about how to rollback your Rancher server to a previous version.\n Rolling back Rancher installed with Docker Rolling back Rancher installed on a Kubernetes cluster  Special Scenarios regarding Rollbacks If you are rolling back to versions in either of these scenarios, you must follow some extra instructions in order to get your clusters working.\n Rolling back from v2.1.6+ to any version between v2.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/upgrades/rollbacks/","expirydate":-62135596800,"fuzzywordcount":600,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"8dfe83379f313fdd258f038448a0c777","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/upgrades/rollbacks/","postref":"8dfe83379f313fdd258f038448a0c777","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/rancher/v2.x/en/upgrades/rollbacks/","section":"rancher","tags":null,"title":"Rollbacks","type":"rancher","url":"/docs/rancher/v2.x/en/upgrades/rollbacks/","weight":1010,"wordcount":522},{"authors":null,"categories":null,"content":"The following instructions will guide you through upgrading a Rancher server that was installed with Docker.\nPrerequisites  Review the known upgrade issues and caveats in the Rancher documentation for the most noteworthy issues to consider when upgrading Rancher. A more complete list of known issues for each Rancher version can be found in the release notes on GitHub and on the Rancher forums. For air gap installs only, collect and populate images for the new Rancher server version.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/upgrades/upgrades/single-node/","expirydate":-62135596800,"fuzzywordcount":2100,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"68f02be98e0bb3a552577f1f3e2b35e7","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/upgrades/upgrades/single-node/","postref":"68f02be98e0bb3a552577f1f3e2b35e7","publishdate":"0001-01-01T00:00:00Z","readingtime":10,"relpermalink":"/docs/rancher/v2.x/en/upgrades/upgrades/single-node/","section":"rancher","tags":null,"title":"Upgrading Rancher Installed with Docker","type":"rancher","url":"/docs/rancher/v2.x/en/upgrades/upgrades/single-node/","weight":1010,"wordcount":2048},{"authors":null,"categories":null,"content":"If a Rancher upgrade does not complete successfully, you&rsquo;ll have to roll back to your Rancher setup that you were using before Docker Upgrade. Rolling back restores:\n Your previous version of Rancher. Your data backup created before upgrade.  Before You Start During rollback to a prior version of Rancher, you&rsquo;ll enter a series of commands, filling placeholders with data from your environment. These placeholders are denoted with angled brackets and all capital letters (&lt;EXAMPLE&gt;).","date":-62135596800,"description":"","dir":"rancher/v2.x/en/upgrades/rollbacks/single-node-rollbacks/","expirydate":-62135596800,"fuzzywordcount":600,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"f6e581543324a640d67a3679576b65b6","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/upgrades/rollbacks/single-node-rollbacks/","postref":"f6e581543324a640d67a3679576b65b6","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/rancher/v2.x/en/upgrades/rollbacks/single-node-rollbacks/","section":"rancher","tags":null,"title":"Docker Rollback","type":"rancher","url":"/docs/rancher/v2.x/en/upgrades/rollbacks/single-node-rollbacks/","weight":1015,"wordcount":565},{"authors":null,"categories":null,"content":"The following instructions will guide you through using Helm to upgrade a Rancher server that was installed on a Kubernetes cluster.\nTo upgrade the components in your Kubernetes cluster, or the definition of the Kubernetes services or add-ons, refer to the upgrade documentation for RKE, the Rancher Kubernetes Engine.\nIf you installed Rancher using the RKE Add-on yaml, follow the directions to migrate or upgrade.\n Notes:\n Let&rsquo;s Encrypt will be blocking cert-manager instances older than 0.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/upgrades/upgrades/ha/","expirydate":-62135596800,"fuzzywordcount":1100,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"f995ea6a52e74be6300dbf4f655f7bb7","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/upgrades/upgrades/ha/","postref":"f995ea6a52e74be6300dbf4f655f7bb7","publishdate":"0001-01-01T00:00:00Z","readingtime":5,"relpermalink":"/docs/rancher/v2.x/en/upgrades/upgrades/ha/","section":"rancher","tags":null,"title":"Upgrading Rancher Installed on Kubernetes","type":"rancher","url":"/docs/rancher/v2.x/en/upgrades/upgrades/ha/","weight":1020,"wordcount":1052},{"authors":null,"categories":null,"content":"If you upgrade Rancher and the upgrade does not complete successfully, you may need to rollback your Rancher Server to its last healthy state.\nTo restore Rancher follow the procedure detailed here: Restoring Backups — Kubernetes installs\nRestoring a snapshot of the Rancher Server cluster will revert Rancher to the version and state at the time of the snapshot.\n Note: Managed cluster are authoritative for their state. This means restoring the rancher server will not revert workload deployments or changes made on managed clusters after the snapshot was taken.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/upgrades/rollbacks/ha-server-rollbacks/","expirydate":-62135596800,"fuzzywordcount":100,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"e4e0952338761340ba4b912148fbadb0","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/upgrades/rollbacks/ha-server-rollbacks/","postref":"e4e0952338761340ba4b912148fbadb0","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rancher/v2.x/en/upgrades/rollbacks/ha-server-rollbacks/","section":"rancher","tags":null,"title":"Kubernetes Rollback","type":"rancher","url":"/docs/rancher/v2.x/en/upgrades/rollbacks/ha-server-rollbacks/","weight":1025,"wordcount":89},{"authors":null,"categories":null,"content":"Important: RKE add-on install is only supported up to Rancher v2.0.8\nIf you are currently using the RKE add-on install method, please follow these directions to migrate to the Helm install.\n The following instructions will help guide you through migrating from the RKE Add-on install to managing Rancher with the Helm package manager.\nYou will need the to have kubectl installed and the kubeconfig YAML file (kube_config_rancher-cluster.yml) generated by RKE.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/upgrades/upgrades/migrating-from-rke-add-on/","expirydate":-62135596800,"fuzzywordcount":600,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"336afb6aab3ebe4749390ebe8a53b98e","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/upgrades/upgrades/migrating-from-rke-add-on/","postref":"336afb6aab3ebe4749390ebe8a53b98e","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/rancher/v2.x/en/upgrades/upgrades/migrating-from-rke-add-on/","section":"rancher","tags":null,"title":"Migrating from a Kubernetes Install with an RKE Add-on","type":"rancher","url":"/docs/rancher/v2.x/en/upgrades/upgrades/migrating-from-rke-add-on/","weight":1030,"wordcount":557},{"authors":null,"categories":null,"content":"This section applies only to Rancher upgrades from v2.0.6 or earlier to v2.0.7 or later. Upgrades from v2.0.7 to later version are unaffected.\n In Rancher v2.0.6 and prior, system namespaces crucial for Rancher and Kubernetes operations were not assigned to any Rancher project by default. Instead, these namespaces existed independently from all Rancher projects, but you could move these namespaces into any project without affecting cluster operations.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/upgrades/upgrades/namespace-migration/","expirydate":-62135596800,"fuzzywordcount":1100,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"a7f94487424da2841a4ccc9f2beb2d78","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/upgrades/upgrades/namespace-migration/","postref":"a7f94487424da2841a4ccc9f2beb2d78","publishdate":"0001-01-01T00:00:00Z","readingtime":5,"relpermalink":"/docs/rancher/v2.x/en/upgrades/upgrades/namespace-migration/","section":"rancher","tags":null,"title":"Upgrading to v2.0.7+ — Namespace Migration","type":"rancher","url":"/docs/rancher/v2.x/en/upgrades/upgrades/namespace-migration/","weight":1040,"wordcount":1051},{"authors":null,"categories":null,"content":"After Helm 3 was released, the instructions for upgrading Rancher on a Kubernetes cluster were updated to use Helm 3.\nIf you are using Helm 2, we recommend migrating to Helm 3 because it is simpler to use and more secure than Helm 2.\nThis section provides a copy of the older instructions for upgrading Rancher with Helm 2, and it is intended to be used if upgrading to Helm 3 is not feasible.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/upgrades/upgrades/ha/helm2/","expirydate":-62135596800,"fuzzywordcount":1200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"5c8efa6725c5486d35cbe4e4776fa8ad","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/upgrades/upgrades/ha/helm2/","postref":"5c8efa6725c5486d35cbe4e4776fa8ad","publishdate":"0001-01-01T00:00:00Z","readingtime":6,"relpermalink":"/docs/rancher/v2.x/en/upgrades/upgrades/ha/helm2/","section":"rancher","tags":null,"title":"Upgrading Rancher Installed on Kubernetes with Helm 2","type":"rancher","url":"/docs/rancher/v2.x/en/upgrades/upgrades/ha/helm2/","weight":1050,"wordcount":1128},{"authors":null,"categories":null,"content":"After installation, the system administrator should configure Rancher to configure authentication, authorization, security, default settings, security policies, drivers and global DNS entries.\nFirst Log In After you log into Rancher for the first time, Rancher will prompt you for a Rancher Server URL.You should set the URL to the main entry point to the Rancher Server. When a load balancer sits in front a Rancher Server cluster, the URL should resolve to the load balancer.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/admin-settings/","expirydate":-62135596800,"fuzzywordcount":600,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"04321c954193562fa57f2f36bdadebf4","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/","postref":"04321c954193562fa57f2f36bdadebf4","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/rancher/v2.x/en/admin-settings/","section":"rancher","tags":null,"title":"Authentication, Permissions and Global Configuration","type":"rancher","url":"/docs/rancher/v2.x/en/admin-settings/","weight":1100,"wordcount":566},{"authors":null,"categories":null,"content":"If you&rsquo;re using Rancher in an internal production environment where you aren&rsquo;t exposing apps publicly, use a certificate from a private certificate authority (CA).\nServices that Rancher needs to access are sometimes configured with a certificate from a custom/internal CA root, also known as self signed certificate. If the presented certificate from the service cannot be validated by Rancher, the following error displays: x509: certificate signed by unknown authority.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/options/custom-ca-root-certificate/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"8c5fcabef34b6608c72d1f5d78a5bccf","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/custom-ca-root-certificate/","postref":"8c5fcabef34b6608c72d1f5d78a5bccf","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rancher/v2.x/en/installation/options/custom-ca-root-certificate/","section":"rancher","tags":null,"title":"About Custom CA Root Certificates","type":"rancher","url":"/docs/rancher/v2.x/en/installation/options/custom-ca-root-certificate/","weight":1110,"wordcount":176},{"authors":null,"categories":null,"content":"Local authentication is the default until you configure an external authentication provider. Local authentication is where Rancher stores the user information, i.e. names and passwords, of who can log in to Rancher. By default, the admin user that logs in to Rancher for the first time is a local user.\nAdding Local Users Regardless of whether you use external authentication, you should create a few local authentication users so that you can continue using Rancher if your external authentication service encounters issues.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/admin-settings/authentication/local/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"a138bb05697b48edac94b2a37cf4113b","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/authentication/local/","postref":"a138bb05697b48edac94b2a37cf4113b","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rancher/v2.x/en/admin-settings/authentication/local/","section":"rancher","tags":null,"title":"Local Authentication","type":"rancher","url":"/docs/rancher/v2.x/en/admin-settings/authentication/local/","weight":1111,"wordcount":106},{"authors":null,"categories":null,"content":"If your organization uses Microsoft Active Directory as central user repository, you can configure Rancher to communicate with an Active Directory server to authenticate users. This allows Rancher admins to control access to clusters and projects based on users and groups managed externally in the Active Directory, while allowing end-users to authenticate with their AD credentials when logging in to the Rancher UI.\nRancher uses LDAP to communicate with the Active Directory server.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/admin-settings/authentication/ad/","expirydate":-62135596800,"fuzzywordcount":2100,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"69e6e26d042c2b353d976f05180cf832","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/authentication/ad/","postref":"69e6e26d042c2b353d976f05180cf832","publishdate":"0001-01-01T00:00:00Z","readingtime":10,"relpermalink":"/docs/rancher/v2.x/en/admin-settings/authentication/ad/","section":"rancher","tags":null,"title":"Configuring Active Directory (AD)","type":"rancher","url":"/docs/rancher/v2.x/en/admin-settings/authentication/ad/","weight":1112,"wordcount":2061},{"authors":null,"categories":null,"content":"Available as of v2.0.5\nIf your organization uses LDAP for user authentication, you can configure Rancher to communicate with an OpenLDAP server to authenticate users. This allows Rancher admins to control access to clusters and projects based on users and groups managed externally in the organisation&rsquo;s central user repository, while allowing end-users to authenticate with their LDAP credentials when logging in to the Rancher UI.\nOpenLDAP Authentication Flow  When a user attempts to login with his LDAP credentials, Rancher creates an initial bind to the LDAP server using a service account with permissions to search the directory and read user/group attributes.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/admin-settings/authentication/openldap/","expirydate":-62135596800,"fuzzywordcount":1400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"c4c87c575af3750bd0ec1b47d7bb8a8f","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/authentication/openldap/","postref":"c4c87c575af3750bd0ec1b47d7bb8a8f","publishdate":"0001-01-01T00:00:00Z","readingtime":7,"relpermalink":"/docs/rancher/v2.x/en/admin-settings/authentication/openldap/","section":"rancher","tags":null,"title":"Configuring OpenLDAP","type":"rancher","url":"/docs/rancher/v2.x/en/admin-settings/authentication/openldap/","weight":1113,"wordcount":1368},{"authors":null,"categories":null,"content":"Available as of v2.0.5\nIf your organization uses FreeIPA for user authentication, you can configure Rancher to allow your users to login using their FreeIPA credentials.\n Prerequisites:\n You must have a FreeIPA Server configured. Create a service account in FreeIPA with read-only access. Rancher uses this account to verify group membership when a user makes a request using an API key. Read External Authentication Configuration and Principal Users.    Sign into Rancher using a local user assigned the administrator role (i.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/admin-settings/authentication/freeipa/","expirydate":-62135596800,"fuzzywordcount":500,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"cde8b77eece27ba2388142ed94c9fcf0","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/authentication/freeipa/","postref":"cde8b77eece27ba2388142ed94c9fcf0","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/admin-settings/authentication/freeipa/","section":"rancher","tags":null,"title":"Configuring FreeIPA","type":"rancher","url":"/docs/rancher/v2.x/en/admin-settings/authentication/freeipa/","weight":1114,"wordcount":408},{"authors":null,"categories":null,"content":"One of the key features that Rancher adds to Kubernetes is centralized user authentication. This feature allows your users to use one set of credentials to authenticate with any of your Kubernetes clusters.\nThis centralized user authentication is accomplished using the Rancher authentication proxy, which is installed along with the rest of Rancher. This proxy authenticates your users and forwards their requests to your Kubernetes clusters using a service account.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/admin-settings/authentication/","expirydate":-62135596800,"fuzzywordcount":900,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"d323f945970e5776cce8b41b5bea4a89","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/authentication/","postref":"d323f945970e5776cce8b41b5bea4a89","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/docs/rancher/v2.x/en/admin-settings/authentication/","section":"rancher","tags":null,"title":"Authentication","type":"rancher","url":"/docs/rancher/v2.x/en/admin-settings/authentication/","weight":1115,"wordcount":807},{"authors":null,"categories":null,"content":"Available as of v2.0.3\nIf you have an instance of Active Directory (AD) hosted in Azure, you can configure Rancher to allow your users to log in using their AD accounts. Configuration of Azure AD external authentication requires you to make configurations in both Azure and Rancher.\n Note: Azure AD integration only supports Service Provider initiated logins.\nPrerequisite: Have an instance of Azure AD configured.\nNote: Most of this procedure takes place from the Microsoft Azure Portal.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/admin-settings/authentication/azure-ad/","expirydate":-62135596800,"fuzzywordcount":900,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"80b65438a32bcb52ba4c32d1ba717886","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/authentication/azure-ad/","postref":"80b65438a32bcb52ba4c32d1ba717886","publishdate":"0001-01-01T00:00:00Z","readingtime":5,"relpermalink":"/docs/rancher/v2.x/en/admin-settings/authentication/azure-ad/","section":"rancher","tags":null,"title":"Configuring Azure AD","type":"rancher","url":"/docs/rancher/v2.x/en/admin-settings/authentication/azure-ad/","weight":1115,"wordcount":896},{"authors":null,"categories":null,"content":"In environments using GitHub, you can configure Rancher to allow sign on using GitHub credentials.\n Prerequisites: Read External Authentication Configuration and Principal Users.\n  Sign into Rancher using a local user assigned the administrator role (i.e., the local principal).\n From the Global view, select Security &gt; Authentication from the main menu.\n Select GitHub.\n Follow the directions displayed to Setup a GitHub Application. Rancher redirects you to GitHub to complete registration.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/admin-settings/authentication/github/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"0209233339c809238af1cbf8b5e1915b","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/authentication/github/","postref":"0209233339c809238af1cbf8b5e1915b","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/admin-settings/authentication/github/","section":"rancher","tags":null,"title":"Configuring GitHub","type":"rancher","url":"/docs/rancher/v2.x/en/admin-settings/authentication/github/","weight":1116,"wordcount":303},{"authors":null,"categories":null,"content":"Within Rancher, each person authenticates as a user, which is a login that grants you access to Rancher. As mentioned in Authentication, users can either be local or external.\nAfter you configure external authentication, the users that display on the Users page changes.\n If you are logged in as a local user, only local users display.\n If you are logged in as an external user, both external and local users display.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/admin-settings/rbac/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"dd60b3b30d25fc8835021eb79d4fd7ed","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/rbac/","postref":"dd60b3b30d25fc8835021eb79d4fd7ed","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rancher/v2.x/en/admin-settings/rbac/","section":"rancher","tags":null,"title":"Role-Based Access Control (RBAC)","type":"rancher","url":"/docs/rancher/v2.x/en/admin-settings/rbac/","weight":1120,"wordcount":156},{"authors":null,"categories":null,"content":"The System Charts repository contains all the catalog items required for features such as monitoring, logging, alerting and global DNS.\nIn an air gapped installation of Rancher, you will need to configure Rancher to use a local copy of the system charts. This section describes how to use local system charts using a CLI flag in Rancher v2.3.0, and using a Git mirror for Rancher versions prior to v2.3.0.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/options/local-system-charts/","expirydate":-62135596800,"fuzzywordcount":500,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"790cc957055f4a35ae059832577cb4a3","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/local-system-charts/","postref":"790cc957055f4a35ae059832577cb4a3","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/installation/options/local-system-charts/","section":"rancher","tags":null,"title":"Setting up Local System Charts for Air Gapped Installations","type":"rancher","url":"/docs/rancher/v2.x/en/installation/options/local-system-charts/","weight":1120,"wordcount":416},{"authors":null,"categories":null,"content":"Available as of v2.3.0\nThe RKE metadata feature allows you to provision clusters with new versions of Kubernetes as soon as they are released, without upgrading Rancher. This feature is useful for taking advantage of patch versions of Kubernetes, for example, if you want to upgrade to Kubernetes v1.14.7 when your Rancher server originally supported v1.14.6.\n Note: The Kubernetes API can change between minor versions. Therefore, we don&rsquo;t support introducing minor Kubernetes versions, such as introducing v1.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/admin-settings/k8s-metadata/","expirydate":-62135596800,"fuzzywordcount":1000,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"aba753b557bb8684bd93405ffec8c8dc","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/k8s-metadata/","postref":"aba753b557bb8684bd93405ffec8c8dc","publishdate":"0001-01-01T00:00:00Z","readingtime":5,"relpermalink":"/docs/rancher/v2.x/en/admin-settings/k8s-metadata/","section":"rancher","tags":null,"title":"Upgrading Kubernetes without Upgrading Rancher","type":"rancher","url":"/docs/rancher/v2.x/en/admin-settings/k8s-metadata/","weight":1120,"wordcount":922},{"authors":null,"categories":null,"content":"Permissions are individual access rights that you can assign when selecting a custom permission for a user.\nGlobal Permissions define user authorization outside the scope of any particular cluster. Out-of-the-box, there are two default global permissions: Administrator and Standard User.\n Administrator: These users have full control over the entire Rancher system and all clusters within it.\n Standard User: These users can create new clusters and use them. Standard users can also assign other users permissions to their clusters.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/admin-settings/rbac/global-permissions/","expirydate":-62135596800,"fuzzywordcount":1700,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"8438d2e10528e421f75e037545c4bfb9","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/rbac/global-permissions/","postref":"8438d2e10528e421f75e037545c4bfb9","publishdate":"0001-01-01T00:00:00Z","readingtime":8,"relpermalink":"/docs/rancher/v2.x/en/admin-settings/rbac/global-permissions/","section":"rancher","tags":null,"title":"Global Permissions","type":"rancher","url":"/docs/rancher/v2.x/en/admin-settings/rbac/global-permissions/","weight":1126,"wordcount":1671},{"authors":null,"categories":null,"content":"Cluster and project roles define user authorization inside a cluster or project. You can manage these roles from the Global &gt; Security &gt; Roles page.\nMembership and Role Assignment The projects and clusters accessible to non-administrative users is determined by membership. Membership is a list of users who have access to a specific cluster or project based on the roles they were assigned in that cluster or project. Each cluster and project includes a tab that a user with the appropriate permissions can use to manage membership.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/admin-settings/rbac/cluster-project-roles/","expirydate":-62135596800,"fuzzywordcount":1700,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"efd331a45e96ba80bd54b9889f120816","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/rbac/cluster-project-roles/","postref":"efd331a45e96ba80bd54b9889f120816","publishdate":"0001-01-01T00:00:00Z","readingtime":8,"relpermalink":"/docs/rancher/v2.x/en/admin-settings/rbac/cluster-project-roles/","section":"rancher","tags":null,"title":"Cluster and Project Roles","type":"rancher","url":"/docs/rancher/v2.x/en/admin-settings/rbac/cluster-project-roles/","weight":1127,"wordcount":1622},{"authors":null,"categories":null,"content":"Within Rancher, roles determine what actions a user can make within a cluster or project.\nNote that roles are different from permissions, which determine what clusters and projects you can access.\nThis section covers the following topics:\n Prerequisites Creating a custom role for a cluster or project Creating a custom global role that copies rules from an existing role Creating a custom global role that does not copy rules from another role Deleting a custom global role Assigning a custom global role to a group  Prerequisites To complete the tasks on this page, one of the following permissions are required:","date":-62135596800,"description":"","dir":"rancher/v2.x/en/admin-settings/rbac/default-custom-roles/","expirydate":-62135596800,"fuzzywordcount":1400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"42165d7fdeedb5921a546c4e7f0ba966","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/rbac/default-custom-roles/","postref":"42165d7fdeedb5921a546c4e7f0ba966","publishdate":"0001-01-01T00:00:00Z","readingtime":7,"relpermalink":"/docs/rancher/v2.x/en/admin-settings/rbac/default-custom-roles/","section":"rancher","tags":null,"title":"Custom Roles","type":"rancher","url":"/docs/rancher/v2.x/en/admin-settings/rbac/default-custom-roles/","weight":1128,"wordcount":1338},{"authors":null,"categories":null,"content":"You can set roles to a status of locked. Locking roles prevent them from being assigned users in the future.\nLocked roles:\n Cannot be assigned to users that don&rsquo;t already have it assigned. Are not listed in the Member Roles drop-down when you are adding a user to a cluster or project. Do not affect users assigned the role before you lock the role. These users retain access that the role provides.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/admin-settings/rbac/locked-roles/","expirydate":-62135596800,"fuzzywordcount":300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"1268d2c894d46f0e6c9439a8c729d757","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/rbac/locked-roles/","postref":"1268d2c894d46f0e6c9439a8c729d757","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/admin-settings/rbac/locked-roles/","section":"rancher","tags":null,"title":"Locked Roles","type":"rancher","url":"/docs/rancher/v2.x/en/admin-settings/rbac/locked-roles/","weight":1129,"wordcount":270},{"authors":null,"categories":null,"content":"Pod Security Policies (or PSPs) are objects that control security-sensitive aspects of pod specification (like root privileges). If a pod does not meet the conditions specified in the PSP, Kubernetes will not allow it to start, and Rancher will display an error message of Pod &lt;NAME&gt; is forbidden: unable to validate....\n Note: Assigning Pod Security Policies are only available for clusters that are launched using RKE.\n  You can assign PSPs at the cluster or project level.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/admin-settings/pod-security-policies/","expirydate":-62135596800,"fuzzywordcount":600,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"fb3267c9309282ce8a4aa6d9308c3a77","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/pod-security-policies/","postref":"fb3267c9309282ce8a4aa6d9308c3a77","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/rancher/v2.x/en/admin-settings/pod-security-policies/","section":"rancher","tags":null,"title":"Pod Security Policies","type":"rancher","url":"/docs/rancher/v2.x/en/admin-settings/pod-security-policies/","weight":1135,"wordcount":520},{"authors":null,"categories":null,"content":"Drivers in Rancher allow you to manage which providers can be used to deploy hosted Kubernetes clusters or nodes in an infrastructure provider to allow Rancher to deploy and manage Kubernetes.\nRancher Drivers With Rancher drivers, you can enable/disable existing built-in drivers that are packaged in Rancher. Alternatively, you can add your own driver if Rancher has not yet implemented it.\nThere are two types of drivers within Rancher:","date":-62135596800,"description":"","dir":"rancher/v2.x/en/admin-settings/drivers/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"73e28b7be8a8732e547ae8883e97b3e5","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/drivers/","postref":"73e28b7be8a8732e547ae8883e97b3e5","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/admin-settings/drivers/","section":"rancher","tags":null,"title":"Provisioning Drivers","type":"rancher","url":"/docs/rancher/v2.x/en/admin-settings/drivers/","weight":1140,"wordcount":326},{"authors":null,"categories":null,"content":"Available as of v2.1.0\nIf your organization uses Keycloak Identity Provider (IdP) for user authentication, you can configure Rancher to allow your users to log in using their IdP credentials.\nPrerequisites  You must have a Keycloak IdP Server configured. In Keycloak, create a new SAML client, with the settings below. See the Keycloak documentation for help.\n   Setting Value     Sign Documents ON 1   Sign Assertions ON 1   All other ON/OFF Settings OFF   Client ID https://yourRancherHostURL/v1-saml/keycloak/saml/metadata2   Client Name  (e.","date":-62135596800,"description":"Create a Keycloak SAML client and configure Rancher to work with Keycloak. By the end your users will be able to sign into Rancher using their Keycloak logins","dir":"rancher/v2.x/en/admin-settings/authentication/keycloak/","expirydate":-62135596800,"fuzzywordcount":900,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"091e999ff095aa1bd96ce7d9fc10c43b","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/authentication/keycloak/","postref":"091e999ff095aa1bd96ce7d9fc10c43b","publishdate":"0001-01-01T00:00:00Z","readingtime":5,"relpermalink":"/docs/rancher/v2.x/en/admin-settings/authentication/keycloak/","section":"rancher","tags":null,"title":"Configuring Keycloak (SAML)","type":"rancher","url":"/docs/rancher/v2.x/en/admin-settings/authentication/keycloak/","weight":1200,"wordcount":877},{"authors":null,"categories":null,"content":"Available as of v2.0.7\nIf your organization uses Ping Identity Provider (IdP) for user authentication, you can configure Rancher to allow your users to log in using their IdP credentials.\n Prerequisites:\n You must have a Ping IdP Server configured. Following are the Rancher Service Provider URLs needed for configuration: Metadata URL: https://&lt;rancher-server&gt;/v1-saml/ping/saml/metadata Assertion Consumer Service (ACS) URL: https://&lt;rancher-server&gt;/v1-saml/ping/saml/acs Export a metadata.xml file from your IdP Server. For more information, see the PingIdentity documentation.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/admin-settings/authentication/ping-federate/","expirydate":-62135596800,"fuzzywordcount":500,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"fb92d49fcdc90ec6bc85ec9dc8018798","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/authentication/ping-federate/","postref":"fb92d49fcdc90ec6bc85ec9dc8018798","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/rancher/v2.x/en/admin-settings/authentication/ping-federate/","section":"rancher","tags":null,"title":"Configuring PingIdentity (SAML)","type":"rancher","url":"/docs/rancher/v2.x/en/admin-settings/authentication/ping-federate/","weight":1200,"wordcount":455},{"authors":null,"categories":null,"content":"Before configuring Rancher to support AD FS users, you must add Rancher as a relying party trust in AD FS.\n Log into your AD server as an administrative user.\n Open the AD FS Management console. Select Add Relying Party Trust&hellip; from the Actions menu and click Start.\n Select Enter data about the relying party manually as the option for obtaining data about the relying party.\n Enter your desired Display name for your Relying Party Trust.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/admin-settings/authentication/microsoft-adfs/microsoft-adfs-setup/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"f4adc603d49a3cf23d0ee4b0af4bdb85","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/authentication/microsoft-adfs/microsoft-adfs-setup/","postref":"f4adc603d49a3cf23d0ee4b0af4bdb85","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/admin-settings/authentication/microsoft-adfs/microsoft-adfs-setup/","section":"rancher","tags":null,"title":"1 — Configuring Microsoft AD FS for Rancher","type":"rancher","url":"/docs/rancher/v2.x/en/admin-settings/authentication/microsoft-adfs/microsoft-adfs-setup/","weight":1205,"wordcount":309},{"authors":null,"categories":null,"content":"Available as of v2.0.7\nAfter you complete Configuring Microsoft AD FS for Rancher, enter your AD FS information into Rancher to allow AD FS users to authenticate with Rancher.\n Important Notes For Configuring Your AD FS Server:\n The SAML 2.0 WebSSO Protocol Service URL is: https://&lt;RANCHER_SERVER&gt;/v1-saml/adfs/saml/acs The Relying Party Trust identifier URL is: https://&lt;RANCHER_SERVER&gt;/v1-saml/adfs/saml/metadata You must export the federationmetadata.xml file from your AD FS server. This can be found at: https://&lt;AD_SERVER&gt;/federationmetadata/2007-06/federationmetadata.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/admin-settings/authentication/microsoft-adfs/rancher-adfs-setup/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"8f1dcfc1458558ab698474e989fde90f","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/authentication/microsoft-adfs/rancher-adfs-setup/","postref":"8f1dcfc1458558ab698474e989fde90f","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/admin-settings/authentication/microsoft-adfs/rancher-adfs-setup/","section":"rancher","tags":null,"title":"2 — Configuring Rancher for Microsoft AD FS","type":"rancher","url":"/docs/rancher/v2.x/en/admin-settings/authentication/microsoft-adfs/rancher-adfs-setup/","weight":1205,"wordcount":346},{"authors":null,"categories":null,"content":"Available as of v2.0.7\nIf your organization uses Microsoft Active Directory Federation Services (AD FS) for user authentication, you can configure Rancher to allow your users to log in using their AD FS credentials.\nPrerequisites  You must have Rancher installed.\n Obtain your Rancher Server URL. During AD FS configuration, substitute this URL for the &lt;RANCHER_SERVER&gt; placeholder.\n You must have a global administrator account on your Rancher installation.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/admin-settings/authentication/microsoft-adfs/","expirydate":-62135596800,"fuzzywordcount":300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"2a64df5a1cb589bc0966765b7d912d04","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/authentication/microsoft-adfs/","postref":"2a64df5a1cb589bc0966765b7d912d04","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/admin-settings/authentication/microsoft-adfs/","section":"rancher","tags":null,"title":"Configuring Microsoft Active Directory Federation Service (SAML)","type":"rancher","url":"/docs/rancher/v2.x/en/admin-settings/authentication/microsoft-adfs/","weight":1205,"wordcount":296},{"authors":null,"categories":null,"content":"Available as of v2.2.0\nIf your organization uses Okta Identity Provider (IdP) for user authentication, you can configure Rancher to allow your users to log in using their IdP credentials.\n Note: Okta integration only supports Service Provider initiated logins.\n Prerequisites In Okta, create a SAML Application with the settings below. See the Okta documentation for help.\n   Setting Value     Single Sign on URL https://yourRancherHostURL/v1-saml/okta/saml/acs   Audience URI (SP Entity ID) https://yourRancherHostURL/v1-saml/okta/saml/metadata    Configuring Okta in Rancher  From the Global view, select Security &gt; Authentication from the main menu.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/admin-settings/authentication/okta/","expirydate":-62135596800,"fuzzywordcount":500,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"a826c89179cb72eae424c517f0228e66","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/authentication/okta/","postref":"a826c89179cb72eae424c517f0228e66","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/rancher/v2.x/en/admin-settings/authentication/okta/","section":"rancher","tags":null,"title":"Configuring Okta (SAML)","type":"rancher","url":"/docs/rancher/v2.x/en/admin-settings/authentication/okta/","weight":1210,"wordcount":432},{"authors":null,"categories":null,"content":"Rancher simplifies the creation of clusters by allowing you to create them through the Rancher UI rather than more complex alternatives. Rancher provides multiple options for launching a cluster. Use the option that best fits your use case.\nThis section assumes a basic familiarity with Docker and Kubernetes. For a brief explanation of how Kubernetes components work together, refer to the concepts page.\nFor a conceptual overview of how the Rancher server provisions clusters and what tools it uses to provision them, refer to the architecture page.","date":-62135596800,"description":"Provisioning Kubernetes Clusters","dir":"rancher/v2.x/en/cluster-provisioning/","expirydate":-62135596800,"fuzzywordcount":700,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"a53656025e59ba11520a6ed68fb266d9","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/","postref":"a53656025e59ba11520a6ed68fb266d9","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/rancher/v2.x/en/cluster-provisioning/","section":"rancher","tags":null,"title":"Setting up Kubernetes Clusters in Rancher","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-provisioning/","weight":2000,"wordcount":608},{"authors":null,"categories":null,"content":"In this section, we recommend best practices for creating the production-ready Kubernetes clusters that will run your apps and services.\nFor a list of requirements for your cluster, including the requirements for OS/Docker, hardware, and networking, refer to the section on node requirements.\nThis is a shortlist of best practices that we strongly recommend for all production clusters.\nFor a full list of all the best practices that we recommend, refer to the best practices section.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-provisioning/production/","expirydate":-62135596800,"fuzzywordcount":500,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"63a5ab15a31512b0c29ba7e06b606d99","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/production/","postref":"63a5ab15a31512b0c29ba7e06b606d99","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/rancher/v2.x/en/cluster-provisioning/production/","section":"rancher","tags":null,"title":"Checklist for Production-Ready Clusters","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-provisioning/production/","weight":2005,"wordcount":429},{"authors":null,"categories":null,"content":"After you provision a cluster in Rancher, you can begin using powerful Kubernetes features to deploy and scale your containerized applications in development, testing, or production environments.\nThis page covers the following topics:\n Switching between clusters Managing clusters in Rancher Configuring tools   This section assumes a basic familiarity with Docker and Kubernetes. For a brief explanation of how Kubernetes components work together, refer to the concepts page.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-admin/","expirydate":-62135596800,"fuzzywordcount":300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"bfd0cc60c3a83b81af3257c929141efe","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/","postref":"bfd0cc60c3a83b81af3257c929141efe","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/cluster-admin/","section":"rancher","tags":null,"title":"Cluster Administration","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-admin/","weight":2005,"wordcount":290},{"authors":null,"categories":null,"content":"This section describes how to manipulate your downstream Kubernetes cluster with kubectl from the Rancher UI or from your workstation.\nFor more information on using kubectl, see Kubernetes Documentation: Overview of kubectl.\n Accessing clusters with kubectl shell in the Rancher UI Accessing clusters with kubectl from your workstation Note on Resources created using kubectl Authenticating Directly with a Downstream Cluster  Connecting Directly to Clusters with FQDN Defined Connecting Directly to Clusters without FQDN Defined   Accessing Clusters with kubectl Shell in the Rancher UI You can access and manage your clusters by logging into Rancher and opening the kubectl shell in the UI.","date":-62135596800,"description":"Learn how you can access and manage your Kubernetes clusters using kubectl with kubectl Shell or with kubectl CLI and kubeconfig file. A kubeconfig file is used to configure access to Kubernetes. When you create a cluster with Rancher, it automatically creates a kubeconfig for your cluster.","dir":"rancher/v2.x/en/cluster-admin/cluster-access/kubectl/","expirydate":-62135596800,"fuzzywordcount":900,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"53abbb89333c81f86c599c4e6650f565","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/cluster-access/kubectl/","postref":"53abbb89333c81f86c599c4e6650f565","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/docs/rancher/v2.x/en/cluster-admin/cluster-access/kubectl/","section":"rancher","tags":null,"title":"Access a Cluster with Kubectl and kubeconfig","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-admin/cluster-access/kubectl/","weight":2010,"wordcount":845},{"authors":null,"categories":null,"content":"This section describes how the kubectl CLI, the kubeconfig file, and the authorized cluster endpoint work together to allow you to access a downstream Kubernetes cluster directly, without authenticating through the Rancher server. It is intended to provide background information and context to the instructions for how to set up kubectl to directly access a cluster.\nAbout the kubeconfig File The kubeconfig file is a file used to configure access to Kubernetes when used in conjunction with the kubectl command line tool (or other clients).","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-admin/cluster-access/ace/","expirydate":-62135596800,"fuzzywordcount":600,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"93148bbf5c92e4c89af5d3d5c81d4b27","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/cluster-access/ace/","postref":"93148bbf5c92e4c89af5d3d5c81d4b27","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/rancher/v2.x/en/cluster-admin/cluster-access/ace/","section":"rancher","tags":null,"title":"How the Authorized Cluster Endpoint Works","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-admin/cluster-access/ace/","weight":2015,"wordcount":518},{"authors":null,"categories":null,"content":"If you want to provide a user with access and permissions to all projects, nodes, and resources within a cluster, assign the user a cluster membership.\n Tip: Want to provide a user with access to a specific project within a cluster? See Adding Project Members instead.\n There are two contexts where you can add cluster members:\n Adding Members to a New Cluster\nYou can add members to a cluster as you create it (recommended if possible).","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-admin/cluster-access/cluster-members/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"6cb0f2551dfa8c2f05d05387c2f9d08b","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/cluster-access/cluster-members/","postref":"6cb0f2551dfa8c2f05d05387c2f9d08b","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/cluster-admin/cluster-access/cluster-members/","section":"rancher","tags":null,"title":"Adding Users to Clusters","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-admin/cluster-access/cluster-members/","weight":2020,"wordcount":343},{"authors":null,"categories":null,"content":"After you provision a Kubernetes cluster using Rancher, you can still edit options and settings for the cluster. To edit your cluster, open the Global view, make sure the Clusters tab is selected, and then select Ellipsis (&hellip;) &gt; Edit for the cluster that you want to edit.\nTo Edit an Existing Cluster The options and settings available for an existing cluster change based on the method that you used to provision it.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-admin/editing-clusters/","expirydate":-62135596800,"fuzzywordcount":800,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"2f9bd043c8f2cc83115af27131a6b98f","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/editing-clusters/","postref":"2f9bd043c8f2cc83115af27131a6b98f","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/docs/rancher/v2.x/en/cluster-admin/editing-clusters/","section":"rancher","tags":null,"title":"Cluster Configuration","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-admin/editing-clusters/","weight":2025,"wordcount":756},{"authors":null,"categories":null,"content":"After you launch a Kubernetes cluster in Rancher, you can manage individual nodes from the cluster&rsquo;s Node tab. Depending on the option used to provision the cluster, there are different node options available.\nThis page covers the following topics:\n Node options for each type of cluster Cordoning and draining nodes Editing a node Viewing a node API Deleting a node Scaling nodes SSH into a node hosted by an infrastructure provider Managing node pools  To manage individual nodes, browse to the cluster that you want to manage and then select Nodes from the main menu.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-admin/nodes/","expirydate":-62135596800,"fuzzywordcount":1600,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"414a272377e66ab9b8dcfc1f176de815","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/nodes/","postref":"414a272377e66ab9b8dcfc1f176de815","publishdate":"0001-01-01T00:00:00Z","readingtime":8,"relpermalink":"/docs/rancher/v2.x/en/cluster-admin/nodes/","section":"rancher","tags":null,"title":"Nodes and Node Pools","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-admin/nodes/","weight":2030,"wordcount":1532},{"authors":null,"categories":null,"content":"When deploying an application that needs to retain data, you&rsquo;ll need to create persistent storage. Persistent storage allows you to store application data external from the pod running your application. This storage practice allows you to maintain application data, even if the application&rsquo;s pod fails.\nThe documents in this section assume that you understand the Kubernetes concepts of persistent volumes, persistent volume claims, and storage classes. For more information, refer to the section on how storage works.","date":-62135596800,"description":"Learn about the two ways with which you can create persistent storage in Kubernetes: persistent volumes and storage classes","dir":"rancher/v2.x/en/cluster-admin/volumes-and-storage/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"9ecf39af1d9ce62f0ee4da1cbe6dae7b","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/volumes-and-storage/","postref":"9ecf39af1d9ce62f0ee4da1cbe6dae7b","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/cluster-admin/volumes-and-storage/","section":"rancher","tags":null,"title":"Kubernetes Persistent Storage: Volumes and Storage Classes","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-admin/volumes-and-storage/","weight":2031,"wordcount":365},{"authors":null,"categories":null,"content":"A namespace is a Kubernetes concept that allows a virtual cluster within a cluster, which is useful for dividing the cluster into separate &ldquo;virtual clusters&rdquo; that each have their own access control and resource quotas.\nA project is a group of namespaces, and it is a concept introduced by Rancher. Projects allow you to manage multiple namespaces as a group and perform Kubernetes operations in them. You can use projects to support multi-tenancy, so that a team can access a project within a cluster without having access to other projects in the same cluster.","date":-62135596800,"description":"Rancher Projects ease the administrative burden of your cluster and support multi-tenancy. Learn to create projects and divide projects into Kubernetes namespaces","dir":"rancher/v2.x/en/cluster-admin/projects-and-namespaces/","expirydate":-62135596800,"fuzzywordcount":1600,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"458a6ad658a84b66e99f1f3f79164e9e","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/projects-and-namespaces/","postref":"458a6ad658a84b66e99f1f3f79164e9e","publishdate":"0001-01-01T00:00:00Z","readingtime":8,"relpermalink":"/docs/rancher/v2.x/en/cluster-admin/projects-and-namespaces/","section":"rancher","tags":null,"title":"Projects and Kubernetes Namespaces with Rancher","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-admin/projects-and-namespaces/","weight":2032,"wordcount":1504},{"authors":null,"categories":null,"content":"Rancher contains a variety of tools that aren&rsquo;t included in Kubernetes to assist in your DevOps operations. Rancher can integrate with external services to help your clusters run more efficiently. Tools are divided into following categories:\n Notifiers and Alerts Logging Monitoring Istio  Notifiers and Alerts Notifiers and alerts are two features that work together to inform you of events in the Rancher system.\nNotifiers are services that inform you of alert events.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-admin/tools/","expirydate":-62135596800,"fuzzywordcount":300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"b19a8c69435b02b7918dfc448b8c8f24","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/tools/","postref":"b19a8c69435b02b7918dfc448b8c8f24","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/cluster-admin/tools/","section":"rancher","tags":null,"title":"Tools for Logging, Monitoring, and Visibility","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-admin/tools/","weight":2033,"wordcount":286},{"authors":null,"categories":null,"content":"If you have a cluster in Rancher that you want to use as a template for creating similar clusters, you can use Rancher CLI to clone the cluster&rsquo;s configuration, edit it, and then use it to quickly launch the cloned cluster.\nYou can clone clusters only if the nodes in the cluster are hosted by an infrastructure provider, such as EC2, Azure, or DigitalOcean.\nDuplication of imported clusters, clusters in hosted Kubernetes providers, and custom clusters provisioned using Docker machine is not supported.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-admin/cloning-clusters/","expirydate":-62135596800,"fuzzywordcount":600,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"5dfc2401ce49b534e55df64ca42f67c2","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/cloning-clusters/","postref":"5dfc2401ce49b534e55df64ca42f67c2","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/rancher/v2.x/en/cluster-admin/cloning-clusters/","section":"rancher","tags":null,"title":"Cloning Clusters","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-admin/cloning-clusters/","weight":2035,"wordcount":554},{"authors":null,"categories":null,"content":"Warning: Rotating Kubernetes certificates may result in your cluster being temporarily unavailable as components are restarted. For production environments, it&rsquo;s recommended to perform this action during a maintenance window.\n By default, Kubernetes clusters require certificates and Rancher launched Kubernetes clusters automatically generate certificates for the Kubernetes components. Rotating these certificates is important before the certificates expire as well as if a certificate is compromised. After the certificates are rotated, the Kubernetes components are automatically restarted.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-admin/certificate-rotation/","expirydate":-62135596800,"fuzzywordcount":500,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"8718d294898fff5a9ba6e6f712b68430","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/certificate-rotation/","postref":"8718d294898fff5a9ba6e6f712b68430","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/rancher/v2.x/en/cluster-admin/certificate-rotation/","section":"rancher","tags":null,"title":"Certificate Rotation","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-admin/certificate-rotation/","weight":2040,"wordcount":434},{"authors":null,"categories":null,"content":"Rancher uses cert-manager to automatically generate and renew TLS certificates for HA deployments of Rancher. As of Fall 2019, three important changes to cert-manager are set to occur that you need to take action on if you have an HA deployment of Rancher:\n Let&rsquo;s Encrypt will be blocking cert-manager instances older than 0.8.0 starting November 1st 2019. Cert-manager is deprecating and replacing the certificate.spec.acme.solvers field. This change has no exact deadline.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/options/upgrading-cert-manager/","expirydate":-62135596800,"fuzzywordcount":1300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"785051ab1a1db630f372bb5eb6e15b8d","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/upgrading-cert-manager/","postref":"785051ab1a1db630f372bb5eb6e15b8d","publishdate":"0001-01-01T00:00:00Z","readingtime":6,"relpermalink":"/docs/rancher/v2.x/en/installation/options/upgrading-cert-manager/","section":"rancher","tags":null,"title":"Upgrading Cert-Manager","type":"rancher","url":"/docs/rancher/v2.x/en/installation/options/upgrading-cert-manager/","weight":2040,"wordcount":1231},{"authors":null,"categories":null,"content":"Rancher uses cert-manager to automatically generate and renew TLS certificates for HA deployments of Rancher. As of Fall 2019, three important changes to cert-manager are set to occur that you need to take action on if you have an HA deployment of Rancher:\n Let&rsquo;s Encrypt will be blocking cert-manager instances older than 0.8.0 starting November 1st 2019. Cert-manager is deprecating and replacing the certificate.spec.acme.solvers field. This change has no exact deadline.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/options/upgrading-cert-manager/helm-2-instructions/","expirydate":-62135596800,"fuzzywordcount":1000,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"459306a2f412a0ba2f7afa97ab440ea4","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/upgrading-cert-manager/helm-2-instructions/","postref":"459306a2f412a0ba2f7afa97ab440ea4","publishdate":"0001-01-01T00:00:00Z","readingtime":5,"relpermalink":"/docs/rancher/v2.x/en/installation/options/upgrading-cert-manager/helm-2-instructions/","section":"rancher","tags":null,"title":"Upgrading Cert-Manager with Helm 2","type":"rancher","url":"/docs/rancher/v2.x/en/installation/options/upgrading-cert-manager/helm-2-instructions/","weight":2040,"wordcount":993},{"authors":null,"categories":null,"content":"Available as of v2.2.0\nIn the Rancher UI, etcd backup and recovery for Rancher launched Kubernetes clusters can be easily performed. Snapshots of the etcd database are taken and saved either locally onto the etcd nodes or to a S3 compatible target. The advantages of configuring S3 is that if all etcd nodes are lost, your snapshot is saved remotely and can be used to restore the cluster.\nRancher recommends configuring recurrent etcd snapshots for all production clusters.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-admin/backing-up-etcd/","expirydate":-62135596800,"fuzzywordcount":1000,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"9bab843efe5eb0ad70753609e5003e43","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/backing-up-etcd/","postref":"9bab843efe5eb0ad70753609e5003e43","publishdate":"0001-01-01T00:00:00Z","readingtime":5,"relpermalink":"/docs/rancher/v2.x/en/cluster-admin/backing-up-etcd/","section":"rancher","tags":null,"title":"Backing up etcd","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-admin/backing-up-etcd/","weight":2045,"wordcount":984},{"authors":null,"categories":null,"content":"Available as of v2.2.0\netcd backup and recovery for Rancher launched Kubernetes clusters can be easily performed. Snapshots of the etcd database are taken and saved either locally onto the etcd nodes or to a S3 compatible target. The advantages of configuring S3 is that if all etcd nodes are lost, your snapshot is saved remotely and can be used to restore the cluster.\nRancher recommends enabling the ability to set up recurring snapshots of etcd, but one-time snapshots can easily be taken as well.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-admin/restoring-etcd/","expirydate":-62135596800,"fuzzywordcount":700,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"d592744334773a05a91f3a3867cce400","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/restoring-etcd/","postref":"d592744334773a05a91f3a3867cce400","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/rancher/v2.x/en/cluster-admin/restoring-etcd/","section":"rancher","tags":null,"title":"Restoring etcd","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-admin/restoring-etcd/","weight":2050,"wordcount":624},{"authors":null,"categories":null,"content":"This section describes how to disconnect a node from a Rancher-launched Kubernetes cluster and remove all of the Kubernetes components from the node. This process allows you to use the node for other purposes.\nWhen you use Rancher to launch nodes for a cluster, resources (containers/virtual network interfaces) and configuration items (certificates/configuration files) are created.\nWhen removing nodes from your Rancher launched Kubernetes cluster (provided that they are in Active state), those resources are automatically cleaned, and the only action needed is to restart the node.","date":-62135596800,"description":"Learn about cluster cleanup when removing nodes from your Rancher-launched Kubernetes cluster. What is removed, how to do it manually","dir":"rancher/v2.x/en/cluster-admin/cleaning-cluster-nodes/","expirydate":-62135596800,"fuzzywordcount":1300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"acba3909aeca8be197fd562b90be1711","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/cleaning-cluster-nodes/","postref":"acba3909aeca8be197fd562b90be1711","publishdate":"0001-01-01T00:00:00Z","readingtime":7,"relpermalink":"/docs/rancher/v2.x/en/cluster-admin/cleaning-cluster-nodes/","section":"rancher","tags":null,"title":"Removing Kubernetes Components from Nodes","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-admin/cleaning-cluster-nodes/","weight":2055,"wordcount":1280},{"authors":null,"categories":null,"content":"In this scenario, Rancher does not provision Kubernetes because it is installed by providers such as Google Kubernetes Engine (GKE), Amazon Elastic Container Service for Kubernetes, or Azure Kubernetes Service.\nIf you use a Kubernetes provider such as Google GKE, Rancher integrates with its cloud APIs, allowing you to create and manage role-based access control for the hosted cluster from the Rancher UI.\nIn this use case, Rancher sends a request to a hosted provider using the provider&rsquo;s API.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-provisioning/hosted-kubernetes-clusters/","expirydate":-62135596800,"fuzzywordcount":300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"a515306da7b79aec5d1eca6a3da7d0d0","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/hosted-kubernetes-clusters/","postref":"a515306da7b79aec5d1eca6a3da7d0d0","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/cluster-provisioning/hosted-kubernetes-clusters/","section":"rancher","tags":null,"title":"Setting up Clusters from Hosted Kubernetes Providers","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-provisioning/hosted-kubernetes-clusters/","weight":2100,"wordcount":237},{"authors":null,"categories":null,"content":"Prerequisites in Google Kubernetes Engine  Note Deploying to GKE will incur charges.\n Create a service account using Google Kubernetes Engine. GKE uses this account to operate your cluster. Creating this account also generates a private key used for authentication.\nThe service account requires the following roles:\n Compute Viewer: roles/compute.viewer Project Viewer: roles/viewer Kubernetes Engine Admin: roles/container.admin Service Account User: roles/iam.serviceAccountUser  Google Documentation: Creating and Enabling Service Accounts","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-provisioning/hosted-kubernetes-clusters/gke/","expirydate":-62135596800,"fuzzywordcount":300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"16cea34323de6d4ff172da25a69a3717","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/hosted-kubernetes-clusters/gke/","postref":"16cea34323de6d4ff172da25a69a3717","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/cluster-provisioning/hosted-kubernetes-clusters/gke/","section":"rancher","tags":null,"title":"Creating a GKE Cluster","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-provisioning/hosted-kubernetes-clusters/gke/","weight":2105,"wordcount":256},{"authors":null,"categories":null,"content":"Amazon EKS provides a managed control plane for your Kubernetes cluster. Amazon EKS runs the Kubernetes control plane instances across multiple Availability Zones to ensure high availability. Rancher provides an intuitive user interface for managing and deploying the Kubernetes clusters you run in Amazon EKS. With this guide, you will use Rancher to quickly and easily launch an Amazon EKS Kubernetes cluster in your AWS account. For more information on Amazon EKS, see this documentation.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-provisioning/hosted-kubernetes-clusters/eks/","expirydate":-62135596800,"fuzzywordcount":1500,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"5f4e527208564c52965aa6fa9a7aafb2","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/hosted-kubernetes-clusters/eks/","postref":"5f4e527208564c52965aa6fa9a7aafb2","publishdate":"0001-01-01T00:00:00Z","readingtime":7,"relpermalink":"/docs/rancher/v2.x/en/cluster-provisioning/hosted-kubernetes-clusters/eks/","section":"rancher","tags":null,"title":"Creating an EKS Cluster","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-provisioning/hosted-kubernetes-clusters/eks/","weight":2110,"wordcount":1400},{"authors":null,"categories":null,"content":"You can use Rancher to create a cluster hosted in Microsoft Azure Kubernetes Service (AKS).\nPrerequisites in Microsoft Azure  Note Deploying to AKS will incur charges.\n To interact with Azure APIs, an AKS cluster requires an Azure Active Directory (AD) service principal. The service principal is needed to dynamically create and manage other Azure resources, and it provides credentials for your cluster to communicate with AKS. For more information about the service principal, refer to the AKS documentation.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-provisioning/hosted-kubernetes-clusters/aks/","expirydate":-62135596800,"fuzzywordcount":1000,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"55fd214b94a3ff5e5f4230746f4675e6","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/hosted-kubernetes-clusters/aks/","postref":"55fd214b94a3ff5e5f4230746f4675e6","publishdate":"0001-01-01T00:00:00Z","readingtime":5,"relpermalink":"/docs/rancher/v2.x/en/cluster-provisioning/hosted-kubernetes-clusters/aks/","section":"rancher","tags":null,"title":"Creating an AKS Cluster","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-provisioning/hosted-kubernetes-clusters/aks/","weight":2115,"wordcount":972},{"authors":null,"categories":null,"content":"Available as of v2.2.0\nYou can use Rancher to create a cluster hosted in Alibaba Cloud Kubernetes (ACK). Rancher has already implemented and packaged the cluster driver for ACK, but by default, this cluster driver is inactive. In order to launch ACK clusters, you will need to enable the ACK cluster driver. After enabling the cluster driver, you can start provisioning ACK clusters.\nPrerequisites  Note Deploying to ACK will incur charges.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-provisioning/hosted-kubernetes-clusters/ack/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"fb8f533b039c7b99ca5433a9b81b2b1b","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/hosted-kubernetes-clusters/ack/","postref":"fb8f533b039c7b99ca5433a9b81b2b1b","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/cluster-provisioning/hosted-kubernetes-clusters/ack/","section":"rancher","tags":null,"title":"Creating an Aliyun ACK Cluster","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-provisioning/hosted-kubernetes-clusters/ack/","weight":2120,"wordcount":339},{"authors":null,"categories":null,"content":"Available as of v2.2.0\nYou can use Rancher to create a cluster hosted in Tencent Kubernetes Engine (TKE). Rancher has already implemented and packaged the cluster driver for TKE, but by default, this cluster driver is inactive. In order to launch TKE clusters, you will need to enable the TKE cluster driver. After enabling the cluster driver, you can start provisioning TKE clusters.\nPrerequisites in Tencent  Note Deploying to TKE will incur charges.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-provisioning/hosted-kubernetes-clusters/tke/","expirydate":-62135596800,"fuzzywordcount":700,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"cdbcac3e22dacce35b7ef6e51f4b47b6","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/hosted-kubernetes-clusters/tke/","postref":"cdbcac3e22dacce35b7ef6e51f4b47b6","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/docs/rancher/v2.x/en/cluster-provisioning/hosted-kubernetes-clusters/tke/","section":"rancher","tags":null,"title":"Creating a Tencent TKE Cluster","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-provisioning/hosted-kubernetes-clusters/tke/","weight":2125,"wordcount":659},{"authors":null,"categories":null,"content":"Available as of v2.2.0\nYou can use Rancher to create a cluster hosted in Huawei Cloud Container Engine (CCE). Rancher has already implemented and packaged the cluster driver for CCE, but by default, this cluster driver is inactive. In order to launch CCE clusters, you will need to enable the CCE cluster driver. After enabling the cluster driver, you can start provisioning CCE clusters.\nPrerequisites in Huawei  Note Deploying to CCE will incur charges.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-provisioning/hosted-kubernetes-clusters/cce/","expirydate":-62135596800,"fuzzywordcount":1000,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"526b9aa50b37332213cca09b38241a22","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/hosted-kubernetes-clusters/cce/","postref":"526b9aa50b37332213cca09b38241a22","publishdate":"0001-01-01T00:00:00Z","readingtime":5,"relpermalink":"/docs/rancher/v2.x/en/cluster-provisioning/hosted-kubernetes-clusters/cce/","section":"rancher","tags":null,"title":"Creating a Huawei CCE Cluster","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-provisioning/hosted-kubernetes-clusters/cce/","weight":2130,"wordcount":945},{"authors":null,"categories":null,"content":"You can have Rancher launch a Kubernetes cluster using any nodes you want. When Rancher deploys Kubernetes onto these nodes, it uses Rancher Kubernetes Engine (RKE), which is Rancher&rsquo;s own lightweight Kubernetes installer. It can launch Kubernetes on any computers, including:\n Bare-metal servers On-premise virtual machines Virtual machines hosted by an infrastructure provider  Rancher can install Kubernetes on existing nodes, or it can dynamically provision nodes in an infrastructure provider and install Kubernetes on them.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-provisioning/rke-clusters/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"a2421df752ee5b187a4733e45918651d","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/","postref":"a2421df752ee5b187a4733e45918651d","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/","section":"rancher","tags":null,"title":"Launching Kubernetes with Rancher","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/","weight":2200,"wordcount":315},{"authors":null,"categories":null,"content":"Using Rancher, you can create pools of nodes based on a node template. This node template defines the parameters you want to use to launch nodes in your infrastructure providers or cloud providers.\nOne benefit of installing Kubernetes on node pools hosted by an infrastructure provider is that if a node loses connectivity with the cluster, Rancher can automatically create another node to join the cluster to ensure that the count of the node pool is as expected.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-provisioning/rke-clusters/node-pools/","expirydate":-62135596800,"fuzzywordcount":1400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"690a18559c5c94f7365fad603f15ee30","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/node-pools/","postref":"690a18559c5c94f7365fad603f15ee30","publishdate":"0001-01-01T00:00:00Z","readingtime":7,"relpermalink":"/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/node-pools/","section":"rancher","tags":null,"title":"Launching Kubernetes on New Nodes in an Infrastructure Provider","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/node-pools/","weight":2205,"wordcount":1317},{"authors":null,"categories":null,"content":"Use Rancher to create a Kubernetes cluster in Amazon EC2.\nPrerequisites  AWS EC2 Access Key and Secret Key that will be used to create the instances. See Amazon Documentation: Creating Access Keys how to create an Access Key and Secret Key. IAM Policy created to add to the user of the Access Key And Secret Key. See Amazon Documentation: Creating IAM Policies (Console) how to create an IAM policy.","date":-62135596800,"description":"Learn the prerequisites and steps required in order for you to create an Amazon EC2 cluster using Rancher","dir":"rancher/v2.x/en/cluster-provisioning/rke-clusters/node-pools/ec2/","expirydate":-62135596800,"fuzzywordcount":1800,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"3bdc0fc6625189da9d91d09ea30b2466","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/node-pools/ec2/","postref":"3bdc0fc6625189da9d91d09ea30b2466","publishdate":"0001-01-01T00:00:00Z","readingtime":9,"relpermalink":"/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/node-pools/ec2/","section":"rancher","tags":null,"title":"Creating an Amazon EC2 Cluster","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/node-pools/ec2/","weight":2210,"wordcount":1722},{"authors":null,"categories":null,"content":"Use Rancher to create a Kubernetes cluster using DigitalOcean.\n From the Clusters page, click Add Cluster.\n Choose DigitalOcean.\n Enter a Cluster Name.\n  Use Member Roles to configure user authorization for the cluster.\n Click Add Member to add users that can access the cluster. Use the Role drop-down to set permissions for each user.     Use Cluster Options to choose the version of Kubernetes, what network provider will be used and if you want to enable project network isolation.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-provisioning/rke-clusters/node-pools/digital-ocean/","expirydate":-62135596800,"fuzzywordcount":700,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"84c0860c6c7fd42273feb642049b9d2c","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/node-pools/digital-ocean/","postref":"84c0860c6c7fd42273feb642049b9d2c","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/node-pools/digital-ocean/","section":"rancher","tags":null,"title":"Creating a DigitalOcean Cluster","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/node-pools/digital-ocean/","weight":2215,"wordcount":651},{"authors":null,"categories":null,"content":"Use Rancher to create a Kubernetes cluster in Azure.\n From the Clusters page, click Add Cluster.\n Choose Azure.\n Enter a Cluster Name.\n  Use Member Roles to configure user authorization for the cluster.\n Click Add Member to add users that can access the cluster. Use the Role drop-down to set permissions for each user.     Use Cluster Options to choose the version of Kubernetes, what network provider will be used and if you want to enable project network isolation.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-provisioning/rke-clusters/node-pools/azure/","expirydate":-62135596800,"fuzzywordcount":700,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"0021ebc5270fe7831543fb74620bf94c","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/node-pools/azure/","postref":"0021ebc5270fe7831543fb74620bf94c","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/node-pools/azure/","section":"rancher","tags":null,"title":"Creating an Azure Cluster","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/node-pools/azure/","weight":2220,"wordcount":659},{"authors":null,"categories":null,"content":"By using Rancher with vSphere, you can bring cloud operations on-premises.\nRancher can provision nodes in vSphere and install Kubernetes on them. When creating a Kubernetes cluster in vSphere, Rancher first provisions the specified number of virtual machines by communicating with the vCenter API. Then it installs Kubernetes on top of them.\nA vSphere cluster may consist of multiple groups of VMs with distinct properties, such as the amount of memory or the number of vCPUs.","date":-62135596800,"description":"Use Rancher to create a vSphere cluster. It may consist of groups of VMs with distinct properties which allow for fine-grained control over the sizing of nodes.","dir":"rancher/v2.x/en/cluster-provisioning/rke-clusters/node-pools/vsphere/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"c17bbd0aa3f9a41f5f547cdf604009b5","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/node-pools/vsphere/","postref":"c17bbd0aa3f9a41f5f547cdf604009b5","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/node-pools/vsphere/","section":"rancher","tags":null,"title":"Creating a vSphere Cluster","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/node-pools/vsphere/","weight":2225,"wordcount":381},{"authors":null,"categories":null,"content":"When you create a custom cluster, Rancher uses RKE (the Rancher Kubernetes Engine) to create a Kubernetes cluster in on-premise bare-metal servers, on-premise virtual machines, or in any node hosted by an infrastructure provider.\nTo use this option you&rsquo;ll need access to servers you intend to use in your Kubernetes cluster. Provision each server according to the requirements, which includes some hardware specifications and Docker. After you install Docker on each server, run the command provided in the Rancher UI to turn each server into a Kubernetes node.","date":-62135596800,"description":"To create a cluster with custom nodes, you’ll need to access servers in your cluster and provision them according to Rancher requirements","dir":"rancher/v2.x/en/cluster-provisioning/rke-clusters/custom-nodes/","expirydate":-62135596800,"fuzzywordcount":900,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"49db5f712676ab795416166283100d75","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/custom-nodes/","postref":"49db5f712676ab795416166283100d75","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/custom-nodes/","section":"rancher","tags":null,"title":"Launching Kubernetes on Existing Custom Nodes","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/custom-nodes/","weight":2225,"wordcount":846},{"authors":null,"categories":null,"content":"Available as of v2.3.0\nWhen provisioning a custom cluster using Rancher, Rancher uses RKE (the Rancher Kubernetes Engine) to provision the Kubernetes custom cluster on your existing infrastructure.\nYou can use a mix of Linux and Windows hosts as your cluster nodes. Windows nodes can only be used for deploying workloads, while Linux nodes are required for cluster management.\nYou can only add Windows nodes to a cluster if Windows support is enabled.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-provisioning/rke-clusters/windows-clusters/","expirydate":-62135596800,"fuzzywordcount":2200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"b3d745a667545b43c68ef02d1e3b910f","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/windows-clusters/","postref":"b3d745a667545b43c68ef02d1e3b910f","publishdate":"0001-01-01T00:00:00Z","readingtime":11,"relpermalink":"/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/windows-clusters/","section":"rancher","tags":null,"title":"Launching Kubernetes on Windows Clusters","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/windows-clusters/","weight":2240,"wordcount":2147},{"authors":null,"categories":null,"content":"As you configure a new cluster that&rsquo;s provisioned using RKE, you can choose custom Kubernetes options.\nYou can configure Kubernetes options one of two ways:\n Rancher UI: Use the Rancher UI to select options that are commonly customized when setting up a Kubernetes cluster. Cluster Config File: Instead of using the Rancher UI to choose Kubernetes options for the cluster, advanced users can create an RKE config file. Using a config file allows you to set any of the options available in an RKE installation, except for system_images configuration, by specifying them in YAML.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-provisioning/rke-clusters/options/","expirydate":-62135596800,"fuzzywordcount":1900,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"e0a56d94ee917808a6d8662155683bb4","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/options/","postref":"e0a56d94ee917808a6d8662155683bb4","publishdate":"0001-01-01T00:00:00Z","readingtime":9,"relpermalink":"/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/options/","section":"rancher","tags":null,"title":"Cluster Configuration Reference","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/options/","weight":2250,"wordcount":1845},{"authors":null,"categories":null,"content":"A cloud provider is a module in Kubernetes that provides an interface for managing nodes, load balancers, and networking routes. For more information, refer to the official Kubernetes documentation on cloud providers.\nWhen a cloud provider is set up in Rancher, the Rancher server can automatically provision new nodes, load balancers or persistent storage devices when launching Kubernetes definitions, if the cloud provider you&rsquo;re using supports such automation.\n Cloud provider options Setting up the Amazon cloud provider Setting up the Azure cloud provider  Cloud Provider Options By default, the Cloud Provider option is set to None.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-provisioning/rke-clusters/options/cloud-providers/","expirydate":-62135596800,"fuzzywordcount":1300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"51c078cad85d0a923adecc8b5559c5f7","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/options/cloud-providers/","postref":"51c078cad85d0a923adecc8b5559c5f7","publishdate":"0001-01-01T00:00:00Z","readingtime":6,"relpermalink":"/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/options/cloud-providers/","section":"rancher","tags":null,"title":"Setting up Cloud Providers","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/options/cloud-providers/","weight":2255,"wordcount":1229},{"authors":null,"categories":null,"content":"Pod Security Policies are objects that control security-sensitive aspects of pod specification (like root privileges).\nAdding a Default Pod Security Policy When you create a new cluster with RKE, you can configure it to apply a PSP immediately. As you create the cluster, use the Cluster Options to enable a PSP. The PSP assigned to the cluster will be the default PSP for projects within the cluster.\n Prerequisite: Create a Pod Security Policy within Rancher.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-provisioning/rke-clusters/options/pod-security-policies/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"38aedb377defe72ff1f54f81c1235226","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/options/pod-security-policies/","postref":"38aedb377defe72ff1f54f81c1235226","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/options/pod-security-policies/","section":"rancher","tags":null,"title":"Assigning Pod Security Policies","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/options/pod-security-policies/","weight":2260,"wordcount":160},{"authors":null,"categories":null,"content":"What is CNI? CNI (Container Network Interface), a Cloud Native Computing Foundation project, consists of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of plugins. CNI concerns itself only with network connectivity of containers and removing allocated resources when the container is deleted.\nKubernetes uses CNI as an interface between network providers and Kubernetes pod networking.\nFor more information visit CNI GitHub project.","date":-62135596800,"description":"Learn about Container Network Interface (CNI), the CNI providers Rancher provides, the features they offer, and how to choose a provider for you","dir":"rancher/v2.x/en/faq/networking/cni-providers/","expirydate":-62135596800,"fuzzywordcount":1300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"4639cdbb1ea40a8b3725dc72219c9356","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/faq/networking/cni-providers/","postref":"4639cdbb1ea40a8b3725dc72219c9356","publishdate":"0001-01-01T00:00:00Z","readingtime":7,"relpermalink":"/docs/rancher/v2.x/en/faq/networking/cni-providers/","section":"rancher","tags":null,"title":"Container Network Interface (CNI) Providers","type":"rancher","url":"/docs/rancher/v2.x/en/faq/networking/cni-providers/","weight":2300,"wordcount":1295},{"authors":null,"categories":null,"content":"When managing an imported cluster, Rancher connects to a Kubernetes cluster that has already been set up. Therefore, Rancher does not provision Kubernetes, but only sets up the Rancher agents to communicate with the cluster.\nKeep in mind that editing your Kubernetes cluster still has to be done outside of Rancher. Some examples of editing the cluster include adding and removing nodes, upgrading the Kubernetes version, and changing Kubernetes component parameters.","date":-62135596800,"description":"Learn how you can create a cluster in Rancher by importing an existing Kubernetes cluster. Then, you can manage it using Rancher","dir":"rancher/v2.x/en/cluster-provisioning/imported-clusters/","expirydate":-62135596800,"fuzzywordcount":500,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"a0d239114f7784346a6327801016daf4","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/imported-clusters/","postref":"a0d239114f7784346a6327801016daf4","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/cluster-provisioning/imported-clusters/","section":"rancher","tags":null,"title":"Importing Existing Clusters into Rancher","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-provisioning/imported-clusters/","weight":2300,"wordcount":404},{"authors":null,"categories":null,"content":"There are two different agent resources deployed on Rancher managed clusters:\n cattle-cluster-agent cattle-node-agent  For a conceptual overview of how the Rancher server provisions clusters and communicates with them, refer to the architecture\ncattle-cluster-agent The cattle-cluster-agent is used to connect to the Kubernetes API of Rancher Launched Kubernetes clusters. The cattle-cluster-agent is deployed using a Deployment resource.\ncattle-node-agent The cattle-node-agent is used to interact with nodes in a Rancher Launched Kubernetes cluster when performing cluster operations.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-provisioning/rke-clusters/rancher-agents/","expirydate":-62135596800,"fuzzywordcount":300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"1994b73f001878e5de400101cd895c4e","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/rancher-agents/","postref":"1994b73f001878e5de400101cd895c4e","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/rancher-agents/","section":"rancher","tags":null,"title":"Rancher Agents","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/rancher-agents/","weight":2400,"wordcount":226},{"authors":null,"categories":null,"content":"Projects are objects introduced in Rancher that help organize namespaces in your Kubernetes cluster. You can use projects to create multi-tenant clusters, which allows a group of users to share the same underlying resources without interacting with each other&rsquo;s applications.\nIn terms of hierarchy:\n Clusters contain projects Projects contain namespaces  Within Rancher, projects allow you to manage multiple namespaces as a single entity. In native Kubernetes, which does not include projects, features like role-based access rights or cluster resources are assigned to individual namespaces.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/project-admin/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"cc891f40ff93c2e6be3aafd4bea10863","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/project-admin/","postref":"cc891f40ff93c2e6be3aafd4bea10863","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/project-admin/","section":"rancher","tags":null,"title":"Project Administration","type":"rancher","url":"/docs/rancher/v2.x/en/project-admin/","weight":2500,"wordcount":330},{"authors":null,"categories":null,"content":"Rancher deploys an agent on each node to communicate with the node. This pages describes the options that can be passed to the agent. To use these options, you will need to create a cluster with custom nodes and add the options to the generated docker run command when adding a node.\nFor an overview of how Rancher communicates with downstream clusters using node agents, refer to the architecture section.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-provisioning/rke-clusters/custom-nodes/agent-options/","expirydate":-62135596800,"fuzzywordcount":500,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"d6b345637af1edf641a0b311c8be0bc2","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/custom-nodes/agent-options/","postref":"d6b345637af1edf641a0b311c8be0bc2","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/custom-nodes/agent-options/","section":"rancher","tags":null,"title":"Rancher Agent Options","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/custom-nodes/agent-options/","weight":2500,"wordcount":429},{"authors":null,"categories":null,"content":"If you want to provide a user with access and permissions to specific projects and resources within a cluster, assign the user a project membership.\nYou can add members to a project as it is created, or add them to an existing project.\n Tip: Want to provide a user with access to all projects within a cluster? See Adding Cluster Members instead.\n Adding Members to a New Project You can add members to a project as you create it (recommended if possible).","date":-62135596800,"description":"","dir":"rancher/v2.x/en/project-admin/project-members/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"659d241a55ced371e820ddb60567fb9d","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/project-admin/project-members/","postref":"659d241a55ced371e820ddb60567fb9d","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/project-admin/project-members/","section":"rancher","tags":null,"title":"Adding Users to Projects","type":"rancher","url":"/docs/rancher/v2.x/en/project-admin/project-members/","weight":2505,"wordcount":368},{"authors":null,"categories":null,"content":"Available as of v2.1.0\nIn situations where several teams share a cluster, one team may overconsume the resources available: CPU, memory, storage, services, Kubernetes objects like pods or secrets, and so on. To prevent this overconsumption, you can apply a resource quota, which is a Rancher feature that limits the resources available to a project or namespace.\nThis page is a how-to guide for creating resource quotas in existing projects.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/project-admin/resource-quotas/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"44cf6e294b630b6165277a8c0e875d99","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/project-admin/resource-quotas/","postref":"44cf6e294b630b6165277a8c0e875d99","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/project-admin/resource-quotas/","section":"rancher","tags":null,"title":"Project Resource Quotas","type":"rancher","url":"/docs/rancher/v2.x/en/project-admin/resource-quotas/","weight":2515,"wordcount":351},{"authors":null,"categories":null,"content":"Within Rancher, you can further divide projects into different namespaces, which are virtual clusters within a project backed by a physical cluster. Should you require another level of organization beyond projects and the default namespace, you can use multiple namespaces to isolate applications and resources.\nAlthough you assign resources at the project level so that each namespace in the project can use them, you can override this inheritance by assigning resources explicitly to a namespace.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/project-admin/namespaces/","expirydate":-62135596800,"fuzzywordcount":600,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"03cdb208e43b88ede35eecb73435fdbe","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/project-admin/namespaces/","postref":"03cdb208e43b88ede35eecb73435fdbe","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/rancher/v2.x/en/project-admin/namespaces/","section":"rancher","tags":null,"title":"Namespaces","type":"rancher","url":"/docs/rancher/v2.x/en/project-admin/namespaces/","weight":2520,"wordcount":568},{"authors":null,"categories":null,"content":"Rancher contains a variety of tools that aren&rsquo;t included in Kubernetes to assist in your DevOps operations. Rancher can integrate with external services to help your clusters run more efficiently. Tools are divided into following categories:  Notifiers and Alerts Logging Monitoring  Notifiers and Alerts Notifiers and alerts are two features that work together to inform you of events in the Rancher system.\nNotifiers are services that inform you of alert events.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/project-admin/tools/","expirydate":-62135596800,"fuzzywordcount":300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"16947df0429561bace3d45e1bb53ebd7","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/project-admin/tools/","postref":"16947df0429561bace3d45e1bb53ebd7","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/project-admin/tools/","section":"rancher","tags":null,"title":"Tools for Logging, Monitoring, and Visibility","type":"rancher","url":"/docs/rancher/v2.x/en/project-admin/tools/","weight":2525,"wordcount":244},{"authors":null,"categories":null,"content":"To keep your clusters and applications healthy and driving your organizational productivity forward, you need to stay informed of events occurring in your clusters and projects, both planned and unplanned. When an event occurs, your alert is triggered, and you are sent a notification. You can then, if necessary, follow up with corrective actions.\nNotifiers and alerts are built on top of the Prometheus Alertmanager. Leveraging these tools, Rancher can notify cluster owners and project owners of events they need to address.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/project-admin/tools/alerts/","expirydate":-62135596800,"fuzzywordcount":1400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"ce4492a177877452c7ac5f8c60ed91ad","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/project-admin/tools/alerts/","postref":"ce4492a177877452c7ac5f8c60ed91ad","publishdate":"0001-01-01T00:00:00Z","readingtime":7,"relpermalink":"/docs/rancher/v2.x/en/project-admin/tools/alerts/","section":"rancher","tags":null,"title":"Alerts","type":"rancher","url":"/docs/rancher/v2.x/en/project-admin/tools/alerts/","weight":2526,"wordcount":1390},{"authors":null,"categories":null,"content":"Rancher can integrate with a variety of popular logging services and tools that exist outside of your Kubernetes clusters.\nFor background information about how logging integrations work, refer to the cluster administration section.\nRancher supports the following services:\n Elasticsearch Splunk Kafka Syslog Fluentd   Note: You can only configure one logging service per cluster or per project.\n Only administrators, cluster owners or members, or project owners can configure Rancher to send Kubernetes logs to a logging service.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/project-admin/tools/logging/","expirydate":-62135596800,"fuzzywordcount":700,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"76abeba68655940a6214b72fa7b50906","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/project-admin/tools/logging/","postref":"76abeba68655940a6214b72fa7b50906","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/docs/rancher/v2.x/en/project-admin/tools/logging/","section":"rancher","tags":null,"title":"Logging","type":"rancher","url":"/docs/rancher/v2.x/en/project-admin/tools/logging/","weight":2527,"wordcount":681},{"authors":null,"categories":null,"content":"Available as of v2.2.4\nUsing Rancher, you can monitor the state and processes of your cluster nodes, Kubernetes components, and software deployments through integration with Prometheus, a leading open-source monitoring solution.\n For more information about how Prometheus works, refer to the cluster administration section.\n This section covers the following topics:\n Monitoring scope Permissions to configure project monitoring Enabling project monitoring Project-level monitoring resource requirements Project metrics  Monitoring Scope Using Prometheus, you can monitor Rancher at both the cluster level and project level.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/project-admin/tools/monitoring/","expirydate":-62135596800,"fuzzywordcount":600,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"e70c03d00a832d6042d5e9cf822a44c8","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/project-admin/tools/monitoring/","postref":"e70c03d00a832d6042d5e9cf822a44c8","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/rancher/v2.x/en/project-admin/tools/monitoring/","section":"rancher","tags":null,"title":"Monitoring","type":"rancher","url":"/docs/rancher/v2.x/en/project-admin/tools/monitoring/","weight":2528,"wordcount":591},{"authors":null,"categories":null,"content":"When your project is set up, project members can start managing their applications and all the components that comprise it.\nWorkloads Deploy applications to your cluster nodes using workloads, which are objects that contain pods that run your apps, along with metadata that set rules for the deployment&rsquo;s behavior. Workloads can be deployed within the scope of the entire clusters or within a namespace.\nWhen deploying a workload, you can deploy from any image.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/k8s-in-rancher/","expirydate":-62135596800,"fuzzywordcount":700,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"417b029dbbc114173f179f6bb58d8355","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/k8s-in-rancher/","postref":"417b029dbbc114173f179f6bb58d8355","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/rancher/v2.x/en/k8s-in-rancher/","section":"rancher","tags":null,"title":"Kubernetes Resources, Registries and Pipelines","type":"rancher","url":"/docs/rancher/v2.x/en/k8s-in-rancher/","weight":3000,"wordcount":611},{"authors":null,"categories":null,"content":"You can build any complex containerized application in Kubernetes using two basic constructs: pods and workloads. Once you build an application, you can expose it for access either within the same cluster or on the Internet using a third construct: services.\nPods Pods are one or more containers that share network namespaces and storage volumes. Most pods have only one container. Therefore when we discuss pods, the term is often synonymous with containers.","date":-62135596800,"description":"Learn about the two constructs with which you can build any complex containerized application in Kubernetes: Kubernetes workloads and pods","dir":"rancher/v2.x/en/k8s-in-rancher/workloads/","expirydate":-62135596800,"fuzzywordcount":600,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"26a1ceb40cbb6b6381a2e7a32a9a44cc","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/k8s-in-rancher/workloads/","postref":"26a1ceb40cbb6b6381a2e7a32a9a44cc","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/rancher/v2.x/en/k8s-in-rancher/workloads/","section":"rancher","tags":null,"title":"Kubernetes Workloads and Pods","type":"rancher","url":"/docs/rancher/v2.x/en/k8s-in-rancher/workloads/","weight":3025,"wordcount":537},{"authors":null,"categories":null,"content":"Deploy a workload to run an application in one or more containers.\n From the Global view, open the project that you want to deploy a workload to.\n  Click Resources &gt; Workloads. (In versions prior to v2.3.0, click the Workloads tab.) From the Workloads view, click Deploy.  Enter a Name for the workload.\n Select a workload type. The workload defaults to a scalable deployment, by can change the workload type by clicking More options.","date":-62135596800,"description":"Read this step by step guide for deploying workloads. Deploy a workload to run an application in one or more containers.","dir":"rancher/v2.x/en/k8s-in-rancher/workloads/deploy-workloads/","expirydate":-62135596800,"fuzzywordcount":500,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"ddd3938332175936b65cca22c0804337","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/k8s-in-rancher/workloads/deploy-workloads/","postref":"ddd3938332175936b65cca22c0804337","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/rancher/v2.x/en/k8s-in-rancher/workloads/deploy-workloads/","section":"rancher","tags":null,"title":"Deploying Workloads","type":"rancher","url":"/docs/rancher/v2.x/en/k8s-in-rancher/workloads/deploy-workloads/","weight":3026,"wordcount":437},{"authors":null,"categories":null,"content":"The Horizontal Pod Autoscaler (HPA) is a Kubernetes feature that allows you to configure your cluster to automatically scale the services it&rsquo;s running up or down.\nRancher provides some additional features to help manage HPAs, depending on the version of Rancher.\nYou can create, manage, and delete HPAs using the Rancher UI in Rancher v2.3.0-alpha4 and higher versions. It only supports HPA in the autoscaling/v2beta2 API.\nManaging HPAs The way that you manage HPAs is different based on your version of the Kubernetes API:","date":-62135596800,"description":"Learn about the horizontal pod autoscaler (HPA). How to manage HPAs and how to test them with a service deployment","dir":"rancher/v2.x/en/k8s-in-rancher/horitzontal-pod-autoscaler/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"e3d1ec1b6c76ab60a53b51e936025197","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/k8s-in-rancher/horitzontal-pod-autoscaler/","postref":"e3d1ec1b6c76ab60a53b51e936025197","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/k8s-in-rancher/horitzontal-pod-autoscaler/","section":"rancher","tags":null,"title":"The Horizontal Pod Autoscaler","type":"rancher","url":"/docs/rancher/v2.x/en/k8s-in-rancher/horitzontal-pod-autoscaler/","weight":3026,"wordcount":395},{"authors":null,"categories":null,"content":"The Horizontal Pod Autoscaler (HPA) is a Kubernetes feature that allows you to configure your cluster to automatically scale the services it&rsquo;s running up or down. This section provides explanation on how HPA works with Kubernetes.\nWhy Use Horizontal Pod Autoscaler? Using HPA, you can automatically scale the number of pods within a replication controller, deployment, or replica set up or down. HPA automatically scales the number of pods that are running for maximum efficiency.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/k8s-in-rancher/horitzontal-pod-autoscaler/hpa-background/","expirydate":-62135596800,"fuzzywordcount":300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"2bceda28489952ebb59fb3f0c8c863ce","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/k8s-in-rancher/horitzontal-pod-autoscaler/hpa-background/","postref":"2bceda28489952ebb59fb3f0c8c863ce","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/k8s-in-rancher/horitzontal-pod-autoscaler/hpa-background/","section":"rancher","tags":null,"title":"Background Information on HPAs","type":"rancher","url":"/docs/rancher/v2.x/en/k8s-in-rancher/horitzontal-pod-autoscaler/hpa-background/","weight":3027,"wordcount":284},{"authors":null,"categories":null,"content":"Sometimes there is a need to rollback to the previous version of the application, either for debugging purposes or because an upgrade did not go as planned.\n From the Global view, open the project running the workload you want to rollback.\n Find the workload that you want to rollback and select Vertical Ellipsis (&hellip; ) &gt; Rollback.\n Choose the revision that you want to roll back to. Click Rollback.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/k8s-in-rancher/workloads/rollback-workloads/","expirydate":-62135596800,"fuzzywordcount":100,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"132a534810cf94271256afde9bd751f3","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/k8s-in-rancher/workloads/rollback-workloads/","postref":"132a534810cf94271256afde9bd751f3","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rancher/v2.x/en/k8s-in-rancher/workloads/rollback-workloads/","section":"rancher","tags":null,"title":"Rolling Back Workloads","type":"rancher","url":"/docs/rancher/v2.x/en/k8s-in-rancher/workloads/rollback-workloads/","weight":3027,"wordcount":89},{"authors":null,"categories":null,"content":"Available as of v2.3.0\nThe Rancher UI supports creating, managing, and deleting HPAs. You can configure CPU or memory usage as the metric that the HPA uses to scale.\nIf you want to create HPAs that scale based on other metrics than CPU and memory, refer to Configuring HPA to Scale Using Custom Metrics with Prometheus.\nCreating an HPA  From the Global view, open the project that you want to deploy a HPA to.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/k8s-in-rancher/horitzontal-pod-autoscaler/manage-hpa-with-rancher-ui/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"1b7109c17a4781bb1a48e876947a32ca","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/k8s-in-rancher/horitzontal-pod-autoscaler/manage-hpa-with-rancher-ui/","postref":"1b7109c17a4781bb1a48e876947a32ca","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/k8s-in-rancher/horitzontal-pod-autoscaler/manage-hpa-with-rancher-ui/","section":"rancher","tags":null,"title":"Managing HPAs with the Rancher UI","type":"rancher","url":"/docs/rancher/v2.x/en/k8s-in-rancher/horitzontal-pod-autoscaler/manage-hpa-with-rancher-ui/","weight":3028,"wordcount":313},{"authors":null,"categories":null,"content":"When a new version of an application image is released on Docker Hub, you can upgrade any workloads running a previous version of the application to the new one.\n From the Global view, open the project running the workload you want to upgrade.\n Find the workload that you want to upgrade and select Vertical Ellipsis (&hellip; ) &gt; Edit.\n Update the Docker Image to the updated version of the application image on Docker Hub.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/k8s-in-rancher/workloads/upgrade-workloads/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"da6f3d42488ab2c374ee507e84fc5a5c","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/k8s-in-rancher/workloads/upgrade-workloads/","postref":"da6f3d42488ab2c374ee507e84fc5a5c","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rancher/v2.x/en/k8s-in-rancher/workloads/upgrade-workloads/","section":"rancher","tags":null,"title":"Upgrading Workloads","type":"rancher","url":"/docs/rancher/v2.x/en/k8s-in-rancher/workloads/upgrade-workloads/","weight":3028,"wordcount":162},{"authors":null,"categories":null,"content":"A sidecar is a container that extends or enhances the main container in a pod. The main container and the sidecar share a pod, and therefore share the same network space and storage. You can add sidecars to existing workloads by using the Add a Sidecar option.\n From the Global view, open the project running the workload you want to add a sidecar to.\n Click Resources &gt; Workloads.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/k8s-in-rancher/workloads/add-a-sidecar/","expirydate":-62135596800,"fuzzywordcount":300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"68d0d0692d0397b5991a632b1ad15e82","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/k8s-in-rancher/workloads/add-a-sidecar/","postref":"68d0d0692d0397b5991a632b1ad15e82","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/k8s-in-rancher/workloads/add-a-sidecar/","section":"rancher","tags":null,"title":"Adding a Sidecar","type":"rancher","url":"/docs/rancher/v2.x/en/k8s-in-rancher/workloads/add-a-sidecar/","weight":3029,"wordcount":236},{"authors":null,"categories":null,"content":"This section describes HPA management with kubectl. This document has instructions for how to:\n Create an HPA Get information on HPAs Delete an HPA Configure your HPAs to scale with CPU or memory utilization Configure your HPAs to scale using custom metrics, if you use a third-party tool such as Prometheus for metrics  Note For Rancher v2.3.x In Rancher v2.3.x, you can create, view, and delete HPAs from the Rancher UI.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/k8s-in-rancher/horitzontal-pod-autoscaler/manage-hpa-with-kubectl/","expirydate":-62135596800,"fuzzywordcount":1100,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"0588b11e2de99b957d2df1392987f86f","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/k8s-in-rancher/horitzontal-pod-autoscaler/manage-hpa-with-kubectl/","postref":"0588b11e2de99b957d2df1392987f86f","publishdate":"0001-01-01T00:00:00Z","readingtime":6,"relpermalink":"/docs/rancher/v2.x/en/k8s-in-rancher/horitzontal-pod-autoscaler/manage-hpa-with-kubectl/","section":"rancher","tags":null,"title":"Managing HPAs with kubectl","type":"rancher","url":"/docs/rancher/v2.x/en/k8s-in-rancher/horitzontal-pod-autoscaler/manage-hpa-with-kubectl/","weight":3029,"wordcount":1080},{"authors":null,"categories":null,"content":"This document describes how to check the status of your HPAs after scaling them up or down with your load testing tool. For information on how to check the status from the Rancher UI (at least version 2.3.x), refer to Managing HPAs with the Rancher UI.\nFor HPA to work correctly, service deployments should have resources request definitions for containers. Follow this hello-world example to test if HPA is working correctly.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/k8s-in-rancher/horitzontal-pod-autoscaler/testing-hpa/","expirydate":-62135596800,"fuzzywordcount":2300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"c328f68ee7053ae9b22612c1ee05395d","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/k8s-in-rancher/horitzontal-pod-autoscaler/testing-hpa/","postref":"c328f68ee7053ae9b22612c1ee05395d","publishdate":"0001-01-01T00:00:00Z","readingtime":11,"relpermalink":"/docs/rancher/v2.x/en/k8s-in-rancher/horitzontal-pod-autoscaler/testing-hpa/","section":"rancher","tags":null,"title":"Testing HPAs with kubectl","type":"rancher","url":"/docs/rancher/v2.x/en/k8s-in-rancher/horitzontal-pod-autoscaler/testing-hpa/","weight":3031,"wordcount":2207},{"authors":null,"categories":null,"content":"Within Rancher, you can set up load balancers and ingress controllers to redirect service requests.\nLoad Balancers After you launch an application, the app is only available within the cluster. It can&rsquo;t be reached from outside the cluster.\nIf you want your applications to be externally accessible, you must add a load balancer or ingress to your cluster. Load balancers create a gateway for external connections to access your cluster, provided that the user knows the load balancer&rsquo;s IP address and the application&rsquo;s port number.","date":-62135596800,"description":"Learn how you can set up load balancers and ingress controllers to redirect service requests within Rancher, and learn about the limitations of load balancers","dir":"rancher/v2.x/en/k8s-in-rancher/load-balancers-and-ingress/","expirydate":-62135596800,"fuzzywordcount":600,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"d6f02e2b1d65a2e3e5b431700f0e44bd","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/k8s-in-rancher/load-balancers-and-ingress/","postref":"d6f02e2b1d65a2e3e5b431700f0e44bd","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/rancher/v2.x/en/k8s-in-rancher/load-balancers-and-ingress/","section":"rancher","tags":null,"title":"Set Up Load Balancer and Ingress Controller within Rancher","type":"rancher","url":"/docs/rancher/v2.x/en/k8s-in-rancher/load-balancers-and-ingress/","weight":3040,"wordcount":589},{"authors":null,"categories":null,"content":"Kubernetes supports load balancing in two ways: Layer-4 Load Balancing and Layer-7 Load Balancing.\nLayer-4 Load Balancer Layer-4 load balancer (or the external load balancer) forwards traffic to Nodeports. Layer-4 load balancer allows you to forward both HTTP and TCP traffic.\nOften, the Layer-4 load balancer is supported by the underlying cloud provider, so when you deploy RKE clusters on bare-metal servers and vSphere clusters, Layer-4 load balancer is not supported.","date":-62135596800,"description":"Kubernetes supports load balancing in two ways: Layer-4 Load Balancing and Layer-7 Load Balancing. Learn about the support for each way in different deployments","dir":"rancher/v2.x/en/k8s-in-rancher/load-balancers-and-ingress/load-balancers/","expirydate":-62135596800,"fuzzywordcount":600,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"d035bd70f3f860902720e484c10610cd","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/k8s-in-rancher/load-balancers-and-ingress/load-balancers/","postref":"d035bd70f3f860902720e484c10610cd","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/rancher/v2.x/en/k8s-in-rancher/load-balancers-and-ingress/load-balancers/","section":"rancher","tags":null,"title":"Layer 4 and Layer 7 Load Balancing","type":"rancher","url":"/docs/rancher/v2.x/en/k8s-in-rancher/load-balancers-and-ingress/load-balancers/","weight":3041,"wordcount":572},{"authors":null,"categories":null,"content":"Ingress can be added for workloads to provide load balancing, SSL termination and host/path based routing. When using ingresses in a project, you can program the ingress hostname to an external DNS by setting up a Global DNS entry.\n From the Global view, open the project that you want to add ingress to.\n Click Resources in the main navigation bar. Click the Load Balancing tab. (In versions prior to v2.","date":-62135596800,"description":"Ingresses can be added for workloads to provide load balancing, SSL termination and host/path-based routing. Learn how to add Rancher ingress to your project","dir":"rancher/v2.x/en/k8s-in-rancher/load-balancers-and-ingress/ingress/","expirydate":-62135596800,"fuzzywordcount":700,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"6a9a4f3a2b4ba5336d61fbc0ba8e2085","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/k8s-in-rancher/load-balancers-and-ingress/ingress/","postref":"6a9a4f3a2b4ba5336d61fbc0ba8e2085","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/docs/rancher/v2.x/en/k8s-in-rancher/load-balancers-and-ingress/ingress/","section":"rancher","tags":null,"title":"Adding Ingresses to Your Project","type":"rancher","url":"/docs/rancher/v2.x/en/k8s-in-rancher/load-balancers-and-ingress/ingress/","weight":3042,"wordcount":644},{"authors":null,"categories":null,"content":"For every workload created, a complementing Service Discovery entry is created. This Service Discovery entry enables DNS resolution for the workload&rsquo;s pods using the following naming convention: &lt;workload&gt;.&lt;namespace&gt;.svc.cluster.local.\nHowever, you also have the option of creating additional Service Discovery records. You can use these additional records so that a given namespace resolves with one or more external IP addresses, an external hostname, an alias to another DNS record, other workloads, or a set of pods that match a selector that you create.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/k8s-in-rancher/service-discovery/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"3dfae7f0f1d57bb813bf43a5e4f0e0e4","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/k8s-in-rancher/service-discovery/","postref":"3dfae7f0f1d57bb813bf43a5e4f0e0e4","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/k8s-in-rancher/service-discovery/","section":"rancher","tags":null,"title":"Service Discovery","type":"rancher","url":"/docs/rancher/v2.x/en/k8s-in-rancher/service-discovery/","weight":3045,"wordcount":320},{"authors":null,"categories":null,"content":"Notes:\n Pipelines are new and improved for Rancher v2.1! Therefore, if you configured pipelines while using v2.0.x, you&rsquo;ll have to reconfigure them after upgrading to v2.1. Still using v2.0.x? See the pipeline documentation for previous versions.   Before setting up any pipelines, review the pipeline overview and ensure that the project has configured authentication to your version control provider, e.g. GitHub, GitLab, Bitbucket. If you haven&rsquo;t configured a version control provider, you can always use Rancher&rsquo;s example repositories to view some common pipeline deployments.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/k8s-in-rancher/pipelines/","expirydate":-62135596800,"fuzzywordcount":3900,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"f93f7f63067f68b2e728b9b3efd61c8a","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/k8s-in-rancher/pipelines/","postref":"f93f7f63067f68b2e728b9b3efd61c8a","publishdate":"0001-01-01T00:00:00Z","readingtime":18,"relpermalink":"/docs/rancher/v2.x/en/k8s-in-rancher/pipelines/","section":"rancher","tags":null,"title":"Pipelines","type":"rancher","url":"/docs/rancher/v2.x/en/k8s-in-rancher/pipelines/","weight":3047,"wordcount":3833},{"authors":null,"categories":null,"content":"This section describes how to manually install HPAs for clusters created with Rancher prior to v2.0.7. This section also describes how to configure your HPA to scale up or down, and how to assign roles to your HPA.\nBefore you can use HPA in your Kubernetes cluster, you must fulfill some requirements.\nRequirements Be sure that your Kubernetes cluster services are running with these flags at minimum:\n kube-api: requestheader-client-ca-file kubelet: read-only-port at 10255 kube-controller: Optional, just needed if distinct values than default are required.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/k8s-in-rancher/horitzontal-pod-autoscaler/hpa-for-rancher-before-2_0_7/","expirydate":-62135596800,"fuzzywordcount":800,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"2286f7dc4b2216f12badbe94a4ad20ea","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/k8s-in-rancher/horitzontal-pod-autoscaler/hpa-for-rancher-before-2_0_7/","postref":"2286f7dc4b2216f12badbe94a4ad20ea","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/docs/rancher/v2.x/en/k8s-in-rancher/horitzontal-pod-autoscaler/hpa-for-rancher-before-2_0_7/","section":"rancher","tags":null,"title":"Manual HPA Installation for Clusters Created Before Rancher v2.0.7","type":"rancher","url":"/docs/rancher/v2.x/en/k8s-in-rancher/horitzontal-pod-autoscaler/hpa-for-rancher-before-2_0_7/","weight":3050,"wordcount":746},{"authors":null,"categories":null,"content":"This section describes how to set up Amazon&rsquo;s Elastic Block Store in EC2.\n From the EC2 console, go to the ELASTIC BLOCK STORE section in the left panel and click Volumes. Click Create Volume. Optional: Configure the size of the volume or other options. The volume should be created in the same availability zone as the instance it will be attached to. Click Create Volume. Click Close.  Result: Persistent storage has been created.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-admin/volumes-and-storage/examples/ebs/","expirydate":-62135596800,"fuzzywordcount":100,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"9772e6d82ea9733a85465ff26002ee21","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/volumes-and-storage/examples/ebs/","postref":"9772e6d82ea9733a85465ff26002ee21","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rancher/v2.x/en/cluster-admin/volumes-and-storage/examples/ebs/","section":"rancher","tags":null,"title":"Creating Persistent Storage in Amazon's EBS","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-admin/volumes-and-storage/examples/ebs/","weight":3053,"wordcount":95},{"authors":null,"categories":null,"content":"Rancher supports persistent storage with a variety of volume plugins. However, before you use any of these plugins to bind persistent storage to your workloads, you have to configure the storage itself, whether its a cloud-based solution from a service-provider or an on-prem solution that you manage yourself.\nFor your convenience, Rancher offers documentation on how to configure some of the popular storage methods:\n NFS vSphere  ","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-admin/volumes-and-storage/examples/","expirydate":-62135596800,"fuzzywordcount":100,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"67ee1d819e17c952faca9470a099b7fb","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/volumes-and-storage/examples/","postref":"67ee1d819e17c952faca9470a099b7fb","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rancher/v2.x/en/cluster-admin/volumes-and-storage/examples/","section":"rancher","tags":null,"title":"Provisioning Storage Examples","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-admin/volumes-and-storage/examples/","weight":3053,"wordcount":66},{"authors":null,"categories":null,"content":"Before you can use the NFS storage volume plug-in with Rancher deployments, you need to provision an NFS server.\n Note:\n If you already have an NFS share, you don&rsquo;t need to provision a new NFS server to use the NFS volume plugin within Rancher. Instead, skip the rest of this procedure and complete adding storage.\n This procedure demonstrates how to set up an NFS server using Ubuntu, although you should be able to use these instructions for other Linux distros (e.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-admin/volumes-and-storage/examples/nfs/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"b8cf628825465deb75496585c4604e6f","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/volumes-and-storage/examples/nfs/","postref":"b8cf628825465deb75496585c4604e6f","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/cluster-admin/volumes-and-storage/examples/nfs/","section":"rancher","tags":null,"title":"NFS Storage","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-admin/volumes-and-storage/examples/nfs/","weight":3054,"wordcount":397},{"authors":null,"categories":null,"content":"To provide stateful workloads with vSphere storage, we recommend creating a vSphereVolume storage class. This practice dynamically provisions vSphere storage when workloads request volumes through a persistent volume claim.\nPrerequisites In order to provision vSphere volumes in a cluster created with the Rancher Kubernetes Engine (RKE), the vSphere cloud provider must be explicitly enabled in the cluster options.\nCreating A Storage Class  Note:\nThe following steps can also be performed using the kubectl command line tool.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-admin/volumes-and-storage/examples/vsphere/","expirydate":-62135596800,"fuzzywordcount":600,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"78922f3997ce405bec5de33a918c2628","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/volumes-and-storage/examples/vsphere/","postref":"78922f3997ce405bec5de33a918c2628","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/rancher/v2.x/en/cluster-admin/volumes-and-storage/examples/vsphere/","section":"rancher","tags":null,"title":"vSphere Storage","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-admin/volumes-and-storage/examples/vsphere/","weight":3055,"wordcount":563},{"authors":null,"categories":null,"content":"When you create an ingress within Rancher/Kubernetes, you must provide it with a secret that includes a TLS private key and certificate, which are used to encrypt and decrypt communications that come through the ingress. You can make certificates available for ingress use by navigating to its project or namespace, and then uploading the certificate. You can then add the certificate to the ingress deployment.\nAdd SSL certificates to either projects, namespaces, or both.","date":-62135596800,"description":"Learn how to add an SSL (Secure Sockets Layer) certificate or TLS (Transport Layer Security) certificate to either a project, a namespace, or both, so that you can add it to deployments","dir":"rancher/v2.x/en/k8s-in-rancher/certificates/","expirydate":-62135596800,"fuzzywordcount":500,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"71f4475714b3f369ca7cea1bee02cb62","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/k8s-in-rancher/certificates/","postref":"71f4475714b3f369ca7cea1bee02cb62","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/rancher/v2.x/en/k8s-in-rancher/certificates/","section":"rancher","tags":null,"title":"Encrypting HTTP Communication","type":"rancher","url":"/docs/rancher/v2.x/en/k8s-in-rancher/certificates/","weight":3060,"wordcount":473},{"authors":null,"categories":null,"content":"While most types of Kubernetes secrets store sensitive information, ConfigMaps store general configuration information, such as a group of config files. Because ConfigMaps don&rsquo;t store sensitive information, they can be updated automatically, and therefore don&rsquo;t require their containers to be restarted following update (unlike most secret types, which require manual updates and a container restart to take effect).\nConfigMaps accept key value pairs in common string formats, like config files or JSON blobs.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/k8s-in-rancher/configmaps/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"da0df7b22b75b48e60570ac69f6334fe","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/k8s-in-rancher/configmaps/","postref":"da0df7b22b75b48e60570ac69f6334fe","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/k8s-in-rancher/configmaps/","section":"rancher","tags":null,"title":"ConfigMaps","type":"rancher","url":"/docs/rancher/v2.x/en/k8s-in-rancher/configmaps/","weight":3061,"wordcount":352},{"authors":null,"categories":null,"content":"Secrets store sensitive data like passwords, tokens, or keys. They may contain one or more key value pairs.\n This page is about secrets in general. For details on setting up a private registry, refer to the section on registries.\n When configuring a workload, you&rsquo;ll be able to choose which secrets to include. Like config maps, secrets can be referenced by workloads as either an environment variable or a volume mount.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/k8s-in-rancher/secrets/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"b6d1bbafa14970d98df1ce5656fb614c","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/k8s-in-rancher/secrets/","postref":"b6d1bbafa14970d98df1ce5656fb614c","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/k8s-in-rancher/secrets/","section":"rancher","tags":null,"title":"Secrets","type":"rancher","url":"/docs/rancher/v2.x/en/k8s-in-rancher/secrets/","weight":3062,"wordcount":346},{"authors":null,"categories":null,"content":"Registries are Kubernetes secrets containing credentials used to authenticate with private Docker registries.\nThe word &ldquo;registry&rdquo; can mean two things, depending on whether it is used to refer to a Docker or Kubernetes registry:\n A Docker registry contains Docker images that you can pull in order to use them in your deployment. The registry is a stateless, scalable server side application that stores and lets you distribute Docker images.","date":-62135596800,"description":"Learn about the Docker registry and Kubernetes registry, their use cases and how to use a private registry with the Rancher UI","dir":"rancher/v2.x/en/k8s-in-rancher/registries/","expirydate":-62135596800,"fuzzywordcount":800,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"73ee3371b3a27bc5f7c79acaddaaa94e","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/k8s-in-rancher/registries/","postref":"73ee3371b3a27bc5f7c79acaddaaa94e","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/docs/rancher/v2.x/en/k8s-in-rancher/registries/","section":"rancher","tags":null,"title":"Kubernetes Registry and Docker Registry","type":"rancher","url":"/docs/rancher/v2.x/en/k8s-in-rancher/registries/","weight":3063,"wordcount":776},{"authors":null,"categories":null,"content":"Available as of v2.3.0\nUsing Rancher, you can connect, secure, control, and observe services through integration with Istio, a leading open-source service mesh solution. Istio provides behavioral insights and operational control over the service mesh as a whole, offering a complete solution to satisfy the diverse requirements of microservice applications.\nThis service mesh provides features that include but are not limited to the following:\n Traffic management features Enhanced monitoring and tracing Service discovery and routing Secure connections and service-to-service authentication with mutual TLS Load balancing Automatic retries, backoff, and circuit breaking  Istio needs to be set up by a Rancher administrator or cluster administrator before it can be used in a project for comprehensive data visualizations, traffic management, or any of its other features.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/project-admin/istio/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"a4c38d4f9648ed268cb54426888a3347","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/project-admin/istio/","postref":"a4c38d4f9648ed268cb54426888a3347","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rancher/v2.x/en/project-admin/istio/","section":"rancher","tags":null,"title":"Istio","type":"rancher","url":"/docs/rancher/v2.x/en/project-admin/istio/","weight":3528,"wordcount":145},{"authors":null,"categories":null,"content":"There are default global catalogs packaged as part of Rancher.\nManaging Built-in Global Catalogs  Prerequisites: In order to manage the built-in catalogs or manage global catalogs, you need one of the following permissions:\n Administrator Global Permissions Custom Global Permissions with the Manage Catalogs role assigned.    From the Global view, choose Tools &gt; Catalogs in the navigation bar. In versions prior to v2.2.0, you can select Catalogs directly in the navigation bar.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/catalog/built-in/","expirydate":-62135596800,"fuzzywordcount":300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"b71218c79399eb8923d5df91d742bdf2","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/catalog/built-in/","postref":"b71218c79399eb8923d5df91d742bdf2","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/catalog/built-in/","section":"rancher","tags":null,"title":"Built-in Global Catalogs","type":"rancher","url":"/docs/rancher/v2.x/en/catalog/built-in/","weight":4000,"wordcount":241},{"authors":null,"categories":null,"content":"Rancher provides the ability to use a catalog of Helm charts that make it easy to repeatedly deploy applications.\n Catalogs are GitHub repositories or Helm Chart repositories filled with applications that are ready-made for deployment. Applications are bundled in objects called Helm charts. Helm charts are a collection of files that describe a related set of Kubernetes resources. A single chart might be used to deploy something simple, like a memcached pod, or something complex, like a full web app stack with HTTP servers, databases, caches, and so on.","date":-62135596800,"description":"Rancher enables the use of catalogs to repeatedly deploy applications easily. Catalogs are GitHub or Helm Chart repositories filled with deployment-ready apps.","dir":"rancher/v2.x/en/catalog/","expirydate":-62135596800,"fuzzywordcount":1500,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"a28cf81f6f8bcfd4d4960c063a6fea98","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/catalog/","postref":"a28cf81f6f8bcfd4d4960c063a6fea98","publishdate":"0001-01-01T00:00:00Z","readingtime":7,"relpermalink":"/docs/rancher/v2.x/en/catalog/","section":"rancher","tags":null,"title":"Catalogs, Helm Charts and Apps","type":"rancher","url":"/docs/rancher/v2.x/en/catalog/","weight":4000,"wordcount":1407},{"authors":null,"categories":null,"content":"Rancher&rsquo;s catalog service requires any custom catalogs to be structured in a specific format for the catalog service to be able to leverage it in Rancher.\nChart Types Rancher supports two different types of charts:\n Helm Charts\nNative Helm charts include an application along with other software required to run it. When deploying native Helm charts, you&rsquo;ll learn the chart&rsquo;s parameters and then configure them using Answers, which are sets of key value pairs.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/catalog/custom/creating/","expirydate":-62135596800,"fuzzywordcount":1200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"71256de4b7790163f5709fcb661e7182","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/catalog/custom/creating/","postref":"71256de4b7790163f5709fcb661e7182","publishdate":"0001-01-01T00:00:00Z","readingtime":6,"relpermalink":"/docs/rancher/v2.x/en/catalog/custom/creating/","section":"rancher","tags":null,"title":"Creating Custom Catalogs Apps","type":"rancher","url":"/docs/rancher/v2.x/en/catalog/custom/creating/","weight":4000,"wordcount":1175},{"authors":null,"categories":null,"content":"Using Rancher, you can integrate with a GitHub repository to setup a continuous integration (CI) pipeline.\nTo set up a pipeline, you&rsquo;ll first need to authorize Rancher using your GitHub settings. Directions are provided in the Rancher UI. After authorizing Rancher in GitHub, provide Rancher with a client ID and secret to authenticate.\nAfter configuring Rancher and GitHub, you can deploy containers running Jenkins to automate a pipeline execution:","date":-62135596800,"description":"Use Rancher’s CI/CD pipeline to automatically checkout code, run builds or scripts, publish Docker images, and deploy software to users","dir":"rancher/v2.x/en/project-admin/pipelines/","expirydate":-62135596800,"fuzzywordcount":2200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"e19e082a642566a02a148a3929ff4b9c","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/project-admin/pipelines/","postref":"e19e082a642566a02a148a3929ff4b9c","publishdate":"0001-01-01T00:00:00Z","readingtime":11,"relpermalink":"/docs/rancher/v2.x/en/project-admin/pipelines/","section":"rancher","tags":null,"title":"Rancher's CI/CD Pipelines","type":"rancher","url":"/docs/rancher/v2.x/en/project-admin/pipelines/","weight":4000,"wordcount":2140},{"authors":null,"categories":null,"content":"Custom catalogs can be added into Rancher at any scope of Rancher.\nAdding Global Catalogs  Prerequisites: In order to manage the built-in catalogs or manage global catalogs, you need one of the following permissions:\n Administrator Global Permissions Custom Global Permissions with the Manage Catalogs role assigned.    From the Global view, choose Tools &gt; Catalogs in the navigation bar. In versions prior to v2.2.0, you can select Catalogs directly in the navigation bar.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/catalog/custom/adding/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"2d2f3b7f823cc71f4424ecc237fbd58c","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/catalog/custom/adding/","postref":"2d2f3b7f823cc71f4424ecc237fbd58c","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/catalog/custom/adding/","section":"rancher","tags":null,"title":"Adding Custom Catalogs","type":"rancher","url":"/docs/rancher/v2.x/en/catalog/custom/adding/","weight":4005,"wordcount":393},{"authors":null,"categories":null,"content":"Any user can create custom catalogs to add into Rancher. Besides the content of the catalog, users must ensure their catalogs are able to be added into Rancher.\nTypes of Repositories Rancher supports adding in different types of repositories as a catalog:\n Custom Git Repository Custom Helm Chart Repository  Custom Git Repository The Git URL needs to be one that git clone can handle and must end in .","date":-62135596800,"description":"","dir":"rancher/v2.x/en/catalog/custom/","expirydate":-62135596800,"fuzzywordcount":500,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"2486d4c95976c3276219cb777571c5a9","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/catalog/custom/","postref":"2486d4c95976c3276219cb777571c5a9","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/catalog/custom/","section":"rancher","tags":null,"title":"Custom Catalogs","type":"rancher","url":"/docs/rancher/v2.x/en/catalog/custom/","weight":4020,"wordcount":422},{"authors":null,"categories":null,"content":"This section only applies to RKE clusters.\n In clusters that store data on GlusterFS volumes, you may experience an issue where pods fail to mount volumes after restarting the kubelet. The logging of the kubelet will show: transport endpoint is not connected. To prevent this from happening, you can configure your cluster to mount the systemd-run binary in the kubelet container. There are two requirements before you can change the cluster configuration:","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-admin/volumes-and-storage/glusterfs-volumes/","expirydate":-62135596800,"fuzzywordcount":300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"e45eeb88585e63b682445136dcadc4ce","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/volumes-and-storage/glusterfs-volumes/","postref":"e45eeb88585e63b682445136dcadc4ce","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/cluster-admin/volumes-and-storage/glusterfs-volumes/","section":"rancher","tags":null,"title":"GlusterFS Volumes","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-admin/volumes-and-storage/glusterfs-volumes/","weight":5000,"wordcount":236},{"authors":null,"categories":null,"content":"Available as of v2.2.0\nTypically, most applications are deployed on a single Kubernetes cluster, but there will be times you might want to deploy multiple copies of the same application across different clusters and/or projects. In Rancher, a multi-cluster application, is an application deployed using a Helm chart across multiple clusters. With the ability to deploy the same application across multiple clusters, it avoids the repetition of the same action on each cluster, which could introduce user error during application configuration.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/catalog/multi-cluster-apps/","expirydate":-62135596800,"fuzzywordcount":1900,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"c8653ee7945301ea0a3fed89d729be8f","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/catalog/multi-cluster-apps/","postref":"c8653ee7945301ea0a3fed89d729be8f","publishdate":"0001-01-01T00:00:00Z","readingtime":9,"relpermalink":"/docs/rancher/v2.x/en/catalog/multi-cluster-apps/","section":"rancher","tags":null,"title":"Multi-Cluster Apps","type":"rancher","url":"/docs/rancher/v2.x/en/catalog/multi-cluster-apps/","weight":5000,"wordcount":1892},{"authors":null,"categories":null,"content":"Within a project, when you want to deploy applications from catalogs, the applications available in your project will be based on the scope of the catalogs.\nIf your application is using ingresses, you can program the ingress hostname to an external DNS by setting up a Global DNS entry.\nPrerequisites To create a multi-cluster app in Rancher, you must have at least one of the following permissions:\n A project-member role in the target cluster, which gives you the ability to create, read, update, and delete the workloads A cluster owner role for the cluster that include the target project  Launching Catalog Applications After you&rsquo;ve either enabled the built-in global catalogs or added your own custom catalog, you can start launching catalog applications.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/catalog/apps/","expirydate":-62135596800,"fuzzywordcount":1200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"f6053699d5fc33a05e89528d013aa7b0","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/catalog/apps/","postref":"f6053699d5fc33a05e89528d013aa7b0","publishdate":"0001-01-01T00:00:00Z","readingtime":6,"relpermalink":"/docs/rancher/v2.x/en/catalog/apps/","section":"rancher","tags":null,"title":"Apps in a Project","type":"rancher","url":"/docs/rancher/v2.x/en/catalog/apps/","weight":5005,"wordcount":1197},{"authors":null,"categories":null,"content":"Available as of v2.2.0\nRancher&rsquo;s Global DNS feature provides a way to program an external DNS provider to route traffic to your Kubernetes applications. Since the DNS programming supports spanning applications across different Kubernetes clusters, Global DNS is configured at a global level. An application can become highly available as it allows you to have one application run on different Kubernetes clusters. If one of your Kubernetes clusters goes down, the application would still be accessible.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/catalog/globaldns/","expirydate":-62135596800,"fuzzywordcount":1200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"a6fb259c31d24b0bce4a61345fb0519a","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/catalog/globaldns/","postref":"a6fb259c31d24b0bce4a61345fb0519a","publishdate":"0001-01-01T00:00:00Z","readingtime":6,"relpermalink":"/docs/rancher/v2.x/en/catalog/globaldns/","section":"rancher","tags":null,"title":"Global DNS","type":"rancher","url":"/docs/rancher/v2.x/en/catalog/globaldns/","weight":5010,"wordcount":1193},{"authors":null,"categories":null,"content":"These cluster options are only available for clusters in which Rancher has launched Kubernetes.\n You can always assign a pod security policy (PSP) to an existing project if you didn&rsquo;t assign one during creation.\nPrerequisites  Create a Pod Security Policy within Rancher. Before you can assign a default PSP to an existing project, you must have a PSP available for assignment. For instruction, see Creating Pod Security Policies.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/project-admin/pod-security-policies/","expirydate":-62135596800,"fuzzywordcount":300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"9d1ff3c2b091a51168519363a6127c44","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/project-admin/pod-security-policies/","postref":"9d1ff3c2b091a51168519363a6127c44","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/project-admin/pod-security-policies/","section":"rancher","tags":null,"title":"Pod Security Policies","type":"rancher","url":"/docs/rancher/v2.x/en/project-admin/pod-security-policies/","weight":5600,"wordcount":260},{"authors":null,"categories":null,"content":"The Rancher CLI (Command Line Interface) is a unified tool that you can use to interact with Rancher. With this tool, you can operate Rancher using a command line rather than the GUI.\nDownload Rancher CLI The binary can be downloaded directly from the UI. The link can be found in the right hand side of the footer in the UI. We have binaries for Windows, Mac, and Linux. You can also check the releases page for our CLI for direct downloads of the binary.","date":-62135596800,"description":"The Rancher CLI is a unified tool that you can use to interact with Rancher. With it, you can operate Rancher using a command line interface rather than the GUI","dir":"rancher/v2.x/en/cli/","expirydate":-62135596800,"fuzzywordcount":500,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"99358bc407a164ba491e9e71658e365d","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cli/","postref":"99358bc407a164ba491e9e71658e365d","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/rancher/v2.x/en/cli/","section":"rancher","tags":null,"title":"Using the Rancher Command Line Interface","type":"rancher","url":"/docs/rancher/v2.x/en/cli/","weight":6000,"wordcount":483},{"authors":null,"categories":null,"content":"In Rancher Launched Kubernetes clusters that store data on iSCSI volumes, you may experience an issue where kubelets fail to automatically connect with iSCSI volumes. This failure is likely due to an incompatibility issue involving the iSCSI initiator tool. You can resolve this issue by installing the iSCSI initiator tool on each of your cluster nodes.\nRancher Launched Kubernetes clusters storing data on iSCSI volumes leverage the iSCSI initiator tool, which is embedded in the kubelet&rsquo;s rancher/hyperkube Docker image.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-admin/volumes-and-storage/iscsi-volumes/","expirydate":-62135596800,"fuzzywordcount":300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"6cf91c9304b1240271dcc6011429fb08","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/volumes-and-storage/iscsi-volumes/","postref":"6cf91c9304b1240271dcc6011429fb08","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/cluster-admin/volumes-and-storage/iscsi-volumes/","section":"rancher","tags":null,"title":"iSCSI Volumes","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-admin/volumes-and-storage/iscsi-volumes/","weight":6000,"wordcount":284},{"authors":null,"categories":null,"content":"System Tools is a tool to perform operational tasks on Rancher Launched Kubernetes clusters or RKE cluster as used for installing Rancher on Kubernetes. The tasks include:\n Collect logging and system metrics from nodes. Remove Kubernetes resources created by Rancher.  The following commands are available:\n   Command Description     logs Collect Kubernetes cluster component logs from nodes.   stats Stream system metrics from nodes.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/system-tools/","expirydate":-62135596800,"fuzzywordcount":800,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"00639ea12cbe7ea64ed11a3911090a3f","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/system-tools/","postref":"00639ea12cbe7ea64ed11a3911090a3f","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/docs/rancher/v2.x/en/system-tools/","section":"rancher","tags":null,"title":"System Tools","type":"rancher","url":"/docs/rancher/v2.x/en/system-tools/","weight":6001,"wordcount":714},{"authors":null,"categories":null,"content":"Within Rancher, each user has a number of settings associated with their login: personal preferences, API keys, etc. You can configure these settings by choosing from the User Settings menu. You can open this menu by clicking your avatar, located within the main menu.\nThe available user settings are:\n API &amp; Keys: If you want to interact with Rancher programmatically, you need an API key. Follow the directions in this section to obtain a key.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/user-settings/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"286403e822b0e2f919269f3891c766a9","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/user-settings/","postref":"286403e822b0e2f919269f3891c766a9","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rancher/v2.x/en/user-settings/","section":"rancher","tags":null,"title":"User Settings","type":"rancher","url":"/docs/rancher/v2.x/en/user-settings/","weight":7000,"wordcount":120},{"authors":null,"categories":null,"content":"API Keys and User Authentication If you want to access your Rancher clusters, projects, or other objects using external applications, you can do so using the Rancher API. However, before your application can access the API, you must provide the app with a key used to authenticate with Rancher. You can obtain a key using the Rancher UI.\nAn API key is also required for using Rancher CLI.\nAPI Keys are composed of four components:","date":-62135596800,"description":"","dir":"rancher/v2.x/en/user-settings/api-keys/","expirydate":-62135596800,"fuzzywordcount":500,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"12ca310a5b8c1e5578d322dacaf80d1c","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/user-settings/api-keys/","postref":"12ca310a5b8c1e5578d322dacaf80d1c","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/user-settings/api-keys/","section":"rancher","tags":null,"title":"API Keys","type":"rancher","url":"/docs/rancher/v2.x/en/user-settings/api-keys/","weight":7005,"wordcount":402},{"authors":null,"categories":null,"content":"When you provision a cluster hosted by an infrastructure provider, node templates are used to provision the cluster nodes. These templates use Docker Machine configuration options to define an operating system image and settings/parameters for the node. You can create node templates in two contexts:\n While provisioning a node pool cluster. At any time, from your user settings.  When you create a node template, it is bound to your user profile.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/user-settings/node-templates/","expirydate":-62135596800,"fuzzywordcount":500,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"e3b1f2f21622038fe550ae06791c18dc","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/user-settings/node-templates/","postref":"e3b1f2f21622038fe550ae06791c18dc","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/user-settings/node-templates/","section":"rancher","tags":null,"title":"Managing Node Templates","type":"rancher","url":"/docs/rancher/v2.x/en/user-settings/node-templates/","weight":7010,"wordcount":417},{"authors":null,"categories":null,"content":"Available as of Rancher v2.3.0\nRKE templates are designed to allow DevOps and security teams to standardize and simplify the creation of Kubernetes clusters.\nRKE is the Rancher Kubernetes Engine, which is the tool that Rancher uses to provision Kubernetes clusters.\nWith Kubernetes increasing in popularity, there is a trend toward managing a larger number of smaller clusters. When you want to create many clusters, it’s more important to manage them consistently.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/admin-settings/rke-templates/","expirydate":-62135596800,"fuzzywordcount":1500,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"75c5cf4987b52542d5883bd1501e6cf0","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/rke-templates/","postref":"75c5cf4987b52542d5883bd1501e6cf0","publishdate":"0001-01-01T00:00:00Z","readingtime":7,"relpermalink":"/docs/rancher/v2.x/en/admin-settings/rke-templates/","section":"rancher","tags":null,"title":"RKE Templates","type":"rancher","url":"/docs/rancher/v2.x/en/admin-settings/rke-templates/","weight":7010,"wordcount":1458},{"authors":null,"categories":null,"content":"Available as of v2.2.0\nWhen you create a cluster hosted by an infrastructure provider, node templates are used to provision the cluster nodes. These templates use Docker Machine configuration options to define an operating system image and settings/parameters for the node.\nNode templates can use cloud credentials to access the credential information required to provision nodes in the infrastructure providers. The same cloud credential can be used by multiple node templates.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/user-settings/cloud-credentials/","expirydate":-62135596800,"fuzzywordcount":500,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"2728a2c9fa7ece7d6338ba70bd89b0dc","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/user-settings/cloud-credentials/","postref":"2728a2c9fa7ece7d6338ba70bd89b0dc","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/rancher/v2.x/en/user-settings/cloud-credentials/","section":"rancher","tags":null,"title":"Managing Cloud Credentials","type":"rancher","url":"/docs/rancher/v2.x/en/user-settings/cloud-credentials/","weight":7011,"wordcount":468},{"authors":null,"categories":null,"content":"Each user can choose preferences to personalize their Rancher experience. To change preference settings, open the User Settings menu and then select Preferences.\nTheme Choose your background color for the Rancher UI. If you choose Auto, the background color changes from light to dark at 6 PM, and then changes back at 6 AM.\nMy Account This section displays the Name (your display name) and Username (your login) used for your session.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/user-settings/preferences/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"dbb07fc5f136873d94d0fb772c51fca5","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/user-settings/preferences/","postref":"dbb07fc5f136873d94d0fb772c51fca5","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rancher/v2.x/en/user-settings/preferences/","section":"rancher","tags":null,"title":"User Preferences","type":"rancher","url":"/docs/rancher/v2.x/en/user-settings/preferences/","weight":7012,"wordcount":121},{"authors":null,"categories":null,"content":"How to use the API The API has its own user interface accessible from a web browser. This is an easy way to see resources, perform actions, and see the equivalent cURL or HTTP request &amp; response. To access it, click on your user avatar in the upper right corner. Under API &amp; Keys, you can find the URL endpoint as well as create API keys.\nAuthentication API requests must include authentication information.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/api/","expirydate":-62135596800,"fuzzywordcount":900,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"e9a97f51121835d55a12f5f82b50512e","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/api/","postref":"e9a97f51121835d55a12f5f82b50512e","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/docs/rancher/v2.x/en/api/","section":"rancher","tags":null,"title":"API","type":"rancher","url":"/docs/rancher/v2.x/en/api/","weight":7500,"wordcount":801},{"authors":null,"categories":null,"content":"Security policy Rancher Labs supports responsible disclosure, and endeavours to resolve all issues in a reasonable time frame.  Reporting process Please submit possible security issues by emailing security@rancher.com\n Announcements Subscribe to the Rancher announcements forum for release updates.\n   Security is at the heart of all Rancher features. From integrating with all the popular authentication tools and services, to an enterprise grade RBAC capability, Rancher makes your Kubernetes clusters even more secure.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/security/","expirydate":-62135596800,"fuzzywordcount":1300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"5ef338ea395ff63a67a928aacc540b69","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/security/","postref":"5ef338ea395ff63a67a928aacc540b69","publishdate":"0001-01-01T00:00:00Z","readingtime":6,"relpermalink":"/docs/rancher/v2.x/en/security/","section":"rancher","tags":null,"title":"Security","type":"rancher","url":"/docs/rancher/v2.x/en/security/","weight":7505,"wordcount":1250},{"authors":null,"categories":null,"content":"Available as of v2.2.0\n Important:\nRunning on an ARM64 platform is currently an experimental feature and is not yet officially supported in Rancher. Therefore, we do not recommend using ARM64 based nodes in a production environment.\n The following options are available when using an ARM64 platform:\n Running Rancher on ARM64 based node(s)  Only Docker Install  Create custom cluster and adding ARM64 based node(s)  Kubernetes cluster version must be 1.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/options/arm64-platform/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"3cbdbf780765eba2e5eeabcd384359a3","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/arm64-platform/","postref":"3cbdbf780765eba2e5eeabcd384359a3","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rancher/v2.x/en/installation/options/arm64-platform/","section":"rancher","tags":null,"title":"Running on ARM64 (Experimental)","type":"rancher","url":"/docs/rancher/v2.x/en/installation/options/arm64-platform/","weight":7600,"wordcount":119},{"authors":null,"categories":null,"content":"Available as of v2.3.0\nRancher includes some features that are experimental and disabled by default. You might want to enable these features, for example, if you decide that the benefits of using an unsupported storage type outweighs the risk of using an untested feature. Feature flags were introduced to allow you to try these features that are not enabled by default.\nThe features can be enabled in three ways:","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/options/feature-flags/","expirydate":-62135596800,"fuzzywordcount":1100,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"e75ee4f3ca6570362f698d2f3380b0fe","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/feature-flags/","postref":"e75ee4f3ca6570362f698d2f3380b0fe","publishdate":"0001-01-01T00:00:00Z","readingtime":5,"relpermalink":"/docs/rancher/v2.x/en/installation/options/feature-flags/","section":"rancher","tags":null,"title":"Enabling Experimental Features","type":"rancher","url":"/docs/rancher/v2.x/en/installation/options/feature-flags/","weight":8000,"wordcount":1051},{"authors":null,"categories":null,"content":"This FAQ is a work in progress designed to answers the questions our users most frequently ask about Rancher v2.x.\nSee Technical FAQ, for frequently asked technical questions.\n\nDoes Rancher v2.x support Docker Swarm and Mesos as environment types?\nWhen creating an environment in Rancher v2.x, Swarm and Mesos will no longer be standard options you can select. However, both Swarm and Mesos will continue to be available as Catalog applications you can deploy.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/faq/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"2fba5d52c1b2174cbce764fbf97beb17","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/faq/","postref":"2fba5d52c1b2174cbce764fbf97beb17","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/faq/","section":"rancher","tags":null,"title":"FAQ","type":"rancher","url":"/docs/rancher/v2.x/en/faq/","weight":8000,"wordcount":367},{"authors":null,"categories":null,"content":"Networking FAQ&rsquo;s\n CNI Providers  ","date":-62135596800,"description":"","dir":"rancher/v2.x/en/faq/networking/","expirydate":-62135596800,"fuzzywordcount":100,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"e31568ca4240c15ec45196c3ab667da3","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/faq/networking/","postref":"e31568ca4240c15ec45196c3ab667da3","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rancher/v2.x/en/faq/networking/","section":"rancher","tags":null,"title":"Networking","type":"rancher","url":"/docs/rancher/v2.x/en/faq/networking/","weight":8005,"wordcount":4},{"authors":null,"categories":null,"content":"How can I reset the administrator password? Docker Install:\n$ docker exec -ti &lt;container_id&gt; reset-password New password for default administrator (user-xxxxx): &lt;new_password&gt;  Kubernetes install (Helm):\n$ KUBECONFIG=./kube_config_rancher-cluster.yml $ kubectl --kubeconfig $KUBECONFIG -n cattle-system exec $(kubectl --kubeconfig $KUBECONFIG -n cattle-system get pods -l app=rancher | grep '1/1' | head -1 | awk '{ print $1 }') -- reset-password New password for default administrator (user-xxxxx): &lt;new_password&gt;  Kubernetes install (RKE add-on):","date":-62135596800,"description":"","dir":"rancher/v2.x/en/faq/technical/","expirydate":-62135596800,"fuzzywordcount":1500,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"44bbe8a7346dc54eaf6fce71e172dd3e","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/faq/technical/","postref":"44bbe8a7346dc54eaf6fce71e172dd3e","publishdate":"0001-01-01T00:00:00Z","readingtime":7,"relpermalink":"/docs/rancher/v2.x/en/faq/technical/","section":"rancher","tags":null,"title":"Technical","type":"rancher","url":"/docs/rancher/v2.x/en/faq/technical/","weight":8006,"wordcount":1458},{"authors":null,"categories":null,"content":"Is there a Hardening Guide?\nThe Hardening Guide is now located in the main Security section.\n\nWhat are the results of Rancher&rsquo;s Kubernetes cluster when it is CIS benchmarked?\nWe have run the CIS Kubernetes benchmark against a hardened Rancher Kubernetes cluster. The results of that assessment can be found in the main Security section.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/faq/security/","expirydate":-62135596800,"fuzzywordcount":100,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"efe479318ff293478a1c6fb0e9c5a67b","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/faq/security/","postref":"efe479318ff293478a1c6fb0e9c5a67b","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rancher/v2.x/en/faq/security/","section":"rancher","tags":null,"title":"Security","type":"rancher","url":"/docs/rancher/v2.x/en/faq/security/","weight":8007,"wordcount":55},{"authors":null,"categories":null,"content":"What is Telemetry? Telemetry collects aggregate information about the size of Rancher installations, versions of components used, and which features are used. This information is used by Rancher Labs to help make the product better and is not shared with third-parties.\nWhat information is collected? No specific identifying information like usernames, passwords, or the names or addresses of user resources will ever be collected.\nThe primary things collected include:","date":-62135596800,"description":"","dir":"rancher/v2.x/en/faq/telemetry/","expirydate":-62135596800,"fuzzywordcount":300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"677d923ca2d682ecf50a3d0408c29bd8","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/faq/telemetry/","postref":"677d923ca2d682ecf50a3d0408c29bd8","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/faq/telemetry/","section":"rancher","tags":null,"title":"Telemetry","type":"rancher","url":"/docs/rancher/v2.x/en/faq/telemetry/","weight":8008,"wordcount":250},{"authors":null,"categories":null,"content":"This page is intended to answer questions about what happens if you don&rsquo;t want Rancher anymore, if you don&rsquo;t want a cluster to be managed by Rancher anymore, or if the Rancher server is deleted.\n If the Rancher server is deleted, what happens to the workloads in my downstream clusters? If the Rancher server is deleted, how do I access my downstream clusters? What if I don&rsquo;t want Rancher anymore?","date":-62135596800,"description":"","dir":"rancher/v2.x/en/faq/removing-rancher/","expirydate":-62135596800,"fuzzywordcount":600,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"633cc875a8eb9d067eaef7229009d2d3","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/faq/removing-rancher/","postref":"633cc875a8eb9d067eaef7229009d2d3","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/docs/rancher/v2.x/en/faq/removing-rancher/","section":"rancher","tags":null,"title":"Rancher is No Longer Needed","type":"rancher","url":"/docs/rancher/v2.x/en/faq/removing-rancher/","weight":8010,"wordcount":588},{"authors":null,"categories":null,"content":"This section contains information to help you troubleshoot issues when using Rancher.\n Kubernetes components\nIf you need help troubleshooting core Kubernetes cluster components like:\n etcd kube-apiserver kube-controller-manager kube-scheduler kubelet kube-proxy nginx-proxy  Kubernetes resources\nOptions for troubleshooting Kubernetes resources like Nodes, Ingress Controller and Rancher Agents are described in this section.\n Networking\nSteps to troubleshoot networking issues can be found here.\n DNS\nWhen you experience name resolution issues in your cluster.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/troubleshooting/","expirydate":-62135596800,"fuzzywordcount":100,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"0fcb0a97182297d11b3d8d5e1220ffd1","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/troubleshooting/","postref":"0fcb0a97182297d11b3d8d5e1220ffd1","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rancher/v2.x/en/troubleshooting/","section":"rancher","tags":null,"title":"Troubleshooting","type":"rancher","url":"/docs/rancher/v2.x/en/troubleshooting/","weight":8100,"wordcount":96},{"authors":null,"categories":null,"content":"This section explains the repositories used for Rancher, how to build the repositories, and what information to include when you file an issue.\nFor more detailed information on how to contribute to the development of Rancher projects, refer to the Rancher Developer Wiki. The wiki has resources on many topics, including the following:\n How to set up the Rancher development environment and run tests The typical flow of an issue through the development lifecycle Coding guidelines and development best practices Debugging and troubleshooting Developing the Rancher API  On the Rancher Users Slack, the channel for developers is #developer.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/contributing/","expirydate":-62135596800,"fuzzywordcount":1200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"1fda90268d6b647a6db191c0cee74e0f","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/contributing/","postref":"1fda90268d6b647a6db191c0cee74e0f","publishdate":"0001-01-01T00:00:00Z","readingtime":6,"relpermalink":"/docs/rancher/v2.x/en/contributing/","section":"rancher","tags":null,"title":"Contributing to Rancher","type":"rancher","url":"/docs/rancher/v2.x/en/contributing/","weight":9000,"wordcount":1193},{"authors":null,"categories":null,"content":"Note: This section describes the pipeline feature as implemented in Rancher v2.0.x. If you are using Rancher v2.1 or later, where pipelines have been significantly improved, please refer to the new documentation for v2.1 or later.\n Pipelines help you automate the software delivery process. You can integrate Rancher with GitHub to create a pipeline.\nYou can set up your pipeline to run a series of stages and steps to test your code and deploy it.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/project-admin/pipelines/docs-for-v2.0.x/","expirydate":-62135596800,"fuzzywordcount":700,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"1181d99604204df23d299ccfe25d26f1","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/project-admin/pipelines/docs-for-v2.0.x/","postref":"1181d99604204df23d299ccfe25d26f1","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/docs/rancher/v2.x/en/project-admin/pipelines/docs-for-v2.0.x/","section":"rancher","tags":null,"title":"v2.0.x Pipeline Documentation","type":"rancher","url":"/docs/rancher/v2.x/en/project-admin/pipelines/docs-for-v2.0.x/","weight":9000,"wordcount":660},{"authors":null,"categories":null,"content":"Available from v2.1.0 to v2.1.9 and v2.2.0 to v2.2.3\nThis section describes how to provision Windows clusters in Rancher v2.1.x and v2.2.x. If you are using Rancher v2.3.0 or later, please refer to the new documentation for v2.3.0 or later.\nWhen you create a custom cluster, Rancher uses RKE (the Rancher Kubernetes Engine) to provision the Kubernetes cluster on your existing infrastructure.\nYou can provision a custom Windows cluster using Rancher by using a mix of Linux and Windows hosts as your cluster nodes.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-provisioning/rke-clusters/windows-clusters/docs-for-2.1-and-2.2/","expirydate":-62135596800,"fuzzywordcount":1400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"3903fa9dc119786a55c837434d065aec","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/windows-clusters/docs-for-2.1-and-2.2/","postref":"3903fa9dc119786a55c837434d065aec","publishdate":"0001-01-01T00:00:00Z","readingtime":7,"relpermalink":"/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/windows-clusters/docs-for-2.1-and-2.2/","section":"rancher","tags":null,"title":"v2.1.x and v2.2.x Windows Documentation (Experimental)","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/windows-clusters/docs-for-2.1-and-2.2/","weight":9100,"wordcount":1335},{"authors":null,"categories":null,"content":"You can enable the API audit log to record the sequence of system events initiated by individual users. You can know what happened, when it happened, who initiated it, and what cluster it affected. When you enable this feature, all requests to the Rancher API and all responses from it are written to a log.\nYou can enable API Auditing during Rancher installation or upgrade.\nEnabling API Audit Log The Audit Log is enabled and configured by passing environment variables to the Rancher server container.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/options/api-audit-log/","expirydate":-62135596800,"fuzzywordcount":1600,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"0fb5723df5ebe790dee7cf0cbb1e2700","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/api-audit-log/","postref":"0fb5723df5ebe790dee7cf0cbb1e2700","publishdate":"0001-01-01T00:00:00Z","readingtime":8,"relpermalink":"/docs/rancher/v2.x/en/installation/options/api-audit-log/","section":"rancher","tags":null,"title":"Enabling the API Audit Log to Record System Events","type":"rancher","url":"/docs/rancher/v2.x/en/installation/options/api-audit-log/","weight":10000,"wordcount":1530},{"authors":null,"categories":null,"content":"Rancher v2.x has been rearchitected and rewritten with the goal of providing a complete management solution for Kubernetes and Docker. Due to these extensive changes, there is no direct upgrade path from v1.6 to v2.x, but rather a migration of your v1.6 services into v2.x as Kubernetes workloads. In v1.6, the most common orchestration used was Rancher&rsquo;s own engine called Cattle. The following guide explains and educates our Cattle users on running workloads in a Kubernetes environment.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/v1.6-migration/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"9df68b8bf5441b195821f7f057747d4d","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/v1.6-migration/","postref":"9df68b8bf5441b195821f7f057747d4d","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/v1.6-migration/","section":"rancher","tags":null,"title":"Migrating from v1.6 to v2.x","type":"rancher","url":"/docs/rancher/v2.x/en/v1.6-migration/","weight":10000,"wordcount":322},{"authors":null,"categories":null,"content":"Available as of v2.1.7\nIn Rancher v2.1.7, the default TLS configuration changed to only accept TLS 1.2 and secure TLS cipher suites. TLS 1.3 and TLS 1.3 exclusive cipher suites are not supported.\nConfiguring TLS settings The Audit Log is enabled and configured by passing environment variables to the Rancher server container. See the following to enable on your installation.\n Installing Rancher on a single node with Docker","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/options/tls-settings/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"0a63dfbc1b14f8b43f6373e8ae67ead9","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/tls-settings/","postref":"0a63dfbc1b14f8b43f6373e8ae67ead9","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/rancher/v2.x/en/installation/options/tls-settings/","section":"rancher","tags":null,"title":"TLS settings","type":"rancher","url":"/docs/rancher/v2.x/en/installation/options/tls-settings/","weight":11000,"wordcount":126},{"authors":null,"categories":null,"content":"Some distributions of Linux derived from RHEL, including Oracle Linux, may have default firewall rules that block communication with Helm.\nFor example, one Oracle Linux image in AWS has REJECT rules that stop Helm from communicating with Tiller:\nChain INPUT (policy ACCEPT) target prot opt source destination ACCEPT all -- anywhere anywhere state RELATED,ESTABLISHED ACCEPT icmp -- anywhere anywhere ACCEPT all -- anywhere anywhere ACCEPT tcp -- anywhere anywhere state NEW tcp dpt:ssh REJECT all -- anywhere anywhere reject-with icmp-host-prohibited Chain FORWARD (policy ACCEPT) target prot opt source destination REJECT all -- anywhere anywhere reject-with icmp-host-prohibited Chain OUTPUT (policy ACCEPT) target prot opt source destination  You can check the default firewall rules with this command:","date":-62135596800,"description":"","dir":"rancher/v2.x/en/installation/options/firewall/","expirydate":-62135596800,"fuzzywordcount":400,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"d0a6c797c46d01f832a0243889363519","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/installation/options/firewall/","postref":"d0a6c797c46d01f832a0243889363519","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/installation/options/firewall/","section":"rancher","tags":null,"title":"Opening Ports with firewalld","type":"rancher","url":"/docs/rancher/v2.x/en/installation/options/firewall/","weight":12000,"wordcount":396},{"authors":null,"categories":null,"content":"Available as of v2.3.0\nIf your organization uses G Suite for user authentication, you can configure Rancher to allow your users to log in using their G Suite credentials.\nOnly admins of the G Suite domain have access to the Admin SDK. Therefore, only G Suite admins can configure Google OAuth for Rancher.\nWithin Rancher, only administrators or users with the Manage Authentication global role can configure authentication.\nPrerequisites  You must have a G Suite admin account configured.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/admin-settings/authentication/google/","expirydate":-62135596800,"fuzzywordcount":1200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"41399c655ec749ab1ec9ada1e6a76880","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/admin-settings/authentication/google/","postref":"41399c655ec749ab1ec9ada1e6a76880","publishdate":"0001-01-01T00:00:00Z","readingtime":6,"relpermalink":"/docs/rancher/v2.x/en/admin-settings/authentication/google/","section":"rancher","tags":null,"title":"Configuring Google OAuth","type":"rancher","url":"/docs/rancher/v2.x/en/admin-settings/authentication/google/","weight":0,"wordcount":1100},{"authors":null,"categories":null,"content":"Rancher docs Rancher is open source software that combines everything an organization needs to adopt and run containers in production. Built on Kubernetes, Rancher makes it easy for DevOps teams to test, deploy and manage their applications.             Featured resource Read about how to migrate from Rancher v1.6 Cattle to v2.x\n Read More            2.","date":-62135596800,"description":"Rancher is open source software that combines everything an organization needs to adopt and run containers in production. Built on Kubernetes, Rancher makes it easy for DevOps teams to test, deploy and manage their applications. Operations teams use Rancher to deploy, manage and secure every Kubernetes deployment regardless of where it is running.","dir":"","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"home","lang":"en","lastmod":-62135596800,"objectID":"3971dee9690c45ec29595f6ae97154f9","permalink":"http://jijeesh.github.io/docs/","postref":"3971dee9690c45ec29595f6ae97154f9","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/","section":"","tags":null,"title":"Documentation | Rancher Labs","type":"page","url":"/docs/","weight":0,"wordcount":196},{"authors":null,"categories":null,"content":"Note: The following guide is only for RKE provisioned clusters.\n If you have restrictive Pod Security Policies enabled, then Istio may not be able to function correctly, because it needs certain permissions in order to install itself and manage pod infrastructure. In this section, we will configure a cluster with PSPs enabled for an Istio install, and also set up the Istio CNI plugin.\nThe Istio CNI plugin removes the need for each application pod to have a privileged NET_ADMIN container.","date":-62135596800,"description":"","dir":"rancher/v2.x/en/cluster-admin/tools/istio/setup/enable-istio-in-cluster/enable-istio-with-psp/","expirydate":-62135596800,"fuzzywordcount":300,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"e73500f30ab0bf7d548794fd2090b330","permalink":"http://jijeesh.github.io/docs/rancher/v2.x/en/cluster-admin/tools/istio/setup/enable-istio-in-cluster/enable-istio-with-psp/","postref":"e73500f30ab0bf7d548794fd2090b330","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/docs/rancher/v2.x/en/cluster-admin/tools/istio/setup/enable-istio-in-cluster/enable-istio-with-psp/","section":"rancher","tags":null,"title":"Enable Istio with Pod Security Policies","type":"rancher","url":"/docs/rancher/v2.x/en/cluster-admin/tools/istio/setup/enable-istio-in-cluster/enable-istio-with-psp/","weight":0,"wordcount":298},{"authors":null,"categories":null,"content":"Lightweight Kubernetes. Easy to install, half the memory, all in a binary less than 50mb.\nGreat for:\n Edge IoT CI ARM Situations where a PhD in k8s clusterology is infeasible  What is K3s? K3s is a fully compliant Kubernetes distribution with the following enhancements:\n An embedded SQLite database has replaced etcd as the default datastore. External datastores such as PostgreSQL, MySQL, and etcd are also supported. Simple but powerful &ldquo;batteries-included&rdquo; features have been added, such as: a local storage provider, a service load balancer, a Helm controller, and the Traefik ingress controller.","date":-62135596800,"description":"","dir":"k3s/latest/en/","expirydate":-62135596800,"fuzzywordcount":200,"keywords":[],"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"e0e39269981c95ac058def185a9d13ac","permalink":"http://jijeesh.github.io/docs/k3s/latest/en/","postref":"e0e39269981c95ac058def185a9d13ac","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/docs/k3s/latest/en/","section":"k3s","tags":null,"title":"K3s - 5 less than K8s","type":"k3s","url":"/docs/k3s/latest/en/","weight":0,"wordcount":155}]